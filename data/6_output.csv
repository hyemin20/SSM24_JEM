Authors,Title,Year,Volume,Abstract,T_tokens_,T_tokens,T_tokens_join
Wesolowski B.C.,Predicting Operational Rater-Type Classifications Using Rasch Measurement Theory and Random Forests: A Music Performance Assessment Perspective,2019,56,"The purpose of this study was to build a Random Forest supervised machine learning model in order to predict musical rater-type classifications based upon a Rasch analysis of raters’ differential severity/leniency related to item use. Raw scores (N = 1,704) from 142 raters across nine high school solo and ensemble festivals (grades 9–12) were collected using a 29-item Likert-type rating scale embedded within five domains (tone/intonation, n = 6; balance, n = 5; interpretation, n = 6; rhythm, n = 6; and technical accuracy, n = 6). Data were analyzed using a Many Facets Rasch Partial Credit Model. An a priori k-means cluster analysis of 29 differential rater functioning indices produced a discrete feature vector that classified raters into one of three distinct rater-types: (a) syntactical rater-type, (b) expressive rater-type, or (c) mental representation rater-type. Results of the initial Random Forest model resulted in an out-of-bag error rate of 5.05%, indicating that approximately 95% of the raters were correctly classified. After tuning a set of three hyperparameters (ntree, mtry, and node size), the optimized model demonstrated an improved out-of-bag error rate of 2.02%. Implications for improvements in assessment, research, and rater training in the field of music education are discussed. © 2019 by the National Council on Measurement in Education","['predict', 'Operational', 'RaterType', 'Classifications', 'Rasch', 'Theory', 'Random', 'Forests', 'Music', 'Performance', 'Assessment', 'Perspective']","['predict', 'operational', 'ratertype', 'classifications', 'rasch', 'theory', 'random', 'forests', 'music', 'performance', 'assessment', 'perspective']",predict operational ratertype classifications rasch theory random forests music performance assessment perspective
Wang S.; Lin H.; Chang H.-H.; Douglas J.,Hybrid Computerized Adaptive Testing: From Group Sequential Design to Fully Sequential Design,2016,53,"Computerized adaptive testing (CAT) and multistage testing (MST) have become two of the most popular modes in large-scale computer-based sequential testing.  Though most designs of CAT and MST exhibit strength and weakness in recent large-scale implementations, there is no simple answer to the question of which design is better because different modes may fit different practical situations. This article proposes a hybrid adaptive framework to combine both CAT and MST, inspired by an analysis of the history of CAT and MST. The proposed procedure is a design which transitions from a group sequential design to a fully sequential design. This allows for the robustness of MST in early stages, but also shares the advantages of CAT in later stages with fine tuning of the ability estimator once its neighborhood has been identified. Simulation results showed that hybrid designs following our proposed principles provided comparable or even better estimation accuracy and efficiency than standard CAT and MST designs, especially for examinees at the two ends of the ability range. © 2016 by the National Council on Measurement in Education.","['hybrid', 'Computerized', 'Adaptive', 'Testing', 'Group', 'Sequential', 'Design', 'fully', 'Sequential', 'Design']","['hybrid', 'computerized', 'adaptive', 'testing', 'group', 'sequential', 'design', 'fully', 'sequential', 'design']",hybrid computerized adaptive testing group sequential design fully sequential design
LaHuis D.M.; Bryant-Lees K.B.; Hakoyama S.; Barnes T.; Wiemann A.,A Comparison of Procedures for Estimating Person Reliability Parameters in the Graded Response Model,2018,55,"Person reliability parameters (PRPs) model temporary changes in individuals’ attribute level perceptions when responding to self-report items (higher levels of PRPs represent less fluctuation). PRPs could be useful in measuring careless responding and traitedness. However, it is unclear how well current procedures for estimating PRPs can recover parameter estimates. This study assesses these procedures in terms of mean error (ME), average absolute difference (AAD), and reliability using simulated data with known values. Several prior distributions for PRPs were compared across a number of conditions. Overall, our results revealed little differences between using the χ or lognormal distributions as priors for estimated PRPs. Both distributions produced estimates with reasonable levels of ME; however, the AAD of the estimates was high. AAD did improve slightly as the number of items increased, suggesting that increasing the number of items would ameliorate this problem. Similarly, a larger number of items were necessary to produce reasonable levels of reliability. Based on our results, several conclusions are drawn and implications for future research are discussed. © 2018 by the National Council on Measurement in Education","['Comparison', 'Procedures', 'Estimating', 'Person', 'Reliability', 'Parameters', 'Graded', 'Response']","['comparison', 'procedures', 'estimating', 'person', 'reliability', 'parameters', 'graded', 'response']",comparison procedures estimating person reliability parameters graded response
Kang H.-A.; Zhang S.; Chang H.-H.,Dual-Objective Item Selection Criteria in Cognitive Diagnostic Computerized Adaptive Testing,2017,54,"The development of cognitive diagnostic-computerized adaptive testing (CD-CAT) has provided a new perspective for gaining information about examinees' mastery on a set of cognitive attributes. This study proposes a new item selection method within the framework of dual-objective CD-CAT that simultaneously addresses examinees' attribute mastery status and overall test performance. The new procedure is based on the Jensen-Shannon (JS) divergence, a symmetrized version of the Kullback-Leibler divergence. We show that the JS divergence resolves the noncomparability problem of the dual information index and has close relationships with Shannon entropy, mutual information, and Fisher information. The performance of the JS divergence is evaluated in simulation studies in comparison with the methods available in the literature. Results suggest that the JS divergence achieves parallel or more precise recovery of latent trait variables compared to the existing methods and maintains practical advantages in computation and item pool usage. Copyright © 2017 by the National Council on Measurement in Education","['DualObjective', 'Item', 'Selection', 'Criteria', 'Cognitive', 'Diagnostic', 'Computerized', 'Adaptive', 'Testing']","['dualobjective', 'item', 'selection', 'criteria', 'cognitive', 'diagnostic', 'computerized', 'adaptive', 'testing']",dualobjective item selection criteria cognitive diagnostic computerized adaptive testing
Debeer D.; Ali U.S.; van Rijn P.W.,Evaluating Statistical Targets for Assembling Parallel Mixed-Format Test Forms,2017,54,"Test assembly is the process of selecting items from an item pool to form one or more new test forms. Often new test forms are constructed to be parallel with an existing (or an ideal) test. Within the context of item response theory, the test information function (TIF) or the test characteristic curve (TCC) are commonly used as statistical targets to obtain this parallelism. In a recent study, Ali and van Rijn proposed combining the TIF and TCC as statistical targets, rather than using only a single statistical target. In this article, we propose two new methods using this combined approach, and compare these methods with single statistical targets for the assembly of mixed-format tests. In addition, we introduce new criteria to evaluate the parallelism of multiple forms. The results show that single statistical targets can be problematic, while the combined targets perform better, especially in situations with increasing numbers of polytomous items. Implications of using the combined target are discussed. Copyright © 2017 by the National Council on Measurement in Education","['evaluate', 'statistical', 'Targets', 'assemble', 'Parallel', 'MixedFormat', 'Test', 'form']","['evaluate', 'statistical', 'targets', 'assemble', 'parallel', 'mixedformat', 'test', 'form']",evaluate statistical targets assemble parallel mixedformat test form
Dwyer A.C.,Maintaining Equivalent Cut Scores for Small Sample Test Forms,2016,53,"This study examines the effectiveness of three approaches for maintaining equivalent performance standards across test forms with small samples: (1) common-item equating, (2) resetting the standard, and (3) rescaling the standard. Rescaling the standard (i.e., applying common-item equating methodology to standard setting ratings to account for systematic differences between standard setting panels) has received almost no attention in the literature. Identity equating was also examined to provide context. Data from a standard setting form of a large national certification test (N examinees = 4,397; N panelists = 13) were split into content-equivalent subforms with common items, and resampling methodology was used to investigate the error introduced by each approach. Common-item equating (circle-arc and nominal weights mean) was evaluated at samples of size 10, 25, 50, and 100. The standard setting approaches (resetting and rescaling the standard) were evaluated by resampling (N = 8) and by simulating panelists (N = 8, 13, and 20). Results were inconclusive regarding the relative effectiveness of resetting and rescaling the standard. Small-sample equating, however, consistently produced new form cut scores that were less biased and less prone to random error than new form cut scores based on resetting or rescaling the standard. © 2016 by the National Council on Measurement in Education.","['maintain', 'Equivalent', 'Cut', 'Scores', 'Small', 'Sample', 'test', 'form']","['maintain', 'equivalent', 'cut', 'scores', 'small', 'sample', 'test', 'form']",maintain equivalent cut scores small sample test form
Sinharay S.,A New Person-Fit Statistic for the Lognormal Model for Response Times,2018,55,"Response-time models are of increasing interest in educational and psychological testing. This article focuses on the lognormal model for response times, which is one of the most popular response-time models, and suggests a simple person-fit statistic for the model. The distribution of the statistic under the null hypothesis of no misfit is proved to be a χ2 distribution. A simulation study and a real data example demonstrate the usefulness of the suggested statistic. © 2018 by the National Council on Measurement in Education","['New', 'PersonFit', 'Statistic', 'Lognormal', 'Response', 'Times']","['new', 'personfit', 'statistic', 'lognormal', 'response', 'times']",new personfit statistic lognormal response times
Liu C.; Kolen M.J.,A Comparison of Strategies for Smoothing Parameter Selection for Mixed-Format Tests Under the Random Groups Design,2018,55,"Smoothing techniques are designed to improve the accuracy of equating functions. The main purpose of this study is to compare seven model selection strategies for choosing the smoothing parameter (C) for polynomial loglinear presmoothing and one procedure for model selection in cubic spline postsmoothing for mixed-format pseudo tests under the random groups design. These model selection strategies were compared for four sample sizes (500, 1,000, 2,000, and 3,000) and two content areas (Advanced Placement [AP] Biology and AP Environmental Science). For polynomial loglinear presmoothing, the Akaike information criterion (AIC) was the only statistic that reduced both random equating error and total equating error in all investigated conditions. Cubic spline postsmoothing tended to produce more accurate results than any of the model selection strategies in polynomial loglinear smoothing. © 2018 by the National Council on Measurement in Education","['Comparison', 'strategy', 'Smoothing', 'Parameter', 'Selection', 'MixedFormat', 'Tests', 'Random', 'Groups', 'Design']","['comparison', 'strategy', 'smoothing', 'parameter', 'selection', 'mixedformat', 'tests', 'random', 'groups', 'design']",comparison strategy smoothing parameter selection mixedformat tests random groups design
"Wang J.; Engelhard G., Jr.",Conceptualizing Rater Judgments and Rating Processes for Rater-Mediated Assessments,2019,56,"Rater-mediated assessments exhibit scoring challenges due to the involvement of human raters. The quality of human ratings largely determines the reliability, validity, and fairness of the assessment process. Our research recommends that the evaluation of ratings should be based on two aspects: a theoretical model of human judgment and an appropriate measurement model for evaluating these judgments. In rater-mediated assessments, the underlying constructs and response processes may require the use of different rater judgment models and the application of different measurement models. We describe the use of Brunswik's lens model as an organizing theme for conceptualizing human judgments in rater-mediated assessments. The constructs vary depending on which distal variables are identified in the lens models for the underlying rater-mediated assessment. For example, one lens model can be developed to emphasize the measurement of student proficiency, while another lens model can stress the evaluation of rater accuracy. Next, we describe two measurement models that reflect different response processes (cumulative and unfolding) from raters: Rasch and hyperbolic cosine models. Future directions for the development and evaluation of rater-mediated assessments are suggested. © 2019 by the National Council on Measurement in Education","['conceptualize', 'Rater', 'Judgments', 'Rating', 'Processes', 'RaterMediated', 'assessment']","['conceptualize', 'rater', 'judgments', 'rating', 'processes', 'ratermediated', 'assessment']",conceptualize rater judgments rating processes ratermediated assessment
Wyse A.E.; Babcock B.,Does Maximizing Information at the Cut Score Always Maximize Classification Accuracy and Consistency?,2016,53,"A common suggestion made in the psychometric literature for fixed-length classification tests is that one should design tests so that they have maximum information at the cut score. Designing tests in this way is believed to maximize the classification accuracy and consistency of the assessment. This article uses simulated examples to illustrate that one can obtain higher classification accuracy and consistency by designing tests that have maximum test information at locations other than at the cut score. We show that the location where one should maximize the test information is dependent on the length of the test, the mean of the ability distribution in comparison to the cut score, and, to a lesser degree, whether or not one wants to optimize classification accuracy or consistency. Analyses also suggested that the differences in classification performance between designing tests optimally versus maximizing information at the cut score tended to be greatest when tests were short and the mean of ability distribution was further away from the cut score. Larger differences were also found in the simulated examples that used the 3PL model compared to the examples that used the Rasch model. © 2016 by the National Council on Measurement in Education.","['Maximizing', 'Information', 'Cut', 'Score', 'maximize', 'Classification', 'Accuracy', 'Consistency']","['maximizing', 'information', 'cut', 'score', 'maximize', 'classification', 'accuracy', 'consistency']",maximizing information cut score maximize classification accuracy consistency
Lee W.-Y.; Cho S.-J.,Detecting Differential Item Discrimination (DID) and the Consequences of Ignoring DID in Multilevel Item Response Models,2017,54,"Cross-level invariance in a multilevel item response model can be investigated by testing whether the within-level item discriminations are equal to the between-level item discriminations. Testing the cross-level invariance assumption is important to understand constructs in multilevel data. However, in most multilevel item response model applications, the cross-level invariance is assumed without testing of the cross-level invariance assumption. In this study, the detection methods of differential item discrimination (DID) over levels and the consequences of ignoring DID are illustrated and discussed with the use of multilevel item response models. Simulation results showed that the likelihood ratio test (LRT) performed well in detecting global DID at the test level when some portion of the items exhibited DID. At the item level, the Akaike information criterion (AIC), the sample-size adjusted Bayesian information criterion (saBIC), LRT, and Wald test showed a satisfactory rejection rate (>.8) when some portion of the items exhibited DID and the items had lower intraclass correlations (or higher DID magnitudes). When DID was ignored, the accuracy of the item discrimination estimates and standard errors was mainly problematic. Implications of the findings and limitations are discussed. Copyright © 2017 by the National Council on Measurement in Education","['detect', 'Differential', 'Item', 'Discrimination', 'Consequences', 'Ignoring', 'Multilevel', 'Item', 'Response', 'Models']","['detect', 'differential', 'item', 'discrimination', 'consequences', 'ignoring', 'multilevel', 'item', 'response', 'models']",detect differential item discrimination consequences ignoring multilevel item response models
Albano A.D.; Cai L.; Lease E.M.; McConnell S.R.,Computerized Adaptive Testing in Early Education: Exploring the Impact of Item Position Effects on Ability Estimation,2019,56,"Studies have shown that item difficulty can vary significantly based on the context of an item within a test form. In particular, item position may be associated with practice and fatigue effects that influence item parameter estimation. The purpose of this research was to examine the relevance of item position specifically for assessments used in early education, an area of testing that has received relatively limited psychometric attention. In an initial study, multilevel item response models fit to data from an early literacy measure revealed statistically significant increases in difficulty for items appearing later in a 20-item form. The estimated linear change in logits for an increase of 1 in position was.024, resulting in a predicted change of.46 logits for a shift from the beginning to the end of the form. A subsequent simulation study examined impacts of item position effects on person ability estimation within computerized adaptive testing. Implications and recommendations for practice are discussed. © 2019 by the National Council on Measurement in Education","['Computerized', 'Adaptive', 'Testing', 'early', 'explore', 'Impact', 'Item', 'Position', 'Effects', 'Ability', 'Estimation']","['computerized', 'adaptive', 'testing', 'early', 'explore', 'impact', 'item', 'position', 'effects', 'ability', 'estimation']",computerized adaptive testing early explore impact item position effects ability estimation
Debeer D.; Janssen R.; De Boeck P.,Modeling Skipped and Not-Reached Items Using IRTrees,2017,54,"When dealing with missing responses, two types of omissions can be discerned: items can be skipped or not reached by the test taker. When the occurrence of these omissions is related to the proficiency process the missingness is nonignorable. The purpose of this article is to present a tree-based IRT framework for modeling responses and omissions jointly, taking into account that test takers as well as items can contribute to the two types of omissions. The proposed framework covers several existing models for missing responses, and many IRTree models can be estimated using standard statistical software. Further, simulated data is used to show that ignoring missing responses is less robust than often considered. Finally, as an illustration of its applicability, the IRTree approach is applied to data from the 2009 PISA reading assessment. Copyright © 2017 by the National Council on Measurement in Education","['Skipped', 'NotReached', 'Items', 'irtree']","['skipped', 'notreached', 'items', 'irtree']",skipped notreached items irtree
Wesolowski B.C.; Wind S.A.,Pedagogical Considerations for Examining Rater Variability in Rater-Mediated Assessments: A Three-Model Framework,2019,56,"Rater-mediated assessments are a common methodology for measuring persons, investigating rater behavior, and/or defining latent constructs. The purpose of this article is to provide a pedagogical framework for examining rater variability in the context of rater-mediated assessments using three distinct models. The first model is the observation model, which includes ecological/environmental considerations for the evaluation system. The second model is the measurement model, which includes the transformation of observed, rater response data to linear measures using a measurement model with specific requirements of rater-invariant measurement in order to examine raters’ construct-relevant variability stemming from the evaluative system. The third model is the interaction model, which includes an interaction parameter to allow for the investigation into raters’ systematic, construct-irrelevant variability stemming from the evaluative system. Implications for measurement outcomes and validity are discussed. © 2019 by the National Council on Measurement in Education","['Pedagogical', 'Considerations', 'examine', 'Rater', 'Variability', 'RaterMediated', 'assessment', 'A', 'ThreeModel', 'Framework']","['pedagogical', 'considerations', 'examine', 'rater', 'variability', 'ratermediated', 'assessment', 'a', 'threemodel', 'framework']",pedagogical considerations examine rater variability ratermediated assessment a threemodel framework
Wu Q.; De Laet T.; Janssen R.,Modeling Partial Knowledge on Multiple-Choice Items Using Elimination Testing,2019,56,"Single-best answers to multiple-choice items are commonly dichotomized into correct and incorrect responses, and modeled using either a dichotomous item response theory (IRT) model or a polytomous one if differences among all response options are to be retained. The current study presents an alternative IRT-based modeling approach to multiple-choice items administered with the procedure of elimination testing, which asks test-takers to eliminate all the response options they consider to be incorrect. The partial credit model is derived for the obtained responses. By extracting more information pertaining to test-takers’ partial knowledge on the items, the proposed approach has the advantage of providing more accurate estimation of the latent ability. In addition, it may shed some light on the possible answering processes of test-takers on the items. As an illustration, the proposed approach is applied to a classroom examination of an undergraduate course in engineering science. © 2019 by the National Council on Measurement in Education","['Partial', 'Knowledge', 'MultipleChoice', 'Items', 'Elimination', 'Testing']","['partial', 'knowledge', 'multiplechoice', 'items', 'elimination', 'testing']",partial knowledge multiplechoice items elimination testing
Svetina D.; Liaw Y.-L.; Rutkowski L.; Rutkowski D.,Routing Strategies and Optimizing Design for Multistage Testing in International Large-Scale Assessments,2019,56,"This study investigates the effect of several design and administration choices on item exposure and person/item parameter recovery under a multistage test (MST) design. In a simulation study, we examine whether number-correct (NC) or item response theory (IRT) methods are differentially effective at routing students to the correct next stage(s) and whether routing choices (optimal versus suboptimal routing) have an impact on achievement precision. Additionally, we examine the impact of testlet length on both person and item recovery. Overall, our results suggest that no single approach works best across the studied conditions. With respect to the mean person parameter recovery, IRT scoring (via either Fisher information or preliminary EAP estimates) outperformed classical NC methods, although differences in bias and root mean squared error were generally small. Item exposure rates were found to be more evenly distributed when suboptimal routing methods were used, and item recovery (both difficulty and discrimination) was most precisely observed for items with moderate difficulties. Based on the results of the simulation study, we draw conclusions and discuss implications for practice in the context of international large-scale assessments that recently introduced adaptive assessment in the form of MST. Future research directions are also discussed. © 2019 by the National Council on Measurement in Education","['route', 'Strategies', 'Optimizing', 'Design', 'Multistage', 'Testing', 'International', 'LargeScale', 'assessment']","['route', 'strategies', 'optimizing', 'design', 'multistage', 'testing', 'international', 'largescale', 'assessment']",route strategies optimizing design multistage testing international largescale assessment
Kim H.J.; Brennan R.L.; Lee W.-C.,Structural Zeros and Their Implications With Log-Linear Bivariate Presmoothing Under the Internal-Anchor Design,2017,54,"In equating, when common items are internal and scoring is conducted in terms of the number of correct items, some pairs of total scores (X) and common-item scores (V) can never be observed in a bivariate distribution of X and V; these pairs are called structural zeros. This simulation study examines how equating results compare for different approaches to handling structural zeros. The study considers four approaches: the no-smoothing, unique-common, total-common, and adjusted total-common approaches. This study led to four main findings: (1) the total-common approach generally had the worst results; (2) for relatively small effect sizes, the unique-common approach generally had the smallest overall error; (3) for relatively large effect sizes, the adjusted total-common approach generally had the smallest overall error; and, (4) if sole interest focuses on reducing bias only, the adjusted total-common approach was generally preferable. These results suggest that, when common items are internal and log-linear bivariate presmoothing is performed, structural zeros should be maintained, even if there is some loss in the moment preservation property. Copyright © 2017 by the National Council on Measurement in Education","['Structural', 'Zeros', 'implication', 'LogLinear', 'Bivariate', 'Presmoothing', 'InternalAnchor', 'Design']","['structural', 'zeros', 'implication', 'loglinear', 'bivariate', 'presmoothing', 'internalanchor', 'design']",structural zeros implication loglinear bivariate presmoothing internalanchor design
Andrich D.; Marais I.,Controlling Bias in Both Constructed Response and Multiple-Choice Items When Analyzed With the Dichotomous Rasch Model,2018,55,"Even though guessing biases difficulty estimates as a function of item difficulty in the dichotomous Rasch model, assessment programs with tests which include multiple-choice items often construct scales using this model. Research has shown that when all items are multiple-choice, this bias can largely be eliminated. However, many assessments have a combination of multiple-choice and constructed response items. Using vertically scaled numeracy assessments from a large-scale assessment program, this article shows that eliminating the bias on estimates of the multiple-choice items also impacts on the difficulty estimates of the constructed response items. This implies that the original estimates of the constructed response items were biased by the guessing on the multiple-choice items. This bias has implications for both defining difficulties in item banks for use in adaptive testing composed of both multiple-choice and constructed response items, and for the construction of proficiency scales. Copyright © 2018 by the National Council on Measurement in Education","['control', 'Bias', 'Constructed', 'Response', 'MultipleChoice', 'Items', 'analyze', 'Dichotomous', 'Rasch']","['control', 'bias', 'constructed', 'response', 'multiplechoice', 'items', 'analyze', 'dichotomous', 'rasch']",control bias constructed response multiplechoice items analyze dichotomous rasch
Clauser B.E.; Baldwin P.; Margolis M.J.; Mee J.; Winward M.,An Experimental Study of the Internal Consistency of Judgments Made in Bookmark Standard Setting,2017,54,"Validating performance standards is challenging and complex. Because of the difficulties associated with collecting evidence related to external criteria, validity arguments rely heavily on evidence related to internal criteria—especially evidence that expert judgments are internally consistent. Given its importance, it is somewhat surprising that evidence of this kind has rarely been published in the context of the widely used bookmark standard-setting procedure. In this article we examined the effect of ordered item booklet difficulty on content experts’ bookmark judgments. If panelists make internally consistent judgments, their resultant cut scores should be unaffected by the difficulty of their respective booklets. This internal consistency was not observed: the results suggest that substantial systematic differences in the resultant cut scores can arise when the difficulty of the ordered item booklets varies. These findings raise questions about the ability of content experts to make the judgments required by the bookmark procedure. Copyright © 2017 by the National Council on Measurement in Education","['Experimental', 'Study', 'Internal', 'Consistency', 'Judgments', 'Bookmark', 'Standard', 'Setting']","['experimental', 'study', 'internal', 'consistency', 'judgments', 'bookmark', 'standard', 'setting']",experimental study internal consistency judgments bookmark standard setting
Wiberg M.; González J.,Statistical Assessment of Estimated Transformations in Observed-Score Equating,2016,53,"Equating methods make use of an appropriate transformation function to map the scores of one test form into the scale of another so that scores are comparable and can be used interchangeably. The equating literature shows that the ways of judging the success of an equating (i.e., the score transformation) might differ depending on the adopted framework. Rather than targeting different parts of the equating process and aiming to evaluate the process from different aspects, this article views the equating transformation as a standard statistical estimator and discusses how this estimator should be assessed in an equating framework. For the kernel equating framework, a numerical illustration shows the potentials of viewing the equating transformation as a statistical estimator as opposed to assessing it using equating-specific criteria. A discussion on how this approach can be used to compare other equating estimators from different frameworks is also included. © 2016 by the National Council on Measurement in Education.","['statistical', 'Assessment', 'Estimated', 'Transformations', 'ObservedScore', 'Equating']","['statistical', 'assessment', 'estimated', 'transformations', 'observedscore', 'equating']",statistical assessment estimated transformations observedscore equating
Herborn K.; Mustafić M.; Greiff S.,Mapping an Experiment-Based Assessment of Collaborative Behavior Onto Collaborative Problem Solving in PISA 2015: A Cluster Analysis Approach for Collaborator Profiles,2017,54,"Collaborative problem solving (CPS) assessment is a new academic research field with a number of educational implications. In 2015, the Programme for International Student Assessment (PISA) assessed CPS with a computer-simulated human-agent (H-A) approach that claimed to measure 12 individual CPS skills for the first time. After reviewing the approach, we conceptually embedded a computer-based collaborative behavior assessment (COLBAS) into the overarching PISA 2015 CPS approach. COLBAS is an H-A CPS assessment instrument that can be used to measure certain aspects of CPS. In addition, we applied a model-based cluster analysis to the embedded COLBAS aspects. The analysis revealed three types of student collaborator profiles that differed in cognitive performance and motivation: (a) passive low-performing (non-)collaborators, (b) active high-performing collaborators, and (c) compensating collaborators. Copyright © 2017 by the National Council on Measurement in Education","['mapping', 'ExperimentBased', 'Assessment', 'Collaborative', 'Behavior', 'Onto', 'Collaborative', 'Problem', 'Solving', 'PISA', '2015', 'Cluster', 'Analysis', 'Approach', 'Collaborator', 'Profiles']","['mapping', 'experimentbased', 'assessment', 'collaborative', 'behavior', 'onto', 'collaborative', 'problem', 'solving', 'pisa', '2015', 'cluster', 'analysis', 'approach', 'collaborator', 'profiles']",mapping experimentbased assessment collaborative behavior onto collaborative problem solving pisa 2015 cluster analysis approach collaborator profiles
Wind S.A.; Jones E.,Exploring the Influence of Range Restrictions on Connectivity in Sparse Assessment Networks: An Illustration and Exploration Within the Context of Classroom Observations,2018,55,"Range restrictions, or raters’ tendency to limit their ratings to a subset of available rating scale categories, are well documented in large-scale teacher evaluation systems based on principal observations. When these restrictions occur, the ratings observed during operational teacher evaluations are limited to a subset of the available categories. However, range restrictions are less common within teacher performances that are used to establish links (anchor ratings) in otherwise disconnected assessment systems. As a result, principals’ category use may be different between anchor ratings and operational ratings. The purpose of this study is to explore the consequences of discrepancies in rating scale category use across operational and anchor ratings within the context of teacher evaluation systems based on principal observations. First, we used real data to illustrate the presence of range restriction in operational ratings, and the effect of this restriction on connectivity. Then, we used simulated data to explore these effects using experimental manipulation. Results suggested that discrepancies in range restriction between anchor and operational ratings do not systematically impact the precision of teacher, principal, and teaching practice estimates. We discuss the implications of these results in terms of research and practice for teacher evaluation systems. Copyright © 2018 by the National Council on Measurement in Education","['explore', 'Influence', 'Range', 'Restrictions', 'Connectivity', 'Sparse', 'Assessment', 'Networks', 'Illustration', 'Exploration', 'Context', 'Classroom', 'Observations']","['explore', 'influence', 'range', 'restrictions', 'connectivity', 'sparse', 'assessment', 'networks', 'illustration', 'exploration', 'context', 'classroom', 'observations']",explore influence range restrictions connectivity sparse assessment networks illustration exploration context classroom observations
Rosen Y.,Assessing Students in Human-to-Agent Settings to Inform Collaborative Problem-Solving Learning,2017,54,"In order to understand potential applications of collaborative problem-solving (CPS) assessment tasks, it is necessary to examine empirically the multifaceted student performance that may be distributed across collaboration methods and purposes of the assessment. Ideally, each student should be matched with various types of group members and must apply the skills in varied contexts and tasks. One solution to these assessment demands is to use computer-based (virtual) agents to serve as the collaborators in the interactions with students. This article proposes a human-to-agent (H-A) approach for formative CPS assessment and describes an international pilot study aimed to provide preliminary empirical findings on the use of H-A CPS assessment to inform collaborative learning. Overall, the findings showed promise in terms of using a H-A CPS assessment task as a formative tool for structuring effective groups in the context of CPS online learning. Copyright © 2017 by the National Council on Measurement in Education","['assess', 'student', 'HumantoAgent', 'Settings', 'inform', 'Collaborative', 'ProblemSolving', 'Learning']","['assess', 'student', 'humantoagent', 'settings', 'inform', 'collaborative', 'problemsolving', 'learning']",assess student humantoagent settings inform collaborative problemsolving learning
Jin K.-Y.; Wang W.-C.,A New Facets Model for Rater's Centrality/Extremity Response Style,2018,55,"The Rasch facets model was developed to account for facet data, such as student essays graded by raters, but it accounts for only one kind of rater effect (severity). In practice, raters may exhibit various tendencies such as using middle or extreme scores in their ratings, which is referred to as the rater centrality/extremity response style. To achieve better measurement quality in rater data, it is desirable to simultaneously consider both rater severity and rater centrality/extremity. A new facets model is thus developed by adding to the Rasch facets model a weight parameter for the item thresholds for each rater. The parameters of the new facets model can be estimated with the JAGS freeware. An empirical example is provided to illustrate the implications and applications of the new model. Two simulation studies were conducted. The first simulation was to evaluate the parameter recovery of the new facets model and the consequences of ignoring the effects of rater centrality/extremity on parameter estimation. The second simulation was to illustrate how rater severity affects the relationship between rater centrality and the standard deviation of raw rating scores. © 2018 by the National Council on Measurement in Education","['New', 'Facets', 'Raters', 'CentralityExtremity', 'Response', 'Style']","['new', 'facets', 'raters', 'centralityextremity', 'response', 'style']",new facets raters centralityextremity response style
Wolkowitz A.A.; Wright K.D.,Effectiveness of Equating at the Passing Score for Exams With Small Sample Sizes,2019,56,"This article explores the amount of equating error at a passing score when equating scores from exams with small samples sizes. This article focuses on equating using classical test theory methods of Tucker linear, Levine linear, frequency estimation, and chained equipercentile equating. Both simulation and real data studies were used in the investigation. The results of the study supported past findings that as the sample sizes increase, the amount of bias in the equating at the passing score decreases. The research also highlights the importance for practitioners to understand the data, to have an informed expectation of the results, and to have a documented rationale for an acceptable amount of equating error. © 2019 by the National Council on Measurement in Education","['effectiveness', 'Equating', 'Passing', 'Score', 'exam', 'Small', 'Sample', 'size']","['effectiveness', 'equating', 'passing', 'score', 'exam', 'small', 'sample', 'size']",effectiveness equating passing score exam small sample size
Peabody M.R.; Wind S.A.,Exploring the Influence of Judge Proficiency on Standard-Setting Judgments,2019,56,"Setting performance standards is a judgmental process involving human opinions and values as well as technical and empirical considerations. Although all cut score decisions are by nature somewhat arbitrary, they should not be capricious. Judges selected for standard-setting panels should have the proper qualifications to make the judgments asked of them; however, even qualified judges vary in expertise and in some cases, such as highly specialized areas or when members of the public are involved, it may be difficult to ensure that each member of a standard-setting panel has the requisite expertise to make qualified judgments. Given the subjective nature of these types of judgments, and that a large part of the validity argument for an exam lies in the robustness of its passing standard, an examination of the influence of judge proficiency on the judgments is warranted. This study explores the use of the many-facet Rasch model as a method for adjusting modified Angoff standard-setting ratings based on judges’ proficiency levels. The results suggest differences in the severity and quality of standard-setting judgments across levels of judge proficiency, such that judges who answered easy items incorrectly tended to perceive them as easier, but those who answered correctly tended to provide ratings within normal stochastic limits. © 2019 by the National Council on Measurement in Education","['explore', 'Influence', 'Judge', 'Proficiency', 'StandardSetting', 'Judgments']","['explore', 'influence', 'judge', 'proficiency', 'standardsetting', 'judgments']",explore influence judge proficiency standardsetting judgments
McNeish D.; Dumas D.,Calculating Conditional Reliability for Dynamic Measurement Model Capacity Estimates,2018,55,"Dynamic measurement modeling (DMM) is a recent framework for measuring developing constructs whose manifestation occurs after an assessment is administered (e.g., learning capacity). Empirical studies have suggested that DMM may improve consequential validity of test scores because DMM learning capacity estimates were shown to be much less related to demographic factors like examinees’ socioeconomic status compared to traditional single-administration item response theory (IRT)–based estimates. Though promotion of DMM has hinged on improved validity, no methods for computing reliability (a prerequisite for validity) have been advanced and DMM is sufficiently different from classical test theory (CTT) and IRT that known methods cannot be directly imported. This article advances one method for computing conditional reliability for DMM so that precision of the estimates can be assessed. © 2018 by the National Council on Measurement in Education","['calculate', 'Conditional', 'Reliability', 'Dynamic', 'Capacity', 'estimate']","['calculate', 'conditional', 'reliability', 'dynamic', 'capacity', 'estimate']",calculate conditional reliability dynamic capacity estimate
Johnson M.S.; Sinharay S.,Measures of Agreement to Assess Attribute-Level Classification Accuracy and Consistency for Cognitive Diagnostic Assessments,2018,55,"One of the proposed uses of cognitive diagnostic assessments is to classify the examinees as either masters or nonmasters on each of a number of skills being assessed. As with any test, it is important to report the quality of these binary classifications with measures of their reliability. Cui et al. and Wang et al. have suggested reliability measures that can be calculated from the model parameters of cognitive diagnosis models; these previously suggested indices are measures of agreement between either the estimated and true mastery classifications, or between the estimated classifications from parallel assessments. This article discusses the limitations of these existing methods and suggests the use of other measures of agreement. A simulation study demonstrates that the proposed measures are related to factors that would be expected to be associated with reliability; for example, reliability increases with variability in the population and with item discrimination, whereas the previously suggested measures do not show the same pattern. A real data example is also included. © 2018 by the National Council on Measurement in Education","['measure', 'Agreement', 'Assess', 'AttributeLevel', 'Classification', 'Accuracy', 'Consistency', 'Cognitive', 'Diagnostic', 'assessment']","['measure', 'agreement', 'assess', 'attributelevel', 'classification', 'accuracy', 'consistency', 'cognitive', 'diagnostic', 'assessment']",measure agreement assess attributelevel classification accuracy consistency cognitive diagnostic assessment
Berger S.; Verschoor A.J.; Eggen T.J.H.M.; Moser U.,Efficiency of Targeted Multistage Calibration Designs Under Practical Constraints: A Simulation Study,2019,56,"Calibration of an item bank for computer adaptive testing requires substantial resources. In this study, we investigated whether the efficiency of calibration under the Rasch model could be enhanced by improving the match between item difficulty and student ability. We introduced targeted multistage calibration designs, a design type that considers ability-related background variables and performance for assigning students to suitable items. Furthermore, we investigated whether uncertainty about item difficulty could impair the assembling of efficient designs. The results indicated that targeted multistage calibration designs were more efficient than ordinary targeted designs under optimal conditions. Limited knowledge about item difficulty reduced the efficiency of one of the two investigated targeted multistage calibration designs, whereas targeted designs were more robust. © 2019 by the National Council on Measurement in Education","['efficiency', 'Targeted', 'Multistage', 'calibration', 'design', 'practical', 'Constraints', 'A', 'Simulation', 'Study']","['efficiency', 'targeted', 'multistage', 'calibration', 'design', 'practical', 'constraints', 'a', 'simulation', 'study']",efficiency targeted multistage calibration design practical constraints a simulation study
Shin H.J.; Wilson M.; Choi I.-H.,Structured Constructs Models Based on Change-Point Analysis,2017,54,"This study proposes a structured constructs model (SCM) to examine measurement in the context of a multidimensional learning progression (LP). The LP is assumed to have features that go beyond a typical multidimentional IRT model, in that there are hypothesized to be certain cross-dimensional linkages that correspond to requirements between the levels of the different dimensions. The new model builds on multidimensional item response theory models and change-point analysis to add cut-score and discontinuity parameters that embody these substantive requirements. This modeling strategy allows us to place the examinees in the appropriate LP level and simultaneously to model the hypothesized requirement relations. Results from a simulation study indicate that the proposed change-point SCM recovers the generating parameters well. When the hypothesized requirement relations are ignored, the model fit tends to become worse, and the model parameters appear to be more biased. Moreover, the proposed model can be used to find validity evidence to support or disprove initial theoretical hypothesized links in the LP through empirical data. We illustrate the technique with data from an assessment system designed to measure student progress in a middle-school statistics and modeling curriculum. Copyright © 2017 by the National Council on Measurement in Education","['structure', 'Constructs', 'Models', 'base', 'ChangePoint', 'Analysis']","['structure', 'constructs', 'models', 'base', 'changepoint', 'analysis']",structure constructs models base changepoint analysis
Olsen J.; Aleven V.; Rummel N.,Statistically Modeling Individual Students’ Learning Over Successive Collaborative Practice Opportunities,2017,54,"Within educational data mining, many statistical models capture the learning of students working individually. However, not much work has been done to extend these statistical models of individual learning to a collaborative setting, despite the effectiveness of collaborative learning activities. We extend a widely used model (the additive factors model) to account for the effect of collaboration on individual learning, including having the help of a partner and getting to observe/help a partner. We find evidence that models that include these collaborative features have a better fit than the original models for performance data and that learning rates estimated using the extended models provide insights into how collaboration benefits individual students’ learning outcomes. Copyright © 2017 by the National Council on Measurement in Education","['statistically', 'Individual', 'Students', ""'"", 'learning', 'Successive', 'Collaborative', 'Practice', 'opportunity']","['statistically', 'individual', 'students', ""'"", 'learning', 'successive', 'collaborative', 'practice', 'opportunity']",statistically individual students ' learning successive collaborative practice opportunity
Madison M.J.; Bradshaw L.,Evaluating Intervention Effects in a Diagnostic Classification Model Framework,2018,55,"The evaluation of intervention effects is an important objective of educational research. One way to evaluate the effectiveness of an intervention is to conduct an experiment that assigns individuals to control and treatment groups. In the context of pretest/posttest designed studies, this is referred to as a control-group pretest/posttest design. The transition diagnostic classification model (TDCM) was recently developed to assess growth, defined as change in attribute mastery status over time, in a diagnostic classification model framework. The TDCM, however, does not model multiple groups, and therefore is not able to analyze data from a control-group pretest/posttest designed experiment. In this study, we extend the TDCM to model multiple groups, thereby enabling the examination of group-differential growth in attribute mastery and the evaluation of intervention effects. The utility of the multigroup TDCM is demonstrated in the evaluation of an innovative instructional method in mathematics education. Copyright © 2018 by the National Council on Measurement in Education","['evaluate', 'Intervention', 'Effects', 'Diagnostic', 'Classification', 'Framework']","['evaluate', 'intervention', 'effects', 'diagnostic', 'classification', 'framework']",evaluate intervention effects diagnostic classification framework
Zhang X.; Tao J.; Wang C.; Shi N.-Z.,Bayesian Model Selection Methods for Multilevel IRT Models: A Comparison of Five DIC-Based Indices,2019,56,"Model selection is important in any statistical analysis, and the primary goal is to find the preferred (or most parsimonious) model, based on certain criteria, from a set of candidate models given data. Several recent publications have employed the deviance information criterion (DIC) to do model selection among different forms of multilevel item response theory models (MLIRT). The majority of the practitioners use WinBUGS for implementing MCMC algorithms for MLIRT models, and the default version of DIC provided by WinBUGS focused on the measurement-level parameters only. The results herein show that this version of DIC is inappropriate. This study introduces five variants of DIC as a model selection index for MLIRT models with dichotomous outcomes. Considering a multilevel IRT model with three levels, five forms of DIC are formed: first-level conditional DIC computed from the measurement model only, which is the index given by many software packages such as WinBUGS; second-level marginalized DIC and second-level joint DIC computed from the second-level model; and top-level marginalized DIC and top-level joint DIC computed from the entire model. We evaluate the performance of the five model selection indices via simulation studies. The manipulated factors include the number of groups, the number of second-level covariates, the number of top-level covariates, and the types of measurement models (one-parameter vs. two-parameter). Considering the computational viability and interpretability, the second-level joint DIC is recommended for MLIRT models under our simulated conditions. © 2019 by the National Council on Measurement in Education","['Bayesian', 'Selection', 'Methods', 'Multilevel', 'IRT', 'Models', 'Comparison', 'Five', 'DICBased', 'index']","['bayesian', 'selection', 'methods', 'multilevel', 'irt', 'models', 'comparison', 'five', 'dicbased', 'index']",bayesian selection methods multilevel irt models comparison five dicbased index
Luo X.; Kim D.,A Top-Down Approach to Designing the Computerized Adaptive Multistage Test,2018,55,"The top-down approach to designing a multistage test is relatively understudied in the literature and underused in research and practice. This study introduced a route-based top-down design approach that directly sets design parameters at the test level and utilizes the advanced automated test assembly algorithm seeking global optimality. The design process in this approach consists of five sub-processes: (1) route mapping, (2) setting objectives, (3) setting constraints, (4) routing error control, and (5) test assembly. Results from a simulation study confirmed that the assembly, measurement and routing results of the top-down design eclipsed those of the bottom-up design. Additionally, the top-down design approach provided unique insights into design decisions that could be used to refine the test. Regardless of these advantages, it is recommended applying both top-down and bottom-up approaches in a complementary manner in practice. Copyright © 2018 by the National Council on Measurement in Education","['TopDown', 'Approach', 'design', 'Computerized', 'Adaptive', 'Multistage', 'test']","['topdown', 'approach', 'design', 'computerized', 'adaptive', 'multistage', 'test']",topdown approach design computerized adaptive multistage test
Liu S.; Cai Y.; Tu D.,On-the-Fly Constraint-Controlled Assembly Methods for Multistage Adaptive Testing for Cognitive Diagnosis,2018,55,"This study applied the mode of on-the-fly assembled multistage adaptive testing to cognitive diagnosis (CD-OMST). Several and several module assembly methods for CD-OMST were proposed and compared in terms of measurement precision, test security, and constrain management. The module assembly methods in the study included the maximum priority index method (MPI), the revised maximum priority index (RMPI), the weighted deviation model (WDM), and the two revised Monte Carlo methods (R1-MC, R2-MC). Simulation results showed that on the whole the CD-OMST performs well in that it not only has acceptable attribute pattern correct classification rates but also satisfies both statistical and nonstatistical constraints; the RMPI method was generally better than the MPI method, the R2-MC method was generally better than the R1-MC method, and the two revised Monte Carlo methods performed best in terms of test security and constraint management, whereas the RMPI and WDM methods worked best in terms of measurement precision. The study is not only expected to provide information about how to combine MST and CD using an on-the-fly method and how do these assembled methods in CD-OMST perform relative to each other but also offer guidance for practitioners to assemble modules in CD-OMST with both statistical and nonstatistical constraints. © 2018 by the National Council on Measurement in Education","['OntheFly', 'ConstraintControlled', 'Assembly', 'Methods', 'Multistage', 'Adaptive', 'Testing', 'Cognitive', 'diagnosis']","['onthefly', 'constraintcontrolled', 'assembly', 'methods', 'multistage', 'adaptive', 'testing', 'cognitive', 'diagnosis']",onthefly constraintcontrolled assembly methods multistage adaptive testing cognitive diagnosis
Scoular C.; Care E.; Hesse F.W.,Designs for Operationalizing Collaborative Problem Solving for Automated Assessment,2017,54,"Collaborative problem solving is a complex skill set that draws on social and cognitive factors. The construct remains in its infancy due to lack of empirical evidence that can be drawn upon for validation. The differences and similarities between two large-scale initiatives that reflect this state of the art, in terms of underlying assumptions about the construct and approach to task development, are outlined. The goal is to clarify how definitions of the nature of the construct impact the approach to design of assessment tasks. Illustrations of two different approaches to the development of a task designed to elicit behaviors that manifest the construct are presented. The method highlights the degree to which these approaches might constrain a comprehensive assessment of the construct. Copyright © 2017 by the National Council on Measurement in Education","['design', 'Operationalizing', 'Collaborative', 'Problem', 'Solving', 'Automated', 'Assessment']","['design', 'operationalizing', 'collaborative', 'problem', 'solving', 'automated', 'assessment']",design operationalizing collaborative problem solving automated assessment
Humphry S.M.; Heldsinger S.,A Two-Stage Method for Classroom Assessments of Essay Writing,2019,56,"To capitalize on professional expertise in educational assessment, it is desirable to develop and test methods of rater-mediated assessment that enable classroom teachers to make reliable and informative judgments. Accordingly, this article investigates the reliability of a two-stage method used by classroom teachers to assess primary school students’ persuasive writing. Stage 1 involves pairwise comparisons and stage 2 involves rating against calibrated exemplars from stage 1 plus performance descriptors. A high level of interrater reliability among teachers was obtained. This is consistent with previous evidence that the two-stage method is a viable classroom assessment method without extensive training and moderation. Implications for assessment practices in education are discussed with a focus on the widely expressed desire to value professional expertise. © 2019 by the National Council on Measurement in Education","['TwoStage', 'Method', 'Classroom', 'assessment', 'Essay', 'writing']","['twostage', 'method', 'classroom', 'assessment', 'essay', 'writing']",twostage method classroom assessment essay writing
Trierweiler T.J.; Lewis C.; Smith R.L.,Further Study of the Choice of Anchor Tests in Equating,2016,53,"In this study, we describe what factors influence the observed score correlation between an (external) anchor test and a total test. We show that the anchor to full-test observed score correlation is based on two components: the true score correlation between the anchor and total test, and the reliability of the anchor test. Findings using an analytical approach suggest that making an anchor test a miditest does not generally maximize the anchor to total test correlation. Results are discussed in the context of what conditions maximize the correlations between the anchor and total test. Copyright © 2016 by the National Council on Measurement in Education","['Study', 'Choice', 'Anchor', 'Tests', 'Equating']","['study', 'choice', 'anchor', 'tests', 'equating']",study choice anchor tests equating
Wise S.L.; Kingsbury G.G.,Modeling Student Test-Taking Motivation in the Context of an Adaptive Achievement Test,2016,53,"This study examined the utility of response time-based analyses in understanding the behavior of unmotivated test takers. For the data from an adaptive achievement test, patterns of observed rapid-guessing behavior and item response accuracy were compared to the behavior expected under several types of models that have been proposed to represent unmotivated test taking behavior. Test taker behavior was found to be inconsistent with these models, with the exception of the effort-moderated model. Effort-moderated scoring was found to both yield scores that were more accurate than those found under traditional scoring, and exhibit improved person fit statistics. In addition, an effort-guided adaptive test was proposed and shown by a simulation study to alleviate item difficulty mistargeting caused by unmotivated test taking. © 2016 by the National Council on Measurement in Education.","['student', 'TestTaking', 'Motivation', 'Context', 'Adaptive', 'Achievement', 'test']","['student', 'testtaking', 'motivation', 'context', 'adaptive', 'achievement', 'test']",student testtaking motivation context adaptive achievement test
Lee Y.-H.; Haberman S.J.; Dorans N.J.,Use of Adjustment by Minimum Discriminant Information in Linking Constructed-Response Test Scores in the Absence of Common Items,2019,56,"In many educational tests, both multiple-choice (MC) and constructed-response (CR) sections are used to measure different constructs. In many common cases, security concerns lead to the use of form-specific CR items that cannot be used for equating test scores, along with MC sections that can be linked to previous test forms via common items. In such cases, adjustment by minimum discriminant information may be used to link CR section scores and composite scores based on both MC and CR sections. This approach is an innovative extension that addresses the long-standing issue of linking CR test scores across test forms in the absence of common items in educational measurement. It is applied to a series of administrations from an international language assessment with MC sections for receptive skills and CR sections for productive skills. To assess the linking results, harmonic regression is applied to examine the effects of the proposed linking method on score stability, among several analyses for evaluation. © 2019 by the National Council on Measurement in Education","['Use', 'Adjustment', 'Minimum', 'Discriminant', 'Information', 'Linking', 'ConstructedResponse', 'Test', 'Scores', 'Absence', 'Common', 'Items']","['use', 'adjustment', 'minimum', 'discriminant', 'information', 'linking', 'constructedresponse', 'test', 'scores', 'absence', 'common', 'items']",use adjustment minimum discriminant information linking constructedresponse test scores absence common items
Joo S.-H.; Lee P.; Stark S.,Development of Information Functions and Indices for the GGUM-RANK Multidimensional Forced Choice IRT Model,2018,55,"This research derived information functions and proposed new scalar information indices to examine the quality of multidimensional forced choice (MFC) items based on the RANK model. We also explored how GGUM-RANK information, latent trait recovery, and reliability varied across three MFC formats: pairs (two response alternatives), triplets (three alternatives), and tetrads (four alternatives). As expected, tetrad and triplet measures provided substantially more information than pairs, and MFC items composed of statements with high discrimination parameters were most informative. The methods and findings of this study will help practitioners to construct better MFC items, make informed projections about reliability with different MFC formats, and facilitate the development of MFC triplet- and tetrad-based computerized adaptive tests. © 2018 by the National Council on Measurement in Education","['development', 'Information', 'Functions', 'Indices', 'GGUMRANK', 'Multidimensional', 'Forced', 'Choice', 'IRT']","['development', 'information', 'functions', 'indices', 'ggumrank', 'multidimensional', 'forced', 'choice', 'irt']",development information functions indices ggumrank multidimensional forced choice irt
Cheng Y.; Liu C.,A Short Note on the Relationship Between Pass Rate and Multiple Attempts,2016,53,"For a certification, licensure, or placement exam, allowing examinees to take multiple attempts at the test could effectively change the pass rate. Change in the pass rate can occur without any change in the underlying latent trait, and can be an artifact of multiple attempts and imperfect reliability of the test. By deriving formulae to compute the pass rate under two definitions, this article provides tools for testing practitioners to compute and evaluate the change in the expected pass rate when a certain (maximum) number of attempts are allowed without any change in the latent trait. This article also includes a simulation study that considers change in ability and differential motivation of examinees to retake the test. Results indicate that the general trend shown by the analytical results is maintained—that is, the marginal expected pass rate increases with more attempts when the testing volume is defined as the total number of test takers, and decreases with more attempts when the testing volume is defined as the total number of test attempts. Copyright © 2016 by the National Council on Measurement in Education","['Short', 'Note', 'relationship', 'Pass', 'Rate', 'Multiple', 'attempt']","['short', 'note', 'relationship', 'pass', 'rate', 'multiple', 'attempt']",short note relationship pass rate multiple attempt
Cizek G.J.; Kosh A.E.; Toutkoushian E.K.,Gathering and Evaluating Validity Evidence: The Generalized Assessment Alignment Tool,2018,55,"Alignment is an essential piece of validity evidence for both educational (K-12) and credentialing (licensure and certification) assessments. In this article, a comprehensive review of commonly used contemporary alignment procedures is provided; some key weaknesses in current alignment approaches are identified; principles for evaluating alignment methods are distilled; and a new approach to investigating alignment is proposed and illustrated. The article concludes with suggestions for alignment research and practice. © 2018 by the National Council on Measurement in Education","['gather', 'evaluate', 'Validity', 'Evidence', 'Generalized', 'Assessment', 'Alignment', 'Tool']","['gather', 'evaluate', 'validity', 'evidence', 'generalized', 'assessment', 'alignment', 'tool']",gather evaluate validity evidence generalized assessment alignment tool
Zwick R.; Ye L.; Isham S.,Aggregating Polytomous DIF Results Over Multiple Test Administrations,2018,55,"In typical differential item functioning (DIF) assessments, an item's DIF status is not influenced by its status in previous test administrations. An item that has shown DIF at multiple administrations may be treated the same way as an item that has shown DIF in only the most recent administration. Therefore, much useful information about the item's functioning is ignored. In earlier work, we developed the Bayesian updating (BU) DIF procedure for dichotomous items and showed how it could be used to formally aggregate DIF results over administrations. More recently, we extended the BU method to the case of polytomously scored items. We conducted an extensive simulation study that included four “administrations” of a test. For the single-administration case, we compared the Bayesian approach to an existing polytomous-DIF procedure. For the multiple-administration case, we compared BU to two non-Bayesian methods of aggregating the polytomous-DIF results over administrations. We concluded that both the BU approach and a simple non-Bayesian method show promise as methods of aggregating polytomous DIF results over administrations. Copyright © 2018 by the National Council on Measurement in Education","['aggregate', 'polytomous', 'DIF', 'result', 'Multiple', 'Test', 'Administrations']","['aggregate', 'polytomous', 'dif', 'result', 'multiple', 'test', 'administrations']",aggregate polytomous dif result multiple test administrations
Duckor B.; Holmberg C.,Exploring How to Model Formative Assessment Trajectories of Posing-Pausing-Probing Practices: Toward a Teacher Learning Progressions Framework for the Study of Novice Teachers,2019,56,"A robust body of evidence supports the finding that particular teaching and assessment strategies in the K-12 classroom can improve student achievement. While experts have identified many effective teaching and learning practices in the assessment for learning literature, teachers’ knowledge and use of “high leverage” formative assessment (FA) practices are difficult to model in novice populations. By employing advances in construct modeling, the theoretical underpinnings of learning progressions research, and four principles of evidence-centered design, teacher educators along with psychometricians can test hypotheses about teacher learning progressions. Utilizing an FA moves-based framework, the article examines how beginning teachers’ posing, pausing, and probing practices align with five key strategies of FA. Examples of construct maps, instructional tasks, and turns of talk analysis using scoring guides are provided from an empirical study of novice science preservice teachers in a high-needs school district. © 2019 by the National Council on Measurement in Education","['explore', 'Formative', 'Assessment', 'Trajectories', 'posingpausingprobe', 'Practices', 'teacher', 'Learning', 'Progressions', 'Framework', 'Study', 'Novice', 'Teachers']","['explore', 'formative', 'assessment', 'trajectories', 'posingpausingprobe', 'practices', 'teacher', 'learning', 'progressions', 'framework', 'study', 'novice', 'teachers']",explore formative assessment trajectories posingpausingprobe practices teacher learning progressions framework study novice teachers
Suh Y.,Effect Size Measures for Differential Item Functioning in a Multidimensional IRT Model,2016,53,"This study adapted an effect size measure used for studying differential item functioning (DIF) in unidimensional tests and extended the measure to multidimensional tests. Two effect size measures were considered in a multidimensional item response theory model: signed weighted P-difference and unsigned weighted P-difference. The performance of the effect size measures was investigated under various simulation conditions including different sample sizes and DIF magnitudes. As another way of studying DIF, the χ2 difference test was included to compare the result of statistical significance (statistical tests) with that of practical significance (effect size measures). The adequacy of existing effect size criteria used in unidimensional tests was also evaluated. Both effect size measures worked well in estimating true effect sizes, identifying DIF types, and classifying effect size categories. Finally, a real data analysis was conducted to support the simulation results. Copyright © 2016 by the National Council on Measurement in Education","['effect', 'Size', 'Measures', 'Differential', 'Item', 'Functioning', 'Multidimensional', 'IRT']","['effect', 'size', 'measures', 'differential', 'item', 'functioning', 'multidimensional', 'irt']",effect size measures differential item functioning multidimensional irt
Ke X.; Zeng Y.; Luo H.,Autoscoring Essays Based on Complex Networks,2016,53,"This article presents a novel method, the Complex Dynamics Essay Scorer (CDES), for automated essay scoring using complex network features. Texts produced by college students in China were represented as scale-free networks (e.g., a word adjacency model) from which typical network features, such as the in-/out-degrees, clustering coefficient (CC), and dynamic networks, were obtained. The CDES integrates the classical concepts of network feature representation and essay score series variation. Several experiments indicated that the network measures different essay qualities and can be clearly demonstrated to develop complex networks for autoscoring tasks. The average agreement of the CDES and human rater scores was 86.5%, and the average Pearson correlation was.77. The results indicate that the CDES produced functional complex systems and autoscored Chinese essays in a method consistent with human raters. Our research suggests potential applications in other areas of educational assessment. Copyright © 2016 by the National Council on Measurement in Education","['autoscore', 'Essays', 'base', 'Complex', 'network']","['autoscore', 'essays', 'base', 'complex', 'network']",autoscore essays base complex network
Andrews J.J.; Kerr D.; Mislevy R.J.; von Davier A.; Hao J.; Liu L.,Modeling Collaborative Interaction Patterns in a Simulation-Based Task,2017,54,"Simulations and games offer interactive tasks that can elicit rich data, providing evidence of complex skills that are difficult to measure with more conventional items and tests. However, one notable challenge in using such technologies is making sense of the data generated in order to make claims about individuals or groups. This article presents a novel methodological approach that uses the process data and performance outcomes from a simulation-based collaborative science assessment to explore the propensities of dyads to interact in accordance with certain interaction patterns. Further exploratory analyses examine how the approach can be used to answer important questions in collaboration research regarding gender and cultural differences in collaborative behavior and how interaction patterns relate to performance outcomes. Copyright © 2017 by the National Council on Measurement in Education","['Collaborative', 'Interaction', 'Patterns', 'SimulationBased', 'Task']","['collaborative', 'interaction', 'patterns', 'simulationbased', 'task']",collaborative interaction patterns simulationbased task
Heritage M.; Kingston N.M.,Classroom Assessment and Large-Scale Psychometrics: Shall the Twain Meet? (A Conversation With Margaret Heritage and Neal Kingston),2019,56,"Classroom assessment and large-scale assessment have, for the most part, existed in mutual isolation. Some experts have felt this is for the best and others have been concerned that the schism limits the potential contribution of both forms of assessment. Margaret Heritage has long been a champion of best practices in classroom assessment. Neal Kingston has been involved with the application of psychometrics to large-scale assessments for four decades. Together they discuss what commonalities and differences exist between these two assessment contexts, whether the twain should meet, what impediments or concerns exist, and whether they expect the status quo will change at all in the near future. Based on their joint keynote address at the NCME Special Conference on Classroom Assessment and Large-Scale Psychometrics, they have expanded and constructed this discussion piece. © 2019 by the National Council on Measurement in Education","['Classroom', 'Assessment', 'LargeScale', 'Psychometrics', 'shall', 'Twain', 'Meet', 'A', 'Conversation', 'Margaret', 'Heritage', 'Neal', 'Kingston']","['classroom', 'assessment', 'largescale', 'psychometrics', 'shall', 'twain', 'meet', 'a', 'conversation', 'margaret', 'heritage', 'neal', 'kingston']",classroom assessment largescale psychometrics shall twain meet a conversation margaret heritage neal kingston
Palermo C.; Bunch M.B.; Ridge K.,Scoring Stability in a Large-Scale Assessment Program: A Longitudinal Analysis of Leniency/Severity Effects,2019,56,"Although much attention has been given to rater effects in rater-mediated assessment contexts, little research has examined the overall stability of leniency and severity effects over time. This study examined longitudinal scoring data collected during three consecutive administrations of a large-scale, multi-state summative assessment program. Multilevel models were used to assess the overall extent of rater leniency/severity during scoring and examine the extent to which leniency/severity effects were stable across the three administrations. Model results were then applied to scaled scores to estimate the impact of the stability of leniency/severity effects on students’ scores. Results showed relative scoring stability across administrations in mathematics. In English language arts, short constructed response items showed evidence of slightly increasing severity across administrations, while essays showed mixed results: evidence of both slightly increasing severity and moderately increasing leniency over time, depending on trait. However, when model results were applied to scaled scores, results revealed rater effects had minimal impact on students’ scores. © 2019 by the National Council on Measurement in Education","['Scoring', 'Stability', 'LargeScale', 'Assessment', 'Program', 'A', 'Longitudinal', 'Analysis', 'LeniencySeverity', 'effect']","['scoring', 'stability', 'largescale', 'assessment', 'program', 'a', 'longitudinal', 'analysis', 'leniencyseverity', 'effect']",scoring stability largescale assessment program a longitudinal analysis leniencyseverity effect
Jang Y.; Kim S.-H.; Cohen A.S.,The Impact of Multidimensionality on Extraction of Latent Classes in Mixture Rasch Models,2018,55,"This study investigates the effect of multidimensionality on extraction of latent classes in mixture Rasch models. In this study, two-dimensional data were generated under varying conditions. The two-dimensional data sets were analyzed with one- to five-class mixture Rasch models. Results of the simulation study indicate the mixture Rasch model tended to extract more latent classes than the number of dimensions simulated, particularly when the multidimensional structure of the data was more complex. In addition, the number of extracted latent classes decreased as the dimensions were more highly correlated regardless of multidimensional structure. An analysis of the empirical multidimensional data also shows that the number of latent classes extracted by the mixture Rasch model is larger than the number of dimensions measured by the test. © 2018 by the National Council on Measurement in Education","['Impact', 'Multidimensionality', 'Extraction', 'Latent', 'Classes', 'Mixture', 'Rasch', 'Models']","['impact', 'multidimensionality', 'extraction', 'latent', 'classes', 'mixture', 'rasch', 'models']",impact multidimensionality extraction latent classes mixture rasch models
Wilson M.; Gochyyev P.; Scalise K.,Modeling Data From Collaborative Assessments: Learning in Digital Interactive Social Networks,2017,54,"This article summarizes assessment of cognitive skills through collaborative tasks, using field test results from the Assessment and Teaching of 21st Century Skills (ATC21S) project. This project, sponsored by Cisco, Intel, and Microsoft, aims to help educators around the world enable students with the skills to succeed in future career and college goals. In this article, ATC21S collaborative assessments focus on the project's “ICT Literacy—Learning in digital networks” learning progression. The article includes a description of the development of the learning progression, as well as examples and the logic behind the instrument construction. Assessments took place in random pairs of students in a demonstration digital environment. Modeling of results employed unidimensional and multidimensional item response models, with and without random effects for groups. The results indicated that, based on this data set, the models that take group into consideration in both the unidimensional and the multidimensional analyses fit better. However, the group-level variances were substantially higher than the individual-level variances. This indicates that a total individual estimate of group plus individual is likely a more informative estimate than individual alone but also that the performances of the pairs dominated the performances of the individuals. Implications are discussed in the results and conclusions. Copyright © 2017 by the National Council on Measurement in Education","['datum', 'collaborative', 'assessment', 'Learning', 'Digital', 'Interactive', 'Social', 'Networks']","['datum', 'collaborative', 'assessment', 'learning', 'digital', 'interactive', 'social', 'networks']",datum collaborative assessment learning digital interactive social networks
Wind S.A.; Jones E.,The Effects of Incomplete Rating Designs in Combination With Rater Effects,2019,56,"Researchers have explored a variety of topics related to identifying and distinguishing among specific types of rater effects, as well as the implications of different types of incomplete data collection designs for rater-mediated assessments. In this study, we used simulated data to examine the sensitivity of latent trait model indicators of three rater effects (leniency, central tendency, and severity) in combination with different types of incomplete rating designs (systematic links, anchor performances, and spiral). We used the rating scale model and the partial credit model to calculate rater location estimates, standard errors of rater estimates, model–data fit statistics, and the standard deviation of rating scale category thresholds as indicators of rater effects and we explored the sensitivity of these indicators to rater effects under different conditions. Our results suggest that it is possible to detect rater effects when each of the three types of rating designs is used. However, there are differences in the sensitivity of each indicator related to type of rater effect, type of rating design, and the overall proportion of effect raters. We discuss implications for research and practice related to rater-mediated assessments. © 2019 by the National Council on Measurement in Education","['Effects', 'Incomplete', 'Rating', 'design', 'Combination', 'Rater', 'effect']","['effects', 'incomplete', 'rating', 'design', 'combination', 'rater', 'effect']",effects incomplete rating design combination rater effect
Halpin P.F.; von Davier A.A.; Hao J.; Liu L.,Measuring Student Engagement During Collaboration,2017,54,"This article addresses performance assessments that involve collaboration among students. We apply the Hawkes process to infer whether the actions of one student are associated with increased probability of further actions by his/her partner(s) in the near future. This leads to an intuitive notion of engagement among collaborators, and we consider a model-based index that can be used to quantify this notion. The approach is illustrated using a simulation-based task designed for science education, in which pairs of collaborators interact using online chat. We also consider the empirical relationship between chat engagement and task performance, finding that less engaged collaborators were less likely to revise their responses after being given an opportunity to share their work with their partner. Copyright © 2017 by the National Council on Measurement in Education","['measure', 'Student', 'Engagement', 'collaboration']","['measure', 'student', 'engagement', 'collaboration']",measure student engagement collaboration
Moses T.; Kim Y.,Stabilizing Conditional Standard Errors of Measurement in Scale Score Transformations,2017,54,"The focus of this article is on scale score transformations that can be used to stabilize conditional standard errors of measurement (CSEMs). Three transformations for stabilizing the estimated CSEMs are reviewed, including the traditional arcsine transformation, a recently developed general variance stabilization transformation, and a new method proposed in this article involving cubic transformations. Two examples are provided and the three scale score transformations are compared in terms of how well they stabilize CSEMs estimated from compound binomial and item response theory (IRT) models. Advantages of the cubic transformation are demonstrated with respect to CSEM stabilization and other scaling criteria (e.g., scale score distributions that are more symmetric). Copyright © 2017 by the National Council on Measurement in Education","['stabilize', 'Conditional', 'Standard', 'Errors', 'Scale', 'Score', 'Transformations']","['stabilize', 'conditional', 'standard', 'errors', 'scale', 'score', 'transformations']",stabilize conditional standard errors scale score transformations
Chen Y.-H.; Senk S.L.; Thompson D.R.; Voogt K.,Examining Psychometric Properties and Level Classification of the van Hiele Geometry Test Using CTT and CDM Frameworks,2019,56,"The van Hiele theory and van Hiele Geometry Test have been extensively used in mathematics assessments across countries. The purpose of this study is to use classical test theory (CTT) and cognitive diagnostic modeling (CDM) frameworks to examine psychometric properties of the van Hiele Geometry Test and to compare how various classification criteria assign van Hiele levels to students. The findings support the hierarchical property of the van Hiele theory and levels. Using conventional and combined criteria to determine mastery of a level, the percentages of students classified into an overall level were relatively high. Although some items had aberrant difficulties and low item discrimination, varied selection of the criteria across levels improved item discrimination power, especially for those items with low item discrimination index (IDI) estimates. Based on the findings, we identify items on the van Hiele Geometry Test that might be revised and we suggest changes to classification criteria to increase the number of students who can be assigned an overall level of geometry thinking according to the theory. As a result, practitioners and researchers may be better positioned to use the van Hiele Geometry Test for classroom assessment. © 2019 by the National Council on Measurement in Education","['examine', 'Psychometric', 'Properties', 'Level', 'Classification', 'van', 'Hiele', 'Geometry', 'Test', 'CTT', 'CDM', 'framework']","['examine', 'psychometric', 'properties', 'level', 'classification', 'van', 'hiele', 'geometry', 'test', 'ctt', 'cdm', 'framework']",examine psychometric properties level classification van hiele geometry test ctt cdm framework
Lee S.; Suh Y.,Lord's Wald Test for Detecting DIF in Multidimensional IRT Models: A Comparison of Two Estimation Approaches,2018,55,"Lord's Wald test for differential item functioning (DIF) has not been studied extensively in the context of the multidimensional item response theory (MIRT) framework. In this article, Lord's Wald test was implemented using two estimation approaches, marginal maximum likelihood estimation and Bayesian Markov chain Monte Carlo estimation, to detect uniform and nonuniform DIF under MIRT models. The Type I error and power rates for Lord's Wald test were investigated under various simulation conditions, including different DIF types and magnitudes, different means and correlations of two ability parameters, and different sample sizes. Furthermore, English usage data were analyzed to illustrate the use of Lord's Wald test with the two estimation approaches. Copyright © 2018 by the National Council on Measurement in Education","['Lords', 'Wald', 'Test', 'detect', 'DIF', 'Multidimensional', 'IRT', 'Models', 'Comparison', 'Estimation', 'Approaches']","['lords', 'wald', 'test', 'detect', 'dif', 'multidimensional', 'irt', 'models', 'comparison', 'estimation', 'approaches']",lords wald test detect dif multidimensional irt models comparison estimation approaches
Andersson B.,Asymptotic Standard Errors of Observed-Score Equating With Polytomous IRT Models,2016,53,"In observed-score equipercentile equating, the goal is to make scores on two scales or tests measuring the same construct comparable by matching the percentiles of the respective score distributions. If the tests consist of different items with multiple categories for each item, a suitable model for the responses is a polytomous item response theory (IRT) model. The parameters from such a model can be utilized to derive the score probabilities for the tests and these score probabilities may then be used in observed-score equating. In this study, the asymptotic standard errors of observed-score equating using score probability vectors from polytomous IRT models are derived using the delta method. The results are applied to the equivalent groups design and the nonequivalent groups design with either chain equating or poststratification equating within the framework of kernel equating. The derivations are presented in a general form and specific formulas for the graded response model and the generalized partial credit model are provided. The asymptotic standard errors are accurate under several simulation conditions relating to sample size, distributional misspecification and, for the nonequivalent groups design, anchor test length. Copyright © 2016 by the National Council on Measurement in Education","['Asymptotic', 'Standard', 'Errors', 'ObservedScore', 'Equating', 'Polytomous', 'IRT']","['asymptotic', 'standard', 'errors', 'observedscore', 'equating', 'polytomous', 'irt']",asymptotic standard errors observedscore equating polytomous irt
Sinharay S.; Duong M.Q.; Wood S.W.,A New Statistic for Detection of Aberrant Answer Changes,2017,54,"As noted by Fremer and Olson, analysis of answer changes is often used to investigate testing irregularities because the analysis is readily performed and has proven its value in practice. Researchers such as Belov, Sinharay and Johnson, van der Linden and Jeon, van der Linden and Lewis, and Wollack, Cohen, and Eckerly have suggested several statistics for detection of aberrant answer changes. This article suggests a new statistic that is based on the likelihood ratio test. An advantage of the new statistic is that it follows the standard normal distribution under the null hypothesis of no aberrant answer changes. It is demonstrated in a detailed simulation study that the Type I error rate of the new statistic is very close to the nominal level and the power of the new statistic is satisfactory in comparison to those of several existing statistics for detecting aberrant answer changes. The new statistic and several existing statistics were shown to provide useful information for a real data set. Given the increasing interest in analysis of answer changes, the new statistic promises to be useful to measurement practitioners. Copyright © 2017 by the National Council on Measurement in Education","['New', 'Statistic', 'Detection', 'Aberrant', 'Answer', 'change']","['new', 'statistic', 'detection', 'aberrant', 'answer', 'change']",new statistic detection aberrant answer change
Keuning T.; van Geel M.; Visscher A.; Fox J.-P.,Assessing and Validating Effects of a Data-Based Decision-Making Intervention on Student Growth for Mathematics and Spelling,2019,56,"Data-based decision making (DBDM) is presumed to improve student performance in elementary schools in all subjects. The majority of studies in which DBDM effects have been evaluated have focused on mathematics. A hierarchical multiple single-subject design was used to measure effects of a 2-year training, in which entire school teams learned how to implement and sustain DBDM, in 39 elementary schools. In a multilevel modeling approach, student achievement in mathematics and spelling was analyzed to broaden our understanding of the effects of DBDM interventions. Student achievement data covering the period from August 2010 to July 2014 were retrieved from schools’ student monitoring systems. Student performance on standardized tests was scored on a vertical ability scale per subject for Grades 1 to 6. To investigate intervention effects, linear mixed effect analysis was conducted. Findings revealed a positive intervention effect for both mathematics and spelling. Furthermore, low-SES students and low-SES schools benefitted most from the intervention for mathematics. © 2019 by the National Council on Measurement in Education","['assess', 'validate', 'effect', 'DataBased', 'DecisionMaking', 'Intervention', 'student', 'Growth', 'Mathematics', 'Spelling']","['assess', 'validate', 'effect', 'databased', 'decisionmaking', 'intervention', 'student', 'growth', 'mathematics', 'spelling']",assess validate effect databased decisionmaking intervention student growth mathematics spelling
Wind S.A.; Sebok-Syer S.S.,Examining Differential Rater Functioning Using a Between-Subgroup Outfit Approach,2019,56,"When practitioners use modern measurement models to evaluate rating quality, they commonly examine rater fit statistics that summarize how well each rater's ratings fit the expectations of the measurement model. Essentially, this approach involves examining the unexpected ratings that each misfitting rater assigned (i.e., carrying out analyses of standardized residuals). One can create plots of the standardized residuals, isolating those that resulted from raters’ ratings of particular subgroups. Practitioners can then examine the plots to identify raters who did not maintain a uniform level of severity when they assessed various subgroups (i.e., exhibited evidence of differential rater functioning). In this study, we analyzed simulated and real data to explore the utility of this between-subgroup fit approach. We used standardized between-subgroup outfit statistics to identify misfitting raters and the corresponding plots of their standardized residuals to determine whether there were any identifiable patterns in each rater's misfitting ratings related to subgroups. © 2019 by the National Council on Measurement in Education","['examine', 'Differential', 'Rater', 'Functioning', 'BetweenSubgroup', 'Outfit', 'Approach']","['examine', 'differential', 'rater', 'functioning', 'betweensubgroup', 'outfit', 'approach']",examine differential rater functioning betweensubgroup outfit approach
Vonkova H.; Zamarro G.; Hitt C.,Cross-Country Heterogeneity in Students’ Reporting Behavior: The Use of the Anchoring Vignette Method,2018,55,Self-reports are an indispensable source of information in education research but they are often affected by heterogeneity in reporting behavior. Failing to correct for this heterogeneity can lead to invalid comparisons across groups. The researchers use the parametric anchoring vignette method to correct for cross-country incomparability of students’ reports on teacher's classroom management. Their analysis is based on the data from the Programme for International Student Assessment 2012. The results show significant variation in implicit standards across countries. Correlations between countries’ average teacher classroom management levels and external variables like students test scores and public expenditure per pupil change substantially after vignette adjustment. The researchers conclude that the anchoring vignettes method shows potential to enhance the comparability of self-reported measures in education. Copyright © 2018 by the National Council on Measurement in Education,"['CrossCountry', 'Heterogeneity', 'Students', ""'"", 'report', 'Behavior', 'Use', 'Anchoring', 'Vignette', 'Method']","['crosscountry', 'heterogeneity', 'students', ""'"", 'report', 'behavior', 'use', 'anchoring', 'vignette', 'method']",crosscountry heterogeneity students ' report behavior use anchoring vignette method
Kim K.Y.; Lee W.-C.,Confidence Intervals for Weighted Composite Scores Under the Compound Binomial Error Model,2018,55,"Reporting confidence intervals with test scores helps test users make important decisions about examinees by providing information about the precision of test scores. Although a variety of estimation procedures based on the binomial error model are available for computing intervals for test scores, these procedures assume that items are randomly drawn from a undifferentiated universe of items, and therefore might not be suitable for tests developed according to a table of specifications. To address this issue, four interval estimation procedures that use category subscores for the computation of confidence intervals are presented in this article. All four estimation procedures assume that subscores instead of test scores follow a binomial distribution (i.e., compound binomial error model). The relative performance of the four compound binomial–based interval estimation procedures is compared to each other and to the better known normal approximation and Wilson score procedures based on the binomial error model. Copyright © 2018 by the National Council on Measurement in Education","['confidence', 'Intervals', 'Weighted', 'Composite', 'Scores', 'Compound', 'Binomial', 'Error']","['confidence', 'intervals', 'weighted', 'composite', 'scores', 'compound', 'binomial', 'error']",confidence intervals weighted composite scores compound binomial error
Ames A.; Smith E.,Subjective Priors for Item Response Models: Application of Elicitation by Design,2018,55,"Bayesian methods incorporate model parameter information prior to data collection. Eliciting information from content experts is an option, but has seen little implementation in Bayesian item response theory (IRT) modeling. This study aims to use ethical reasoning content experts to elicit prior information and incorporate this information into Markov Chain Monte Carlo (MCMC) estimation. A six-step elicitation approach is followed, with relevant details at each stage for two IRT items parameters: difficulty and guessing. Results indicate that using content experts is the preferred approach, rather than noninformative priors, for both parameter types. The use of a noninformative prior for small samples provided dramatically different results when compared to results from content expert–elicited priors. The WAMBS (When to worry and how to Avoid the Misuse of Bayesian Statistics) checklist is used to aid in comparisons. © 2018 by the National Council on Measurement in Education","['subjective', 'Priors', 'Item', 'Response', 'Models', 'Application', 'Elicitation', 'Design']","['subjective', 'priors', 'item', 'response', 'models', 'application', 'elicitation', 'design']",subjective priors item response models application elicitation design
Man K.; Harring J.R.; Sinharay S.,Use of Data Mining Methods to Detect Test Fraud,2019,56,"Data mining methods have drawn considerable attention across diverse scientific fields. However, few applications could be found in the areas of psychological and educational measurement, and particularly pertinent to this article, in test security research. In this study, various data mining methods for detecting cheating behaviors on large-scale assessments are explored as an alternative to the traditional methods including person-fit statistics and similarity analysis. A common data set from the Handbook of Quantitative Methods for Detecting Cheating on Tests (Cizek & Wollack) was used for comparing the performance of the different methods. The results indicated that the use of data mining methods may combine multiple sources of information about test takers' performance, which may lead to higher detection rate over traditional item response and response time methods. Several recommendations, all based on our findings, are provided to practitioners. © 2019 by the National Council on Measurement in Education","['Use', 'Data', 'Mining', 'Methods', 'detect', 'Test', 'Fraud']","['use', 'data', 'mining', 'methods', 'detect', 'test', 'fraud']",use data mining methods detect test fraud
Skaggs G.; Hein S.F.; Wilkins J.L.M.,Diagnostic Profiles: A Standard Setting Method for Use With a Cognitive Diagnostic Model,2016,53,"This article introduces the Diagnostic Profiles (DP) standard setting method for setting a performance standard on a test developed from a cognitive diagnostic model (CDM), the outcome of which is a profile of mastered and not-mastered skills or attributes rather than a single test score. In the DP method, the key judgment task for panelists is a decision on whether or not individual cognitive skill profiles meet the performance standard. A randomized experiment was carried out in which secondary mathematics teachers were randomly assigned to either the DP method or the modified Angoff method. The standard setting methods were applied to a test of student readiness to enter high school algebra (Algebra I). While the DP profile judgments were perceived to be more difficult than the Angoff item judgments, there was a high degree of agreement among the panelists for most of the profiles. In order to compare the methods, cut scores were generated from the DP method. The results of the DP group were comparable to the Angoff group, with less cut score variability in the DP group. The DP method shows promise for testing situations in which diagnostic information is needed about examinees and where that information needs to be linked to a performance standard. Copyright © 2016 by the National Council on Measurement in Education","['Diagnostic', 'Profiles', 'Standard', 'Setting', 'Method', 'Use', 'Cognitive', 'Diagnostic']","['diagnostic', 'profiles', 'standard', 'setting', 'method', 'use', 'cognitive', 'diagnostic']",diagnostic profiles standard setting method use cognitive diagnostic
Feuerstahler L.; Wilson M.,Scale Alignment in Between-Item Multidimensional Rasch Models,2019,56,"Scores estimated from multidimensional item response theory (IRT) models are not necessarily comparable across dimensions. In this article, the concept of aligned dimensions is formalized in the context of Rasch models, and two methods are described—delta dimensional alignment (DDA) and logistic regression alignment (LRA)—to transform estimated item parameters so that dimensions are aligned. Both the DDA and LRA methods are applied to real and simulated data, and it is demonstrated that both methods are broadly effective for achieving aligned scales. The routine use of scale alignment methods is recommended prior to comparing scores across dimensions. © 2019 by the National Council on Measurement in Education","['Scale', 'Alignment', 'BetweenItem', 'Multidimensional', 'Rasch', 'Models']","['scale', 'alignment', 'betweenitem', 'multidimensional', 'rasch', 'models']",scale alignment betweenitem multidimensional rasch models
Leighton J.P.,Students’ Interpretation of Formative Assessment Feedback: Three Claims for Why We Know So Little About Something So Important,2019,56,"If K-12 students are to be fully integrated as active participants in their own learning, understanding how they interpret formative assessment feedback is needed. The objective of this article is to advance three claims about why teachers and assessment scholars/specialists may have little understanding of students’ interpretation of formative assessment feedback. The three claims are as follows. First, there is little systematic research of K-12 students’ interpretations of feedback. Systematic research requires gathering substantive evidence of students’ cognitive and emotional processes using psychological methods and tools. Second, there is an overemphasis on the external assessment process at the expense of uncovering learners’ internal reasoning and emotional processes. This overemphasis may be due to vestiges of behavioral approaches and lack of training in social cognitive methods. Third, there are psychological tools such as the clinical interview, pioneered by Piaget and used by psychologists to “enter the child's mind,” which may be helpful in uncovering students’ interpretation of feedback and associated behavioral responses. If the purpose of formative assessment is to change student learning, and feedback is delivered as a conduit to help with this long-term change, understanding students’ interpretation of feedback plays a central role in the validity of the process. © 2019 by the National Council on Measurement in Education","['student', '’', 'Interpretation', 'Formative', 'Assessment', 'Feedback', 'Three', 'Claims', 'know', 'little', 'Something', 'important']","['student', '’', 'interpretation', 'formative', 'assessment', 'feedback', 'three', 'claims', 'know', 'little', 'something', 'important']",student ’ interpretation formative assessment feedback three claims know little something important
Li J.; van der Linden W.J.,A Comparison of Constraint Programming and Mixed-Integer Programming for Automated Test-Form Generation,2018,55,"The final step of the typical process of developing educational and psychological tests is to place the selected test items in a formatted form. The step involves the grouping and ordering of the items to meet a variety of formatting constraints. As this activity tends to be time-intensive, the use of mixed-integer programming (MIP) has been proposed to automate it. The goal of this article is to show how constraint programming (CP) can be used as an alternative to automate test-form generation problems with a large variety of formatting constraints, and how it compares with MIP-based form generation as for its models, solutions, and running times. Two empirical examples are presented: (i) automated generation of a computerized fixed-form; and (ii) automated generation of shadow tests for multistage testing. Both examples show that CP works well with feasible solutions and running times likely to be better than that for MIP-based applications. © 2018 by the National Council on Measurement in Education","['Comparison', 'Constraint', 'Programming', 'MixedInteger', 'Programming', 'Automated', 'TestForm', 'generation']","['comparison', 'constraint', 'programming', 'mixedinteger', 'programming', 'automated', 'testform', 'generation']",comparison constraint programming mixedinteger programming automated testform generation
Ju U.; Falk C.F.,Modeling Response Styles in Cross-Country Self-Reports: An Application of a Multilevel Multidimensional Nominal Response Model,2019,56,"We examined the feasibility and results of a multilevel multidimensional nominal response model (ML-MNRM) for measuring both substantive constructs and extreme response style (ERS) across countries. The ML-MNRM considers within-country clustering while allowing overall item slopes to vary across items and examination of whether certain items were more prone to ERS. We applied this model to survey items from TALIS 2013. Results indicated that self-efficacy items were more likely to trigger ERS compared to need for professional development, and the between-country relationships among constructs can change due to ERS. Simulations assessed the estimation approach and found adequate recovery of model parameters and factor scores. We stress the importance of additional validity studies to improve the cross-cultural comparability of substantive constructs. © 2019 by the National Council on Measurement in Education","['Response', 'Styles', 'CrossCountry', 'SelfReports', 'application', 'Multilevel', 'Multidimensional', 'Nominal', 'Response']","['response', 'styles', 'crosscountry', 'selfreports', 'application', 'multilevel', 'multidimensional', 'nominal', 'response']",response styles crosscountry selfreports application multilevel multidimensional nominal response
Köhler C.; Pohl S.; Carstensen C.H.,Dealing With Item Nonresponse in Large-Scale Cognitive Assessments: The Impact of Missing Data Methods on Estimated Explanatory Relationships,2017,54,"Competence data from low-stakes educational large-scale assessment studies allow for evaluating relationships between competencies and other variables. The impact of item-level nonresponse has not been investigated with regard to statistics that determine the size of these relationships (e.g., correlations, regression coefficients). Classical approaches such as ignoring missing values or treating them as incorrect are currently applied in many large-scale studies, while recent model-based approaches that can account for nonignorable nonresponse have been developed. Estimates of item and person parameters have been demonstrated to be biased for classical approaches when missing data are missing not at random (MNAR). In our study, we focus on parameter estimates of the structural model (i.e., the true regression coefficient when regressing competence on an explanatory variable), simulating data according to various missing data mechanisms. We found that model-based approaches and ignoring missing values performed well in retrieving regression coefficients even when we induced missing data that were MNAR. Treating missing values as incorrect responses can lead to substantial bias. We demonstrate the validity of our approach empirically and discuss the relevance of our results. Copyright © 2017 by the National Council on Measurement in Education","['deal', 'Item', 'Nonresponse', 'LargeScale', 'Cognitive', 'assessment', 'Impact', 'Missing', 'Data', 'Methods', 'Estimated', 'Explanatory', 'Relationships']","['deal', 'item', 'nonresponse', 'largescale', 'cognitive', 'assessment', 'impact', 'missing', 'data', 'methods', 'estimated', 'explanatory', 'relationships']",deal item nonresponse largescale cognitive assessment impact missing data methods estimated explanatory relationships
Sinharay S.,A New Interpretation of Augmented Subscores and Their Added Value in Terms of Parallel Forms,2018,55,The value-added method of Haberman is arguably one of the most popular methods to evaluate the quality of subscores. The method is based on the classical test theory and deems a subscore to be of added value if the subscore predicts the corresponding true subscore better than does the total score. Sinharay provided an interpretation of the added value of subscores in terms of scores and subscores on parallel forms. This article extends the results of Sinharay and considers the prediction of a subscore on a parallel form from both the subscore and the total raw score on the original form. The resulting predictor essentially becomes the augmented subscore suggested by Haberman. The proportional reduction in mean squared error of the resulting predictor is interpreted as a squared multiple correlation coefficient. The practical usefulness of the derived results is demonstrated using an operational data set. Copyright © 2018 by the National Council on Measurement in Education,"['New', 'Interpretation', 'Augmented', 'Subscores', 'Added', 'Value', 'term', 'parallel', 'form']","['new', 'interpretation', 'augmented', 'subscores', 'added', 'value', 'term', 'parallel', 'form']",new interpretation augmented subscores added value term parallel form
Fox J.-P.; Marianti S.,Person-Fit Statistics for Joint Models for Accuracy and Speed,2017,54,"Response accuracy and response time data can be analyzed with a joint model to measure ability and speed of working, while accounting for relationships between item and person characteristics. In this study, person-fit statistics are proposed for joint models to detect aberrant response accuracy and/or response time patterns. The person-fit tests take the correlation between ability and speed into account, as well as the correlation between item characteristics. They are posited as Bayesian significance tests, which have the advantage that the extremeness of a test statistic value is quantified by a posterior probability. The person-fit tests can be computed as by-products of a Markov chain Monte Carlo algorithm. Simulation studies were conducted in order to evaluate their performance. For all person-fit tests, the simulation studies showed good detection rates in identifying aberrant patterns. A real data example is given to illustrate the person-fit statistics for the evaluation of the joint model. Copyright © 2017 by the National Council on Measurement in Education","['PersonFit', 'Statistics', 'Joint', 'Models', 'Accuracy', 'speed']","['personfit', 'statistics', 'joint', 'models', 'accuracy', 'speed']",personfit statistics joint models accuracy speed
Briggs D.C.; Chattergoon R.; Burkhardt A.,Examining the Dual Purpose Use of Student Learning Objectives for Classroom Assessment and Teacher Evaluation,2019,56,"The process of setting and evaluating student learning objectives (SLOs) has become increasingly popular as an example where classroom assessment is intended to fulfill the dual purpose use of informing instruction and holding teachers accountable. A concern is that the high-stakes purpose may lead to distortions in the inferences about students and teachers that SLOs can support. This concern is explored in the present study by contrasting student SLO scores in a large urban school district to performance on a common objective external criterion. This external criterion is used to evaluate the extent to which student growth scores appear to be inflated. Using 2 years of data, growth comparisons are also made at the teacher level for teachers who submit SLOs and have students that take the state-administered large-scale assessment. Although they do show similar relationships with demographic covariates and have the same degree of stability across years, the two different measures of growth are weakly correlated. © 2019 by the National Council on Measurement in Education","['examine', 'Dual', 'Purpose', 'Use', 'Student', 'Learning', 'Objectives', 'Classroom', 'Assessment', 'Teacher', 'Evaluation']","['examine', 'dual', 'purpose', 'use', 'student', 'learning', 'objectives', 'classroom', 'assessment', 'teacher', 'evaluation']",examine dual purpose use student learning objectives classroom assessment teacher evaluation
Steedle J.T.; Radunzel J.; Mattern K.D.,Comparing Academic Readiness Requirements for Different Postsecondary Pathways: What Admissions Tests Tell Us,2019,56,"Ensuring postsecondary readiness is a goal of K-12 education, but it is unclear whether high school students should get different messages about the required levels of academic preparation depending on their postsecondary trajectories. This study estimated readiness benchmark scores on a college admissions test predictive of earning good grades in majors associated with middle-skills occupations at 2-year postsecondary institutions. Results generally indicated similarity between those scores, the corresponding scores for students preparing for high-skills jobs requiring a bachelor's degree, and established readiness benchmarks for the general college-going population. Subsequent analyses revealed small variation between readiness benchmarks for different college majors. Overall, results suggest that high school graduates need a strong academic foundation regardless of the postsecondary path they choose. © 2019 by the National Council on Measurement in Education","['compare', 'Academic', 'Readiness', 'Requirements', 'Different', 'Postsecondary', 'pathway', 'Admissions', 'test', 'tell']","['compare', 'academic', 'readiness', 'requirements', 'different', 'postsecondary', 'pathway', 'admissions', 'test', 'tell']",compare academic readiness requirements different postsecondary pathway admissions test tell
Sinharay S.,How to Compare Parametric and Nonparametric Person-Fit Statistics Using Real Data,2017,54,"Person-fit assessment (PFA) is concerned with uncovering atypical test performance as reflected in the pattern of scores on individual items on a test. Existing person-fit statistics (PFSs) include both parametric and nonparametric statistics. Comparison of PFSs has been a popular research topic in PFA, but almost all comparisons have employed simulated data. This article suggests an approach for comparing the performance of parametric and nonparametric PFSs using real data. This article then shows that there is no clear winner between l*Z, a popular parametric PFS, and HT, a popular nonparametric statistic, in a comparison using the suggested approach. This finding is contradictory to the common finding shown by Karabatsos, Dimitrov and Smith, and Tendeiro and Meijer that HT is more powerful than several parametric PFSs including l*Z and lZ. Copyright © 2017 by the National Council on Measurement in Education","['compare', 'Parametric', 'Nonparametric', 'PersonFit', 'Statistics', 'Real', 'Data']","['compare', 'parametric', 'nonparametric', 'personfit', 'statistics', 'real', 'data']",compare parametric nonparametric personfit statistics real data
Wind S.A.,"Nonparametric Evidence of Validity, Reliability, and Fairness for Rater-Mediated Assessments: An Illustration Using Mokken Scale Analysis",2019,56,"Numerous researchers have proposed methods for evaluating the quality of rater-mediated assessments using nonparametric methods (e.g., kappa coefficients) and parametric methods (e.g., the many-facet Rasch model). Generally speaking, popular nonparametric methods for evaluating rating quality are not based on a particular measurement theory. On the other hand, popular parametric methods for evaluating rating quality are often based on measurement theories such as invariant measurement. However, these methods are based on assumptions and transformations that may not be appropriate for ordinal ratings. In this study, I show how researchers can use Mokken scale analysis (MSA), which is a nonparametric approach to item response theory, to evaluate rating quality within the framework of invariant measurement without the use of potentially inappropriate parametric techniques. I use an illustrative analysis of data from a rater-mediated writing assessment to demonstrate how one can use numeric and graphical indicators from MSA to gather evidence of validity, reliability, and fairness. The results from the analyses suggest that MSA provides a useful framework within which to evaluate rater-mediated assessments for evidence of validity, reliability, and fairness that can supplement existing popular methods for evaluating ratings. © 2019 by the National Council on Measurement in Education","['Nonparametric', 'Evidence', 'Validity', 'Reliability', 'Fairness', 'RaterMediated', 'assessment', 'Illustration', 'Mokken', 'Scale', 'Analysis']","['nonparametric', 'evidence', 'validity', 'reliability', 'fairness', 'ratermediated', 'assessment', 'illustration', 'mokken', 'scale', 'analysis']",nonparametric evidence validity reliability fairness ratermediated assessment illustration mokken scale analysis
Nieto R.; Casabianca J.M.,Accounting for Rater Effects With the Hierarchical Rater Model Framework When Scoring Simple Structured Constructed Response Tests,2019,56,"Many large-scale assessments are designed to yield two or more scores for an individual by administering multiple sections measuring different but related skills. Multidimensional tests, or more specifically, simple structured tests, such as these rely on multiple multiple-choice and/or constructed responses sections of items to generate multiple scores. In the current article, we propose an extension of the hierarchical rater model (HRM) to be applied with simple structured tests with constructed response items. In addition to modeling the appropriate trait structure, the multidimensional HRM (M-HRM) presented here also accounts for rater severity bias and rater variability or inconsistency. We introduce the model formulation, test parameter recovery with a focus on latent traits, and compare the M-HRM to other scoring approaches (unidimensional HRMs and a traditional multidimensional item response theory model) using simulated and empirical data. Results show more precise scores under the M-HRM, with a major improvement in scores when incorporating rater effects versus ignoring them in the traditional multidimensional item response theory model. © 2019 by the National Council on Measurement in Education","['account', 'Rater', 'Effects', 'Hierarchical', 'Rater', 'Framework', 'Scoring', 'Simple', 'Structured', 'Constructed', 'Response', 'test']","['account', 'rater', 'effects', 'hierarchical', 'rater', 'framework', 'scoring', 'simple', 'structured', 'constructed', 'response', 'test']",account rater effects hierarchical rater framework scoring simple structured constructed response test
Embretson S.E.; Kingston N.M.,Automatic Item Generation: A More Efficient Process for Developing Mathematics Achievement Items?,2018,55,"The continual supply of new items is crucial to maintaining quality for many tests. Automatic item generation (AIG) has the potential to rapidly increase the number of items that are available. However, the efficiency of AIG will be mitigated if the generated items must be submitted to traditional, time-consuming review processes. In two studies, generated mathematics achievement items were subjected to multiple stages of qualitative review for measuring the intended skills, followed by empirical tryout in operational testing. High rates of success were found. Further, items generated from the same item structure had predictable psychometric properties. Thus, the feasibility of a more limited and expedient review processes was supported. Additionally, positive results were obtained on measuring the same skills from item structures with reduced cognitive complexity. Copyright © 2018 by the National Council on Measurement in Education","['Automatic', 'Item', 'Generation', 'A', 'efficient', 'Process', 'develop', 'Mathematics', 'Achievement', 'item']","['automatic', 'item', 'generation', 'a', 'efficient', 'process', 'develop', 'mathematics', 'achievement', 'item']",automatic item generation a efficient process develop mathematics achievement item
Guo H.; Robin F.; Dorans N.,Detecting Item Drift in Large-Scale Testing,2017,54,"The early detection of item drift is an important issue for frequently administered testing programs because items are reused over time. Unfortunately, operational data tend to be very sparse and do not lend themselves to frequent monitoring analyses, particularly for on-demand testing. Building on existing residual analyses, the authors propose an item index that requires only moderate-to-small sample sizes to form data for time-series analysis. Asymptotic results are presented to facilitate statistical significance tests. The authors show that the proposed index combined with time-series techniques may be useful in detecting and predicting item drift. Most important, this index is related to a well-known differential item functioning analysis so that a meaningful effect size can be proposed for item drift detection. Copyright © 2017 by the National Council on Measurement in Education","['detect', 'Item', 'Drift', 'LargeScale', 'Testing']","['detect', 'item', 'drift', 'largescale', 'testing']",detect item drift largescale testing
Guo H.; Deane P.D.; van Rijn P.W.; Zhang M.; Bennett R.E.,Modeling Basic Writing Processes From Keystroke Logs,2018,55,"The goal of this study is to model pauses extracted from writing keystroke logs as a way of characterizing the processes students use in essay composition. Low-level timing data were modeled, the interkey interval and its subtype, the intraword duration, thought to reflect processes associated with keyboarding skills and composition fluency. Heavy-tailed probability distributions (lognormal and stable distributions) were fit to individual students' data. Both density functions fit reasonably well, and estimated parameters were found to be robust across prompts designed to assess student proficiency for the same writing purpose. In addition, estimated parameters for both density functions were statistically significantly associated with human essay scores after accounting for total time spent writing the essay, a result consistent with cognitive theory on the role of low-level processes in writing. Copyright © 2018 by the National Council on Measurement in Education","['Basic', 'Writing', 'Processes', 'Keystroke', 'Logs']","['basic', 'writing', 'processes', 'keystroke', 'logs']",basic writing processes keystroke logs
Lee S.; Bolt D.M.,An Alternative to the 3PL: Using Asymmetric Item Characteristic Curves to Address Guessing Effects,2018,55,"Both the statistical and interpretational shortcomings of the three-parameter logistic (3PL) model in accommodating guessing effects on multiple-choice items are well documented. We consider the use of a residual heteroscedasticity (RH) model as an alternative, and compare its performance to the 3PL with real test data sets and through simulation analyses. Our results suggest advantages to the RH approach, including closer fit to real data, more interpretable parameter estimates, and greater psychological plausibility. Copyright © 2018 by the National Council on Measurement in Education","['alternative', '3PL', 'Asymmetric', 'Item', 'Characteristic', 'curve', 'Address', 'Guessing', 'effect']","['alternative', '3pl', 'asymmetric', 'item', 'characteristic', 'curve', 'address', 'guessing', 'effect']",alternative 3pl asymmetric item characteristic curve address guessing effect
Liu B.; Kennedy P.C.; Seipel B.; Carlson S.E.; Biancarosa G.; Davison M.L.,"Can We Learn From Student Mistakes in a Formative, Reading Comprehension Assessment?",2019,56,"This article describes an ongoing project to develop a formative, inferential reading comprehension assessment of causal story comprehension. It has three features to enhance classroom use: equated scale scores for progress monitoring within and across grades, a scale score to distinguish among low-scoring students based on patterns of mistakes, and a reading efficiency index. Instead of two response types for each multiple-choice item, correct and incorrect, each item has three response types: correct and two incorrect response types. Prior results on reliability, convergent and discriminant validity, and predictive utility of mistake subscores are briefly described. The three-response-type structure of items required rethinking the item response theory (IRT) modeling. IRT-modeling results are presented, and implications for formative assessments and instructional use are discussed. © 2019 by the National Council on Measurement in Education","['learn', 'student', 'mistake', 'formative', 'Reading', 'Comprehension', 'Assessment']","['learn', 'student', 'mistake', 'formative', 'reading', 'comprehension', 'assessment']",learn student mistake formative reading comprehension assessment
Wang W.; Song L.; Chen P.; Ding S.,An Item-Level Expected Classification Accuracy and Its Applications in Cognitive Diagnostic Assessment,2019,56,"Most of the existing classification accuracy indices of attribute patterns lose effectiveness when the response data is absent in diagnostic testing. To handle this issue, this article proposes new indices to predict the correct classification rate of a diagnostic test before administering the test under the deterministic noise input “and” gate (DINA) model. The new indices include an item-level expected classification accuracy (ECA) for attributes and a test-level ECA for attributes and attribute patterns, and both of them are calculated based solely on the known item parameters and Q-matrix. Theoretical analysis showed that the item-level ECA could be regarded as a measure of correct classification rates of attributes contributed by an item. This article also illustrates how to apply the item-level ECA for attributes to estimate the correct classification rate of attributes patterns at the test level. Simulation results showed that two test-level ECA indices, ECA_I_W (an index based on the independence assumption and the weighted sum of the item-level ECAs) and ECA_C_M (an index based on Gaussian Copula function that incorporates the dependence structure of the events of attribute classification and the simple average of the item-level ECAs), could make an accurate prediction for correct classification rates of attribute patterns. © 2019 by the National Council on Measurement in Education","['ItemLevel', 'Expected', 'Classification', 'Accuracy', 'Applications', 'Cognitive', 'Diagnostic', 'Assessment']","['itemlevel', 'expected', 'classification', 'accuracy', 'applications', 'cognitive', 'diagnostic', 'assessment']",itemlevel expected classification accuracy applications cognitive diagnostic assessment
Shear B.R.,Using Hierarchical Logistic Regression to Study DIF and DIF Variance in Multilevel Data,2018,55,"When contextual features of test-taking environments differentially affect item responding for different test takers and these features vary across test administrations, they may cause differential item functioning (DIF) that varies across test administrations. Because many common DIF detection methods ignore potential DIF variance, this article proposes the use of random coefficient hierarchical logistic regression (RC-HLR) models to test for both uniform DIF and DIF variance simultaneously. A simulation study and real data analysis are used to demonstrate and evaluate the proposed RC-HLR model. Results show the RC-HLR model can detect uniform DIF and DIF variance more accurately than standard logistic regression DIF models in terms of bias and Type I error rates. © 2018 by the National Council on Measurement in Education","['hierarchical', 'Logistic', 'regression', 'study', 'DIF', 'DIF', 'Variance', 'Multilevel', 'Data']","['hierarchical', 'logistic', 'regression', 'study', 'dif', 'dif', 'variance', 'multilevel', 'data']",hierarchical logistic regression study dif dif variance multilevel data
Zhang Z.; Zhao M.,"Standard Errors of IRT Parameter Scale Transformation Coefficients: Comparison of Bootstrap Method, Delta Method, and Multiple Imputation Method",2019,56,"The present study evaluated the multiple imputation method, a procedure that is similar to the one suggested by Li and Lissitz (2004), and compared the performance of this method with that of the bootstrap method and the delta method in obtaining the standard errors for the estimates of the parameter scale transformation coefficients in item response theory (IRT) equating in the context of the common-item nonequivalent groups design. Two different estimation procedures for the variance-covariance matrix of the IRT item parameter estimates, which were used in both the delta method and the multiple imputation method, were considered: empirical cross-product (XPD) and supplemented expectation maximization (SEM). The results of the analyses with simulated and real data indicate that the multiple imputation method generally produced very similar results to the bootstrap method and the delta method in most of the conditions. The differences between the estimated standard errors obtained by the methods using the XPD matrices and the SEM matrices were very small when the sample size was reasonably large. When the sample size was small, the methods using the XPD matrices appeared to yield slight upward bias for the standard errors of the IRT parameter scale transformation coefficients. © 2019 by the National Council on Measurement in Education","['Standard', 'error', 'IRT', 'Parameter', 'Scale', 'Transformation', 'Coefficients', 'Comparison', 'Bootstrap', 'Method', 'Delta', 'Method', 'Multiple', 'Imputation', 'Method']","['standard', 'error', 'irt', 'parameter', 'scale', 'transformation', 'coefficients', 'comparison', 'bootstrap', 'method', 'delta', 'method', 'multiple', 'imputation', 'method']",standard error irt parameter scale transformation coefficients comparison bootstrap method delta method multiple imputation method
Hopster-den Otter D.; Wools S.; Eggen T.J.H.M.; Veldkamp B.P.,A General Framework for the Validation of Embedded Formative Assessment,2019,56,"In educational practice, test results are used for several purposes. However, validity research is especially focused on the validity of summative assessment. This article aimed to provide a general framework for validating formative assessment. The authors applied the argument-based approach to validation to the context of formative assessment. This resulted in a proposed interpretation and use argument consisting of a score interpretation and a score use. The former involves inferences linking specific task performance to an interpretation of a student's general performance. The latter involves inferences regarding decisions about actions and educational consequences. The validity argument should focus on critical claims regarding score interpretation and score use, since both are critical to the effectiveness of formative assessment. The proposed framework is illustrated by an operational example including a presentation of evidence that can be collected on the basis of the framework. © 2019 by the National Council on Measurement in Education","['General', 'Framework', 'Validation', 'Embedded', 'Formative', 'Assessment']","['general', 'framework', 'validation', 'embedded', 'formative', 'assessment']",general framework validation embedded formative assessment
Ip E.H.; Strachan T.; Fu Y.; Lay A.; Willse J.T.; Chen S.-H.; Rutkowski L.; Ackerman T.,Bias and Bias Correction Method for Nonproportional Abilities Requirement (NPAR) Tests,2019,56,"Test items must often be broad in scope to be ecologically valid. It is therefore almost inevitable that secondary dimensions are introduced into a test during test development. A cognitive test may require one or more abilities besides the primary ability to correctly respond to an item, in which case a unidimensional test score overestimates the primary ability and creates interpretability problems. In this article, we demonstrate the nonproportional abilities requirement, a phenomenon with which secondary abilities are more required for difficult items. A novel and practical method for correcting bias in the primary ability is proposed and illustrated using a real data set from an international assessment. Simulation data are also used to evaluate the performance of the method. © 2019 by the National Council on Measurement in Education","['Bias', 'Bias', 'Correction', 'Method', 'Nonproportional', 'Abilities', 'Requirement', 'NPAR', 'test']","['bias', 'bias', 'correction', 'method', 'nonproportional', 'abilities', 'requirement', 'npar', 'test']",bias bias correction method nonproportional abilities requirement npar test
Cui Z.; Liu C.; He Y.; Chen H.,Evaluation of a New Method for Providing Full Review Opportunities in Computerized Adaptive Testing—Computerized Adaptive Testing With Salt,2018,55,"Allowing item review in computerized adaptive testing (CAT) is getting more attention in the educational measurement field as more and more testing programs adopt CAT. The research literature has shown that allowing item review in an educational test could result in more accurate estimates of examinees’ abilities. The practice of item review in CAT, however, is hindered by the potential danger of test-manipulation strategies. To provide review opportunities to examinees while minimizing the effect of test-manipulation strategies, researchers have proposed different algorithms to implement CAT with restricted revision options. In this article, we propose and evaluate a new method that implements CAT without any restriction on item review. In particular, we evaluate the new method in terms of the accuracy on ability estimates and the robustness against test-manipulation strategies. This study shows that the newly proposed method is promising in a win-win situation: examinees have full freedom to review and change answers, and the impacts of test-manipulation strategies are undermined. © 2018 by the National Council on Measurement in Education","['evaluation', 'New', 'Method', 'provide', 'Full', 'Review', 'Opportunities', 'Computerized', 'Adaptive', 'Testing', '—', 'Computerized', 'Adaptive', 'Testing', 'salt']","['evaluation', 'new', 'method', 'provide', 'full', 'review', 'opportunities', 'computerized', 'adaptive', 'testing', '—', 'computerized', 'adaptive', 'testing', 'salt']",evaluation new method provide full review opportunities computerized adaptive testing — computerized adaptive testing salt
Wyse A.E.; Babcock B.,A Method for Detecting Regression of Hard and Easy Item Angoff Ratings,2019,56,"One common phenomenon in Angoff standard setting is that panelists regress their ratings in toward the middle of the probability scale. This study describes two indices based on taking ratios of standard deviations that can be utilized with a scatterplot of item ratings versus expected probabilities of success to identify whether ratings are regressed in toward the middle of the probability scale. Results from a simulation study show that the standard deviation ratio indices can successfully detect ratings for hard and easy items that are regressed in toward the middle of the probability scale in Angoff standard-setting data, where previously proposed indices often do not work as well to detect these effects. Results from a real data set show that, while virtually all raters improve from Round 1 to Round 2 as measured by previously developed indices, the standard deviation ratios in conjunction with a scatterplot of item ratings versus expected probabilities of success can identify individuals who may still be regressing their ratings in toward the middle of the probability scale even after receiving feedback. The authors suggest using the scatterplot along with the standard deviation ratio indices and other statistics for measuring the quality of Angoff standard-setting data. © 2019 by the National Council on Measurement in Education","['Method', 'detect', 'Regression', 'Hard', 'Easy', 'Item', 'Angoff', 'Ratings']","['method', 'detect', 'regression', 'hard', 'easy', 'item', 'angoff', 'ratings']",method detect regression hard easy item angoff ratings
Fay D.M.; Levy R.; Mehta V.,Investigating Psychometric Isomorphism for Traditional and Performance-Based Assessment,2018,55,"A common practice in educational assessment is to construct multiple forms of an assessment that consists of tasks with similar psychometric properties. This study utilizes a Bayesian multilevel item response model and descriptive graphical representations to evaluate the psychometric similarity of variations of the same task. These approaches for describing the psychometric similarity of task variants were applied to two different types of assessments (one traditional assessment and one performance-based assessment) with markedly different response formats. Due to the general nature of the multilevel item response model and graphical approaches that were utilized, the methods used for this work can readily be applied to many assessment contexts for the purposes of evaluating the psychometric similarity of tasks. Copyright © 2018 by the National Council on Measurement in Education","['investigate', 'Psychometric', 'Isomorphism', 'Traditional', 'PerformanceBased', 'Assessment']","['investigate', 'psychometric', 'isomorphism', 'traditional', 'performancebased', 'assessment']",investigate psychometric isomorphism traditional performancebased assessment
Lin C.-K.; Zhang J.,Detecting Nonadditivity in Single-Facet Generalizability Theory Applications: Tukey's Test,2018,55,"Under the generalizability-theory (G-theory) framework, the estimation precision of variance components (VCs) is of significant importance in that they serve as the foundation of estimating reliability. Zhang and Lin advanced the discussion of nonadditivity in data from a theoretical perspective and showed the adverse effects of nonadditivity on the estimation precision of VCs in 2016. Contributing to this line of research, the current article directs the discussion of nonadditivity from a theoretical perspective to a practical application and highlights the importance of detecting nonadditivity in G-theory applications. To this end, Tukey's test for nonadditivity is the only method to date that is appropriate for the typical single-facet G-theory design, in which a single observation is made per element within a facet. The current article evaluates the Type I and Type II error rates of Tukey's test. Results show that Tukey's test is satisfactory in controlling for falsely detecting nonadditivity when the data are actually additive and that it is generally powerful in detecting nonadditivity when it exists. Finally, the article demonstrates an application of Tukey's test in detecting nonadditivity in a judgmental study of educational standards and shows how Tukey's test results can be used to correct imprecision in the estimated VC in the presence of nonadditivity. Copyright © 2018 by the National Council on Measurement in Education","['detect', 'Nonadditivity', 'SingleFacet', 'Generalizability', 'Theory', 'Applications', 'Tukeys', 'Test']","['detect', 'nonadditivity', 'singlefacet', 'generalizability', 'theory', 'applications', 'tukeys', 'test']",detect nonadditivity singlefacet generalizability theory applications tukeys test
Barrett M.D.; van der Linden W.J.,Optimal Linking Design for Response Model Parameters,2017,54,"Linking functions adjust for differences between identifiability restrictions used in different instances of the estimation of item response model parameters. These adjustments are necessary when results from those instances are to be compared. As linking functions are derived from estimated item response model parameters, parameter estimation error automatically propagates into linking error. This article explores an optimal linking design approach in which mixed-integer programming is used to select linking items to minimize linking error. Results indicate that the method holds promise for selection of linking items. Copyright © 2017 by the National Council on Measurement in Education","['optimal', 'Linking', 'Design', 'Response', 'Parameters']","['optimal', 'linking', 'design', 'response', 'parameters']",optimal linking design response parameters
Harik P.; Clauser B.E.; Grabovsky I.; Baldwin P.; Margolis M.J.; Bucak D.; Jodoin M.; Walsh W.; Haist S.,A Comparison of Experimental and Observational Approaches to Assessing the Effects of Time Constraints in a Medical Licensing Examination,2018,55,"Test administrators are appropriately concerned about the potential for time constraints to impact the validity of score interpretations; psychometric efforts to evaluate the impact of speededness date back more than half a century. The widespread move to computerized test delivery has led to the development of new approaches to evaluating how examinees use testing time and to new metrics designed to provide evidence about the extent to which time limits impact performance. Much of the existing research is based on these types of observational metrics; relatively few studies use randomized experiments to evaluate the impact time limits on scores. Of those studies that do report on randomized experiments, none directly compare the experimental results to evidence from observational metrics to evaluate the extent to which these metrics are able to sensitively identify conditions in which time constraints actually impact scores. The present study provides such evidence based on data from a medical licensing examination. The results indicate that these observational metrics are useful but provide an imprecise evaluation of the impact of time constraints on test performance. Copyright © 2018 by the National Council on Measurement in Education","['Comparison', 'Experimental', 'Observational', 'Approaches', 'assess', 'effect', 'Time', 'Constraints', 'Medical', 'Licensing', 'Examination']","['comparison', 'experimental', 'observational', 'approaches', 'assess', 'effect', 'time', 'constraints', 'medical', 'licensing', 'examination']",comparison experimental observational approaches assess effect time constraints medical licensing examination
Drabinová A.; Martinková P.,Detection of Differential Item Functioning with Nonlinear Regression: A Non-IRT Approach Accounting for Guessing,2017,54,"In this article we present a general approach not relying on item response theory models (non-IRT) to detect differential item functioning (DIF) in dichotomous items with presence of guessing. The proposed nonlinear regression (NLR) procedure for DIF detection is an extension of method based on logistic regression. As a non-IRT approach, NLR can be seen as a proxy of detection based on the three-parameter IRT model which is a standard tool in the study field. Hence, NLR fills a logical gap in DIF detection methodology and as such is important for educational purposes. Moreover, the advantages of the NLR procedure as well as comparison to other commonly used methods are demonstrated in a simulation study. A real data analysis is offered to demonstrate practical use of the method. Copyright © 2017 by the National Council on Measurement in Education","['detection', 'Differential', 'Item', 'Functioning', 'Nonlinear', 'Regression', 'NonIRT', 'Approach', 'Accounting', 'Guessing']","['detection', 'differential', 'item', 'functioning', 'nonlinear', 'regression', 'nonirt', 'approach', 'accounting', 'guessing']",detection differential item functioning nonlinear regression nonirt approach accounting guessing
Bergner Y.; Choi I.; Castellano K.E.,Item Response Models for Multiple Attempts With Incomplete Data,2019,56,"Allowance for multiple chances to answer constructed response questions is a prevalent feature in computer-based homework and exams. We consider the use of item response theory in the estimation of item characteristics and student ability when multiple attempts are allowed but no explicit penalty is deducted for extra tries. This is common practice in online formative assessments, where the number of attempts is often unlimited. In these environments, some students may not always answer-until-correct, but may rather terminate a response process after one or more incorrect tries. We contrast the cases of graded and sequential item response models, both unidimensional models which do not explicitly account for factors other than ability. These approaches differ not only in terms of log-odds assumptions but, importantly, in terms of handling incomplete data. We explore the consequences of model misspecification through a simulation study and with four online homework data sets. Our results suggest that model selection is insensitive for complete data, but quite sensitive to whether missing responses are regarded as informative (of inability) or not (e.g., missing at random). Under realistic conditions, a sequential model with similar parametric degrees of freedom to a graded model can account for more response patterns and outperforms the latter in terms of model fit. © 2019 by the National Council on Measurement in Education","['Item', 'Response', 'Models', 'Multiple', 'Attempts', 'Incomplete', 'Data']","['item', 'response', 'models', 'multiple', 'attempts', 'incomplete', 'data']",item response models multiple attempts incomplete data
Bolt D.M.; Kim J.-S.,Parameter Invariance and Skill Attribute Continuity in the DINA Model,2018,55,"Cognitive diagnosis models (CDMs) typically assume skill attributes with discrete (often binary) levels of skill mastery, making the existence of skill continuity an anticipated form of model misspecification. In this article, misspecification due to skill continuity is argued to be of particular concern for several CDM applications due to the lack of invariance it yields in CDM skill attribute metrics, or what in this article are viewed as the “thresholds” applied to continuous attributes in distinguishing masters from nonmasters. Using the deterministic input noisy and (DINA) model as an illustration, the effects observed in real data are found to be systematic, with higher thresholds for mastery tending to emerge in higher ability populations. The results are shown to have significant implications for applications of CDMs that rely heavily upon the parameter invariance properties of the models, including, for example, applications toward the measurement of growth and differential item functioning analyses. Copyright © 2018 by the National Council on Measurement in Education","['Parameter', 'Invariance', 'Skill', 'Attribute', 'Continuity', 'DINA']","['parameter', 'invariance', 'skill', 'attribute', 'continuity', 'dina']",parameter invariance skill attribute continuity dina
Huang H.-Y.,Multilevel Cognitive Diagnosis Models for Assessing Changes in Latent Attributes,2017,54,"Cognitive diagnosis models (CDMs) have been developed to evaluate the mastery status of individuals with respect to a set of defined attributes or skills that are measured through testing. When individuals are repeatedly administered a cognitive diagnosis test, a new class of multilevel CDMs is required to assess the changes in their attributes and simultaneously estimate the model parameters from the different measurements. In this study, the most general CDM of the generalized deterministic input, noisy “and” gate (G-DINA) model was extended to a multilevel higher order CDM by embedding a multilevel structure into higher order latent traits. A series of simulations based on diverse factors was conducted to assess the quality of the parameter estimation. The results demonstrate that the model parameters can be recovered fairly well and attribute mastery can be precisely estimated if the sample size is large and the test is sufficiently long. The range of the location parameters had opposing effects on the recovery of the item and person parameters. Ignoring the multilevel structure in the data by fitting a single-level G-DINA model decreased the attribute classification accuracy and the precision of latent trait estimation. The number of measurement occasions had a substantial impact on latent trait estimation. Satisfactory model and person parameter recoveries could be achieved even when assumptions of the measurement invariance of the model parameters over time were violated. A longitudinal basic ability assessment is outlined to demonstrate the application of the new models. Copyright © 2017 by the National Council on Measurement in Education","['Multilevel', 'Cognitive', 'Diagnosis', 'Models', 'assess', 'Changes', 'Latent', 'Attributes']","['multilevel', 'cognitive', 'diagnosis', 'models', 'assess', 'changes', 'latent', 'attributes']",multilevel cognitive diagnosis models assess changes latent attributes
Liu C.-W.; Wang W.-C.,Parameter Estimation in Rasch Models for Examinee-Selected Items,2017,54,"The examinee-selected-item (ESI) design, in which examinees are required to respond to a fixed number of items in a given set of items (e.g., choose one item to respond from a pair of items), always yields incomplete data (i.e., only the selected items are answered and the others have missing data) that are likely nonignorable. Therefore, using standard item response theory models, which assume ignorable missing data, can yield biased parameter estimates so that examinees taking different sets of items to answer cannot be compared. To solve this fundamental problem, in this study the researchers utilized the specific objectivity of Rasch models by adopting the conditional maximum likelihood estimation (CMLE) and pairwise estimation (PE) methods to analyze ESI data, and conducted a series of simulations to demonstrate the advantages of the CMLE and PE methods over traditional estimation methods in recovering item parameters in ESI data. An empirical data set obtained from an experiment on the ESI design was analyzed to illustrate the implications and applications of the proposed approach to ESI data. Copyright © 2017 by the National Council on Measurement in Education","['Parameter', 'Estimation', 'Rasch', 'Models', 'ExamineeSelected', 'Items']","['parameter', 'estimation', 'rasch', 'models', 'examineeselected', 'items']",parameter estimation rasch models examineeselected items
Sinharay S.,Assessment of Person Fit Using Resampling-Based Approaches,2016,53,"De la Torre and Deng suggested a resampling-based approach for person-fit assessment (PFA). The approach involves the use of the statistic, a corrected expected a posteriori estimate of the examinee ability, and the Monte Carlo (MC) resampling method. The Type I error rate of the approach was closer to the nominal level than that of the traditional approach of using along with the assumption of a standard normal null distribution. This article suggests a generalized resampling-based approach for PFA that allows one to employ or another person-fit statistic (PFS) based on item response theory, the corrected expected a posteriori estimate or another ability estimate, and the MC method or another resampling method. The suggested approach includes the approach of de la Torre and Deng as a special case. Several approaches belonging to the generalized approach perform very similarly to the approach of de la Torre and Deng's in two simulation studies and in applications to three real data sets, irrespective of the PFS used. The generalized approach promises to be useful to those interested in resampling-based PFA. © 2016 by the National Council on Measurement in Education.","['assessment', 'Person', 'Fit', 'ResamplingBased', 'Approaches']","['assessment', 'person', 'fit', 'resamplingbased', 'approaches']",assessment person fit resamplingbased approaches
Shang Y.,Measurement Error Adjustment Using the SIMEX Method: An Application to Student Growth Percentiles,2012,49,"Growth models are used extensively in the context of educational accountability to evaluate student-, class-, and school-level growth. However, when error-prone test scores are used as independent variables or right-hand-side controls, the estimation of such growth models can be substantially biased. This article introduces a simulation-extrapolation (SIMEX) method that corrects measurement error induced bias. The SIMEX method is applied to quantile regression, which is the basis of Student Growth Percentile, a descriptive growth model adopted in a number of states to diagnose and project student growth. A simulation study is conducted to demonstrate the performance of the SIMEX method in reducing bias and mean squared error in quantile regression with a mismeasured predictor. One of the simulation cases is based on longitudinal state assessment data. The analysis shows that measurement error differentially biases growth percentile results for students at different achievement levels and that the SIMEX method corrects such biases and closely reproduces conditional distributions of current test scores given past true scores. The potential applications and limitations of the method are discussed at the end of this paper with suggestions for further studies. © 2012 by the National Council on Measurement in Education.","['Error', 'Adjustment', 'simex', 'Method', 'application', 'Student', 'Growth', 'Percentiles']","['error', 'adjustment', 'simex', 'method', 'application', 'student', 'growth', 'percentiles']",error adjustment simex method application student growth percentiles
Moses T.,Adjoined piecewise linear approximations (APLAs) for equating: Accuracy evaluations of a postsmoothing equating method,2013,50,"The purpose of this study was to evaluate the use of adjoined and piecewise linear approximations (APLAs) of raw equipercentile equating functions as a postsmoothing equating method. APLAs are less familiar than other postsmoothing equating methods (i.e., cubic splines), but their use has been described in historical equating practices of large-scale testing programs. This study used simulations to evaluate APLA equating results and compare these results with those from cubic spline postsmoothing and from several presmoothing equating methods. The overall results suggested that APLAs based on four line segments have accuracy advantages similar to or better than cubic splines and can sometimes produce more accurate smoothed equating functions than those produced using presmoothing methods. © 2013 by the National Council on Measurement in Education.","['adjoined', 'piecewise', 'linear', 'approximation', 'apla', 'equate', 'Accuracy', 'evaluation', 'postsmoothe', 'equating', 'method']","['adjoined', 'piecewise', 'linear', 'approximation', 'apla', 'equate', 'accuracy', 'evaluation', 'postsmoothe', 'equating', 'method']",adjoined piecewise linear approximation apla equate accuracy evaluation postsmoothe equating method
Sinharay S.; Wan P.; Choi S.W.; Kim D.-I.,Assessing individual-level impact of interruptions during online testing,2015,52,"With an increase in the number of online tests, the number of interruptions during testing due to unexpected technical issues seems to be on the rise. For example, interruptions occurred during several recent state tests. When interruptions occur, it is important to determine the extent of their impact on the examinees' scores. Researchers such as Hill and Sinharay et al. examined the impact of interruptions at an aggregate level. However, there is a lack of research on the assessment of impact of interruptions at an individual level. We attempt to fill that void. We suggest four methodological approaches, primarily based on statistical hypothesis testing, linear regression, and item response theory, which can provide evidence on the individual-level impact of interruptions. We perform a realistic simulation study to compare the Type I error rate and power of the suggested approaches. We then apply the approaches to data from the 2013 Indiana Statewide Testing for Educational Progress-Plus (ISTEP+) test that experienced interruptions. © 2015 by the National Council on Measurement in Education.","['assess', 'individuallevel', 'impact', 'interruption', 'online', 'testing']","['assess', 'individuallevel', 'impact', 'interruption', 'online', 'testing']",assess individuallevel impact interruption online testing
Yao L.,Multidimensional CAT item selection methods for domain scores and composite scores with item exposure control and content constraints,2014,51,"The intent of this research was to find an item selection procedure in the multidimensional computer adaptive testing (CAT) framework that yielded higher precision for both the domain and composite abilities, had a higher usage of the item pool, and controlled the exposure rate. Five multidimensional CAT item selection procedures (minimum angle volume minimum error variance of the linear combination; minimum error variance of the composite score with optimized weight; and Kullback-Leibler information) were studied and compared with two methods for item exposure control (the Sympson-Hetter procedure and the fixed-rate procedure, the latter simply refers to putting a limit on the item exposure rate) using simulated data. The maximum priority index method was used for the content constraints. Results showed that the Sympson-Hetter procedure yielded better precision than the fixed-rate procedure but had much lower item pool usage and took more time. The five item selection procedures performed similarly under Sympson-Hetter. For the fixed-rate procedure, there was a trade-off between the precision of the ability estimates and the item pool usage: the five procedures had different patterns. It was found that (1) Kullback-Leibler had better precision but lower item pool usage (2) minimum angle and volume had balanced precision and item pool usage and (3) the two methods minimizing the error variance had the best item pool usage and comparable overall score recovery but less precision for certain domains. The priority index for content constraints and item exposure was implemented successfully. © 2014 by the National Council on Measurement in Education.","['Multidimensional', 'CAT', 'item', 'selection', 'method', 'domain', 'score', 'composite', 'score', 'item', 'exposure', 'control', 'content', 'constraint']","['multidimensional', 'cat', 'item', 'selection', 'method', 'domain', 'score', 'composite', 'score', 'item', 'exposure', 'control', 'content', 'constraint']",multidimensional cat item selection method domain score composite score item exposure control content constraint
Shermis M.D.; Lottridge S.; Mayfield E.,The Impact of Anonymization for Automated Essay Scoring,2015,52,"This study investigated the impact of anonymizing text on predicted scores made by two kinds of automated scoring engines: one that incorporates elements of natural language processing (NLP) and one that does not. Eight data sets (N = 22,029) were used to form both training and test sets in which the scoring engines had access to both text and human rater scores for training, but only the text for the test set. Machine ratings were applied under three conditions: (a) both the training and test were conducted with the original data, (b) the training was modeled on the anonymized data, but the predictions were made on the original data, and (c) both the training and test were conducted on the anonymized text. The first condition served as the baseline for subsequent comparisons on the mean, standard deviation, and quadratic weighted kappa. With one exception, results on scoring scales in the range of 1-6 were not significantly different. The results on scales that were much wider did show significant differences. The conclusion was that anonymizing text for operational use may have a differential impact on machine score predictions for both NLP and non-NLP applications. © 2015 by the National Council on Measurement in Education.","['Impact', 'Anonymization', 'Automated', 'Essay', 'Scoring']","['impact', 'anonymization', 'automated', 'essay', 'scoring']",impact anonymization automated essay scoring
González B. J.; von Davier M.,Statistical models and inference for the true equating transformation in the context of local equating,2013,50,"Based on Lord's criterion of equity of equating, van der Linden (this issue) revisits the so-called local equating method and offers alternative as well as new thoughts on several topics including the types of transformations, symmetry, reliability, and population invariance appropriate for equating. A remarkable aspect is to define equating as a standard statistical inference problem in which the true equating transformation is the parameter of interest that has to be estimated and assessed as any standard evaluation of an estimator of an unknown parameter in statistics. We believe that putting equating methods in a general statistical model framework would be an interesting and useful next step in the area. van der Linden's conceptual article on equating is certainly an important contribution to this task. © 2013 by the National Council on Measurement in Education.","['statistical', 'inference', 'true', 'equating', 'transformation', 'context', 'local', 'equating']","['statistical', 'inference', 'true', 'equating', 'transformation', 'context', 'local', 'equating']",statistical inference true equating transformation context local equating
Wang W.-C.; Su C.-M.; Qiu X.-L.,Item response models for local dependence among multiple ratings,2014,51,"Ratings given to the same item response may have a stronger correlation than those given to different item responses, especially when raters interact with one another before giving ratings. The rater bundle model was developed to account for such local dependence by forming multiple ratings given to an item response as a bundle and assigning fixed-effect parameters to describe response patterns in the bundle. Unfortunately, this model becomes difficult to manage when a polytomous item is graded by more than two raters. In this study, by adding random-effect parameters to the facets model, we propose a class of generalized rater models to account for the local dependence among multiple ratings and intrarater variation in severity. A series of simulations was conducted with the freeware WinBUGS to evaluate parameter recovery of the new models and consequences of ignoring the local dependence or intrarater variation in severity. The results revealed a good parameter recovery when the data-generating models were fit, and a poor estimation of parameters and test reliability when the local dependence or intrarater variation in severity was ignored. An empirical example is provided. © 2014 by the National Council on Measurement in Education.","['item', 'response', 'local', 'dependence', 'multiple', 'rating']","['item', 'response', 'local', 'dependence', 'multiple', 'rating']",item response local dependence multiple rating
Guo H.; Puhan G.,Section preequating under the equivalent groups design without IRT,2014,51,"In this article, we introduce a section preequating (SPE) method (linear and nonlinear) under the randomly equivalent groups design. In this equating design, sections of Test X (a future new form) and another existing Test Y (an old form already on scale) are administered. The sections of Test X are equated to Test Y, after adjusting for the imperfect correlation between sections of Test X, to obtain the equated score on the complete form of X. Simulations and a real-data application show that the proposed SPE method is fairly simple and accurate. © 2014 by the National Council on Measurement in Education.","['section', 'preequate', 'equivalent', 'group', 'design', 'IRT']","['section', 'preequate', 'equivalent', 'group', 'design', 'irt']",section preequate equivalent group design irt
Terzi R.; Suh Y.,An Odds Ratio Approach for Detecting DDF Under the Nested Logit Modeling Framework,2015,52,"An odds ratio approach (ORA) under the framework of a nested logit model was proposed for evaluating differential distractor functioning (DDF) in multiple-choice items and was compared with an existing ORA developed under the nominal response model. The performances of the two ORAs for detecting DDF were investigated through an extensive simulation study. The impact of model misfit on the performance of each ORA was also examined. To facilitate practical interpretation of each method, effect size measures were obtained and compared. Finally, data from a college-level mathematics placement test were analyzed using the two approaches. © 2015 by the National Council on Measurement in Education.","['Odds', 'ratio', 'Approach', 'detect', 'DDF', 'Nested', 'Logit', 'modeling', 'Framework']","['odds', 'ratio', 'approach', 'detect', 'ddf', 'nested', 'logit', 'modeling', 'framework']",odds ratio approach detect ddf nested logit modeling framework
Holland P.W.,Comments on van der Linden's critique and proposal for equating,2013,50,"While agreeing with van der Linden (this issue) that test equating needs better theoretical underpinnings, my comments criticize several aspects of his article. His examples are, for the most part, worthless; he does not use well-established terminology correctly; his view of 100 years of attempts to give a theoretical basis for equating is unreasonably dismissive; he exhibits no understanding of the role of the synthetic population for anchor test equating for the nonequivalent groups with anchor test design; he is obtuse regarding the condition of symmetry, requiring it of the estimand but not of the estimator; and his proposal for a foundational basis for all test equating, the ""true equating transformation,"" allows a different equating function for every examinee, which is way past what equating actually does or hopes to achieve. Most importantly, he appears to think that criticism of others is more important than improved insight that moves a field forward based on the work of many other theorists whose contributions have improved the practice of equating. © 2013 by the National Council on Measurement in Education.","['comment', 'van', 'der', 'Lindens', 'critique', 'proposal', 'equate']","['comment', 'van', 'der', 'lindens', 'critique', 'proposal', 'equate']",comment van der lindens critique proposal equate
Milla J.; Martín E.S.; Van Bellegem S.,Higher Education Value Added Using Multiple Outcomes,2016,53,"In this article we develop a methodology for the joint value added analysis of multiple outcomes that takes into account the inherent correlation between them. This is especially crucial in the analysis of higher education institutions. We use a unique Colombian database on universities, which contains scores in five domains tested in a standardized exit examination that is compulsory in order to graduate. We develop a new estimation procedure that accommodates any number of outcomes. Another novelty of our method is related to the structure of the random effect covariance matrix. Effects of the same school can be correlated and this correlation is allowed to vary among schools. Copyright © 2016 by the National Council on Measurement in Education","['high', 'Value', 'add', 'Multiple', 'Outcomes']","['high', 'value', 'add', 'multiple', 'outcomes']",high value add multiple outcomes
Briggs D.C.,Measuring growth with vertical scales,2013,50,"A vertical score scale is needed to measure growth across multiple tests in terms of absolute changes in magnitude. Since the warrant for subsequent growth interpretations depends upon the assumption that the scale has interval properties, the validation of a vertical scale would seem to require methods for distinguishing interval scales from ordinal scales. In taking up this issue, two different perspectives on educational measurement are contrasted: a metaphorical perspective and a classical perspective. Although the metaphorical perspective is more predominant, at present it provides no objective methods whereby the properties of a vertical scale can be validated. In contrast, when taking a classical perspective, the axioms of additive conjoint measurement can be used to test the hypothesis that the latent variable underlying a vertical scale is quantitative (supporting ratio or interval properties) rather than merely qualitative (supporting ordinal or nominal properties). The application of such an approach is illustrated with both a hypothetical example and by drawing upon recent research that has been conducted on the Lexile scale for reading comprehension. © 2013 by the National Council on Measurement in Education.","['measure', 'growth', 'vertical', 'scale']","['measure', 'growth', 'vertical', 'scale']",measure growth vertical scale
Fitzpatrick J.; Skorupski W.P.,Equating With Miditests Using IRT,2016,53,"The equating performance of two internal anchor test structures—miditests and minitests—is studied for four IRT equating methods using simulated data. Originally proposed by Sinharay and Holland, miditests are anchors that have the same mean difficulty as the overall test but less variance in item difficulties. Four popular IRT equating methods were tested, and both the means and SDs of the true ability of the group to be equated were varied. We evaluate equating accuracy marginally and conditional on true ability. Our results suggest miditests perform about as well as traditional minitests for most conditions. Findings are discussed in terms of comparability to the typical minitest design and the trade-off between accuracy and flexibility in test construction. Copyright © 2016 by the National Council on Measurement in Education","['equate', 'miditest', 'IRT']","['equate', 'miditest', 'irt']",equate miditest irt
Liu O.L.; Liu H.; Roohr K.C.; McCaffrey D.F.,Investigating College Learning Gain: Exploring a Propensity Score Weighting Approach,2016,53,"Learning outcomes assessment has been widely used by higher education institutions both nationally and internationally. One of its popular uses is to document learning gains of students. Prior studies have recognized the potential imbalance between freshmen and seniors in terms of their background characteristics and their prior academic performance and have used linear regression adjustments for these differences, which some researchers have argued are not fully adequate. We explored an alternative adjustment via propensity score weighting to balance the samples on background variables including SAT score, gender, and ethnicity. Results involving a cross-sectional sample of freshmen and seniors from seven groups of majors within a large research university showed that students in most of the majors demonstrated significant learning gain. Additionally, there was a slight difference in learning gain rankings across major groupings when compared to multiple regression results. Copyright © 2016 by the National Council on Measurement in Education","['investigate', 'College', 'Learning', 'Gain', 'explore', 'Propensity', 'Score', 'Weighting', 'approach']","['investigate', 'college', 'learning', 'gain', 'explore', 'propensity', 'score', 'weighting', 'approach']",investigate college learning gain explore propensity score weighting approach
Chen J.; de la Torre J.; Zhang Z.,Relative and absolute fit evaluation in cognitive diagnosis modeling,2013,50,"As with any psychometric models, the validity of inferences from cognitive diagnosis models (CDMs) determines the extent to which these models can be useful. For inferences from CDMs to be valid, it is crucial that the fit of the model to the data is ascertained. Based on a simulation study, this study investigated the sensitivity of various fit statistics for absolute or relative fit under different CDM settings. The investigation covered various types of model-data misfit that can occur with the misspecifications of the Q-matrix, the CDM, or both. Six fit statistics were considered: -2 log likelihood (-2LL), Akaike's information criterion (AIC), Bayesian information criterion (BIC), and residuals based on the proportion correct of individual items (p), the correlations (r), and the log-odds ratio of item pairs (l). An empirical example involving real data was used to illustrate how the different fit statistics can be employed in conjunction with each other to identify different types of misspecifications. With these statistics and the saturated model serving as the basis, relative and absolute fit evaluation can be integrated to detect misspecification efficiently. © 2013 by the National Council on Measurement in Education.","['relative', 'absolute', 'fit', 'evaluation', 'cognitive', 'diagnosis', 'modeling']","['relative', 'absolute', 'fit', 'evaluation', 'cognitive', 'diagnosis', 'modeling']",relative absolute fit evaluation cognitive diagnosis modeling
Powers S.; Kolen M.J.,Evaluating equating accuracy and assumptions for groups that differ in performance,2014,51,"Accurate equating results are essential when comparing examinee scores across exam forms. Previous research indicates that equating results may not be accurate when group differences are large. This study compared the equating results of frequency estimation, chained equipercentile, item response theory (IRT) true-score, and IRT observed-score equating methods. Using mixed-format test data, equating results were evaluated for group differences ranging from 0 to .75 standard deviations. As group differences increased, equating results became increasingly biased and dissimilar across equating methods. Results suggest that the size of group differences, the likelihood that equating assumptions are violated, and the equating error associated with an equating method should be taken into consideration when choosing an equating method. © 2014 by the National Council on Measurement in Education.","['evaluate', 'equate', 'accuracy', 'assumption', 'group', 'differ', 'performance']","['evaluate', 'equate', 'accuracy', 'assumption', 'group', 'differ', 'performance']",evaluate equate accuracy assumption group differ performance
Veldkamp B.P.,On the Issue of Item Selection in Computerized Adaptive Testing With Response Times,2016,53,"Many standardized tests are now administered via computer rather than paper-and-pencil format. The computer-based delivery mode brings with it certain advantages. One advantage is the ability to adapt the difficulty level of the test to the ability level of the test taker in what has been termed computerized adaptive testing (CAT). A second advantage is the ability to record not only the test taker's response to each item (i.e., question), but also the amount of time the test taker spends considering and answering each item. Combining these two advantages, various methods were explored for utilizing response time data in selecting appropriate items for an individual test taker. Four strategies for incorporating response time data were evaluated, and the precision of the final test-taker score was assessed by comparing it to a benchmark value that did not take response time information into account. While differences in measurement precision and testing times were expected, results showed that the strategies did not differ much with respect to measurement precision but that there were differences with regard to the total testing time. Copyright © 2016 by the National Council on Measurement in Education","['issue', 'Item', 'Selection', 'Computerized', 'Adaptive', 'Testing', 'Response', 'Times']","['issue', 'item', 'selection', 'computerized', 'adaptive', 'testing', 'response', 'times']",issue item selection computerized adaptive testing response times
Raymond M.R.; Swygert K.A.; Kahraman N.,Psychometric equivalence of ratings for repeat examinees on a performance assessment for physician licensure,2012,49,"Although a few studies report sizable score gains for examinees who repeat performance-based assessments, research has not yet addressed the reliability and validity of inferences based on ratings of repeat examinees on such tests. This study analyzed scores for 8,457 single-take examinees and 4,030 repeat examinees who completed a 6-hour clinical skills assessment required for physician licensure. Each examinee was rated in four skill domains: data gathering, communication-interpersonal skills, spoken English proficiency, and documentation proficiency. Conditional standard errors of measurement computed for single-take and multiple-take examinees indicated that ratings were of comparable precision for the two groups within each of the four skill domains; however, conditional errors were larger for low-scoring examinees regardless of retest status. In addition, on their first attempt multiple-take examinees exhibited less score consistency across the skill domains but on their second attempt their scores became more consistent. Further, the median correlation between scores on the four clinical skill domains and three external measures was .15 for multiple-take examinees on their first attempt but increased to .27 for their second attempt, a value, which was comparable to the median correlation of .26 for single-take examinees. The findings support the validity of inferences based on scores from the second attempt. © 2012 by the National Council on Measurement in Education.","['psychometric', 'equivalence', 'rating', 'repeat', 'examinee', 'performance', 'assessment', 'physician', 'licensure']","['psychometric', 'equivalence', 'rating', 'repeat', 'examinee', 'performance', 'assessment', 'physician', 'licensure']",psychometric equivalence rating repeat examinee performance assessment physician licensure
Hsu C.-L.; Wang W.-C.,Variable-Length Computerized Adaptive Testing Using the Higher Order DINA Model,2015,52,"Cognitive diagnosis models provide profile information about a set of latent binary attributes, whereas item response models yield a summary report on a latent continuous trait. To utilize the advantages of both models, higher order cognitive diagnosis models were developed in which information about both latent binary attributes and latent continuous traits is available. To facilitate the utility of cognitive diagnosis models, corresponding computerized adaptive testing (CAT) algorithms were developed. Most of them adopt the fixed-length rule to terminate CAT and are limited to ordinary cognitive diagnosis models. In this study, the higher order deterministic-input, noisy-and-gate (DINA) model was used as an example, and three criteria based on the minimum-precision termination rule were implemented: one for the latent class, one for the latent trait, and the other for both. The simulation results demonstrated that all of the termination criteria were successful when items were selected according to the Kullback-Leibler information and the posterior-weighted Kullback-Leibler information, and the minimum-precision rule outperformed the fixed-length rule with a similar test length in recovering the latent attributes and the latent trait. © 2015 by the National Council on Measurement in Education.","['VariableLength', 'Computerized', 'Adaptive', 'Testing', 'high', 'Order', 'DINA']","['variablelength', 'computerized', 'adaptive', 'testing', 'high', 'order', 'dina']",variablelength computerized adaptive testing high order dina
Huang H.-Y.; Wang W.-C.,The random-effect DINA model,2014,51,"The DINA (deterministic input, noisy, and gate) model has been widely used in cognitive diagnosis tests and in the process of test development. The outcomes known as slip and guess are included in the DINA model function representing the responses to the items. This study aimed to extend the DINA model by using the random-effect approach to allow examinees to have different probabilities of slipping and guessing. Two extensions of the DINA model were developed and tested to represent the random components of slipping and guessing. The first model assumed that a random variable can be incorporated in the slipping parameters to allow examinees to have different levels of caution. The second model assumed that the examinees' ability may increase the probability of a correct response if they have not mastered all of the required attributes of an item. The results of a series of simulations based on Markov chain Monte Carlo methods showed that the model parameters and attribute-mastery profiles can be recovered relatively accurately from the generating models and that neglect of the random effects produces biases in parameter estimation. Finally, a fraction subtraction test was used as an empirical example to demonstrate the application of the new models. © 2014 by the National Council on Measurement in Education.","['randomeffect', 'DINA']","['randomeffect', 'dina']",randomeffect dina
Oh H.; Moses T.,Comparison of the One- and Bi-Direction Chained Equipercentile Equating,2012,49,"This study investigated differences between two approaches to chained equipercentile (CE) equating (one- and bi-direction CE equating) in nearly equal groups and relatively unequal groups. In one-direction CE equating, the new form is linked to the anchor in one sample of examinees and the anchor is linked to the reference form in the other sample. In bi-direction CE equating, the anchor is linked to the new form in one sample of examinees and to the reference form in the other sample. The two approaches were evaluated in comparison to a criterion equating function (i.e., equivalent groups equating) using indexes such as root expected squared difference, bias, standard error of equating, root mean squared error, and number of gaps and bumps. The overall results across the equating situations suggested that the two CE equating approaches produced very similar results, whereas the bi-direction results were slightly less erratic, smoother (i.e., fewer gaps and bumps), usually closer to the criterion function, and also less variable. © 2012 by the National Council on Measurement in Education.","['Comparison', 'One', 'BiDirection', 'chain', 'Equipercentile', 'Equating']","['comparison', 'one', 'bidirection', 'chain', 'equipercentile', 'equating']",comparison one bidirection chain equipercentile equating
Guo H.; Oh H.J.; Eignor D.,Situations where it is appropriate to use frequency estimation equipercentile equating,2013,50,"In operational equating situations, frequency estimation equipercentile equating is considered only when the old and new groups have similar abilities. The frequency estimation assumptions are investigated in this study under various situations from both the levels of theoretical interest and practical use. It shows that frequency estimation equating can be used under circumstances when it is not normally used. To link theoretical results with practice, statistical methods are proposed for checking frequency estimation assumptions based on available data: observed-score distributions and item difficulty distributions of the forms. In addition to the conventional use of frequency estimation equating when the group abilities are similar, three situations are identified when the group abilities are dissimilar: (a) when the two forms and the observed conditional score distributions are similar the two forms and the observed conditional score distributions are similar (in this situation, the frequency estimation equating assumptions are likely to hold, and frequency estimation equating is appropriate); (b) when forms are similar but the observed conditional score distributions are not (in this situation, frequency estimation equating is not appropriate); and (c) when forms are not similar but the observed conditional score distributions are (frequency estimation equating is not appropriate). Statistical analysis procedures for comparing distributions are provided. Data from a large-scale test are used to illustrate the use of frequency estimation equating when the group difference in ability is large. © 2013 by the National Council on Measurement in Education.","['situation', 'appropriate', 'frequency', 'estimation', 'equipercentile', 'equating']","['situation', 'appropriate', 'frequency', 'estimation', 'equipercentile', 'equating']",situation appropriate frequency estimation equipercentile equating
Samudra P.G.; Min I.; Cortina K.S.; Miller K.F.,No Second Chance to Make a First Impression: The “Thin-Slice” Effect on Instructor Ratings and Learning Outcomes in Higher Education,2016,53,"Prior research has found strong and persistent effects of instructor first impressions on student evaluations. Because these studies look at real classroom lessons, this finding fits two different interpretations: (1) first impressions may color student experience of instruction regardless of lesson quality, or (2) first impressions may provide valid evidence for instructional quality. By using scripted lessons, we experimentally investigated how first impression and instruction quality related to learning and evaluation of instruction among college students. Results from two studies indicate that quality of instruction is the strongest determinant of student factual and conceptual learning, but that both instructional quality and first impressions affect evaluations of the instructor. First impressions matter, but our findings suggest that lesson quality matters more. Copyright © 2016 by the National Council on Measurement in Education","['Second', 'Chance', 'Impression', '""', 'ThinSlice', '""', 'Effect', 'Instructor', 'Ratings', 'Learning', 'Outcomes', 'high']","['second', 'chance', 'impression', '""', 'thinslice', '""', 'effect', 'instructor', 'ratings', 'learning', 'outcomes', 'high']","second chance impression "" thinslice "" effect instructor ratings learning outcomes high"
Wang W.; Song L.; Chen P.; Meng Y.; Ding S.,Attribute-Level and Pattern-Level Classification Consistency and Accuracy Indices for Cognitive Diagnostic Assessment,2015,52,"Classification consistency and accuracy are viewed as important indicators for evaluating the reliability and validity of classification results in cognitive diagnostic assessment (CDA). Pattern-level classification consistency and accuracy indices were introduced by Cui, Gierl, and Chang. However, the indices at the attribute level have not yet been constructed. This study puts forward a simple approach to estimating the indices at both the attribute and the pattern level through one single test administration. Detailed elaboration is made on how the upper and lower bounds for the attribute-level accuracy can be derived from the variance of error of the attribute mastery probability estimate. In addition, based on Cui's pattern-level indices, an alternative approach to estimating the attribute-level indices is also proposed. Comparative analysis of simulation results indicate that the new indices are very desirable for evaluating test-retest consistency and correct classification rate. © 2015 by the National Council on Measurement in Education.","['AttributeLevel', 'PatternLevel', 'Classification', 'Consistency', 'Accuracy', 'Indices', 'Cognitive', 'Diagnostic', 'Assessment']","['attributelevel', 'patternlevel', 'classification', 'consistency', 'accuracy', 'indices', 'cognitive', 'diagnostic', 'assessment']",attributelevel patternlevel classification consistency accuracy indices cognitive diagnostic assessment
Belov D.I.,Robust Detection of Examinees With Aberrant Answer Changes,2015,52,"The statistical analysis of answer changes (ACs) has uncovered multiple testing irregularities on large-scale assessments and is now routinely performed at testing organizations. However, AC data has an uncertainty caused by technological or human factors. Therefore, existing statistics (e.g., number of wrong-to-right ACs) used to detect examinees with aberrant ACs capitalize on the uncertainty, which may result in a large Type I error. In this article, the information about ACs is used only for the partitioning of administered items into two disjoint subtests: items where ACs did not occur, and items where ACs did occur. A new statistic is based on the difference in performance between these subtests (measured as Kullback-Leibler divergence between corresponding posteriors of latent traits), where, in order to avoid the uncertainty, only final responses are used. One of the subtests can be filtered such that the asymptotic distribution of the statistic is chi-square with one degree of freedom. In computer simulations, the presented statistic demonstrated a strong robustness to the uncertainty and higher detection rates in contrast to two popular statistics based on wrong-to-right ACs. © 2015 by the National Council on Measurement in Education.","['robust', 'Detection', 'Examinees', 'Aberrant', 'Answer', 'change']","['robust', 'detection', 'examinees', 'aberrant', 'answer', 'change']",robust detection examinees aberrant answer change
Kim S.,Generalization of the Lord-Wingersky algorithm to computing the distribution of summed test scores based on real-number item scores,2013,50,"With known item response theory (IRT) item parameters, Lord and Wingersky provided a recursive algorithm for computing the conditional frequency distribution of number-correct test scores, given proficiency. This article presents a generalized algorithm for computing the conditional distribution of summed test scores involving real-number item scores. The generalized algorithm is distinct from the Lord-Wingersky algorithm in that it explicitly incorporates the task of figuring out all possible unique real-number test scores in each recursion. Some applications of the generalized recursive algorithm, such as IRT test score reliability estimation and IRT proficiency estimation based on summed test scores, are illustrated with a short test by varying scoring schemes for its items. © 2013 by the National Council on Measurement in Education.","['generalization', 'LordWingersky', 'algorithm', 'compute', 'distribution', 'sum', 'test', 'score', 'base', 'realnumber', 'item', 'score']","['generalization', 'lordwingersky', 'algorithm', 'compute', 'distribution', 'sum', 'test', 'score', 'base', 'realnumber', 'item', 'score']",generalization lordwingersky algorithm compute distribution sum test score base realnumber item score
Wiberg M.; Van Der Linden W.J.; Von Davier A.A.,Local observed-score kernel equating,2014,51,"Three local observed-score kernel equating methods that integrate methods from the local equating and kernel equating frameworks are proposed. The new methods were compared with their earlier counterparts with respect to such measures as bias-as defined by Lord's criterion of equity-and percent relative error. The local kernel item response theory observed-score equating method, which can be used for any of the common equating designs, had a small amount of bias, a low percent relative error, and a relatively low kernel standard error of equating, even when the accuracy of the test was reduced. The local kernel equating methods for the nonequivalent groups with anchor test generally had low bias and were quite stable against changes in the accuracy or length of the anchor test. Although all proposed methods showed small percent relative errors, the local kernel equating methods for the nonequivalent groups with anchor test design had somewhat larger standard error of equating than their kernel method counterparts. © 2014 by the National Council on Measurement in Education.","['local', 'observedscore', 'kernel', 'equating']","['local', 'observedscore', 'kernel', 'equating']",local observedscore kernel equating
Puhan G.,Choosing Among Tucker or Chained Linear Equating in Two Testing Situations: Rater Comparability Scoring and Randomly Equivalent Groups With an Anchor,2012,49,"Tucker and chained linear equatings were evaluated in two testing scenarios. In Scenario 1, referred to as rater comparability scoring and equating, the anchor-to-total correlation is often very high for the new form but moderate for the reference form. This may adversely affect the results of Tucker equating, especially if the new and reference form samples differ in ability. In Scenario 2, the new and reference form samples are randomly equivalent but the correlation between the anchor and total scores is low. When the correlation between the anchor and total scores is low, Tucker equating assumes that the new and reference form samples are similar in ability (which, with randomly equivalents groups, is the correct assumption). Thus Tucker equating should produce accurate results. Results indicated that in Scenario 1, the Tucker results were less accurate than the chained linear equating results. However, in Scenario 2, the Tucker results were more accurate than the chained linear equating results. Some implications are discussed. © 2012 by the National Council on Measurement in Education.","['choose', 'Tucker', 'chain', 'Linear', 'Equating', 'Testing', 'Situations', 'Rater', 'Comparability', 'Scoring', 'Randomly', 'Equivalent', 'group', 'anchor']","['choose', 'tucker', 'chain', 'linear', 'equating', 'testing', 'situations', 'rater', 'comparability', 'scoring', 'randomly', 'equivalent', 'group', 'anchor']",choose tucker chain linear equating testing situations rater comparability scoring randomly equivalent group anchor
French B.F.; Finch W.H.,Transforming SIBTEST to Account for Multilevel Data Structures,2015,52,"SIBTEST is a differential item functioning (DIF) detection method that is accurate and effective with small samples, in the presence of group mean differences, and for assessment of both uniform and nonuniform DIF. The presence of multilevel data with DIF detection has received increased attention. Ignoring such structure can inflate Type I error. This simulation study examines the performance of newly developed multilevel adaptations of SIBTEST in the presence of multilevel data. Data were simulated in a multilevel framework and both uniform and nonuniform DIF were assessed. Study results demonstrated that naïve SIBTEST and Crossing SIBTEST, ignoring the multilevel data structure, yield inflated Type I error rates, while certain multilevel extensions provided better error and accuracy control. © 2015 by the National Council on Measurement in Education.","['transform', 'SIBTEST', 'Account', 'Multilevel', 'Data', 'Structures']","['transform', 'sibtest', 'account', 'multilevel', 'data', 'structures']",transform sibtest account multilevel data structures
Chen H.,A Comparison Between Linear IRT Observed-Score Equating and Levine Observed-Score Equating Under the Generalized Kernel Equating Framework,2012,49,"In this article, linear item response theory (IRT) observed-score equating is compared under a generalized kernel equating framework with Levine observed-score equating for nonequivalent groups with anchor test design. Interestingly, these two equating methods are closely related despite being based on different methodologies. Specifically, when using data from IRT models, linear IRT observed-score equating is virtually identical to Levine observed-score equating. This leads to the conclusion that poststratification equating based on true anchor scores can be viewed as the curvilinear Levine observed-score equating. © 2012 by the National Council on Measurement in Education.","['Comparison', 'Linear', 'IRT', 'ObservedScore', 'Equating', 'Levine', 'ObservedScore', 'Equating', 'Generalized', 'Kernel', 'Equating', 'Framework']","['comparison', 'linear', 'irt', 'observedscore', 'equating', 'levine', 'observedscore', 'equating', 'generalized', 'kernel', 'equating', 'framework']",comparison linear irt observedscore equating levine observedscore equating generalized kernel equating framework
Andersson B.; von Davier A.A.,Improving the bandwidth selection in kernel equating,2014,51,"We investigate the current bandwidth selection methods in kernel equating and propose a method based on Silverman's rule of thumb for selecting the bandwidth parameters. In kernel equating, the bandwidth parameters have previously been obtained by minimizing a penalty function. This minimization process has been criticized by practitioners for being too complex and that it does not offer sufficient smoothing in certain cases. In addition, the bandwidth parameters have been treated as constants in the derivation of the standard error of equating even when they were selected by considering the observed data. Here, the bandwidth selection is simplified, and modified standard errors of equating (SEEs) that reflect the bandwidth selection method are derived. The method is illustrated with real data examples and simulated data. © 2014 by the National Council on Measurement in Education.","['improve', 'bandwidth', 'selection', 'kernel', 'equating']","['improve', 'bandwidth', 'selection', 'kernel', 'equating']",improve bandwidth selection kernel equating
Li F.; Cohen A.; Shen L.,Investigating the Effect of Item Position in Computer-Based Tests,2012,49,"Computer-based tests (CBTs) often use random ordering of items in order to minimize item exposure and reduce the potential for answer copying. Little research has been done, however, to examine item position effects for these tests. In this study, different versions of a Rasch model and different response time models were examined and applied to data from a CBT administration of a medical licensure examination. The models specifically were used to investigate whether item position affected item difficulty and item intensity estimates. Results indicated that the position effect was negligible. © 2012 by the National Council on Measurement in Education.","['investigate', 'Effect', 'Item', 'Position', 'ComputerBased', 'Tests']","['investigate', 'effect', 'item', 'position', 'computerbased', 'tests']",investigate effect item position computerbased tests
Sachse K.A.; Roppelt A.; Haag N.,A Comparison of Linking Methods for Estimating National Trends in International Comparative Large-Scale Assessments in the Presence of Cross-National DIF,2016,53,"Trend estimation in international comparative large-scale assessments relies on measurement invariance between countries. However, cross-national differential item functioning (DIF) has been repeatedly documented. We ran a simulation study using national item parameters, which required trends to be computed separately for each country, to compare trend estimation performances to two linking methods employing international item parameters across several conditions. The trend estimates based on the national item parameters were more accurate than the trend estimates based on the international item parameters when cross-national DIF was present. Moreover, the use of fixed common item parameter calibrations led to biased trend estimates. The detection and elimination of DIF can reduce this bias but is also likely to increase the total error. Copyright © 2016 by the National Council on Measurement in Education","['Comparison', 'Linking', 'Methods', 'estimate', 'National', 'Trends', 'International', 'Comparative', 'LargeScale', 'assessment', 'Presence', 'CrossNational', 'DIF']","['comparison', 'linking', 'methods', 'estimate', 'national', 'trends', 'international', 'comparative', 'largescale', 'assessment', 'presence', 'crossnational', 'dif']",comparison linking methods estimate national trends international comparative largescale assessment presence crossnational dif
Shu L.; Schwarz R.D.,IRT-estimated reliability for tests containing mixed item formats,2014,51,"As a global measure of precision, item response theory (IRT) estimated reliability is derived for four coefficients (Cronbach's α, Feldt-Raju, stratified α, and marginal reliability). Models with different underlying assumptions concerning test-part similarity are discussed. A detailed computational example is presented for the targeted coefficients. A comparison of the IRT model-derived coefficients is made and the impact of varying ability distributions is evaluated. The advantages of IRT-derived reliability coefficients for problems such as automated test form assembly and vertical scaling are discussed. © 2014 by the National Council on Measurement in Education.","['irtestimate', 'reliability', 'test', 'contain', 'mixed', 'item', 'format']","['irtestimate', 'reliability', 'test', 'contain', 'mixed', 'item', 'format']",irtestimate reliability test contain mixed item format
Haberman S.; Yao L.,Repeater Analysis for Combining Information From Different Assessments,2015,52,"Admission decisions frequently rely on multiple assessments. As a consequence, it is important to explore rational approaches to combine the information from different educational tests. For example, U.S. graduate schools usually receive both TOEFL iBT® scores and GRE® General scores of foreign applicants for admission; however, little guidance has been given to combine information from these two assessments, even though the relationships between such sections as GRE Verbal and TOEFL iBT Reading are obvious. In this study, principles are provided to explore the extent to which different assessments complement one another and are distinguishable. Augmentation approaches developed for individual tests are applied to provide an accurate evaluation of combined assessments. Because augmentation methods require estimates of measurement error and internal reliability data are unavailable, required estimates of measurement error are obtained from repeaters, examinees who took the same test more than once. Because repeaters are not representative of all examinees in typical assessments, minimum discriminant information adjustment techniques are applied to the available sample of repeaters to treat the effect of selection bias. To illustrate methodology, combining information from TOEFL iBT scores and GRE General scores is examined. Analysis suggests that information from the GRE General and TOEFL iBT assessments is complementary but not redundant, indicating that the two tests measure related but somewhat different constructs. The proposed methodology can be readily applied to other situations where multiple assessments are needed. © 2015 by the National Council on Measurement in Education.","['Repeater', 'Analysis', 'combine', 'Information', 'different', 'assessment']","['repeater', 'analysis', 'combine', 'information', 'different', 'assessment']",repeater analysis combine information different assessment
Wang C.; Zheng C.; Chang H.-H.,An Enhanced Approach to Combine Item Response Theory With Cognitive Diagnosis in Adaptive Testing,2014,51,"Computerized adaptive testing offers the possibility of gaining information on both the overall ability and cognitive profile in a single assessment administration. Some algorithms aiming for these dual purposes have been proposed, including the shadow test approach, the dual information method (DIM), and the constraint weighted method. The current study proposed two new methods, aggregate ranked information index (ARI) and aggregate standardized information index (ASI), which appropriately addressed the noncompatibility issue inherent in the original DIM method. More flexible weighting schemes that put different emphasis on information about general ability (i.e., θ in item response theory) and information about cognitive profile (i.e., α in cognitive diagnostic modeling) were also explored. Two simulation studies were carried out to investigate the effectiveness of the new methods and weighting schemes. Results showed that the new methods with the flexible weighting schemes could produce more accurate estimation of both overall ability and cognitive profile than the original DIM. Among them, the ASI with both empirical and theoretical weights is recommended, and attribute-level weighting scheme is preferred if some attributes are considered more important from a substantive perspective. © 2014 by the National Council on Measurement in Education.","['enhanced', 'Approach', 'Combine', 'Item', 'Response', 'Theory', 'cognitive', 'Diagnosis', 'Adaptive', 'Testing']","['enhanced', 'approach', 'combine', 'item', 'response', 'theory', 'cognitive', 'diagnosis', 'adaptive', 'testing']",enhanced approach combine item response theory cognitive diagnosis adaptive testing
Debeer D.; Janssen R.,Modeling item-position effects within an IRT framework,2013,50,"Changing the order of items between alternate test forms to prevent copying and to enhance test security is a common practice in achievement testing. However, these changes in item order may affect item and test characteristics. Several procedures have been proposed for studying these item-order effects. The present study explores the use of descriptive and explanatory models from item response theory for detecting and modeling these effects in a one-step procedure. The framework also allows for consideration of the impact of individual differences in position effect on item difficulty. A simulation was conducted to investigate the impact of a position effect on parameter recovery in a Rasch model. As an illustration, the framework was applied to a listening comprehension test for French as a foreign language and to data from the PISA 2006 assessment.Copyright © 2013 by the National Council on Measurement in Education.","['itemposition', 'effect', 'IRT', 'framework']","['itemposition', 'effect', 'irt', 'framework']",itemposition effect irt framework
Kane M.T.,"Validation as a Pragmatic, Scientific Activity",2013,50,"This response to the comments contains three main sections, each addressing a subset of the comments. In the first section, I will respond to the comments by Brennan, Haertel, and Moss. All of these comments suggest ways in which my presentation could be extended or improved; I generally agree with their suggestions, so my response to their comments is brief. In the second section, I will respond to suggestions by Newton and Sireci that my framework be simplified by employing only one kind of argument, a validity argument, and dropping the interpretation/use argument (IUA); I am sympathetic to their desire for greater simplicity, but I see considerable value in keeping the IUA as a framework for the validation effort and will argue for keeping both the IUA and the validity argument. In the third section, I will respond to Borsboom and Markus, who raise a fundamental objection to my approach to validation, suggesting that I give too much attention to justification and too little to truth as a criterion for validity; I don't accept their proposed conception of validity, and I will indicate why. © 2013 by the National Council on Measurement in Education.","['validation', 'pragmatic', 'Scientific', 'Activity']","['validation', 'pragmatic', 'scientific', 'activity']",validation pragmatic scientific activity
Ranger J.; Kuhn J.-T.,Assessing Fit of Item Response Models Using the Information Matrix Test,2012,49,"The information matrix can equivalently be determined via the expectation of the Hessian matrix or the expectation of the outer product of the score vector. The identity of these two matrices, however, is only valid in case of a correctly specified model. Therefore, differences between the two versions of the observed information matrix indicate model misfit. The equality of both matrices can be tested with the so-called information matrix test as a general test of misspecification. This test can be adapted to item response models in order to evaluate the fit of single items and the fit of the whole scale. The performance of different versions of the test is compared in a simulation study with existing tests of model fit, among them thetest of Orlando and Thissen, the score test of local independence due to Glas and Suarez-Falcon, and the limited information approach of Maydeu-Olivares and Joe. In general, the different versions of the information matrix test adhere to the nominal Type I error rate and have high power for detecting misspecified item characteristic curves. Additionally, some versions of the test can be used in order to detect violations of the local independence assumption. © 2012 by the National Council on Measurement in Education.","['assess', 'Fit', 'Item', 'Response', 'Models', 'Information', 'Matrix', 'test']","['assess', 'fit', 'item', 'response', 'models', 'information', 'matrix', 'test']",assess fit item response models information matrix test
Moses T.,Relationships of measurement error and prediction error in observed-score regression,2012,49,The focus of this paper is assessing the impact of measurement errors on the prediction error of an observed-score regression. Measures are presented and described for decomposing the linear regression's prediction error variance into parts attributable to the true score variance and the error variances of the dependent variable and the predictor variable(s). These measures are demonstrated for regression situations reflecting a range of true score correlations and reliabilities and using one and two predictors. Simulation results also are presented which show that the measures of prediction error variance and its parts are generally well estimated for the considered ranges of true score correlations and reliabilities and for homoscedastic and heteroscedastic data. The final discussion considers how the decomposition might be useful for addressing additional questions about regression functions' prediction error variances. © 2012 by the National Council on Measurement in Education.,"['relationship', 'error', 'prediction', 'error', 'observedscore', 'regression']","['relationship', 'error', 'prediction', 'error', 'observedscore', 'regression']",relationship error prediction error observedscore regression
Belov D.I.,Detection of test collusion via Kullback-Leibler divergence,2013,50,"The development of statistical methods for detecting test collusion is a new research direction in the area of test security. Test collusion may be described as large-scale sharing of test materials, including answers to test items. Current methods of detecting test collusion are based on statistics also used in answer-copying detection. Therefore, in computerized adaptive testing (CAT) these methods lose power because the actual test varies across examinees. This article addresses that problem by introducing a new approach that works in two stages: in Stage 1, test centers with an unusual distribution of a person-fit statistic are identified via Kullback-Leibler divergence; in Stage 2, examinees from identified test centers are analyzed further using the person-fit statistic, where the critical value is computed without data from the identified test centers. The approach is extremely flexible. One can employ any existing person-fit statistic. The approach can be applied to all major testing programs: paper-and-pencil testing (P&P), computer-based testing (CBT), multiple-stage testing (MST), and CAT. Also, the definition of test center is not limited by the geographic location (room, class, college) and can be extended to support various relations between examinees (from the same undergraduate college, from the same test-prep center, from the same group at a social network). The suggested approach was found to be effective in CAT for detecting groups of examinees with item pre-knowledge, meaning those with access (possibly unknown to us) to one or more subsets of items prior to the exam. © 2013 by the National Council on Measurement in Education.","['detection', 'test', 'collusion', 'KullbackLeibler', 'divergence']","['detection', 'test', 'collusion', 'kullbackleibler', 'divergence']",detection test collusion kullbackleibler divergence
Li X.; Wang W.-C.,Assessment of differential item functioning under cognitive diagnosis models: The DINA model example,2015,52,"The assessment of differential item functioning (DIF) is routinely conducted to ensure test fairness and validity. Although many DIF assessment methods have been developed in the context of classical test theory and item response theory, they are not applicable for cognitive diagnosis models (CDMs), as the underlying latent attributes of CDMs are multidimensional and binary. This study proposes a very general DIF assessment method in the CDM framework which is applicable for various CDMs, more than two groups of examinees, and multiple grouping variables that are categorical, continuous, observed, or latent. The parameters can be estimated with Markov chain Monte Carlo algorithms implemented in the freeware WinBUGS. Simulation results demonstrated a good parameter recovery and advantages in DIF assessment for the new method over the Wald method. © 2015 by the National Council on Measurement in Education.","['assessment', 'differential', 'item', 'function', 'cognitive', 'diagnosis', 'DINA', 'example']","['assessment', 'differential', 'item', 'function', 'cognitive', 'diagnosis', 'dina', 'example']",assessment differential item function cognitive diagnosis dina example
Puhan G.,Rater comparability scoring and equating: Does choice of target population weights matter in this context?,2013,50,"When a constructed-response test form is reused, raw scores from the two administrations of the form may not be comparable. The solution to this problem requires a rescoring, at the current administration, of examinee responses from the previous administration. The scores from this ""rescoring"" can be used as an anchor for equating. In this equating, the choice of weights for combining the samples to define the target population can be critical. In rescored data, the anchor usually correlates very strongly with the new form but only moderately with the reference form. This difference has a predictable impact: the equating results are most accurate when the target population is the reference form sample, least accurate when the target population is the new form sample, and somewhere in the middle when the new form and reference form samples are equally weighted in forming the target population. © 2013 by the National Council on Measurement in Education.","['Rater', 'comparability', 'scoring', 'equating', 'choice', 'target', 'population', 'weight', 'matter', 'context']","['rater', 'comparability', 'scoring', 'equating', 'choice', 'target', 'population', 'weight', 'matter', 'context']",rater comparability scoring equating choice target population weight matter context
Albano A.D.,A general linear method for equating with small samples,2015,52,"Research on equating with small samples has shown that methods with stronger assumptions and fewer statistical estimates can lead to decreased error in the estimated equating function. This article introduces a new approach to linear observed-score equating, one which provides flexible control over how form difficulty is assumed versus estimated to change across the score scale. A general linear method is presented as an extension of traditional linear methods. The general method is then compared to other linear and nonlinear methods in terms of accuracy in estimating a criterion equating function. Results from two parametric bootstrapping studies based on real data demonstrate the usefulness of the general linear method. © 2015 by the National Council on Measurement in Education.","['general', 'linear', 'method', 'equate', 'small', 'sample']","['general', 'linear', 'method', 'equate', 'small', 'sample']",general linear method equate small sample
Li Z.,Power and Sample Size Calculations for Logistic Regression Tests for Differential Item Functioning,2014,51,"Logistic regression is a popular method for detecting uniform and nonuniform differential item functioning (DIF) effects. Theoretical formulas for the power and sample size calculations are derived for likelihood ratio tests and Wald tests based on the asymptotic distribution of the maximum likelihood estimators for the logistic regression model. The power is related to the item response function (IRF) for the studied item, the latent trait distributions, and the sample sizes for the reference and focal groups. Simulation studies show that the theoretical values calculated from the formulas derived in the article are close to what are observed in the simulated data when the assumptions are satisfied. The robustness of the power formulas are studied with simulations when the assumptions are violated. © 2014 by the National Council on Measurement in Education .","['power', 'Sample', 'Size', 'Calculations', 'Logistic', 'regression', 'test', 'Differential', 'Item', 'Functioning']","['power', 'sample', 'size', 'calculations', 'logistic', 'regression', 'test', 'differential', 'item', 'functioning']",power sample size calculations logistic regression test differential item functioning
Sireci S.G.,Agreeing on Validity Arguments,2013,50,"Kane (this issue) presents a comprehensive review of validity theory and reminds us that the focus of validation is on test score interpretations and use. In reacting to his article, I support the argument-based approach to validity and all of the major points regarding validation made by Dr. Kane. In addition, I call for a simpler, three-step method for developing validity arguments, one that focuses on explicit testing purposes, as suggested by the Standards for Educational and Psychological Testing. If testing purposes are appropriately articulated, the process of developing an interpretive argument becomes unnecessary and validation can directly address intended interpretations and uses. © 2013 by the National Council on Measurement in Education.","['agree', 'Validity', 'argument']","['agree', 'validity', 'argument']",agree validity argument
Liang T.; Wells C.S.; Hambleton R.K.,An assessment of the nonparametric approach for evaluating the fit of item response models,2014,51,"As item response theory has been more widely applied, investigating the fit of a parametric model becomes an important part of the measurement process. There is a lack of promising solutions to the detection of model misfit in IRT. Douglas and Cohen introduced a general nonparametric approach, RISE (Root Integrated Squared Error), for detecting model misfit. The purposes of this study were to extend the use of RISE to more general and comprehensive applications by manipulating a variety of factors (e.g., test length, sample size, IRT models, ability distribution). The results from the simulation study demonstrated that RISE outperformed G2 and S-X2 in that it controlled Type I error rates and provided adequate power under the studied conditions. In the empirical study, RISE detected reasonable numbers of misfitting items compared to G2 and S-X2, and RISE gave a much clearer picture of the location and magnitude of misfit for each misfitting item. In addition, there was no practical consequence to classification before and after replacement of misfitting items detected by three fit statistics. © 2014 by the National Council on Measurement in Education.","['assessment', 'nonparametric', 'approach', 'evaluate', 'fit', 'item', 'response']","['assessment', 'nonparametric', 'approach', 'evaluate', 'fit', 'item', 'response']",assessment nonparametric approach evaluate fit item response
Chang Y.-W.; Huang W.-K.; Tsai R.-C.,DIF Detection Using Multiple-Group Categorical CFA With Minimum Free Baseline Approach,2015,52,"The aim of this study is to assess the efficiency of using the multiple-group categorical confirmatory factor analysis (MCCFA) and the robust chi-square difference test in differential item functioning (DIF) detection for polytomous items under the minimum free baseline strategy. While testing for DIF items, despite the strong assumption that all but the examined item are set to be DIF-free, MCCFA with such a constrained baseline approach is commonly used in the literature. The present study relaxes this strong assumption and adopts the minimum free baseline approach where, aside from those parameters constrained for identification purpose, parameters of all but the examined item are allowed to differ among groups. Based on the simulation results, the robust chi-square difference test statistic with the mean and variance adjustment is shown to be efficient in detecting DIF for polytomous items in terms of the empirical power and Type I error rates. To sum up, MCCFA under the minimum free baseline strategy is useful for DIF detection for polytomous items. © 2015 by the National Council on Measurement in Education.","['DIF', 'Detection', 'MultipleGroup', 'Categorical', 'CFA', 'Minimum', 'Free', 'Baseline', 'Approach']","['dif', 'detection', 'multiplegroup', 'categorical', 'cfa', 'minimum', 'free', 'baseline', 'approach']",dif detection multiplegroup categorical cfa minimum free baseline approach
Van der Linden W.J.,Some conceptual issues in observed-score equating,2013,50,"In spite of all of the technical progress in observed-score equating, several of the more conceptual aspects of the process still are not well understood. As a result, the equating literature struggles with rather complex criteria of equating, lack of a test-theoretic foundation, confusing terminology, and ad hoc analyses. A return to Lord's foundational criterion of equity of equating, a derivation of the true equating transformation from it, and mainstream statistical treatment of the problem of estimating the transformation for various data-collection designs exist as a solution to the problem. © 2013 by the National Council on Measurement in Education.","['conceptual', 'issue', 'observedscore', 'equate']","['conceptual', 'issue', 'observedscore', 'equate']",conceptual issue observedscore equate
Sinharay S.; Wan P.; Whitaker M.; Kim D.-I.; Zhang L.; Choi S.W.,Determining the Overall Impact of Interruptions During Online Testing,2014,51,"With an increase in the number of online tests, interruptions during testing due to unexpected technical issues seem unavoidable. For example, interruptions occurred during several recent state tests. When interruptions occur, it is important to determine the extent of their impact on the examinees' scores. There is a lack of research on this topic due to the novelty of the problem. This article is an attempt to fill that void. Several methods, primarily based on propensity score matching, linear regression, and item response theory, were suggested to determine the overall impact of the interruptions on the examinees' scores. A realistic simulation study shows that the suggested methods have satisfactory Type I error rate and power. Then the methods were applied to data from the Indiana Statewide Testing for Educational Progress-Plus (ISTEP+) test that experienced interruptions in 2013. The results indicate that the interruptions did not have a significant overall impact on the student scores for the ISTEP+ test. © 2014 by the National Council on Measurement in Education.","['determine', 'overall', 'Impact', 'interruption', 'Online', 'Testing']","['determine', 'overall', 'impact', 'interruption', 'online', 'testing']",determine overall impact interruption online testing
Schroeders U.; Robitzsch A.; Schipolowski S.,A Comparison of Different Psychometric Approaches to Modeling Testlet Structures: An Example with C-Tests,2014,51,"C-tests are a specific variant of cloze tests that are considered time-efficient, valid indicators of general language proficiency. They are commonly analyzed with models of item response theory assuming local item independence. In this article we estimated local interdependencies for 12 C-tests and compared the changes in item difficulties, reliability estimates, and person parameter estimates for different modeling approaches: (a) Rasch, (b) testlet, (c) partial credit, and (d) copula models. The results are complemented with findings of a simulation study in which sample size, number of testlets, and strength of residual correlations between items were systematically manipulated. Results are discussed with regard to the pivotal question whether residual dependencies between items are an artifact or part of the construct. © 2014 by the National Council on Measurement in Education .","['Comparison', 'Different', 'Psychometric', 'Approaches', 'Testlet', 'Structures', 'example', 'ctest']","['comparison', 'different', 'psychometric', 'approaches', 'testlet', 'structures', 'example', 'ctest']",comparison different psychometric approaches testlet structures example ctest
Mislevy R.J.,How Developments in Psychology and Technology Challenge Validity Argumentation,2016,53,"Validity is the sine qua non of properties of educational assessment. While a theory of validity and a practical framework for validation has emerged over the past decades, most of the discussion has addressed familiar forms of assessment and psychological framings. Advances in digital technologies and in cognitive and social psychology have expanded the range of purposes, targets of inference, contexts of use, forms of activity, and sources of evidence we now see in educational assessment. This article discusses some of these developments and how concepts and representations that are employed to design and use assessments, hence to frame validity arguments, can be extended accordingly. Ideas are illustrated with a variety of examples, with an emphasis on assessment in higher education. Copyright © 2016 by the National Council on Measurement in Education","['Developments', 'psychology', 'Technology', 'Challenge', 'Validity', 'Argumentation']","['developments', 'psychology', 'technology', 'challenge', 'validity', 'argumentation']",developments psychology technology challenge validity argumentation
Bolt D.M.; Deng S.; Lee S.,IRT model misspecification and measurement of growth in vertical scaling,2014,51,"Functional form misfit is frequently a concern in item response theory (IRT), although the practical implications of misfit are often difficult to evaluate. In this article, we illustrate how seemingly negligible amounts of functional form misfit, when systematic, can be associated with significant distortions of the score metric in vertical scaling contexts. Our analysis uses two- and three-parameter versions of Samejima's logistic positive exponent model (LPE) as a data generating model. Consistent with prior work, we find LPEs generally provide a better comparative fit to real item response data than traditional IRT models (2PL, 3PL). Further, our simulation results illustrate how 2PL- or 3PL-based vertical scaling in the presence of LPE-induced misspecification leads to an artificial growth deceleration across grades, consistent with that commonly seen in vertical scaling studies. The results raise further concerns about the use of standard IRT models in measuring growth, even apart from the frequently cited concerns of construct shift/multidimensionality across grades. © 2014 by the National Council on Measurement in Education.","['IRT', 'misspecification', 'growth', 'vertical', 'scaling']","['irt', 'misspecification', 'growth', 'vertical', 'scaling']",irt misspecification growth vertical scaling
Guo R.; Zheng Y.; Chang H.-H.,A Stepwise Test Characteristic Curve Method to Detect Item Parameter Drift,2015,52,"An important assumption of item response theory is item parameter invariance. Sometimes, however, item parameters are not invariant across different test administrations due to factors other than sampling error; this phenomenon is termed item parameter drift. Several methods have been developed to detect drifted items. However, most of the existing methods were designed to detect drifts in individual items, which may not be adequate for test characteristic curve-based linking or equating. One example is the item response theory-based true score equating, whose goal is to generate a conversion table to relate number-correct scores on two forms based on their test characteristic curves. This article introduces a stepwise test characteristic curve method to detect item parameter drift iteratively based on test characteristic curves without needing to set any predetermined critical values. Comparisons are made between the proposed method and two existing methods under the three-parameter logistic item response model through simulation and real data analysis. Results show that the proposed method produces a small difference in test characteristic curves between administrations, an accurate conversion table, and a good classification of drifted and nondrifted items and at the same time keeps a large amount of linking items. © 2015 by the National Council on Measurement in Education.","['Stepwise', 'Test', 'Characteristic', 'Curve', 'Method', 'detect', 'Item', 'Parameter', 'Drift']","['stepwise', 'test', 'characteristic', 'curve', 'method', 'detect', 'item', 'parameter', 'drift']",stepwise test characteristic curve method detect item parameter drift
Bridgeman B.,A simple answer to a simple question on changing answers,2012,49,"In an article in the Winter 2011 issue of the Journal of Educational Measurement, van der Linden, Jeon, and Ferrara suggested that ""test takers should trust their initial instincts and retain their initial responses when they have the opportunity to review test items."" They presented a complex IRT model that appeared to show that students would be worse off by changing answers. As noted in a subsequent erratum, this conclusion was based on flawed data, and that the correct data could not be analyzed by their method because the model failed to converge. This left their basic question on the value of answer changing unanswered. A much more direct approach is to simply count the number of examinees whose scores after an opportunity to change answers are higher, lower, or the same as their initial scores. Using the same data set as the original article, an overwhelming majority of the students received higher scores after the opportunity to change answers. © 2012 by the National Council on Measurement in Education.","['simple', 'answer', 'simple', 'question', 'change', 'answer']","['simple', 'answer', 'simple', 'question', 'change', 'answer']",simple answer simple question change answer
Kahraman N.,Unidimensional interpretations for multidimensional test items,2013,50,"This article considers potential problems that can arise in estimating a unidimensional item response theory (IRT) model when some test items are multidimensional (i.e., show a complex factorial structure). More specifically, this study examines (1) the consequences of model misfit on IRT item parameter estimates due to unintended minor item-level multidimensionality, and (2) whether a Projection IRT model can provide a useful remedy. A real-data example is used to illustrate the problem and also is used as a base model for a simulation study. The results suggest that ignoring item-level multidimensionality might lead to inflated item discrimination parameter estimates when the proportion of multidimensional test items to unidimensional test items is as low as 1:5. The Projection IRT model appears to be a useful tool for updating unidimensional item parameter estimates of multidimensional test items for a purified unidimensional interpretation. Copyright © 2013 by the National Council on Measurement in Education.","['unidimensional', 'interpretation', 'multidimensional', 'test', 'item']","['unidimensional', 'interpretation', 'multidimensional', 'test', 'item']",unidimensional interpretation multidimensional test item
Tendeiro J.N.; Meijer R.R.,Detection of invalid test scores: The usefulness of simple nonparametric statistics,2014,51,In recent guidelines for fair educational testing it is advised to check the validity of individual test scores through the use of person-fit statistics. For practitioners it is unclear on the basis of the existing literature which statistic to use. An overview of relatively simple existing nonparametric approaches to identify atypical response patterns is provided. A simulation study was conducted to compare the different approaches and on the basis of the literature review and the simulation study guidelines for the use of person-fit approaches are given. © 2014 by the National Council on Measurement in Education.,"['detection', 'invalid', 'test', 'score', 'usefulness', 'simple', 'nonparametric', 'statistic']","['detection', 'invalid', 'test', 'score', 'usefulness', 'simple', 'nonparametric', 'statistic']",detection invalid test score usefulness simple nonparametric statistic
Rutkowski L.; Zhou Y.,Correcting Measurement Error in Latent Regression Covariates via the MC-SIMEX Method,2015,52,"Given the importance of large-scale assessments to educational policy conversations, it is critical that subpopulation achievement is estimated reliably and with sufficient precision. Despite this importance, biased subpopulation estimates have been found to occur when variables in the conditioning model side of a latent regression model contain measurement error. As such, this article proposes a method to correct for misclassification in the conditioning model by way of the misclassification simulation extrapolation (MC-SIMEX) method. Although the proposed method is computationally intensive, results from a simulation study show that the MC-SIMEX method improves latent regression coefficients and associated subpopulation achievement estimates. The method is demonstrated with PIRLS 2006 data. The importance of collecting high-priority, policy-relevant contextual data from at least two sources is emphasized and practical applications are discussed. © 2015 by the National Council on Measurement in Education.","['correct', 'Error', 'Latent', 'Regression', 'Covariates', 'MCSIMEX', 'Method']","['correct', 'error', 'latent', 'regression', 'covariates', 'mcsimex', 'method']",correct error latent regression covariates mcsimex method
Borsboom D.; Markus K.A.,Truth and Evidence in Validity Theory,2013,50,"According to Kane (this issue), ""the validity of a proposed interpretation or use depends on how well the evidence supports"" the claims being made. Because truth and evidence are distinct, this means that the validity of a test score interpretation could be high even though the interpretation is false. As an illustration, we discuss the case of phlogiston measurement as it existed in the 18th century. At face value, Kane's theory would seem to imply that interpretations of phlogiston measurement were valid in the 18th century (because the evidence for them was strong), even though amounts of phlogiston do not exist and hence cannot be measured. We suggest that this neglects an important aspect of validity and suggest various ways in which Kane's theory could meet this challenge. © 2013 by the National Council on Measurement in Education.","['truth', 'Evidence', 'Validity', 'theory']","['truth', 'evidence', 'validity', 'theory']",truth evidence validity theory
Newton P.E.,Two Kinds of Argument?,2013,50,"Kane distinguishes between two kinds of argument: the interpretation/use argument and the validity argument. This commentary considers whether there really are two kinds of argument, two arguments, or just one. It concludes that there is just one argument: the validity argument. © 2013 by the National Council on Measurement in Education.","['Kinds', 'Argument']","['kinds', 'argument']",kinds argument
Zhu M.; Shu Z.; von Davier A.A.,Using Networks to Visualize and Analyze Process Data for Educational Assessment,2016,53,"New technology enables interactive and adaptive scenario-based tasks (SBTs) to be adopted in educational measurement. At the same time, it is a challenging problem to build appropriate psychometric models to analyze data collected from these tasks, due to the complexity of the data. This study focuses on process data collected from SBTs. We explore the potential of using concepts and methods from social network analysis to represent and analyze process data. Empirical data were collected from the assessment of Technology and Engineering Literacy, conducted as part of the National Assessment of Educational Progress. For the activity sequences in the process data, we created a transition network using weighted directed networks, with nodes representing actions and directed links connecting two actions only if the first action is followed by the second action in the sequence. This study shows how visualization of the transition networks represents process data and provides insights for item design. This study also explores how network measures are related to existing scoring rubrics and how detailed network measures can be used to make intergroup comparisons. Copyright © 2016 by the National Council on Measurement in Education","['network', 'Visualize', 'Analyze', 'Process', 'Data', 'Educational', 'Assessment']","['network', 'visualize', 'analyze', 'process', 'data', 'educational', 'assessment']",network visualize analyze process data educational assessment
Pohl S.,Longitudinal multistage testing,2013,50,"This article introduces longitudinal multistage testing (lMST), a special form of multistage testing (MST), as a method for adaptive testing in longitudinal large-scale studies. In lMST designs, test forms of different difficulty levels are used, whereas the values on a pretest determine the routing to these test forms. Since lMST allows for testing in paper and pencil mode, lMST may represent an alternative to conventional testing (CT) in assessments for which other adaptive testing designs are not applicable. In this article the performance of lMST is compared to CT in terms of test targeting as well as bias and efficiency of ability and change estimates. Using a simulation study, the effect of the stability of ability across waves, the difficulty level of the different test forms, and the number of link items between the test forms were investigated. © 2013 by the National Council on Measurement in Education.","['longitudinal', 'multistage', 'testing']","['longitudinal', 'multistage', 'testing']",longitudinal multistage testing
Brinkhuis M.J.S.; Bakker M.; Maris G.,Filtering Data for Detecting Differential Development,2015,52,"The amount of data available in the context of educational measurement has vastly increased in recent years. Such data are often incomplete, involve tests administered at different time points and during the course of many years, and can therefore be quite challenging to model. In addition, intermediate results like grades or report cards being available to pupils, teachers, parents, and policy makers might influence future performance, which adds to the modeling difficulties. We propose the use of simple data filters to obtain a reduced set of relevant data, which allows for simple checks on the relative development of persons, items, or both. © 2015 by the National Council on Measurement in Education.","['Filtering', 'Data', 'Detecting', 'Differential', 'Development']","['filtering', 'data', 'detecting', 'differential', 'development']",filtering data detecting differential development
Harrison G.M.,Non-numeric Intrajudge Consistency Feedback in an Angoff Procedure,2015,52,"The credibility of standard-setting cut scores depends in part on two sources of consistency evidence: intrajudge and interjudge consistency. Although intrajudge consistency feedback has often been provided to Angoff judges in practice, more evidence is needed to determine whether it achieves its intended effect. In this randomized experiment with 36 judges, non-numeric item-level intrajudge consistency feedback was provided to treatment-group judges after the first and second rounds of Angoff ratings. Compared to the judges in the control condition, those receiving the feedback significantly improved their intrajudge consistency, with the effect being stronger after the first round than after the second round. To examine whether this feedback has deleterious effects on between-judge consistency, I also examined interjudge consistency at the cut score level and the item level using generalizability theory. The results showed that without the feedback, cut score variability worsened; with the feedback, idiosyncratic item-level variability improved. These results suggest that non-numeric intrajudge consistency feedback achieves its intended effect and potentially improves interjudge consistency. The findings contribute to standard-setting feedback research and provide empirical evidence for practitioners planning Angoff procedures. © 2015 by the National Council on Measurement in Education.","['Nonnumeric', 'Intrajudge', 'Consistency', 'Feedback', 'Angoff', 'Procedure']","['nonnumeric', 'intrajudge', 'consistency', 'feedback', 'angoff', 'procedure']",nonnumeric intrajudge consistency feedback angoff procedure
Bradlow E.T.,"Comments on ""Some conceptual issues in observed-score equating"" by Wim J. van der Linden",2013,50,"The van der Linden article (this issue) provides a roadmap for future research in equating. My belief is that the roadmap begins and ends with collecting auxiliary data that can be utilized to provide improved equating, especially when data are sparse or equating beyond simple moments is desired. © 2013 by the National Council on Measurement in Education.","['comment', 'conceptual', 'issue', 'observedscore', 'equate', 'Wim', 'J', 'van', 'der', 'Linden']","['comment', 'conceptual', 'issue', 'observedscore', 'equate', 'wim', 'j', 'van', 'der', 'linden']",comment conceptual issue observedscore equate wim j van der linden
Häggström J.; Wiberg M.,Optimal bandwidth selection in observed-score kernel equating,2014,51,"The selection of bandwidth in kernel equating is important because it has a direct impact on the equated test scores. The aim of this article is to examine the use of double smoothing when selecting bandwidths in kernel equating and to compare double smoothing with the commonly used penalty method. This comparison was made using both an equivalent groups design and a nonequivalent group with anchor test design. The performance of the methods was evaluated through simulation studies using both symmetric and skewed score distributions. In addition, the bandwidth selection methods were applied to real data from a college admissions test. The results show that the traditional penalty method works well although double smoothing is a viable alternative because it performs reasonably well compared to the traditional method. © 2014 by the National Council on Measurement in Education.","['optimal', 'bandwidth', 'selection', 'observedscore', 'kernel', 'equating']","['optimal', 'bandwidth', 'selection', 'observedscore', 'kernel', 'equating']",optimal bandwidth selection observedscore kernel equating
Albano A.D.,Multilevel modeling of item position effects,2013,50,"In many testing programs it is assumed that the context or position in which an item is administered does not have a differential effect on examinee responses to the item. Violations of this assumption may bias item response theory estimates of item and person parameters. This study examines the potentially biasing effects of item position. A hierarchical generalized linear model is formulated for estimating item-position effects. The model is demonstrated using data from a pilot administration of the GRE wherein the same items appeared in different positions across the test form. Methods for detecting and assessing position effects are discussed, as are applications of the model in the contexts of test development and item analysis. © 2013 by the National Council on Measurement in Education.","['multilevel', 'modeling', 'item', 'position', 'effect']","['multilevel', 'modeling', 'item', 'position', 'effect']",multilevel modeling item position effect
Schmidt S.; Zlatkin-Troitschanskaia O.; Fox J.-P.,Pretest-Posttest-Posttest Multilevel IRT Modeling of Competence Growth of Students in Higher Education in Germany,2016,53,"Longitudinal research in higher education faces several challenges. Appropriate methods of analyzing competence growth of students are needed to deal with those challenges and thereby obtain valid results. In this article, a pretest-posttest-posttest multivariate multilevel IRT model for repeated measures is introduced which is designed to address educational research questions according to a German research project. In this model, dependencies between repeated observations of the same students are considered not, as usual, by clustering observations within participants but rather by clustering observations within semesters. Estimation of the model is conducted within a Bayesian framework. Results indicate that competences grew over time. Gender, intelligence, motivation, and prior education could explain differences in the level of competence among business and economics students. Copyright © 2016 by the National Council on Measurement in Education","['pretestposttestposttest', 'Multilevel', 'IRT', 'modeling', 'Competence', 'Growth', 'Students', 'high', 'Germany']","['pretestposttestposttest', 'multilevel', 'irt', 'modeling', 'competence', 'growth', 'students', 'high', 'germany']",pretestposttestposttest multilevel irt modeling competence growth students high germany
Hou L.; La Torre J.D.; Nandakumar R.,Differential item functioning assessment in cognitive diagnostic modeling: Application of the wald test to investigate DIF in the DINA model,2014,51,"Analyzing examinees' responses using cognitive diagnostic models (CDMs) has the advantage of providing diagnostic information. To ensure the validity of the results from these models, differential item functioning (DIF) in CDMs needs to be investigated. In this article, the Wald test is proposed to examine DIF in the context of CDMs. This study explored the effectiveness of the Wald test in detecting both uniform and nonuniform DIF in the DINA model through a simulation study. Results of this study suggest that for relatively discriminating items, the Wald test had Type I error rates close to the nominal level. Moreover, its viability was underscored by the medium to high power rates for most investigated DIF types when DIF size was large. Furthermore, the performance of the Wald test in detecting uniform DIF was compared to that of the traditional Mantel-Haenszel (MH) and SIBTEST procedures. The results of the comparison study showed that the Wald test was comparable to or outperformed the MH and SIBTEST procedures. Finally, the strengths and limitations of the proposed method and suggestions for future studies are discussed. © 2014 by the National Council on Measurement in Education.","['differential', 'item', 'functioning', 'assessment', 'cognitive', 'diagnostic', 'modeling', 'application', 'wald', 'test', 'investigate', 'DIF', 'DINA']","['differential', 'item', 'functioning', 'assessment', 'cognitive', 'diagnostic', 'modeling', 'application', 'wald', 'test', 'investigate', 'dif', 'dina']",differential item functioning assessment cognitive diagnostic modeling application wald test investigate dif dina
Kane M.T.,Validating the Interpretations and Uses of Test Scores,2013,50,"To validate an interpretation or use of test scores is to evaluate the plausibility of the claims based on the scores. An argument-based approach to validation suggests that the claims based on the test scores be outlined as an argument that specifies the inferences and supporting assumptions needed to get from test responses to score-based interpretations and uses. Validation then can be thought of as an evaluation of the coherence and completeness of this interpretation/use argument and of the plausibility of its inferences and assumptions. In outlining the argument-based approach to validation, this paper makes eight general points. First, it is the proposed score interpretations and uses that are validated and not the test or the test scores. Second, the validity of a proposed interpretation or use depends on how well the evidence supports the claims being made. Third, more-ambitious claims require more support than less-ambitious claims. Fourth, more-ambitious claims (e.g., construct interpretations) tend to be more useful than less-ambitious claims, but they are also harder to validate. Fifth, interpretations and uses can change over time in response to new needs and new understandings leading to changes in the evidence needed for validation. Sixth, the evaluation of score uses requires an evaluation of the consequences of the proposed uses; negative consequences can render a score use unacceptable. Seventh, the rejection of a score use does not necessarily invalidate a prior, underlying score interpretation. Eighth, the validation of the score interpretation on which a score use is based does not validate the score use. © 2013 by the National Council on Measurement in Education.","['validate', 'Interpretations', 'Uses', 'Test', 'Scores']","['validate', 'interpretations', 'uses', 'test', 'scores']",validate interpretations uses test scores
Moss P.A.,Validity in Action: Lessons From Studies of Data Use,2013,50,Studies of data use illuminate ways in which education professionals have used test scores and other evidence relevant to students' learning-in action in their own contexts of work-to make decisions about their practice. These studies raise instructive challenges for a validity theory that focuses on intended interpretations and uses of test scores as Kane's (this issue) does. This commentary explores implications of data use studies for elaborating Kane's approach to validation to accommodate the ways test scores are used with other sources of evidence to address users' questions. © 2013 by the National Council on Measurement in Education.,"['validity', 'Action', 'Lessons', 'Studies', 'Data', 'Use']","['validity', 'action', 'lessons', 'studies', 'data', 'use']",validity action lessons studies data use
Chalmers R.P.,Extended Mixed-Effects Item Response Models With the MH-RM Algorithm,2015,52,"A mixed-effects item response theory (IRT) model is presented as a logical extension of the generalized linear mixed-effects modeling approach to formulating explanatory IRT models. Fixed and random coefficients in the extended model are estimated using a Metropolis-Hastings Robbins-Monro (MH-RM) stochastic imputation algorithm to accommodate for increased dimensionality due to modeling multiple design- and trait-based random effects. As a consequence of using this algorithm, more flexible explanatory IRT models, such as the multidimensional four-parameter logistic model, are easily organized and efficiently estimated for unidimensional and multidimensional tests. Rasch versions of the linear latent trait and latent regression model, along with their extensions, are presented and discussed, Monte Carlo simulations are conducted to determine the efficiency of parameter recovery of the MH-RM algorithm, and an empirical example using the extended mixed-effects IRT model is presented. © 2015 by the National Council on Measurement in Education.","['Extended', 'MixedEffects', 'Item', 'Response', 'Models', 'mhrm', 'Algorithm']","['extended', 'mixedeffects', 'item', 'response', 'models', 'mhrm', 'algorithm']",extended mixedeffects item response models mhrm algorithm
Brückner S.; Pellegrino J.W.,Integrating the Analysis of Mental Operations Into Multilevel Models to Validate an Assessment of Higher Education Students’ Competency in Business and Economics,2016,53,"The Standards for Educational and Psychological Testing indicate that validation of assessments should include analyses of participants’ response processes. However, such analyses typically are conducted only to supplement quantitative field studies with qualitative data, and seldom are such data connected to quantitative data on student or item performance. This paper presents an example of how data from an analysis of mental operations collected using a sociocognitive approach can be quantitatively integrated with other data on student and item performance to validate in part an assessment of higher education students’ competency in business and economics. Evidence of forward reasoning and paraphrasing as mental operations is obtained using the think-aloud method. As part of the validity argument and to enhance credibility of the findings, the generalized linear models are expressed as multilevel models in which the analyses of response processes are aligned with quantitative findings from large-scale field studies. Copyright © 2016 by the National Council on Measurement in Education","['integrate', 'Analysis', 'Mental', 'Operations', 'Multilevel', 'Models', 'validate', 'Assessment', 'high', 'Students', ""'"", 'Competency', 'Business', 'Economics']","['integrate', 'analysis', 'mental', 'operations', 'multilevel', 'models', 'validate', 'assessment', 'high', 'students', ""'"", 'competency', 'business', 'economics']",integrate analysis mental operations multilevel models validate assessment high students ' competency business economics
"Raczynski K.R.; Cohen A.S.; Engelhard G., Jr.; Lu Z.",Comparing the Effectiveness of Self-Paced and Collaborative Frame-of-Reference Training on Rater Accuracy in a Large-Scale Writing Assessment,2015,52,"There is a large body of research on the effectiveness of rater training methods in the industrial and organizational psychology literature. Less has been reported in the measurement literature on large-scale writing assessments. This study compared the effectiveness of two widely used rater training methods-self-paced and collaborative frame-of-reference training-in the context of a large-scale writing assessment. Sixty-six raters were randomly assigned to the training methods. After training, all raters scored the same 50 representative essays prescored by a group of expert raters. A series of generalized linear mixed models were then fitted to the rating data. Results suggested that the self-paced method was equivalent in effectiveness to the more time-intensive and expensive collaborative method. Implications for large-scale writing assessments and suggestions for further research are discussed. © 2015 by the National Council on Measurement in Education.","['compare', 'Effectiveness', 'SelfPaced', 'Collaborative', 'FrameofReference', 'Training', 'Rater', 'Accuracy', 'LargeScale', 'Writing', 'Assessment']","['compare', 'effectiveness', 'selfpaced', 'collaborative', 'frameofreference', 'training', 'rater', 'accuracy', 'largescale', 'writing', 'assessment']",compare effectiveness selfpaced collaborative frameofreference training rater accuracy largescale writing assessment
Han K.T.,An Efficiency Balanced Information Criterion for Item Selection in Computerized Adaptive Testing,2012,49,"Successful administration of computerized adaptive testing (CAT) programs in educational settings requires that test security and item exposure control issues be taken seriously. Developing an item selection algorithm that strikes the right balance between test precision and level of item pool utilization is the key to successful implementation and long-term quality control of CAT. This study proposed a new item selection method using the ""efficiency balanced information"" criterion to address issues with the maximum Fisher information method and stratification methods. According to the simulation results, the new efficiency balanced information method had desirable advantages over the other studied item selection methods in terms of improving the optimality of CAT assembly and utilizing items with low a-values while eliminating the need for item pool stratification. © 2012 by the National Council on Measurement in Education.","['Efficiency', 'Balanced', 'Information', 'Criterion', 'Item', 'Selection', 'Computerized', 'Adaptive', 'Testing']","['efficiency', 'balanced', 'information', 'criterion', 'item', 'selection', 'computerized', 'adaptive', 'testing']",efficiency balanced information criterion item selection computerized adaptive testing
Jiao H.; Wang S.; He W.,Estimation methods for one-parameter testlet models,2013,50,"This study demonstrated the equivalence between the Rasch testlet model and the three-level one-parameter testlet model and explored the Markov Chain Monte Carlo (MCMC) method for model parameter estimation in WINBUGS. The estimation accuracy from the MCMC method was compared with those from the marginalized maximum likelihood estimation (MMLE) with the expectation-maximization algorithm in ConQuest and the sixth-order Laplace approximation estimation in HLM6. The results indicated that the estimation methods had significant effects on the bias of the testlet variance and ability variance estimation, the random error in the ability parameter estimation, and the bias in the item difficulty parameter estimation. The Laplace method best recovered the testlet variance while the MMLE best recovered the ability variance. The Laplace method resulted in the smallest random error in the ability parameter estimation while the MCMC method produced the smallest bias in item parameter estimates. Analyses of three real tests generally supported the findings from the simulation and indicated that the estimates for item difficulty and ability parameters were highly correlated across estimation methods. Copyright © 2013 by the National Council on Measurement in Education.","['estimation', 'method', 'oneparameter', 'testlet']","['estimation', 'method', 'oneparameter', 'testlet']",estimation method oneparameter testlet
Falk C.F.; Cai L.,Semiparametric Item Response Functions in the Context of Guessing,2016,53,"We present a logistic function of a monotonic polynomial with a lower asymptote, allowing additional flexibility beyond the three-parameter logistic model. We develop a maximum marginal likelihood-based approach to estimate the item parameters. The new item response model is demonstrated on math assessment data from a state, and a computationally efficient strategy for choosing the order of the polynomial is demonstrated. Finally, our approach is tested through simulations and compared to response function estimation using smoothed isotonic regression. Results indicate that our approach can result in small gains in item response function recovery and latent trait estimation. Copyright © 2016 by the National Council on Measurement in Education","['Semiparametric', 'Item', 'Response', 'Functions', 'Context', 'Guessing']","['semiparametric', 'item', 'response', 'functions', 'context', 'guessing']",semiparametric item response functions context guessing
Von Davier M.; B. J.G.; von Davier A.A.,"Local equating using the rasch model, the OPLM, and the 2PL IRT model-or-what is it anyway if the model captures everything there is to know about the test takers?",2013,50,"Local equating (LE) is based on Lord's criterion of equity. It defines a family of true transformations that aim at the ideal of equitable equating. van der Linden (this issue) offers a detailed discussion of common issues in observed-score equating relative to this local approach. By assuming an underlying item response theory model, one of the main features of LE is that it adjusts the equated raw scores using conditional distributions of raw scores given an estimate of the ability of interest. In this article, we argue that this feature disappears when using a Rasch model for the estimation of the true transformation, while the one-parameter logistic model and the two-parameter logistic model do provide a local adjustment of the equated score. © 2013 by the National Council on Measurement in Education.","['local', 'equating', 'rasch', 'OPLM', '2PL', 'IRT', 'modelorwhat', 'capture', 'know', 'test', 'taker']","['local', 'equating', 'rasch', 'oplm', '2pl', 'irt', 'modelorwhat', 'capture', 'know', 'test', 'taker']",local equating rasch oplm 2pl irt modelorwhat capture know test taker
"Davison M.L.; Davenport E.C., Jr.; Chang Y.-F.; Vue K.; Su S.",Criterion-Related Validity: Assessing the Value of Subscores,2015,52,"Criterion-related profile analysis (CPA) can be used to assess whether subscores of a test or test battery account for more criterion variance than does a single total score. Application of CPA to subscore evaluation is described, compared to alternative procedures, and illustrated using SAT data. Considerations other than validity and reliability are discussed, including broad societal goals (e.g., affirmative action), fairness, and ties in expected criterion predictions. In simulation data, CPA results were sensitive to subscore correlations, sample size, and the proportion of criterion-related variance accounted for by the subscores. CPA can be a useful component in a thorough subscore evaluation encompassing subscore reliability, validity, distinctiveness, fairness, and broader societal goals. © 2015 by the National Council on Measurement in Education.","['CriterionRelated', 'Validity', 'assess', 'Value', 'Subscores']","['criterionrelated', 'validity', 'assess', 'value', 'subscores']",criterionrelated validity assess value subscores
Clauser J.C.; Margolis M.J.; Clauser B.E.,An examination of the replicability of angoff standard setting results within a generalizability theory framework,2014,51,"Evidence of stable standard setting results over panels or occasions is an important part of the validity argument for an established cut score. Unfortunately, due to the high cost of convening multiple panels of content experts, standards often are based on the recommendation from a single panel of judges. This approach implicitly assumes that the variability across panels will be modest, but little evidence is available to support this assertion. This article examines the stability of Angoff standard setting results across panels. Data were collected for six independent standard setting exercises, with three panels participating in each exercise. The results show that although in some cases the panel effect is negligible, for four of the six data sets the panel facet represented a large portion of the overall error variance. Ignoring the often hidden panel/occasion facet can result in artificially optimistic estimates of the cut score stability. Results based on a single panel should not be viewed as a reasonable estimate of the results that would be found over multiple panels. Instead, the variability seen in a single panel can best be viewed as a lower bound of the expected variability when the exercise is replicated. © 2014 by the National Board of Medical Examiners.","['examination', 'replicability', 'angoff', 'standard', 'set', 'result', 'generalizability', 'theory', 'framework']","['examination', 'replicability', 'angoff', 'standard', 'set', 'result', 'generalizability', 'theory', 'framework']",examination replicability angoff standard set result generalizability theory framework
Wang W.-C.; Jin K.-Y.; Qiu X.-L.; Wang L.,Item response models for examinee-selected items,2012,49,"In some tests, examinees are required to choose a fixed number of items from a set of given items to answer. This practice creates a challenge to standard item response models, because more capable examinees may have an advantage by making wiser choices. In this study, we developed a new class of item response models to account for the choice effect of examinee-selected items. The results of a series of simulation studies showed: (1) that the parameters of the new models were recovered well, (2) the parameter estimates were almost unbiased when the new models were fit to data that were simulated from standard item response models, (3) failing to consider the choice effect yielded shrunken parameter estimates for examinee-selected items, and (4) even when the missingness mechanism in examinee-selected items did not follow the item response functions specified in the new models, the new models still yielded a better fit than did standard item response models. An empirical example of a college entrance examination supported the use of the new models: in general, the higher the examinee's ability, the better his or her choice of items. © 2012 by the National Council on Measurement in Education.","['item', 'response', 'examineeselecte', 'item']","['item', 'response', 'examineeselecte', 'item']",item response examineeselecte item
van der Palm D.W.; van der Ark L.A.; Sijtsma K.,A Flexible Latent Class Approach to Estimating Test-Score Reliability,2014,51,"The latent class reliability coefficient (LCRC) is improved by using the divisive latent class model instead of the unrestricted latent class model. This results in the divisive latent class reliability coefficient (DLCRC), which unlike LCRC avoids making subjective decisions about the best solution and thus avoids judgment error. A computational study using large numbers of items shows that DLCRC also is faster than LCRC and fast enough for practical purposes. Speed and objectivity render DLCRC superior to LCRC. A decisive feature of DLCRC is that it aims at closely approximating the multivariate distribution of item scores, which might render the method suited when test data are multidimensional. A simulation study focusing on multidimensionality shows that DLCRC in general has little bias relative to the true reliability and is relatively accurate compared to LCRC and classical lower bound methods coefficients α and λ2 and the greatest lower bound. © 2014 by the National Council on Measurement in Education.","['Flexible', 'Latent', 'Class', 'Approach', 'estimate', 'TestScore', 'Reliability']","['flexible', 'latent', 'class', 'approach', 'estimate', 'testscore', 'reliability']",flexible latent class approach estimate testscore reliability
Haertel E.,Getting the Help We Need,2013,50,"In validating uses of testing, it is helpful to distinguish those that rely directly on the information provided by scores or score distributions (direct uses and consequences) versus those that instead capitalize on the motivational effects of testing, or use testing and test reporting to shape public opinion (indirect uses and consequences). Some uses and consequences, both direct and indirect, are intended; others are unintended. Unintended consequences pose greater challenges in test validation because they must be identified before they can be investigated. Validation of uses and consequences can employ theories and methods from various social science disciplines. Educational measurement is most closely allied with psychology and statistics, but sociologists, anthropologists, economists, linguists, and others also could help in theorizing and investigating the consequences of test use, especially indirect and unintended consequences. © 2013 by the National Council on Measurement in Education.","['help', 'need']","['help', 'need']",help need
Cher Wong C.,Asymptotic standard errors for item response theory true score equating of polytomous items,2015,52,"Building on previous works by Lord and Ogasawara for dichotomous items, this article proposes an approach to derive the asymptotic standard errors of item response theory true score equating involving polytomous items, for equivalent and nonequivalent groups of examinees. This analytical approach could be used in place of empirical methods like the bootstrap method, to obtain standard errors of equated scores. Formulas are introduced to obtain the derivatives for computing the asymptotic standard errors. The approach was validated using mean-mean, mean-sigma, random-groups, or concurrent calibration equating of simulated samples, for tests modeled using the generalized partial credit model or the graded response model. © 2015 by the National Council on Measurement in Education.","['asymptotic', 'standard', 'error', 'item', 'response', 'theory', 'true', 'score', 'equating', 'polytomous', 'item']","['asymptotic', 'standard', 'error', 'item', 'response', 'theory', 'true', 'score', 'equating', 'polytomous', 'item']",asymptotic standard error item response theory true score equating polytomous item
Mislevy R.J.; Zwick R.,"Scaling, Linking, and Reporting a Periodic Assessment System",2012,49,"A new entry the testing lexicon is through-course summative assessment, a system consisting of components administered periodically during the academic year. As defined the Race to the Top program, these assessments are intended to yield a yearly summative score for accountability purposes. They must provide for both individual and group proficiency estimates and allow for the measurement of growth. They must accommodate students who vary their patterns of curricular exposure. Because they are meant to provide actionable information to teachers they must be instructionally sensitive, so item-operating characteristics can be expected to change relative to one another as a function of patterns of curricular exposure. This paper discusses methodology one can draw upon to tackle this ambitious collection of inferences. We consider a modeling framework that consists of an item response theory component and a population component, as the National Assessment of Educational Progress, and show how performance and growth could be expressed terms of expected performance on a market basket of tasks. We discuss conditions under which modeling simplifications might be possible and discuss studies that would be needed to fit models, estimate parameters, and evaluate data requirements. © 2012 by the National Council on Measurement in Education.","['scale', 'Linking', 'report', 'Periodic', 'Assessment', 'system']","['scale', 'linking', 'report', 'periodic', 'assessment', 'system']",scale linking report periodic assessment system
Mittelhaëuser M.-A.; Béguin A.A.; Sijtsma K.,The Effect of Differential Motivation on IRT Linking,2015,52,"The purpose of this study was to investigate whether simulated differential motivation between the stakes for operational tests and anchor items produces an invalid linking result if the Rasch model is used to link the operational tests. This was done for an external anchor design and a variation of a pretest design. The study also investigated whether a constrained mixture Rasch model could identify latent classes in such a way that one latent class represented high-stakes responding while the other represented low-stakes responding. The results indicated that for an external anchor design, the Rasch linking result was only biased when the motivation level differed between the subpopulations to which the anchor items were administered. However, the mixture Rasch model did not identify the classes representing low-stakes and high-stakes responding. When a pretest design was used to link the operational tests by means of a Rasch model, the linking result was found to be biased in each condition. Bias increased as percentage of students showing low-stakes responding to the anchor items increased. The mixture Rasch model only identified the classes representing low-stakes and high-stakes responding under a limited number of conditions. © 2015 by the National Council on Measurement in Education.","['Effect', 'Differential', 'Motivation', 'IRT', 'link']","['effect', 'differential', 'motivation', 'irt', 'link']",effect differential motivation irt link
Naumann A.; Hochweber J.; Hartig J.,Modeling Instructional Sensitivity Using a Longitudinal Multilevel Differential Item Functioning Approach,2014,51,"Students' performance in assessments is commonly attributed to more or less effective teaching. This implies that students' responses are significantly affected by instruction. However, the assumption that outcome measures indeed are instructionally sensitive is scarcely investigated empirically. In the present study, we propose a longitudinal multilevel-differential item functioning (DIF) model to combine two existing yet independent approaches to evaluate items' instructional sensitivity. The model permits for a more informative judgment of instructional sensitivity, allowing the distinction of global and differential sensitivity. Exemplarily, the model is applied to two empirical data sets, with classical indices (Pretest-Posttest Difference Index and posttest multilevel-DIF) computed for comparison. Results suggest that the approach works well in the application to empirical data, and may provide important information to test developers. © 2014 by the National Council on Measurement in Education.","['Instructional', 'Sensitivity', 'Longitudinal', 'Multilevel', 'Differential', 'Item', 'Functioning', 'approach']","['instructional', 'sensitivity', 'longitudinal', 'multilevel', 'differential', 'item', 'functioning', 'approach']",instructional sensitivity longitudinal multilevel differential item functioning approach
De la Torre J.; Lee Y.-S.,Evaluating the wald test for item-level comparison of saturated and reduced models in cognitive diagnosis,2013,50,"This article used the Wald test to evaluate the item-level fit of a saturated cognitive diagnosis model (CDM) relative to the fits of the reduced models it subsumes. A simulation study was carried out to examine the Type I error and power of the Wald test in the context of the G-DINA model. Results show that when the sample size is small and a larger number of attributes are required, the Type I error rate of the Wald test for the DINA and DINO models can be higher than the nominal significance levels, while the Type I error rate of the A-CDM is closer to the nominal significance levels. However, with larger sample sizes, the Type I error rates for the three models are closer to the nominal significance levels. In addition, the Wald test has excellent statistical power to detect when the true underlying model is none of the reduced models examined even for relatively small sample sizes. The performance of the Wald test was also examined with real data. With an increasing number of CDMs from which to choose, this article provides an important contribution toward advancing the use of CDMs in practical educational settings. © 2013 by the National Council on Measurement in Education.","['evaluate', 'wald', 'test', 'itemlevel', 'comparison', 'saturated', 'reduce', 'cognitive', 'diagnosis']","['evaluate', 'wald', 'test', 'itemlevel', 'comparison', 'saturated', 'reduce', 'cognitive', 'diagnosis']",evaluate wald test itemlevel comparison saturated reduce cognitive diagnosis
Levy R.; Xu Y.; Yel N.; Svetina D.,A Standardized Generalized Dimensionality Discrepancy Measure and a Standardized Model-Based Covariance for Dimensionality Assessment for Multidimensional Models,2015,52,"The standardized generalized dimensionality discrepancy measure and the standardized model-based covariance are introduced as tools to critique dimensionality assumptions in multidimensional item response models. These tools are grounded in a covariance theory perspective and associated connections between dimensionality and local independence. Relative to their precursors, they allow for dimensionality assessment in a more readily interpretable metric of correlations. A simulation study demonstrates the utility of the discrepancy measures' application at multiple levels of dimensionality analysis, and compares them to factor analytic and item response theoretic approaches. An example illustrates their use in practice. © 2015 by the National Council on Measurement in Education.","['Standardized', 'Generalized', 'Dimensionality', 'Discrepancy', 'measure', 'Standardized', 'ModelBased', 'Covariance', 'Dimensionality', 'Assessment', 'Multidimensional', 'Models']","['standardized', 'generalized', 'dimensionality', 'discrepancy', 'measure', 'standardized', 'modelbased', 'covariance', 'dimensionality', 'assessment', 'multidimensional', 'models']",standardized generalized dimensionality discrepancy measure standardized modelbased covariance dimensionality assessment multidimensional models
Keller L.A.; Hambleton R.K.,The Long-Term Sustainability of IRT Scaling Methods in Mixed-Format Tests,2013,50,"Due to recent research in equating methodologies indicating that some methods may be more susceptible to the accumulation of equating error over multiple administrations, the sustainability of several item response theory methods of equating over time was investigated. In particular, the paper is focused on two equating methodologies: fixed common item parameter scaling (with two variations, FCIP-1 and FCIP-2) and the Stocking and Lord characteristic curve scaling technique in the presence of nonequivalent groups. Results indicated that the improvements made to fixed common item parameter scaling in the FCIP-2 method were sustained over time. FCIP-2 and Stocking and Lord characteristic curve scaling performed similarly in many instances and produced more accurate results than FCIP-1. The relative performance of FCIP-2 and Stocking and Lord characteristic curve scaling depended on the nature of the change in the ability distribution: Stocking and Lord characteristic curve scaling captured the change in the distribution more accurately than FCIP-2 when the change was different across the ability distribution; FCIP-2 captured the changes more accurately when the change was consistent across the ability distribution. © 2013 by the National Council on Measurement in Education.","['LongTerm', 'Sustainability', 'IRT', 'Scaling', 'Methods', 'MixedFormat', 'test']","['longterm', 'sustainability', 'irt', 'scaling', 'methods', 'mixedformat', 'test']",longterm sustainability irt scaling methods mixedformat test
Dorans N.J.,"On attempting to do what lord said was impossible: Commentary on van der Linden's ""Some Conceptual Issues in Observed-Score Equating""",2013,50,"van der Linden (this issue) uses words differently than Holland and Dorans. This difference in language usage is a source of some confusion in van der Linden's critique of what he calls equipercentile equating. I address these differences in language. van der Linden maintains that there are only two requirements for score equating. I maintain that the requirements he discards have practical utility and are testable. The score equity requirement proposed by Lord suggests that observed score equating was either unnecessary or impossible. Strong equity serves as the fulcrum for van der Linden's thesis. His proposed solution to the equity problem takes inequitable measures and aligns conditional error score distributions, resulting in a family of linking functions, one for each level of θ. In reality, θ is never known. Use of an anchor test as a proxy poses many practical problems, including defensibility. © 2013 by the National Council on Measurement in Education.","['attempt', 'lord', 'impossible', 'Commentary', 'van', 'der', 'Lindens', 'Conceptual', 'Issues', 'ObservedScore', 'Equating']","['attempt', 'lord', 'impossible', 'commentary', 'van', 'der', 'lindens', 'conceptual', 'issues', 'observedscore', 'equating']",attempt lord impossible commentary van der lindens conceptual issues observedscore equating
Zhang J.; Li J.,Monitoring Items in Real Time to Enhance CAT Security,2016,53,"An IRT-based sequential procedure is developed to monitor items for enhancing test security. The procedure uses a series of statistical hypothesis tests to examine whether the statistical characteristics of each item under inspection have changed significantly during CAT administration. This procedure is compared with a previously developed CTT-based procedure through simulation studies. The results show that when the total number of examinees is fixed both procedures can control the rate of type I errors at any reasonable significance level by choosing an appropriate cutoff point and meanwhile maintain a low rate of type II errors. Further, the IRT-based method has a much lower type II error rate or more power than the CTT-based method when the number of compromised items is small (e.g., 5), which can be achieved if the IRT-based procedure can be applied in an active mode in the sense that flagged items can be replaced with new items. Copyright © 2016 by the National Council on Measurement in Education","['monitor', 'Items', 'real', 'Time', 'enhance', 'CAT', 'Security']","['monitor', 'items', 'real', 'time', 'enhance', 'cat', 'security']",monitor items real time enhance cat security
Jin K.-Y.; Wang W.-C.,Item response theory models for performance decline during testing,2014,51,"Sometimes, test-takers may not be able to attempt all items to the best of their ability (with full effort) due to personal factors (e.g., low motivation) or testing conditions (e.g., time limit), resulting in poor performances on certain items, especially those located toward the end of a test. Standard item response theory (IRT) models fail to consider such testing behaviors. In this study, a new class of mixture IRT models was developed to account for such testing behavior in dichotomous and polytomous items, by assuming test-takers were composed of multiple latent classes and by adding a decrement parameter to each latent class to describe performance decline. Parameter recovery, effect of model misspecification, and robustness of the linearity assumption in performance decline were evaluated using simulations. It was found that the parameters in the new models were recovered fairly well by using the freeware WinBUGS; the failure to account for such behavior by fitting standard IRT models resulted in overestimation of difficulty parameters on items located toward the end of the test and overestimation of test reliability; and the linearity assumption in performance decline was rather robust. An empirical example is provided to illustrate the applications and the implications of the new class of models. © 2014 by the National Council on Measurement in Education.","['item', 'response', 'theory', 'performance', 'decline', 'testing']","['item', 'response', 'theory', 'performance', 'decline', 'testing']",item response theory performance decline testing
Van der Linden W.J.,More issues in observed-score equating,2013,50,"This article is a response to the commentaries on the position paper on observed-score equating by van der Linden (this issue). The response focuses on the more general issues in these commentaries, such as the nature of the observed scores that are equated, the importance of test-theory assumptions in equating, the necessity to use multiple equating transformations, and the choice of conditioning variables in equating. © 2013 by the National Council on Measurement in Education.","['More', 'issue', 'observedscore', 'equate']","['more', 'issue', 'observedscore', 'equate']",more issue observedscore equate
Meng X.-B.; Tao J.; Chang H.-H.,A conditional joint modeling approach for locally dependent item responses and response times,2015,52,"The assumption of conditional independence between the responses and the response times (RTs) for a given person is common in RT modeling. However, when the speed of a test taker is not constant, this assumption will be violated. In this article we propose a conditional joint model for item responses and RTs, which incorporates a covariance structure to explain the local dependency between speed and accuracy. To obtain information about the population of test takers, the new model was embedded in the hierarchical framework proposed by van der Linden (). A fully Bayesian approach using a straightforward Markov chain Monte Carlo (MCMC) sampler was developed to estimate all parameters in the model. The deviance information criterion (DIC) and the Bayes factor (BF) were employed to compare the goodness of fit between the models with two different parameter structures. The Bayesian residual analysis method was also employed to evaluate the fit of the RT model. Based on the simulations, we conclude that (1) the new model noticeably improves the parameter recovery for both the item parameters and the examinees' latent traits when the assumptions of conditional independence between the item responses and the RTs are relaxed and (2) the proposed MCMC sampler adequately estimates the model parameters. The applicability of our approach is illustrated with an empirical example, and the model fit indices indicated a preference for the new model. © 2015 by the National Council on Measurement in Education.","['conditional', 'joint', 'modeling', 'approach', 'locally', 'dependent', 'item', 'response', 'response', 'time']","['conditional', 'joint', 'modeling', 'approach', 'locally', 'dependent', 'item', 'response', 'response', 'time']",conditional joint modeling approach locally dependent item response response time
Sinharay S.,Analysis of added value of subscores with respect to classification,2014,51,"Brennan noted that users of test scores often want (indeed, demand) that subscores be reported, along with total test scores, for diagnostic purposes. Haberman suggested a method based on classical test theory (CTT) to determine if subscores have added value over the total score. One way to interpret the method is that a subscore has added value only if it has a better agreement than the total score with the corresponding subscore on a parallel form. The focus of this article is on classification of the examinees into ""pass"" and ""fail"" (or master and nonmaster) categories based on subscores. A new CTT-based method is suggested to assess whether classification based on a subscore is in better agreement, than classification based on the total score, with classification based on the corresponding subscore on a parallel form. The method can be considered as an assessment of the added value of subscores with respect to classification. The suggested method is applied to data from several operational tests. The added value of subscores with respect to classification is found to be very similar, except at extreme cutscores, to their added value from a value-added analysis of Haberman. © 2014 by the National Council on Measurement in Education.","['analysis', 'add', 'value', 'subscore', 'respect', 'classification']","['analysis', 'add', 'value', 'subscore', 'respect', 'classification']",analysis add value subscore respect classification
Kim S.; Moses T.; Yoo H.,A comparison of IRT proficiency estimation methods under adaptive multistage testing,2015,52,"This inquiry is an investigation of item response theory (IRT) proficiency estimators' accuracy under multistage testing (MST). We chose a two-stage MST design that includes four modules (one at Stage 1, three at Stage 2) and three difficulty paths (low, middle, high). We assembled various two-stage MST panels (i.e., forms) by manipulating two assembly conditions in each module, such as difficulty level and module length. For each panel, we investigated the accuracy of examinees' proficiency levels derived from seven IRT proficiency estimators. The choice of Bayesian (prior) versus non-Bayesian (no prior) estimators was of more practical significance than the choice of number-correct versus item-pattern scoring estimators. The Bayesian estimators were slightly more efficient than the non-Bayesian estimators, resulting in smaller overall error. Possible score changes caused by the use of different proficiency estimators would be nonnegligible, particularly for low- and high-performing examinees. © 2015 by the National Council on Measurement in Education.","['comparison', 'IRT', 'proficiency', 'estimation', 'method', 'adaptive', 'multistage', 'testing']","['comparison', 'irt', 'proficiency', 'estimation', 'method', 'adaptive', 'multistage', 'testing']",comparison irt proficiency estimation method adaptive multistage testing
Suh Y.; Cho S.-J.; Wollack J.A.,A Comparison of Item Calibration Procedures in the Presence of Test Speededness,2012,49,"In the presence of test speededness, the parameter estimates of item response theory models can be poorly estimated due to conditional dependencies among items, particularly for end-of-test items (i.e., speeded items). This article conducted a systematic comparison of five-item calibration procedures-a two-parameter logistic (2PL) model, a one-dimensional mixture model, a two-step strategy (a combination of the one-dimensional mixture and the 2PL), a two-dimensional mixture model, and a hybrid model--by examining how sample size, percentage of speeded examinees, percentage of missing responses, and way of scoring missing responses (incorrect vs. omitted) affect the item parameter estimation in speeded tests. For nonspeeded items, all five procedures showed similar results in recovering item parameters. For speeded items, the one-dimensional mixture model, the two-step strategy, and the two-dimensional mixture model provided largely similar results and performed better than the 2PL model and the hybrid model in calibrating slope parameters. However, those three procedures performed similarly to the hybrid model in estimating intercept parameters. As expected, the 2PL model did not appear to be as accurate as the other models in recovering item parameters, especially when there were large numbers of examinees showing speededness and a high percentage of missing responses with incorrect scoring. Real data analysis further described the similarities and differences between the five procedures. © 2012 by the National Council on Measurement in Education.","['Comparison', 'Item', 'Calibration', 'procedure', 'Presence', 'Test', 'Speededness']","['comparison', 'item', 'calibration', 'procedure', 'presence', 'test', 'speededness']",comparison item calibration procedure presence test speededness
Lathrop Q.N.; Cheng Y.,A nonparametric approach to estimate classification accuracy and consistency,2014,51,"When cut scores for classifications occur on the total score scale, popular methods for estimating classification accuracy (CA) and classification consistency (CC) require assumptions about a parametric form of the test scores or about a parametric response model, such as item response theory (IRT). This article develops an approach to estimate CA and CC nonparametrically by replacing the role of the parametric IRT model in Lee's classification indices with a modified version of Ramsay's kernel-smoothed item response functions. The performance of the nonparametric CA and CC indices are tested in simulation studies in various conditions with different generating IRT models, test lengths, and ability distributions. The nonparametric approach to CA often outperforms Lee's method and Livingston and Lewis's method, showing robustness to nonnormality in the simulated ability. The nonparametric CC index performs similarly to Lee's method and outperforms Livingston and Lewis's method when the ability distributions are nonnormal. © 2014 by the National Council on Measurement in Education.","['nonparametric', 'approach', 'estimate', 'classification', 'accuracy', 'consistency']","['nonparametric', 'approach', 'estimate', 'classification', 'accuracy', 'consistency']",nonparametric approach estimate classification accuracy consistency
Zu J.; Puhan G.,Preequating with empirical item characteristic curves: An observed-score preequating method,2014,51,"Preequating is in demand because it reduces score reporting time. In this article, we evaluated an observed-score preequating method: the empirical item characteristic curve (EICC) method, which makes preequating without item response theory (IRT) possible. EICC preequating results were compared with a criterion equating and with IRT true-score preequating conversions. Results suggested that the EICC preequating method worked well under the conditions considered in this study. The difference between the EICC preequating conversion and the criterion equating was smaller than .5 raw-score points (a practical criterion often used to evaluate equating quality) between the 5th and 95th percentiles of the new form total score distribution. EICC preequating also performed similarly or slightly better than IRT true-score preequating. © 2014 by the National Council on Measurement in Education.","['preequate', 'empirical', 'item', 'characteristic', 'curve', 'observedscore', 'preequate', 'method']","['preequate', 'empirical', 'item', 'characteristic', 'curve', 'observedscore', 'preequate', 'method']",preequate empirical item characteristic curve observedscore preequate method
Wiberg M.; van der Linden W.J.,Local linear observed-score equating,2011,48,"Two methods of local linear observed-score equating for use with anchor-test and single-group designs are introduced. In an empirical study, the two methods were compared with the current traditional linear methods for observed-score equating. As a criterion, the bias in the equated scores relative to true equating based on definition of equity was used. The local method for the anchor-test design yielded minimum bias, even for considerable variation of the relative difficulties of the two test forms and the length of the anchor test. Among the traditional methods, the method of chain equating performed best. The local method for single-group designs yielded equated scores with bias comparable to the traditional methods. This method, however, appears to be of theoretical interest because it forces us to rethink the relationship between score equating and regression. © 2011 by the National Council on Measurement in Education.","['local', 'linear', 'observedscore', 'equate']","['local', 'linear', 'observedscore', 'equate']",local linear observedscore equate
Cui Y.; Gierl M.J.; Chang H.-H.,Estimating Classification Consistency and Accuracy for Cognitive Diagnostic Assessment,2012,49,"This article introduces procedures for the computation and asymptotic statistical inference for classification consistency and accuracy indices specifically designed for cognitive diagnostic assessments. The new classification indices can be used as important indicators of the reliability and validity of classification results produced by cognitive diagnostic assessments. For tests with known or previously calibrated item parameters, the sampling distributions of the two new indices are shown to be asymptotically normal. To illustrate the computations of the new indices, we apply them to the real diagnostic data from a fraction subtraction test (Tatsuoka). We also use simulated data to evaluate their performances and distributional properties. © 2012 by the National Council on Measurement in Education.","['Estimating', 'Classification', 'Consistency', 'Accuracy', 'Cognitive', 'Diagnostic', 'Assessment']","['estimating', 'classification', 'consistency', 'accuracy', 'cognitive', 'diagnostic', 'assessment']",estimating classification consistency accuracy cognitive diagnostic assessment
Bränberg K.; Wiberg M.,Observed score linear equating with covariates,2011,48,"This paper examined observed score linear equating in two different data collection designs, the equivalent groups design and the nonequivalent groups design, when information from covariates (i.e., background variables correlated with the test scores) was included. The main purpose of the study was to examine the effect (i.e., bias, variance, and mean squared error) on the estimators of including this additional information. A model for observed score linear equating with covariates first was suggested. As a second step, the model was used in a simulation study to show that the use of covariates such as gender and education can increase the accuracy of an equating by reducing the mean squared error of the estimators. Finally, data from two administrations of the Swedish Scholastic Assessment Test were used to illustrate the use of the model. © 2011 by the National Council on Measurement in Education.","['observe', 'score', 'linear', 'equate', 'covariate']","['observe', 'score', 'linear', 'equate', 'covariate']",observe score linear equate covariate
Jiao H.; Kamata A.; Wang S.; Jin Y.,A Multilevel Testlet Model for Dual Local Dependence,2012,49,"The applications of item response theory (IRT) models assume local item independence and that examinees are independent of each other. When a representative sample for psychometric analysis is selected using a cluster sampling method in a testlet-based assessment, both local item dependence and local person dependence are likely to be induced. This study proposed a four-level IRT model to simultaneously account for dual local dependence due to item clustering and person clustering. Model parameter estimation was explored using the Markov Chain Monte Carlo method. Model parameter recovery was evaluated in a simulation study in comparison with three other related models: the Rasch model, the Rasch testlet model, and the three-level Rasch model for person clustering. In general, the proposed model recovered the item difficulty and person ability parameters with the least total error. The bias in both item and person parameter estimation was not affected but the standard error (SE) was affected. In some simulation conditions, the difference in classification accuracy between models could go up to 11%. The illustration using the real data generally supported model performance observed in the simulation study. © 2012 by the National Council on Measurement in Education.","['Multilevel', 'Testlet', 'Dual', 'Local', 'Dependence']","['multilevel', 'testlet', 'dual', 'local', 'dependence']",multilevel testlet dual local dependence
French B.F.; Finch W.H.,Hierarchical logistic regression: Accounting for multilevel data in DIF detection,2010,47,"The purpose of this study was to examine the performance of differential item functioning (DIF) assessment in the presence of a multilevel structure that often underlies data from large-scale testing programs. Analyses were conducted using logistic regression (LR), a popular, flexible, and effective tool for DIF detection. Data were simulated using a hierarchical framework, such as might be seen when examinees are clustered in schools, for example. Both standard and hierarchical LR (accounting for multilevel data) approaches to DIF detection were employed. Results highlight the differences in DIF detection rates when the analytic strategy matches the data structure. Specifically, when the grouping variable was within clusters, LR and HLR performed similarly in terms of Type I error control and power. However, when the grouping variable was between clusters, LR failed to maintain the nominal Type I error rate of .05. HLR was able to maintain this rate. However, power for HLR tended to be low under many conditions in the between cluster variable case. © 2010 by the National Council on Measurement in Education.","['hierarchical', 'logistic', 'regression', 'Accounting', 'multilevel', 'datum', 'DIF', 'detection']","['hierarchical', 'logistic', 'regression', 'accounting', 'multilevel', 'datum', 'dif', 'detection']",hierarchical logistic regression accounting multilevel datum dif detection
Sinharay S.; Haberman S.J.; Lee Y.-H.,When does scale anchoring work? A case study,2011,48,"Providing information to test takers and test score users about the abilities of test takers at different score levels has been a persistent problem in educational and psychological measurement. Scale anchoring, a technique which describes what students at different points on a score scale know and can do, is a tool to provide such information. Scale anchoring for a test involves a substantial amount of work, both by the statistical analysts and test developers involved with the test. In addition, scale anchoring involves considerable use of subjective judgment, so its conclusions may be questionable. We describe statistical procedures that can be used to determine if scale anchoring is likely to be successful for a test. If these procedures indicate that scale anchoring is unlikely to be successful, then there is little reason to perform a detailed scale anchoring study. The procedures are applied to several data sets from a teachers' licensing test. © 2011 by the National Council on Measurement in Education.","['scale', 'anchoring', 'work', 'case', 'study']","['scale', 'anchoring', 'work', 'case', 'study']",scale anchoring work case study
Rijmen F.,"Formal relations and an empirical comparison among the bi-factor, the testlet, and a second-order multidimensional IRT model",2010,47,"Testlet effects can be taken into account by incorporating specific dimensions in addition to the general dimension into the item response theory model. Three such multidimensional models are described: the bi-factor model, the testlet model, and a second-order model. It is shown how the second-order model is formally equivalent to the testlet model. In turn, both models are constrained bi-factor models. Therefore, the efficient full maximum likelihood estimation method that has been established for the bi-factor model can be modified to estimate the parameters of the two other models. An application on a testlet-based international English assessment indicated that the bi-factor model was the preferred model for this particular data set. © 2010 by the National Council on Measurement in Education.","['formal', 'relation', 'empirical', 'comparison', 'bifactor', 'testlet', 'secondorder', 'multidimensional', 'IRT']","['formal', 'relation', 'empirical', 'comparison', 'bifactor', 'testlet', 'secondorder', 'multidimensional', 'irt']",formal relation empirical comparison bifactor testlet secondorder multidimensional irt
Sinharay S.; Haberman S.J.,Equating of augmented subscores,2011,48,"Recently, there has been an increasing level of interest in subscores for their potential diagnostic value. suggested reporting an augmented subscore that is a linear combination of a subscore and the total score. and showed that augmented subscores often lead to more accurate diagnostic information than subscores. In order to report augmented subscores operationally, they should be comparable across the different forms of a test. One way to achieve comparability is to equate them. We suggest several methods for equating augmented subscores. Results from several operational and simulated data sets show that the error in the equating of augmented subscores appears to be small in most practical situations. Copyright © 2011 by the National Council on Measurement in Education.","['equating', 'augmented', 'subscore']","['equating', 'augmented', 'subscore']",equating augmented subscore
Baldwin P.,A Strategy for developing a common metric in item response theory when parameter posterior distributions are known,2011,48,"Growing interest in fully Bayesian item response models begs the question: To what extent can model parameter posterior draws enhance existing practices? One practice that has traditionally relied on model parameter point estimates but may be improved by using posterior draws is the development of a common metric for two independently calibrated test forms. Before parameter estimates from independently calibrated forms can be compared, at least one form's estimates must be adjusted such that both forms share a common metric. Because this adjustment is estimated, there is a propagation of error effect when it is applied. This effect is typically ignored, which leads to overconfidence in the adjusted estimates; yet, when model parameter posterior draws are available, it may be accounted for with a simple sampling strategy. In this paper, it is shown using simulated data that the proposed sampling strategy results in adjusted posteriors with superior coverage properties than those obtained using traditional point-estimate-based methods. © 2011 by the National Council on Measurement in Education.","['strategy', 'develop', 'common', 'metric', 'item', 'response', 'theory', 'parameter', 'posterior', 'distribution', 'know']","['strategy', 'develop', 'common', 'metric', 'item', 'response', 'theory', 'parameter', 'posterior', 'distribution', 'know']",strategy develop common metric item response theory parameter posterior distribution know
Deng H.; Ansley T.; Chang H.-H.,Stratified and maximum information item selection procedures in computer adaptive testing,2010,47,"In this study we evaluated and compared three item selection procedures: the maximum Fisher information procedure (F), the a-stratified multistage computer adaptive testing (CAT) (STR), and a refined stratification procedure that allows more items to be selected from the high a strata and fewer items from the low a strata (USTR), along with completely random item selection (RAN). The comparisons were with respect to error variances, reliability of ability estimates and item usage through CATs simulated under nine test conditions of various practical constraints and item selection space. The results showed that F had an apparent precision advantage over STR and USTR under unconstrained item selection, but with very poor item usage. USTR reduced error variances for STR under various conditions, with small compromises in item usage. Compared to F, USTR enhanced item usage while achieving comparable precision in ability estimates; it achieved a precision level similar to F with improved item usage when items were selected under exposure control and with limited item selection space. The results provide implications for choosing an appropriate item selection procedure in applied settings. © 2010 by the National Council on Measurement in Education.","['stratified', 'maximum', 'information', 'item', 'selection', 'procedure', 'computer', 'adaptive', 'testing']","['stratified', 'maximum', 'information', 'item', 'selection', 'procedure', 'computer', 'adaptive', 'testing']",stratified maximum information item selection procedure computer adaptive testing
Alexeev N.; Templin J.; Cohen A.S.,Spurious latent classes in the mixture Rasch model,2011,48,"Mixture Rasch models have been used to study a number of psychometric issues such as goodness of fit, response strategy differences, strategy shifts, and multidimensionality. Although these models offer the potential for improving understanding of the latent variables being measured, under some conditions overextraction of latent classes may occur, potentially leading to misinterpretation of results. In this study, a mixture Rasch model was applied to data from a statewide test that was initially calibrated to conform to a 3-parameter logistic (3PL) model. Results suggested how latent classes could be explained and also suggested that these latent classes might be due to applying a mixture Rasch model to 3PL data. To support this latter conjecture, a simulation study was presented to demonstrate how data generated to fit a one-class 2-parameter logistic (2PL) model required more than one class when fit with a mixture Rasch model. © 2011 by the National Council on Measurement in Education.","['spurious', 'latent', 'class', 'mixture', 'Rasch']","['spurious', 'latent', 'class', 'mixture', 'rasch']",spurious latent class mixture rasch
Omar M.H.,Statistical process control charts for measuring and monitoring temporal consistency of ratings,2010,47,"Methods of statistical process control were briefly investigated in the field of educational measurement as early as 1999. However, only the use of a cumulative sum chart was explored. In this article other methods of statistical quality control are introduced and explored. In particular, methods in the form of Shewhart mean and standard deviation charts are introduced as techniques for ensuring quality in a measurement process for rating performance items in operational assessments. Several strengths and weaknesses of the procedures are explored with illustrative real and simulated rating data. Further research directions are also suggested. © 2010 by the National Council on Measurement in Education.","['statistical', 'process', 'control', 'chart', 'measure', 'monitor', 'temporal', 'consistency', 'rating']","['statistical', 'process', 'control', 'chart', 'measure', 'monitor', 'temporal', 'consistency', 'rating']",statistical process control chart measure monitor temporal consistency rating
Jiang Y.; von Davier A.A.; Chen H.,Evaluating Equating Results: Percent Relative Error for Chained Kernel Equating,2012,49,"This article presents a method for evaluating equating results. Within the kernel equating framework, the percent relative error (PRE) for chained equipercentile equating was computed under the nonequivalent groups with anchor test (NEAT) design. The method was applied to two data sets to obtain the PRE, which can be used to measure equating effectiveness. The study compared the PRE results for chained and poststratification equating. The results indicated that the chained method transformed the new form score distribution to the reference form scale more effectively than the poststratification method. In addition, the study found that in chained equating, the population weight had impact on score distributions over the target population but not on the equating and PRE results. © 2012 by the National Council on Measurement in Education.","['evaluate', 'Equating', 'Results', 'Percent', 'Relative', 'Error', 'Chained', 'Kernel', 'Equating']","['evaluate', 'equating', 'results', 'percent', 'relative', 'error', 'chained', 'kernel', 'equating']",evaluate equating results percent relative error chained kernel equating
Petscher Y.; Schatschneider C.,A Simulation study on the performance of the simple difference and covariance-adjusted scores in randomized experimental designs,2011,48,"Research by demonstrated that the covariance-adjusted score is more powerful than the simple difference score, yet recent reviews indicate researchers are equally likely to use either score type in two-wave randomized experimental designs. A Monte Carlo simulation was conducted to examine the conditions under which the simple difference and covariance-adjusted scores were more or less powerful to detect treatment effects when relaxing certain assumptions made by Four factors were manipulated in the design including sample size, normality of the pretest and posttest distributions, the correlation between pretest and posttest, and posttest variance. A 5 × 5 × 4 × 3 mostly crossed design was run with 1,000 replications per condition, resulting in 226,000 unique samples. The gain score was nearly as powerful as the covariance-adjusted score when pretest and posttest variances were equal, and as powerful in fan-spread growth conditions; thus, under certain circumstances the gain score could be used in two-wave randomized experimental designs. © 2011 by the National Council on Measurement in Education.","['Simulation', 'study', 'performance', 'simple', 'difference', 'covarianceadjuste', 'score', 'randomize', 'experimental', 'design']","['simulation', 'study', 'performance', 'simple', 'difference', 'covarianceadjuste', 'score', 'randomize', 'experimental', 'design']",simulation study performance simple difference covarianceadjuste score randomize experimental design
Kim S.; Livingston S.A.,Comparisons among small sample equating methods in a common-item design,2010,47,"Score equating based on small samples of examinees is often inaccurate for the examinee populations. We conducted a series of resampling studies to investigate the accuracy of five methods of equating in a common-item design. The methods were chained equipercentile equating of smoothed distributions, chained linear equating, chained mean equating, the symmetric circle-arc method, and the simplified circle-arc method. Four operational test forms, each containing at least 110 items, were used for the equating, with new-form samples of 100, 50, 25, and 10 examinees and reference-form samples three times as large. Accuracy was described in terms of the root-mean-squared difference (over 1,000 replications) of the sample equatings from the criterion equating. Overall, chained mean equating produced the most accurate results for low scores, but the two circle-arc methods produced the most accurate results, particularly in the upper half of the score distribution. The difference in equating accuracy between the two circle-arc methods was negligible. © 2010 by the National Council on Measurement in Education.","['comparison', 'small', 'sample', 'equate', 'method', 'commonitem', 'design']","['comparison', 'small', 'sample', 'equate', 'method', 'commonitem', 'design']",comparison small sample equate method commonitem design
Kahraman N.; Thompson T.,Relating unidimensional IRT parameters to a multidimensional response space: A review of two alternative projection IRT models for scoring subscales,2011,48,"A practical concern for many existing tests is that subscore test lengths are too short to provide reliable and meaningful measurement. A possible method of improving the subscale reliability and validity would be to make use of collateral information provided by items from other subscales of the same test. To this end, the purpose of this article is to compare two different formulations of an alternative Item Response Theory (IRT) model developed to parameterize unidimensional projections of multidimensional test items: Analytical and Empirical formulations. Two real data applications are provided to illustrate how the projection IRT model can be used in practice, as well as to further examine how ability estimates from the projection IRT model compare to external examinee measures. The results suggest that collateral information extracted by a projection IRT model can be used to improve reliability and validity of subscale scores, which in turn can be used to provide diagnostic information about strength and weaknesses of examinees helping stakeholders to link instruction or curriculum to assessment results. Copyright © 2011 by the National Council on Measurement in Education.","['relate', 'unidimensional', 'IRT', 'parameter', 'multidimensional', 'response', 'space', 'review', 'alternative', 'projection', 'IRT', 'score', 'subscale']","['relate', 'unidimensional', 'irt', 'parameter', 'multidimensional', 'response', 'space', 'review', 'alternative', 'projection', 'irt', 'score', 'subscale']",relate unidimensional irt parameter multidimensional response space review alternative projection irt score subscale
Van Der Linden W.J.; Diao Q.,Automated test-form generation,2011,48,"In automated test assembly (ATA), the methodology of mixed-integer programming is used to select test items from an item bank to meet the specifications for a desired test form and optimize its measurement accuracy. The same methodology can be used to automate the formatting of the set of selected items into the actual test form. Three different cases are discussed: (i) computerized test forms in which the items are presented on a screen one at a time and only their optimal order has to be determined; (ii) paper forms in which the items need to be ordered and paginated and the typical goal is to minimize paper use; and (iii) published test forms with the same requirements but a more sophisticated layout (e.g., double-column print). For each case, a menu of possible test-form specifications is identified, and it is shown how they can be modeled as linear constraints using 0-1 decision variables. The methodology is demonstrated using two empirical examples. © 2011 by the National Council on Measurement in Education.","['automate', 'testform', 'generation']","['automate', 'testform', 'generation']",automate testform generation
Robusto E.; Stefanutti L.; Anselmi P.,The gain-loss model: A probabilistic skill multimap model for assessing learning processes,2010,47,"Within the theoretical framework of knowledge space theory, a probabilistic skill multimap model for assessing learning processes is proposed. The learning process of a student is modeled as a function of the student's knowledge and of an educational intervention on the attainment of specific skills required to solve problems in a knowledge domain. Model parameters are initial probabilities of the skills, effects of learning objects on gaining and losing the skills, and careless error and lucky guess probabilities of the problems. An empirical application shows that the model is effective in assessing knowledge and effectiveness of educational intervention at both classroom and student levels. Practical implications for teaching and learning are discussed. © 2010 by the National Council on Measurement in Education.","['gainloss', 'probabilistic', 'skill', 'multimap', 'assess', 'learn', 'process']","['gainloss', 'probabilistic', 'skill', 'multimap', 'assess', 'learn', 'process']",gainloss probabilistic skill multimap assess learn process
Suh Y.; Bolt D.M.,A nested logit approach for investigating distractors as causes of differential item functioning,2011,48,"In multiple-choice items, differential item functioning (DIF) in the correct response may or may not be caused by differentially functioning distractors. Identifying distractors as causes of DIF can provide valuable information for potential item revision or the design of new test items. In this paper, we examine a two-step approach based on application of a nested logit model for this purpose. The approach separates testing of differential distractor functioning (DDF) from DIF, thus allowing for clearer evaluations of where distractors may be responsible for DIF. The approach is contrasted against competing methods and evaluated in simulation and real data analyses. © 2011 by the National Council on Measurement in Education.","['nest', 'logit', 'approach', 'investigate', 'distractor', 'cause', 'differential', 'item', 'function']","['nest', 'logit', 'approach', 'investigate', 'distractor', 'cause', 'differential', 'item', 'function']",nest logit approach investigate distractor cause differential item function
Wang C.; Gierl M.J.,Using the attribute hierarchy method to make diagnostic inferences about examinees' cognitive skills in critical reading,2011,48,"The purpose of this study is to apply the attribute hierarchy method (AHM) to a subset of SAT critical reading items and illustrate how the method can be used to promote cognitive diagnostic inferences. The AHM is a psychometric procedure for classifying examinees' test item responses into a set of attribute mastery patterns associated with different components from a cognitive model. The study was conducted in two steps. In step 1, three cognitive models were developed by reviewing selected literature in reading comprehension as well as research related to SAT Critical Reading. Then, the cognitive models were validated by having a sample of students think aloud as they solved each item. In step 2, psychometric analyses were conducted on the SAT critical reading cognitive models by evaluating the model-data fit between the expected and observed response patterns produced from two random samples of 2,000 examinees who wrote the items. The model that provided best data-model fit was then used to calculate attribute probabilities for 15 examinees to illustrate our diagnostic testing procedure. Copyright © 2011 by the National Council on Measurement in Education.","['attribute', 'hierarchy', 'method', 'diagnostic', 'inference', 'examine', 'cognitive', 'skill', 'critical', 'reading']","['attribute', 'hierarchy', 'method', 'diagnostic', 'inference', 'examine', 'cognitive', 'skill', 'critical', 'reading']",attribute hierarchy method diagnostic inference examine cognitive skill critical reading
Sinharay S.,How often do subscores have added value? Results from operational and simulated data,2010,47,"Recently, there has been an increasing level of interest in subscores for their potential diagnostic value. Haberman suggested a method based on classical test theory to determine whether subscores have added value over total scores. In this article I first provide a rich collection of results regarding when subscores were found to have added value for several operational data sets. Following that I provide results from a detailed simulation study that examines what properties subscores should possess in order to have added value. The results indicate that subscores have to satisfy strict standards of reliability and correlation to have added value. A weighted average of the subscore and the total score was found to have added value more often. © 2010 by the National Council on Measurement in Education.","['subscore', 'add', 'value', 'result', 'operational', 'simulated', 'datum']","['subscore', 'add', 'value', 'result', 'operational', 'simulated', 'datum']",subscore add value result operational simulated datum
de la Torre J.; Hong Y.; Deng W.,Factors affecting the item parameter estimation and classification accuracy of the DINA model,2010,47,"To better understand the statistical properties of the deterministic inputs, noisy "" and"" gate cognitive diagnosis (DINA) model, the impact of several factors on the quality of the item parameter estimates and classification accuracy was investigated. Results of the simulation study indicate that the fully Bayes approach is most accurate when the prior distribution matches the latent class structure. However, when the latent classes are of indefinite structure, the empirical Bayes method in conjunction with an unstructured prior distribution provides much better estimates and classification accuracy. Moreover, using empirical Bayes with an unstructured prior does not lead to extremely poor results as other prior-estimation method combinations do. The simulation results also show that increasing the sample size reduces the variability, and to some extent the bias, of item parameter estimates, whereas lower level of guessing and slip parameter is associated with higher quality item parameter estimation and classification accuracy. © 2010 by the National Council on Measurement in Education.","['factor', 'affect', 'item', 'parameter', 'estimation', 'classification', 'accuracy', 'DINA']","['factor', 'affect', 'item', 'parameter', 'estimation', 'classification', 'accuracy', 'dina']",factor affect item parameter estimation classification accuracy dina
Livingston S.A.; Kim S.,Random-groups equating with samples of 50 to 400 test takers,2010,47,"Five methods for equating in a random groups design were investigated in a series of resampling studies with samples of 400, 200, 100, and 50 test takers. Six operational test forms, each taken by 9,000 or more test takers, were used as item pools to construct pairs of forms to be equated. The criterion equating was the direct equipercentile equating in the group of all test takers. Equating accuracy was indicated by the root-mean-squared deviation, over 1,000 replications, of the sample equatings from the criterion equating. The methods investigated were equipercentile equating of smoothed distributions, linear equating, mean equating, symmetric circle-arc equating, and simplified circle-arc equating. The circle-arc methods produced the most accurate results for all sample sizes investigated, particularly in the upper half of the score distribution. The difference in equating accuracy between the two circle-arc methods was negligible. © 2010 by the National Council on Measurement in Education.","['randomgroup', 'equate', 'sample', '50', '400', 'test', 'taker']","['randomgroup', 'equate', 'sample', '50', '400', 'test', 'taker']",randomgroup equate sample 50 400 test taker
Leckie G.; Baird J.-A.,"Rater effects on essay scoring: A multilevel analysis of severity drift, central tendency, and rater experience",2011,48,"This study examined rater effects on essay scoring in an operational monitoring system from England's 2008 national curriculum English writing test for 14-year-olds. We fitted two multilevel models and analyzed: (1) drift in rater severity effects over time; (2) rater central tendency effects; and (3) differences in rater severity and central tendency effects by raters' previous rating experience. We found no significant evidence of rater drift and, while raters with less experience appeared more severe than raters with more experience, this result also was not significant. However, we did find that there was a central tendency to raters' scoring. We also found that rater severity was significantly unstable over time. We discuss the theoretical and practical questions that our findings raise. © 2011 by the National Council on Measurement in Education.","['Rater', 'effect', 'essay', 'scoring', 'multilevel', 'analysis', 'severity', 'drift', 'central', 'tendency', 'rater', 'experience']","['rater', 'effect', 'essay', 'scoring', 'multilevel', 'analysis', 'severity', 'drift', 'central', 'tendency', 'rater', 'experience']",rater effect essay scoring multilevel analysis severity drift central tendency rater experience
Wang C.; Chang H.-H.; Huebner A.,Restrictive stochastic item selection methods in cognitive diagnostic computerized adaptive testing,2011,48,"This paper proposes two new item selection methods for cognitive diagnostic computerized adaptive testing: the restrictive progressive method and the restrictive threshold method. They are built upon the posterior weighted Kullback-Leibler (KL) information index but include additional stochastic components either in the item selection index or in the item selection procedure. Simulation studies show that both methods are successful at simultaneously suppressing overexposed items and increasing the usage of underexposed items. Compared to item selection based upon (1) pure KL information and (2) the Sympson-Hetter method, the two new methods strike a better balance between item exposure control and measurement accuracy. The two new methods are also compared with progressive method and proportional method. © 2011 by the National Council on Measurement in Education.","['restrictive', 'stochastic', 'item', 'selection', 'method', 'cognitive', 'diagnostic', 'computerized', 'adaptive', 'testing']","['restrictive', 'stochastic', 'item', 'selection', 'method', 'cognitive', 'diagnostic', 'computerized', 'adaptive', 'testing']",restrictive stochastic item selection method cognitive diagnostic computerized adaptive testing
Puhan G.,Futility of log-linear smoothing when equating with unrepresentative small samples,2011,48,"The impact of log-linear presmoothing on the accuracy of small sample chained equipercentile equating was evaluated under two conditions. In the first condition the small samples differed randomly in ability from the target population. In the second condition the small samples were systematically different from the target population. Results showed that equating with small samples (e.g., N < 25 or 50) using either raw or smoothed score distributions led to considerable large random equating error (although smoothing reduced random equating error). Moreover, when the small samples were not representative of the target population, the amount of equating bias also was quite large. It is concluded that although presmoothing can reduce random equating error, it is not likely to reduce equating bias caused by using an unrepresentative sample. Other alternatives to the small sample equating problem (e.g., the SiGNET design) which focus more on improving data collection are discussed. © 2011 by the National Council on Measurement in Education.","['futility', 'loglinear', 'smooth', 'equate', 'unrepresentative', 'small', 'sample']","['futility', 'loglinear', 'smooth', 'equate', 'unrepresentative', 'small', 'sample']",futility loglinear smooth equate unrepresentative small sample
Zwick R.; Himelfarb I.,The effect of high school socioeconomic status on the predictive validity of SAT scores and high school grade-point average,2011,48,"Research has often found that, when high school grades and SAT scores are used to predict first-year college grade-point average (FGPA) via regression analysis, African-American and Latino students, are, on average, predicted to earn higher FGPAs than they actually do. Under various plausible models, this phenomenon can be explained in terms of the unreliability of predictor variables. Attributing overprediction to measurement error, however, is not fully satisfactory: Might the measurement errors in the predictor variables be systematic in part, and could they be reduced? The research hypothesis in the current study was that the overprediction of Latino and African-American performance occurs, at least in part, because these students are more likely than White students to attend high schools with fewer resources. The study provided some support for this hypothesis and showed that the prediction of college grades can be improved using information about high school socioeconomic status. An interesting peripheral finding was that grades provided by students' high schools were stronger predictors of FGPA than were students' self-reported high school grades. Correlations between the two types of high school grades (computed for each of 18 colleges) ranged from .59 to .85. © 2011 by the National Council on Measurement in Education.","['effect', 'high', 'school', 'socioeconomic', 'status', 'predictive', 'validity', 'SAT', 'score', 'high', 'school', 'gradepoint', 'average']","['effect', 'high', 'school', 'socioeconomic', 'status', 'predictive', 'validity', 'sat', 'score', 'high', 'school', 'gradepoint', 'average']",effect high school socioeconomic status predictive validity sat score high school gradepoint average
Sinharay S.; Holland P.W.,A new approach to comparing several equating methods in the context of the NEAT design,2010,47,"The nonequivalent groups with anchor test (NEAT) design involves missing data that are missing by design. Three equating methods that can be used with a NEAT design are the frequency estimation equipercentile equating method, the chain equipercentile equating method, and the item-response-theory observed-score-equating method. We suggest an approach to perform a fair comparison of the three methods. The approach is then applied to compare the three equating methods using three data sets from operational tests. For each data set, we examine how the three equating methods perform when the missing data satisfy the assumptions made by only one of these equating methods. The chain equipercentile equating method is somewhat more satisfactory overall than the other methods. © 2010 by the National Council on Measurement in Education.","['new', 'approach', 'compare', 'equate', 'method', 'context', 'NEAT', 'design']","['new', 'approach', 'compare', 'equate', 'method', 'context', 'neat', 'design']",new approach compare equate method context neat design
Zu J.; Yuan K.-H.,Standard Error of Linear Observed-Score Equating for the NEAT Design With Nonnormally Distributed Data,2012,49,"In the nonequivalent groups with anchor test (NEAT) design, the standard error of linear observed-score equating is commonly estimated by an estimator derived assuming multivariate normality. However, real data are seldom normally distributed, causing this normal estimator to be inconsistent. A general estimator, which does not rely on the normality assumption, would be preferred, because it is asymptotically accurate regardless of the distribution of the data. In this article, an analytical formula for the standard error of linear observed-score equating, which characterizes the effect of nonnormality, is obtained under elliptical distributions. Using three large-scale real data sets as the populations, resampling studies are conducted to empirically evaluate the normal and general estimators of the standard error of linear observed-score equating. The effect of sample size (50, 100, 250, or 500) and equating method (chained linear, Tucker, or Levine observed-score equating) are examined. Results suggest that the general estimator has smaller bias than the normal estimator in all 36 conditions; it has larger standard error when the sample size is at least 100; and it has smaller root mean squared error in all but one condition. An R program is also provided to facilitate the use of the general estimator. © 2012 by the National Council on Measurement in Education.","['Standard', 'Error', 'Linear', 'ObservedScore', 'Equating', 'NEAT', 'Design', 'Nonnormally', 'distribute', 'Data']","['standard', 'error', 'linear', 'observedscore', 'equating', 'neat', 'design', 'nonnormally', 'distribute', 'data']",standard error linear observedscore equating neat design nonnormally distribute data
Moses T.; Holland P.W.,The effects of selection strategies for bivariate loglinear smoothing models on neat equating Functions,2010,47,"In this study, eight statistical strategies were evaluated for selecting the parameterizations of loglinear models for smoothing the bivariate test score distributions used in nonequivalent groups with anchor test (NEAT) equating. Four of the strategies were based on significance tests of chi-square statistics (Likelihood Ratio, Pearson, Freeman-Tukey, and Cressie-Read) and four additional strategies were based on different evaluations of the Likelihood Ratio Chi-Square statistic (Akaike Information Criterion, Bayesian Information Criterion, Consistent Akaike Information Criterion, and an index traced to Goodman). The focus was the implications of the selection strategies' selection tendencies for the accuracy of chained and poststratification equating functions. The results differentiated the strategies in terms of their tendencies to select models with particular bivariate parameterizations and the implications of these tendencies for equating bias and variability. © 2010 by the National Council on Measurement in Education.","['effect', 'selection', 'strategy', 'bivariate', 'loglinear', 'smooth', 'neat', 'equate', 'function']","['effect', 'selection', 'strategy', 'bivariate', 'loglinear', 'smooth', 'neat', 'equate', 'function']",effect selection strategy bivariate loglinear smooth neat equate function
Yao L.,Reporting valid and reliable overall scores and domain scores,2010,47,"In educational assessment, overall scores obtained by simply averaging a number of domain scores are sometimes reported. However, simply averaging the domain scores ignores the fact that different domains have different score points, that scores from those domains are related, and that at different score points the relationship between overall score and domain score may be different. To report reliable and valid overall scores and domain scores, I investigated the performance of four methods using both real and simulation data: (a) the unidimensional IRT model; (b) the higher-order IRT model, which simultaneously estimates the overall ability and domain abilities; (c) the multidimensional IRT (MIRT) model, which estimates domain abilities and uses the maximum information method to obtain the overall ability; and (d) the bifactor general model. My findings suggest that the MIRT model not only provides reliable domain scores, but also produces reliable overall scores. The overall score from the MIRT maximum information method has the smallest standard error of measurement. In addition, unlike the other models, there is no linear relationship assumed between overall score and domain scores. Recommendations for sizes of correlations between domains and the number of items needed for reporting purposes are provided. © 2010 by the National Council on Measurement in EducationNo claim to original US government works.","['report', 'valid', 'reliable', 'overall', 'score', 'domain', 'score']","['report', 'valid', 'reliable', 'overall', 'score', 'domain', 'score']",report valid reliable overall score domain score
Kim S.; Walker M.E.; McHale F.,Investigating the effectiveness of equating designs for constructed-response tests in large-scale assessments,2010,47,"Using data from a large-scale exam, in this study we compared various designs for equating constructed-response (CR) tests to determine which design was most effective in producing equivalent scores across the two tests to be equated. In the context of classical equating methods, four linking designs were examined: (a) an anchor set containing common CR items, (b) an anchor set incorporating common CR items rescored, (c) an external multiple-choice (MC) anchor test, and (d) an equivalent groups design incorporating rescored CR items (no anchor test). The use of CR items without rescoring resulted in much larger bias than the other designs. The use of an external MC anchor resulted in the next largest bias. The use of a rescored CR anchor and the equivalent groups design led to similar levels of equating error. © 2010 by the National Council on Measurement in Education.","['investigate', 'effectiveness', 'equate', 'design', 'constructedresponse', 'test', 'largescale', 'assessment']","['investigate', 'effectiveness', 'equate', 'design', 'constructedresponse', 'test', 'largescale', 'assessment']",investigate effectiveness equate design constructedresponse test largescale assessment
Paek I.,A Note on Three Statistical Tests in the Logistic Regression DIF Procedure,2012,49,"Although logistic regression became one of the well-known methods in detecting differential item functioning (DIF), its three statistical tests, the Wald, likelihood ratio (LR), and score tests, which are readily available under the maximum likelihood, do not seem to be consistently distinguished in DIF literature. This paper provides a clarifying note on those three tests when logistic regression is applied for DIF detection. © 2012 by the National Council on Measurement in Education.","['note', 'statistical', 'Tests', 'Logistic', 'Regression', 'dif', 'procedure']","['note', 'statistical', 'tests', 'logistic', 'regression', 'dif', 'procedure']",note statistical tests logistic regression dif procedure
Puhan G.,A comparison of chained linear and poststratification linear equating under different testing conditions,2010,47,"In this study I compared results of chained linear, Tucker, and Levine-observed score equatings under conditions where the new and old forms samples were similar in ability and also when they were different in ability. The length of the anchor test was also varied to examine its effect on the three different equating methods. The three equating methods were compared to a criterion equating to obtain estimates of random equating error, bias, and root mean squared error (RMSE). Results showed that, for most studied conditions, chained linear equating produced fairly good equating results in terms of low bias and RMSE. Levine equating also produced low bias and RMSE in some conditions. Although the Tucker method always produced the lowest random equating error, it produced a larger bias and RMSE than either of the other equating methods. As noted in the literature, these results also suggest that either chained linear or Levine equating be used when new and old form samples differ on ability and/or when the anchor-to-total correlation is not very high. Finally, by testing the missing data assumptions of the three equating methods, this study also shows empirically why an equating method is more or less accurate under certain conditions. © 2010 by the National Council on Measurement in Education.","['comparison', 'chain', 'linear', 'poststratification', 'linear', 'equate', 'different', 'testing', 'condition']","['comparison', 'chain', 'linear', 'poststratification', 'linear', 'equate', 'different', 'testing', 'condition']",comparison chain linear poststratification linear equate different testing condition
Rutkowski L.,The impact of missing background data on subpopulation estimation,2011,48,"Although population modeling methods are well established, a paucity of literature appears to exist regarding the effect of missing background data on subpopulation achievement estimates. Using simulated data that follows typical large-scale assessment designs with known parameters and a number of missing conditions, this paper examines the extent to which missing background data impacts subpopulation achievement estimates. In particular, the paper compares achievement estimates under a model with fully observed background data to achievement estimates for a variety of missing background data conditions. The findings suggest that sub-population differences are preserved under all analyzed conditions while point estimates for subpopulation achievement values are influenced by missing at random conditions. Implications for cross-population comparisons are discussed. © 2011 by the National Council on Measurement in Education.","['impact', 'miss', 'background', 'datum', 'subpopulation', 'estimation']","['impact', 'miss', 'background', 'datum', 'subpopulation', 'estimation']",impact miss background datum subpopulation estimation
Van der Linden W.J.,Test design and speededness,2011,48,"A critical component of test speededness is the distribution of the test taker's total time on the test. A simple set of constraints on the item parameters in the lognormal model for response times is derived that can be used to control the distribution when assembling a new test form. As the constraints are linear in the item parameters, they can easily be included in a mixed integer programming model for test assembly. The use of the constraints is demonstrated for the problems of assembling a new test form to be equally speeded as a reference form, test assembly in which the impact of a change in the content specifications on speededness is to be neutralized, and the assembly of test forms with a revised level of speededness. © 2011 by the National Council on Measurement in Education.","['test', 'design', 'speededness']","['test', 'design', 'speededness']",test design speededness
Lee W.-C.,Classification consistency and accuracy for complex assessments using item response theory,2010,47,"In this article, procedures are described for estimating single-administration classification consistency and accuracy indices for complex assessments using item response theory (IRT). This IRT approach was applied to real test data comprising dichotomous and polytomous items. Several different IRT model combinations were considered. Comparisons were also made between the IRT approach and two non-IRT approaches including the Livingston-Lewis and compound multinomial procedures. Results for various IRT model combinations were not substantially different. The estimated classification consistency and accuracy indices for the non-IRT procedures were almost always lower than those for the IRT procedures. © 2010 by the National Council on Measurement in Education.","['classification', 'consistency', 'accuracy', 'complex', 'assessment', 'item', 'response', 'theory']","['classification', 'consistency', 'accuracy', 'complex', 'assessment', 'item', 'response', 'theory']",classification consistency accuracy complex assessment item response theory
Dorans N.J.; Middleton K.,Addressing the Extreme Assumptions of Presumed Linkings,2012,49,"The interpretability of score comparisons depends on the design and execution of a sound data collection plan and the establishment of linkings between these scores. When comparisons are made between scores from two or more assessments that are built to different specifications and are administered to different populations under different conditions, the validity of the comparisons hinges on untestable assumptions. For example, tests administered across different disability groups or tests administered to different language groups produce scores for which implicit linkings are presumed to hold. Presumed linking makes use of extreme assumptions to produce links between scores on tests in the absence of common test material or equivalent groups of test takers. These presumed linkings lead to dubious interpretations. This article suggests an approach that indirectly assesses the validity of these presumed linkings among scores on assessments that contain neither equivalent groups nor common anchor material. © 2012 by the National Council on Measurement in Education.","['address', 'Extreme', 'Assumptions', 'Presumed', 'linking']","['address', 'extreme', 'assumptions', 'presumed', 'linking']",address extreme assumptions presumed linking
Kunina-Habenicht O.; Rupp A.A.; Wilhelm O.,The Impact of Model Misspecification on Parameter Estimation and Item-Fit Assessment in Log-Linear Diagnostic Classification Models,2012,49,"Using a complex simulation study we investigated parameter recovery, classification accuracy, and performance of two item-fit statistics for correct and misspecified diagnostic classification models within a log-linear modeling framework. The basic manipulated test design factors included the number of respondents (1,000 vs. 10,000), attributes (3 vs. 5), and items (25 vs. 50) as well as different attribute correlations (50 vs. 80) and marginal attribute difficulties (equal vs. different). We investigated misspecifications of interaction effect parameters under correct Q-matrix specification and two types of Q-matrix misspecification. While the misspecification of interaction effects had little impact on classification accuracy, invalid Q-matrix specifications led to notably decreased classification accuracy. Two proposed item-fit indexes were more strongly sensitive to overspecification of Q-matrix entries for items than to underspecification. Information-based fit indexes AIC and BIC were sensitive to both over- and underspecification. © 2012 by the National Council on Measurement in Education.","['Impact', 'Misspecification', 'Parameter', 'Estimation', 'ItemFit', 'Assessment', 'LogLinear', 'Diagnostic', 'Classification', 'Models']","['impact', 'misspecification', 'parameter', 'estimation', 'itemfit', 'assessment', 'loglinear', 'diagnostic', 'classification', 'models']",impact misspecification parameter estimation itemfit assessment loglinear diagnostic classification models
Frederickx S.; Tuerlinckx F.; De Boeck P.; Magis D.,RIM: A Random Item Mixture Model to Detect Differential Item Functioning,2010,47,"In this paper we present a new methodology for detecting differential item functioning (DIF). We introduce a DIF model, called the random item mixture (RIM), that is based on a Rasch model with random item difficulties (besides the common random person abilities). In addition, a mixture model is assumed for the item difficulties such that the items may belong to one of two classes: a DIF or a non-DIF class. The crucial difference between the DIF class and the non-DIF class is that the item difficulties in the DIF class may differ according to the observed person groups while they are equal across the person groups for the items from the non-DIF class. Statistical inference for the RIM is carried out in a Bayesian framework. The performance of the RIM is evaluated using a simulation study in which it is compared with traditional procedures, like the likelihood ratio test, the Mantel-Haenszel procedure and the standardized p -DIF procedure. In this comparison, the RIM performs better than the other methods. Finally, the usefulness of the model is also demonstrated on a real life data set. © 2010 by the National Council on Measurement in Education.","['RIM', 'A', 'Random', 'Item', 'Mixture', 'detect', 'Differential', 'Item', 'Functioning']","['rim', 'a', 'random', 'item', 'mixture', 'detect', 'differential', 'item', 'functioning']",rim a random item mixture detect differential item functioning
van der Linden W.J.,Linking response-time parameters onto a common scale,2010,47,"Although response times on test items are recorded on a natural scale, the scale for some of the parameters in the lognormal response-time model (van der Linden, 2006) is not fixed. As a result, when the model is used to periodically calibrate new items in a testing program, the parameter are not automatically mapped onto a common scale. Several combinations of linking designs and procedures for the lognormal model are examined that do map parameter estimates onto a common scale. For each of the designs, the standard error of linking is derived. The results are illustrated using examples with simulated data. © 2010 by the National Council on Measurement in Education.","['link', 'responsetime', 'parameter', 'common', 'scale']","['link', 'responsetime', 'parameter', 'common', 'scale']",link responsetime parameter common scale
Zu J.; Liu J.,Observed Score Equating Using Discrete and Passage-Based Anchor Items,2010,47,"Equating of tests composed of both discrete and passage-based multiple choice items using the nonequivalent groups with anchor test design is popular in practice. In this study, we compared the effect of discrete and passage-based anchor items on observed score equating via simulation. Results suggested that an anchor with a larger proportion of passage-based items, more items in each passage, and/or a larger degree of local dependence among items within one passage produces larger equating errors, especially when the groups taking the new form and the reference form differ in ability. Our findings challenge the common belief that an anchor should be a miniature version of the tests to be equated. Suggestions to practitioners regarding anchor design are also given. © 2010 by the National Council on Measurement in Education.","['observe', 'Score', 'Equating', 'Discrete', 'PassageBased', 'Anchor', 'Items']","['observe', 'score', 'equating', 'discrete', 'passagebased', 'anchor', 'items']",observe score equating discrete passagebased anchor items
Attali Y.,An Analysis of Variance Approach for the Estimation of Response Time Distributions in Tests,2010,47,"Generalizability theory and analysis of variance methods are employed, together with the concept of objective time pressure, to estimate response time distributions and the degree of time pressure in timed tests. By estimating response time variance components due to person, item, and their interaction, and fixed effects due to item types and examinee time pressure, one can predict the distribution (mean and variance) of total response time for a population of examinees and a particular time limit. Furthermore, these variance components and fixed effects can be used in a simulation approach to estimate the distributions of time pressure during the test to help test developers evaluate the appropriateness of specific time limits. I present theoretical considerations and empirical results from two tests. © 2010 by the National Council on Measurement in Education.","['Analysis', 'Variance', 'Approach', 'Estimation', 'Response', 'Time', 'Distributions', 'test']","['analysis', 'variance', 'approach', 'estimation', 'response', 'time', 'distributions', 'test']",analysis variance approach estimation response time distributions test
Penfield R.D.,Distinguishing between net and global DIF in polytomous items,2010,47,"In this article, I address two competing conceptions of differential item functioning (DIF) in polytomously scored items. The first conception, referred to as net DIF, concerns between-group differences in the conditional expected value of the polytomous response variable. The second conception, referred to as global DIF, concerns the conditional dependence of group membership and the polytomous response variable. The distinction between net and global DIF is important because different DIF evaluation methods are appropriate for net and global DIF; no currently available method is universally the best for detecting both net and global DIF. Net and global DIF definitions are presented under two different, yet compatible, modeling frameworks: a traditional item response theory (IRT) framework, and a differential step functioning (DSF) framework. The theoretical relationship between the IRT and DSF frameworks is presented. Available methods for evaluating net and global DIF are described, and an applied example of net and global DIF is presented. © 2010 by the National Council on Measurement in Education.","['distinguish', 'net', 'global', 'DIF', 'polytomous', 'item']","['distinguish', 'net', 'global', 'dif', 'polytomous', 'item']",distinguish net global dif polytomous item
Liu J.; Sinharay S.; Holland P.W.; Curley E.; Feigenbaum M.,Test score equating using a mini-version anchor and a midi anchor: A case study using SAT® data,2011,48,"This study explores an anchor that is different from the traditional miniature anchor in test score equating. In contrast to a traditional ""mini"" anchor that has the same spread of item difficulties as the tests to be equated, the studied anchor, referred to as a ""midi"" anchor (Sinharay & Holland), has a smaller spread of item difficulties than the tests to be equated. Both anchors were administered in an operational SAT administration and the impact of anchor type on equating was evaluated with respect to systematic error or equating bias. Contradicting the popular belief that the mini anchor is best, the results showed that the mini anchor does not always produce more accurate equating functions than the midi anchor; the midi anchor was found to perform as well as or even better than the mini anchor. Because testing programs usually have more middle difficulty items and few very hard or very easy items, midi external anchors are operationally easier to build. Therefore, the results of our study provide evidence in favor of the midi anchor, the use of which will lead to cost saving with no reduction in equating quality. © 2011 by the National Council on Measurement in Education.","['test', 'score', 'equating', 'miniversion', 'anchor', 'midi', 'anchor', 'case', 'study', 'SAT', '®', 'datum']","['test', 'score', 'equating', 'miniversion', 'anchor', 'midi', 'anchor', 'case', 'study', 'sat', '®', 'datum']",test score equating miniversion anchor midi anchor case study sat ® datum
Seo M.; Roussos L.A.,Formulation of a DIMTEST Effect Size Measure (DESM) and Evaluation of the DESM Estimator Bias,2010,47,"DIMTEST is a widely used and studied method for testing the hypothesis of test unidimensionality as represented by local item independence. However, DIMTEST does not report the amount of multidimensionality that exists in data when rejecting its null. To provide more information regarding the degree to which data depart from unidimensionality, a DIMTEST-based Effect Size Measure (DESM) was formulated. In addition to detailing the development of the DESM estimate, the current study describes the theoretical formulation of a DESM parameter. To evaluate the efficacy of the DESM estimator according to test length, sample size, and correlations between dimensions, Monte Carlo simulations were conducted. The results of the simulation study indicated that the DESM estimator converged to its parameter as test length increased, and, as desired, its expected value did not increase with sample size (unlike the DIMTEST statistic in the case of multidimensionality). Also as desired, the standard error of DESM decreased as sample size increased. © 2010 by the National Council on Measurement in Education.","['formulation', 'dimtest', 'Effect', 'Size', 'measure', 'DESM', 'Evaluation', 'DESM', 'Estimator', 'Bias']","['formulation', 'dimtest', 'effect', 'size', 'measure', 'desm', 'evaluation', 'desm', 'estimator', 'bias']",formulation dimtest effect size measure desm evaluation desm estimator bias
DeCarlo L.T.; Kim Y.; Johnson M.S.,"A hierarchical rater model for constructed responses, with a signal detection rater model",2011,48,"The hierarchical rater model (HRM) re-cognizes the hierarchical structure of data that arises when raters score constructed response items. In this approach, raters' scores are not viewed as being direct indicators of examinee proficiency but rather as indicators of essay quality; the (latent categorical) quality of an examinee's essay in turn serves as an indicator of the examinee's proficiency, thus yielding a hierarchical structure. Here it is shown that a latent class model motivated by signal detection theory (SDT) is a natural candidate for the first level of the HRM, the rater model. The latent class SDT model provides measures of rater precision and various rater effects, above and beyond simply severity or leniency. The HRM-SDT model is applied to data from a large-scale assessment and is shown to provide a useful summary of various aspects of the raters' performance. © 2011 by the National Council on Measurement in Education.","['hierarchical', 'rater', 'construct', 'response', 'signal', 'detection', 'rater']","['hierarchical', 'rater', 'construct', 'response', 'signal', 'detection', 'rater']",hierarchical rater construct response signal detection rater
Mislevy J.L.; Rupp A.A.; Harring J.R.,Detecting Local Item Dependence in Polytomous Adaptive Data,2012,49,"A rapidly expanding arena for item response theory (IRT) is in attitudinal and health-outcomes survey applications, often with polytomous items. In particular, there is interest in computer adaptive testing (CAT). Meeting model assumptions is necessary to realize the benefits of IRT in this setting, however. Although initial investigations of local item dependence have been studied both for polytomous items in fixed-form settings and for dichotomous items in CAT settings, there have been no publications applying local item dependence detection methodology to polytomous items in CAT despite its central importance to these applications. The current research uses a simulation study to investigate the extension of widely used pairwise statistics, Yen's Q 3 Statistic and Pearson's Statistic X 2, in this context. The simulation design and results are contextualized throughout with a real item bank of this type from the Patient-Reported Outcomes Measurement Information System (PROMIS). © 2012 by the National Council on Measurement in Education.","['detect', 'Local', 'Item', 'Dependence', 'Polytomous', 'Adaptive', 'Data']","['detect', 'local', 'item', 'dependence', 'polytomous', 'adaptive', 'data']",detect local item dependence polytomous adaptive data
Li D.; Jiang Y.; von Davier A.A.,The Accuracy and Consistency of a Series of IRT True Score Equatings,2012,49,"This study investigates a sequence of item response theory (IRT) true score equatings based on various scale transformation approaches and evaluates equating accuracy and consistency over time. The results show that the biases and sample variances for the IRT true score equating (both direct and indirect) are quite small (except for the mean/sigma method). The biases and sample variances for the equating functions based on the characteristic curve methods and concurrent calibrations for adjacent forms are smaller than the biases and variances for the equating functions based on the moment methods. In addition, the IRT true score equating is also compared to the chained equipercentile equating, and we observe that the sample variances for the chained equipercentile equating are much smaller than the variances for the IRT true score equating with an exception at the low scores. © 2012 by the National Council on Measurement in Education.","['Accuracy', 'Consistency', 'Series', 'IRT', 'true', 'Score', 'equating']","['accuracy', 'consistency', 'series', 'irt', 'true', 'score', 'equating']",accuracy consistency series irt true score equating
Zhu X.; Stone C.A.,Assessing fit of unidimensional graded response models using bayesian methods,2011,48,"The posterior predictive model checking method is a flexible Bayesian model-checking tool and has recently been used to assess fit of dichotomous IRT models. This paper extended previous research to polytomous IRT models. A simulation study was conducted to explore the performance of posterior predictive model checking in evaluating different aspects of fit for unidimensional graded response models. A variety of discrepancy measures (test-level, item-level, and pair-wise measures) that reflected different threats to applications of graded IRT models to performance assessments were considered. Results showed that posterior predictive model checking exhibited adequate power in detecting different aspects of misfit for graded IRT models when appropriate discrepancy measures were used. Pair-wise measures were found more powerful in detecting violations of the unidimensionality and local independence assumptions. © 2011 by the National Council on Measurement in Education.","['assess', 'fit', 'unidimensional', 'grade', 'response', 'bayesian', 'method']","['assess', 'fit', 'unidimensional', 'grade', 'response', 'bayesian', 'method']",assess fit unidimensional grade response bayesian method
Chon K.H.; Lee W.-C.; Dunbar S.B.,A comparison of item fit statistics for mixed IRT models,2010,47,"In this study we examined procedures for assessing model-data fit of item response theory (IRT) models for mixed format data. The model fit indices used in this study include PARSCALE's G2, Orlando and Thissen's S - X2 and S - G2, and Stone's χ2* and G2*. To investigate the relative performance of the fit statistics at the item level, we conducted two simulation studies: Type I error and power studies. We evaluated the performance of the item fit indices for various conditions of test length, sample size, and IRT models. Among the competing measures, the summed score-based indices S - X2 and S - G2 were found to be the sensible and efficient choice for assessing model fit for mixed format data. These indices performed well, particularly with short tests. The pseudo-observed score indices, χ2* and G2*, showed inflated Type I error rates in some simulation conditions. Consistent with the findings of current literature, the PARSCALE's G2 index was rarely useful, although it provided reasonable results for long tests. © 2010 by the National Council on Measurement in Education.","['comparison', 'item', 'fit', 'statistic', 'mixed', 'IRT']","['comparison', 'item', 'fit', 'statistic', 'mixed', 'irt']",comparison item fit statistic mixed irt
de la Torre J.; Lee Y.-S.,A note on the invariance of the dina model parameters,2010,47,"Cognitive diagnosis models (CDMs), as alternative approaches to unidimensional item response models, have received increasing attention in recent years. CDMs are developed for the purpose of identifying the mastery or nonmastery of multiple fine-grained attributes or skills required for solving problems in a domain. For CDMs to receive wider use, researchers and practitioners need to understand the basic properties of these models. The article focuses on one CDM, the deterministic inputs, noisy "" and"" gate (DINA) model, and the invariance property of its parameters. Using simulated data involving different attribute distributions, the article demonstrates that the DINA model parameters are absolutely invariant when the model perfectly fits the data. An additional example involving different ability groups illustrates how noise in real data can contribute to the lack of invariance in these parameters. Some practical implications of these findings are discussed. © 2010 by the National Council on Measurement in Education.","['note', 'invariance', 'dina', 'parameter']","['note', 'invariance', 'dina', 'parameter']",note invariance dina parameter
Kane M.,The errors of our ways,2011,48,"Errors don't exist in our data, but they serve a vital function. Reality is complicated, but our models need to be simple in order to be manageable. We assume that attributes are invariant over some conditions of observation, and once we do that we need some way of accounting for the variability in observed scores over these conditions of observation. We relegate this inconvenient variability to errors of measurement. The seriousness of errors of measurement depends on the intended interpretations and uses of the scores and the context in which they are used. Errors are too large if they interfere with the intended interpretations and uses, and otherwise are acceptable. The errors of measurement have to be small compared to the tolerance for error, and errors that are too large have to be controlled in some way. We have several ways of doing this. We can redefine the attribute of interest, we can standardize the assessments and leave the attribute alone, and/or we can sample the relevant performance domain more thoroughly. It is particularly important to control the larger sources of error. If a source of error (systematic or random) is small compared to the dominant sources of error for a testing procedure, it can generally be ignored. © 2011 by the National Council on Measurement in Education.","['error', 'way']","['error', 'way']",error way
Kim S.; Walker M.E.; McHale F.,Comparisons among designs for equating mixed-format tests in large-scale assessments,2010,47,"In this study we examined variations of the nonequivalent groups equating design for tests containing both multiple-choice (MC) and constructed-response (CR) items to determine which design was most effective in producing equivalent scores across the two tests to be equated. Using data from a large-scale exam, this study investigated the use of anchor CR item rescoring (known as trend scoring) in the context of classical equating methods. Four linking designs were examined: an anchor with only MC items, a mixed-format anchor test containing both MC and CR items; a mixed-format anchor test incorporating common CR item rescoring; and an equivalent groups (EG) design with CR item rescoring, thereby avoiding the need for an anchor test. Designs using either MC items alone or a mixed anchor without CR item rescoring resulted in much larger bias than the other two designs. The EG design with trend scoring resulted in the smallest bias, leading to the smallest root mean squared error value. © 2010 by the National Council on Measurement in Education.","['comparison', 'design', 'equate', 'mixedformat', 'test', 'largescale', 'assessment']","['comparison', 'design', 'equate', 'mixedformat', 'test', 'largescale', 'assessment']",comparison design equate mixedformat test largescale assessment
Wang W.-C.; Wu S.-L.,The random-effect generalized rating scale model,2011,48,"Rating scale items have been widely used in educational and psychological tests. These items require people to make subjective judgments, and these subjective judgments usually involve randomness. To account for this randomness, Wang, Wilson, and Shih proposed the random-effect rating scale model in which the threshold parameters are treated as random effects rather than fixed effects. In the present study, the Wang et al. model was further extended to incorporate slope parameters and embed the new model within the framework of multilevel nonlinear mixed-effect models. This was done so that (1) no efforts are needed to derive parameter estimation procedures, and (2) existing computer programs can be applied directly. A brief simulation study was conducted to ascertain parameter recovery using the SAS NLMIXED procedure. An empirical example regarding students' interest in learning science is presented to demonstrate the implications and applications of the new model. © 2011 by the National Council on Measurement in Education.","['randomeffect', 'generalize', 'rating', 'scale']","['randomeffect', 'generalize', 'rating', 'scale']",randomeffect generalize rating scale
Liu J.; Dorans N.J.,Assessing the Practical Equivalence of Conversions When Measurement Conditions Change,2012,49,"At times, the same set of test questions is administered under different measurement conditions that might affect the psychometric properties of the test scores enough to warrant different score conversions for the different conditions. We propose a procedure for assessing the practical equivalence of conversions developed for the same set of test questions but administered under different measurement conditions. This procedure assesses whether the use of separate conversions for each condition has a desirable or undesirable effect. We distinguish effects due to differences in difficulty from effects due to rounding conventions. The proposed procedure provides objective empirical information that assists in deciding to report a common conversion for a set of items or a different conversion for the set of items when the set is administered under different measurement conditions. To illustrate the use of the procedure, we consider the case where a scrambled test form is used along with a base test form. If section order effects are detected between the scrambled and base forms, a decision needs to be made whether to report a single common conversion for both forms or to report separate conversions. © 2012 by the National Council on Measurement in Education.","['assess', 'Practical', 'Equivalence', 'Conversions', 'Conditions', 'Change']","['assess', 'practical', 'equivalence', 'conversions', 'conditions', 'change']",assess practical equivalence conversions conditions change
