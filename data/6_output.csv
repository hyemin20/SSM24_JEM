Authors,Title,Year,Volume,Abstract,Title_spell,Abstract_spell,A_tokens,T_tokens,A_tokens_join,T_tokens_join,lda_0,lda_1,lda_2,lda_3,lda_4,nmf_0,nmf_1,nmf_2,nmf_3,nmf_4
Wesolowski B.C.,Predicting Operational Rater-Type Classifications Using Rasch Measurement Theory and Random Forests: A Music Performance Assessment Perspective,2019,56,"The purpose of this study was to build a Random Forest supervised machine learning model in order to predict musical rater-type classifications based upon a Rasch analysis of raters’ differential severity/leniency related to item use. Raw scores (N = 1,704) from 142 raters across nine high school solo and ensemble festivals (grades 9–12) were collected using a 29-item Likert-type rating scale embedded within five domains (tone/intonation, n = 6; balance, n = 5; interpretation, n = 6; rhythm, n = 6; and technical accuracy, n = 6). Data were analyzed using a Many Facets Rasch Partial Credit Model. An a priori k-means cluster analysis of 29 differential rater functioning indices produced a discrete feature vector that classified raters into one of three distinct rater-types: (a) syntactical rater-type, (b) expressive rater-type, or (c) mental representation rater-type. Results of the initial Random Forest model resulted in an out-of-bag error rate of 5.05%, indicating that approximately 95% of the raters were correctly classified. After tuning a set of three hyperparameters (ntree, mtry, and node size), the optimized model demonstrated an improved out-of-bag error rate of 2.02%. Implications for improvements in assessment, research, and rater training in the field of music education are discussed. © 2019 by the National Council on Measurement in Education",Predicting Operational Rater-Type Classifications Using Rasch Measurement Theory and Random Forests: A Music Performance Assessment Perspective,"The purpose of this study was to build a Random Forest supervised machine learning model in order to predict musical rater-type classifications based upon a Rasch analysis of raters’ differential severity/leniency related to item use. Raw scores (N = 1,704) from 142 raters across nine high school solo and ensemble festivals (grades 9–12) were collected using a 29-item Likert-type rating scale embedded within five domains (tone/intonation, n = 6; balance, n = 5; interpretation, n = 6; rhythm, n = 6; and technical accuracy, n = 6). Data were analyzed using a Many Facets Rasch Partial Credit Model. An a priori k-means cluster analysis of 29 differential rater functioning indices produced a discrete feature vector that classified raters into one of three distinct rater-types: (a) syntactical rater-type, (b) expressive rater-type, or (c) mental representation rater-type. Results of the initial Random Forest model resulted in an out-of-bag error rate of 5.05%, indicating that approximately 95% of the raters were correctly classified. After tuning a set of three hyperparameters (ntree, mtry, and node size), the optimized model demonstrated an improved out-of-bag error rate of 2.02%. Implications for improvements in assessment, research, and rater training in the field of music education are discussed. © 2019 by the National Council on Measurement in Education","['purpose', 'study', 'build', 'Random', 'Forest', 'supervise', 'machine', 'learn', 'order', 'predict', 'musical', 'ratertype', 'classification', 'base', 'Rasch', 'analysis', 'rater', '’', 'differential', 'severityleniency', 'relate', 'item', 'raw', 'score', 'n', '1704', '142', 'rater', 'high', 'school', 'solo', 'ensemble', 'festival', 'grade', '9–12', 'collect', '29item', 'Likerttype', 'rating', 'scale', 'embed', 'domain', 'toneintonation', 'n', '6', 'balance', 'n', '5', 'interpretation', 'n', '6', 'rhythm', 'n', '6', 'technical', 'accuracy', 'n', '6', 'Data', 'analyze', 'facet', 'Rasch', 'Partial', 'Credit', 'An', 'priori', 'kmeans', 'cluster', 'analysis', '29', 'differential', 'rater', 'function', 'index', 'produce', 'discrete', 'feature', 'vector', 'classify', 'rater', 'distinct', 'ratertype', 'syntactical', 'ratertype', 'b', 'expressive', 'ratertype', 'c', 'mental', 'representation', 'ratertype', 'result', 'initial', 'Random', 'Forest', 'result', 'outofbag', 'error', 'rate', '505', 'indicate', 'approximately', '95', 'rater', 'correctly', 'classify', 'tune', 'set', 'hyperparameter', 'ntree', 'mtry', 'node', 'size', 'optimize', 'demonstrate', 'improve', 'outofbag', 'error', 'rate', '202', 'Implications', 'improvement', 'assessment', 'research', 'rater', 'training', 'field', 'music', 'discuss', '©', '2019', 'National', 'Council']","['predict', 'Operational', 'RaterType', 'Classifications', 'Rasch', 'Theory', 'Random', 'Forests', 'Music', 'Performance', 'Assessment', 'Perspective']",purpose study build Random Forest supervise machine learn order predict musical ratertype classification base Rasch analysis rater ’ differential severityleniency relate item raw score n 1704 142 rater high school solo ensemble festival grade 9–12 collect 29item Likerttype rating scale embed domain toneintonation n 6 balance n 5 interpretation n 6 rhythm n 6 technical accuracy n 6 Data analyze facet Rasch Partial Credit An priori kmeans cluster analysis 29 differential rater function index produce discrete feature vector classify rater distinct ratertype syntactical ratertype b expressive ratertype c mental representation ratertype result initial Random Forest result outofbag error rate 505 indicate approximately 95 rater correctly classify tune set hyperparameter ntree mtry node size optimize demonstrate improve outofbag error rate 202 Implications improvement assessment research rater training field music discuss © 2019 National Council,predict Operational RaterType Classifications Rasch Theory Random Forests Music Performance Assessment Perspective,0.023117630102950343,0.9072958340858079,0.023209687212535135,0.022791571736558075,0.02358527686214825,0.0019100624298991405,0.0038481400996120967,0.010295227055072872,0.0027695894937798697,0.13850149220892302
Wang S.; Lin H.; Chang H.-H.; Douglas J.,Hybrid Computerized Adaptive Testing: From Group Sequential Design to Fully Sequential Design,2016,53,"Computerized adaptive testing (CAT) and multistage testing (MST) have become two of the most popular modes in large-scale computer-based sequential testing.  Though most designs of CAT and MST exhibit strength and weakness in recent large-scale implementations, there is no simple answer to the question of which design is better because different modes may fit different practical situations. This article proposes a hybrid adaptive framework to combine both CAT and MST, inspired by an analysis of the history of CAT and MST. The proposed procedure is a design which transitions from a group sequential design to a fully sequential design. This allows for the robustness of MST in early stages, but also shares the advantages of CAT in later stages with fine tuning of the ability estimator once its neighborhood has been identified. Simulation results showed that hybrid designs following our proposed principles provided comparable or even better estimation accuracy and efficiency than standard CAT and MST designs, especially for examinees at the two ends of the ability range. © 2016 by the National Council on Measurement in Education.",Hybrid Computerized Adaptive Testing: From Group Sequential Design to Fully Sequential Design,"Computerized adaptive testing (CAT) and multistage testing (MST) have become two of the most popular modes in large-scale computer-based sequential testing.  Though most designs of CAT and MST exhibit strength and weakness in recent large-scale implementations, there is no simple answer to the question of which design is better because different modes may fit different practical situations. This article proposes a hybrid adaptive framework to combine both CAT and MST, inspired by an analysis of the history of CAT and MST. The proposed procedure is a design which transitions from a group sequential design to a fully sequential design. This allows for the robustness of MST in early stages, but also shares the advantages of CAT in later stages with fine tuning of the ability estimator once its neighborhood has been identified. Simulation results showed that hybrid designs following our proposed principles provided comparable or even better estimation accuracy and efficiency than standard CAT and MST designs, especially for examinees at the two ends of the ability range. © 2016 by the National Council on Measurement in Education.","['computerized', 'adaptive', 'testing', 'CAT', 'multistage', 'testing', 'MST', 'popular', 'mode', 'largescale', 'computerbase', 'sequential', 'testing', 'design', 'CAT', 'mst', 'exhibit', 'strength', 'weakness', 'recent', 'largescale', 'implementation', 'simple', 'answer', 'question', 'design', 'different', 'mode', 'fit', 'different', 'practical', 'situation', 'article', 'propose', 'hybrid', 'adaptive', 'framework', 'combine', 'CAT', 'MST', 'inspire', 'analysis', 'history', 'CAT', 'MST', 'propose', 'procedure', 'design', 'transition', 'group', 'sequential', 'design', 'fully', 'sequential', 'design', 'allow', 'robustness', 'MST', 'early', 'stage', 'share', 'advantage', 'CAT', 'later', 'stage', 'fine', 'tuning', 'ability', 'estimator', 'neighborhood', 'identify', 'Simulation', 'result', 'hybrid', 'design', 'follow', 'propose', 'principle', 'provide', 'comparable', 'estimation', 'accuracy', 'efficiency', 'standard', 'CAT', 'MST', 'design', 'especially', 'examinee', 'end', 'ability', 'range', '©', '2016', 'National', 'Council']","['hybrid', 'Computerized', 'Adaptive', 'Testing', 'Group', 'Sequential', 'Design', 'fully', 'Sequential', 'Design']",computerized adaptive testing CAT multistage testing MST popular mode largescale computerbase sequential testing design CAT mst exhibit strength weakness recent largescale implementation simple answer question design different mode fit different practical situation article propose hybrid adaptive framework combine CAT MST inspire analysis history CAT MST propose procedure design transition group sequential design fully sequential design allow robustness MST early stage share advantage CAT later stage fine tuning ability estimator neighborhood identify Simulation result hybrid design follow propose principle provide comparable estimation accuracy efficiency standard CAT MST design especially examinee end ability range © 2016 National Council,hybrid Computerized Adaptive Testing Group Sequential Design fully Sequential Design,0.02897748518349154,0.02846010299356589,0.028431864359400125,0.028285655372357568,0.8858448920911849,0.03872269464929152,0.0,0.010156228183940916,0.009067927299298924,0.0
LaHuis D.M.; Bryant-Lees K.B.; Hakoyama S.; Barnes T.; Wiemann A.,A Comparison of Procedures for Estimating Person Reliability Parameters in the Graded Response Model,2018,55,"Person reliability parameters (PRPs) model temporary changes in individuals’ attribute level perceptions when responding to self-report items (higher levels of PRPs represent less fluctuation). PRPs could be useful in measuring careless responding and traitedness. However, it is unclear how well current procedures for estimating PRPs can recover parameter estimates. This study assesses these procedures in terms of mean error (ME), average absolute difference (AAD), and reliability using simulated data with known values. Several prior distributions for PRPs were compared across a number of conditions. Overall, our results revealed little differences between using the χ or lognormal distributions as priors for estimated PRPs. Both distributions produced estimates with reasonable levels of ME; however, the AAD of the estimates was high. AAD did improve slightly as the number of items increased, suggesting that increasing the number of items would ameliorate this problem. Similarly, a larger number of items were necessary to produce reasonable levels of reliability. Based on our results, several conclusions are drawn and implications for future research are discussed. © 2018 by the National Council on Measurement in Education",A Comparison of Procedures for Estimating Person Reliability Parameters in the Graded Response Model,"Person reliability parameters (PRPs) model temporary changes in individuals’ attribute level perceptions when responding to self-report items (higher levels of PRPs represent less fluctuation). PRPs could be useful in measuring careless responding and traitedness. However, it is unclear how well current procedures for estimating PRPs can recover parameter estimates. This study assesses these procedures in terms of mean error (ME), average absolute difference (AAD), and reliability using simulated data with known values. Several prior distributions for PRPs were compared across a number of conditions. Overall, our results revealed little differences between using the χ or lognormal distributions as priors for estimated PRPs. Both distributions produced estimates with reasonable levels of ME; however, the AAD of the estimates was high. AAD did improve slightly as the number of items increased, suggesting that increasing the number of items would ameliorate this problem. Similarly, a larger number of items were necessary to produce reasonable levels of reliability. Based on our results, several conclusions are drawn and implications for future research are discussed. © 2018 by the National Council on Measurement in Education","['person', 'reliability', 'parameter', 'prp', 'temporary', 'change', 'individual', '’', 'attribute', 'level', 'perception', 'respond', 'selfreport', 'item', 'high', 'level', 'prp', 'represent', 'fluctuation', 'prp', 'useful', 'measure', 'careless', 'respond', 'traitedness', 'unclear', 'current', 'procedure', 'estimate', 'prp', 'recover', 'parameter', 'estimate', 'study', 'assess', 'procedure', 'term', 'mean', 'error', 'ME', 'average', 'absolute', 'difference', 'aad', 'reliability', 'simulate', 'datum', 'know', 'value', 'prior', 'distribution', 'prp', 'compare', 'number', 'condition', 'overall', 'result', 'reveal', 'little', 'difference', 'χ', 'lognormal', 'distribution', 'prior', 'estimate', 'prp', 'distribution', 'produce', 'estimate', 'reasonable', 'level', 'I', 'AAD', 'estimate', 'high', 'AAD', 'improve', 'slightly', 'number', 'item', 'increase', 'suggest', 'increase', 'number', 'item', 'ameliorate', 'problem', 'similarly', 'large', 'number', 'item', 'necessary', 'produce', 'reasonable', 'level', 'reliability', 'base', 'result', 'conclusion', 'draw', 'implication', 'future', 'research', 'discuss', '©', '2018', 'National', 'Council']","['Comparison', 'Procedures', 'Estimating', 'Person', 'Reliability', 'Parameters', 'Graded', 'Response']",person reliability parameter prp temporary change individual ’ attribute level perception respond selfreport item high level prp represent fluctuation prp useful measure careless respond traitedness unclear current procedure estimate prp recover parameter estimate study assess procedure term mean error ME average absolute difference aad reliability simulate datum know value prior distribution prp compare number condition overall result reveal little difference χ lognormal distribution prior estimate prp distribution produce estimate reasonable level I AAD estimate high AAD improve slightly number item increase suggest increase number item ameliorate problem similarly large number item necessary produce reasonable level reliability base result conclusion draw implication future research discuss © 2018 National Council,Comparison Procedures Estimating Person Reliability Parameters Graded Response,0.02730627234107581,0.8905123899134055,0.02717697060492434,0.02736632041796701,0.027638046722627395,0.050692410744170184,0.015444991407490938,0.01181862459469984,0.0,0.0
Kang H.-A.; Zhang S.; Chang H.-H.,Dual-Objective Item Selection Criteria in Cognitive Diagnostic Computerized Adaptive Testing,2017,54,"The development of cognitive diagnostic-computerized adaptive testing (CD-CAT) has provided a new perspective for gaining information about examinees' mastery on a set of cognitive attributes. This study proposes a new item selection method within the framework of dual-objective CD-CAT that simultaneously addresses examinees' attribute mastery status and overall test performance. The new procedure is based on the Jensen-Shannon (JS) divergence, a symmetrized version of the Kullback-Leibler divergence. We show that the JS divergence resolves the noncomparability problem of the dual information index and has close relationships with Shannon entropy, mutual information, and Fisher information. The performance of the JS divergence is evaluated in simulation studies in comparison with the methods available in the literature. Results suggest that the JS divergence achieves parallel or more precise recovery of latent trait variables compared to the existing methods and maintains practical advantages in computation and item pool usage. Copyright © 2017 by the National Council on Measurement in Education",Dual-Objective Item Selection Criteria in Cognitive Diagnostic Computerized Adaptive Testing,"The development of cognitive diagnostic-computerized adaptive testing (CD-CAT) has provided a new perspective for gaining information about examinees' mastery on a set of cognitive attributes. This study proposes a new item selection method within the framework of dual-objective CD-CAT that simultaneously addresses examinees' attribute mastery status and overall test performance. The new procedure is based on the Jensen-Shannon (JS) divergence, a symmetrized version of the Kullback-Leibler divergence. We show that the JS divergence resolves the noncomparability problem of the dual information index and has close relationships with Shannon entropy, mutual information, and Fisher information. The performance of the JS divergence is evaluated in simulation studies in comparison with the methods available in the literature. Results suggest that the JS divergence achieves parallel or more precise recovery of latent trait variables compared to the existing methods and maintains practical advantages in computation and item pool usage. Copyright © 2017 by the National Council on Measurement in Education","['development', 'cognitive', 'diagnosticcomputerize', 'adaptive', 'testing', 'CDCAT', 'provide', 'new', 'perspective', 'gain', 'information', 'examine', 'mastery', 'set', 'cognitive', 'attribute', 'study', 'propose', 'new', 'item', 'selection', 'method', 'framework', 'dualobjective', 'CDCAT', 'simultaneously', 'address', 'examine', 'attribute', 'mastery', 'status', 'overall', 'test', 'performance', 'new', 'procedure', 'base', 'JensenShannon', 'JS', 'divergence', 'symmetrized', 'version', 'KullbackLeibler', 'divergence', 'JS', 'divergence', 'resolve', 'noncomparability', 'problem', 'dual', 'information', 'index', 'close', 'relationship', 'Shannon', 'entropy', 'mutual', 'information', 'fisher', 'information', 'performance', 'JS', 'divergence', 'evaluate', 'simulation', 'study', 'comparison', 'method', 'available', 'literature', 'result', 'suggest', 'JS', 'divergence', 'achieve', 'parallel', 'precise', 'recovery', 'latent', 'trait', 'variable', 'compare', 'exist', 'method', 'maintain', 'practical', 'advantage', 'computation', 'item', 'pool', 'usage', 'copyright', '©', '2017', 'National', 'Council']","['DualObjective', 'Item', 'Selection', 'Criteria', 'Cognitive', 'Diagnostic', 'Computerized', 'Adaptive', 'Testing']",development cognitive diagnosticcomputerize adaptive testing CDCAT provide new perspective gain information examine mastery set cognitive attribute study propose new item selection method framework dualobjective CDCAT simultaneously address examine attribute mastery status overall test performance new procedure base JensenShannon JS divergence symmetrized version KullbackLeibler divergence JS divergence resolve noncomparability problem dual information index close relationship Shannon entropy mutual information fisher information performance JS divergence evaluate simulation study comparison method available literature result suggest JS divergence achieve parallel precise recovery latent trait variable compare exist method maintain practical advantage computation item pool usage copyright © 2017 National Council,DualObjective Item Selection Criteria Cognitive Diagnostic Computerized Adaptive Testing,0.8936760040687354,0.026597262826249735,0.02638103095225094,0.026629401574387247,0.02671630057837654,0.05071217527443522,0.008582504519161847,0.011424260488073922,0.0,0.0
Debeer D.; Ali U.S.; van Rijn P.W.,Evaluating Statistical Targets for Assembling Parallel Mixed-Format Test Forms,2017,54,"Test assembly is the process of selecting items from an item pool to form one or more new test forms. Often new test forms are constructed to be parallel with an existing (or an ideal) test. Within the context of item response theory, the test information function (TIF) or the test characteristic curve (TCC) are commonly used as statistical targets to obtain this parallelism. In a recent study, Ali and van Rijn proposed combining the TIF and TCC as statistical targets, rather than using only a single statistical target. In this article, we propose two new methods using this combined approach, and compare these methods with single statistical targets for the assembly of mixed-format tests. In addition, we introduce new criteria to evaluate the parallelism of multiple forms. The results show that single statistical targets can be problematic, while the combined targets perform better, especially in situations with increasing numbers of polytomous items. Implications of using the combined target are discussed. Copyright © 2017 by the National Council on Measurement in Education",Evaluating Statistical Targets for Assembling Parallel Mixed-Format Test Forms,"Test assembly is the process of selecting items from an item pool to form one or more new test forms. Often new test forms are constructed to be parallel with an existing (or an ideal) test. Within the context of item response theory, the test information function (TIF) or the test characteristic curve (TCC) are commonly used as statistical targets to obtain this parallelism. In a recent study, Ali and van Rijn proposed combining the TIF and TCC as statistical targets, rather than using only a single statistical target. In this article, we propose two new methods using this combined approach, and compare these methods with single statistical targets for the assembly of mixed-format tests. In addition, we introduce new criteria to evaluate the parallelism of multiple forms. The results show that single statistical targets can be problematic, while the combined targets perform better, especially in situations with increasing numbers of polytomous items. Implications of using the combined target are discussed. Copyright © 2017 by the National Council on Measurement in Education","['test', 'assembly', 'process', 'select', 'item', 'item', 'pool', 'form', 'new', 'test', 'form', 'new', 'test', 'form', 'construct', 'parallel', 'exist', 'ideal', 'test', 'context', 'item', 'response', 'theory', 'test', 'information', 'function', 'TIF', 'test', 'characteristic', 'curve', 'TCC', 'commonly', 'statistical', 'target', 'obtain', 'parallelism', 'recent', 'study', 'Ali', 'van', 'Rijn', 'propose', 'combine', 'TIF', 'TCC', 'statistical', 'target', 'single', 'statistical', 'target', 'article', 'propose', 'new', 'method', 'combine', 'approach', 'compare', 'method', 'single', 'statistical', 'target', 'assembly', 'mixedformat', 'test', 'addition', 'introduce', 'new', 'criterion', 'evaluate', 'parallelism', 'multiple', 'form', 'result', 'single', 'statistical', 'target', 'problematic', 'combine', 'target', 'perform', 'especially', 'situation', 'increase', 'number', 'polytomous', 'item', 'Implications', 'combine', 'target', 'discuss', 'Copyright', '©', '2017', 'National', 'Council']","['evaluate', 'statistical', 'Targets', 'assemble', 'Parallel', 'MixedFormat', 'Test', 'form']",test assembly process select item item pool form new test form new test form construct parallel exist ideal test context item response theory test information function TIF test characteristic curve TCC commonly statistical target obtain parallelism recent study Ali van Rijn propose combine TIF TCC statistical target single statistical target article propose new method combine approach compare method single statistical target assembly mixedformat test addition introduce new criterion evaluate parallelism multiple form result single statistical target problematic combine target perform especially situation increase number polytomous item Implications combine target discuss Copyright © 2017 National Council,evaluate statistical Targets assemble Parallel MixedFormat Test form,0.8773764598662808,0.030572475405911837,0.030687713578034608,0.030782777357221586,0.030580573792551077,0.05494062699916624,0.012261303061758898,0.0033352894519188556,0.021153423113236272,0.0
Dwyer A.C.,Maintaining Equivalent Cut Scores for Small Sample Test Forms,2016,53,"This study examines the effectiveness of three approaches for maintaining equivalent performance standards across test forms with small samples: (1) common-item equating, (2) resetting the standard, and (3) rescaling the standard. Rescaling the standard (i.e., applying common-item equating methodology to standard setting ratings to account for systematic differences between standard setting panels) has received almost no attention in the literature. Identity equating was also examined to provide context. Data from a standard setting form of a large national certification test (N examinees = 4,397; N panelists = 13) were split into content-equivalent subforms with common items, and resampling methodology was used to investigate the error introduced by each approach. Common-item equating (circle-arc and nominal weights mean) was evaluated at samples of size 10, 25, 50, and 100. The standard setting approaches (resetting and rescaling the standard) were evaluated by resampling (N = 8) and by simulating panelists (N = 8, 13, and 20). Results were inconclusive regarding the relative effectiveness of resetting and rescaling the standard. Small-sample equating, however, consistently produced new form cut scores that were less biased and less prone to random error than new form cut scores based on resetting or rescaling the standard. © 2016 by the National Council on Measurement in Education.",Maintaining Equivalent Cut Scores for Small Sample Test Forms,"This study examines the effectiveness of three approaches for maintaining equivalent performance standards across test forms with small samples: (1) common-item equating, (2) resetting the standard, and (3) rescaling the standard. Rescaling the standard (i.e., applying common-item equating methodology to standard setting ratings to account for systematic differences between standard setting panels) has received almost no attention in the literature. Identity equating was also examined to provide context. Data from a standard setting form of a large national certification test (N examinees = 4,397; N panelists = 13) were split into content-equivalent subforms with common items, and resampling methodology was used to investigate the error introduced by each approach. Common-item equating (circle-arc and nominal weights mean) was evaluated at samples of size 10, 25, 50, and 100. The standard setting approaches (resetting and rescaling the standard) were evaluated by resampling (N = 8) and by simulating panelists (N = 8, 13, and 20). Results were inconclusive regarding the relative effectiveness of resetting and rescaling the standard. Small-sample equating, however, consistently produced new form cut scores that were less biased and less prone to random error than new form cut scores based on resetting or rescaling the standard. © 2016 by the National Council on Measurement in Education.","['study', 'examine', 'effectiveness', 'approach', 'maintain', 'equivalent', 'performance', 'standard', 'test', 'form', 'small', 'sample', '1', 'commonitem', 'equate', '2', 'reset', 'standard', '3', 'rescale', 'standard', 'rescale', 'standard', 'ie', 'apply', 'commonitem', 'equate', 'methodology', 'standard', 'set', 'rating', 'account', 'systematic', 'difference', 'standard', 'setting', 'panel', 'receive', 'attention', 'literature', 'Identity', 'equating', 'examine', 'provide', 'context', 'datum', 'standard', 'set', 'form', 'large', 'national', 'certification', 'test', 'N', 'examine', '4397', 'n', 'panelist', '13', 'split', 'contentequivalent', 'subform', 'common', 'item', 'resample', 'methodology', 'investigate', 'error', 'introduce', 'approach', 'Commonitem', 'equate', 'circlearc', 'nominal', 'weight', 'mean', 'evaluate', 'sample', 'size', '10', '25', '50', '100', 'standard', 'set', 'approach', 'reset', 'rescale', 'standard', 'evaluate', 'resample', 'N', '8', 'simulate', 'panelist', 'n', '8', '13', '20', 'result', 'inconclusive', 'regard', 'relative', 'effectiveness', 'reset', 'rescale', 'standard', 'Smallsample', 'equating', 'consistently', 'produce', 'new', 'form', 'cut', 'score', 'biased', 'prone', 'random', 'error', 'new', 'form', 'cut', 'score', 'base', 'reset', 'rescale', 'standard', '©', '2016', 'National', 'Council']","['maintain', 'Equivalent', 'Cut', 'Scores', 'Small', 'Sample', 'test', 'form']",study examine effectiveness approach maintain equivalent performance standard test form small sample 1 commonitem equate 2 reset standard 3 rescale standard rescale standard ie apply commonitem equate methodology standard set rating account systematic difference standard setting panel receive attention literature Identity equating examine provide context datum standard set form large national certification test N examine 4397 n panelist 13 split contentequivalent subform common item resample methodology investigate error introduce approach Commonitem equate circlearc nominal weight mean evaluate sample size 10 25 50 100 standard set approach reset rescale standard evaluate resample N 8 simulate panelist n 8 13 20 result inconclusive regard relative effectiveness reset rescale standard Smallsample equating consistently produce new form cut score biased prone random error new form cut score base reset rescale standard © 2016 National Council,maintain Equivalent Cut Scores Small Sample test form,0.027910902253095043,0.02660358792999202,0.026673499112011237,0.026580421031148375,0.8922315896737533,0.014660307541234097,0.007262090629914245,0.007844175232398668,0.06737179380140157,0.02521787696037029
Sinharay S.,A New Person-Fit Statistic for the Lognormal Model for Response Times,2018,55,"Response-time models are of increasing interest in educational and psychological testing. This article focuses on the lognormal model for response times, which is one of the most popular response-time models, and suggests a simple person-fit statistic for the model. The distribution of the statistic under the null hypothesis of no misfit is proved to be a χ2 distribution. A simulation study and a real data example demonstrate the usefulness of the suggested statistic. © 2018 by the National Council on Measurement in Education",A New Person-Fit Statistic for the Lognormal Model for Response Times,"Response-time models are of increasing interest in educational and psychological testing. This article focuses on the lognormal model for response times, which is one of the most popular response-time models, and suggests a simple person-fit statistic for the model. The distribution of the statistic under the null hypothesis of no misfit is proved to be a χ2 distribution. A simulation study and a real data example demonstrate the usefulness of the suggested statistic. © 2018 by the National Council on Measurement in Education","['responsetime', 'increase', 'interest', 'educational', 'psychological', 'testing', 'article', 'focus', 'lognormal', 'response', 'time', 'popular', 'responsetime', 'suggest', 'simple', 'personfit', 'statistic', 'distribution', 'statistic', 'null', 'hypothesis', 'misfit', 'prove', 'χ2', 'distribution', 'simulation', 'study', 'real', 'datum', 'example', 'demonstrate', 'usefulness', 'suggest', 'statistic', '©', '2018', 'National', 'Council']","['New', 'PersonFit', 'Statistic', 'Lognormal', 'Response', 'Times']",responsetime increase interest educational psychological testing article focus lognormal response time popular responsetime suggest simple personfit statistic distribution statistic null hypothesis misfit prove χ2 distribution simulation study real datum example demonstrate usefulness suggest statistic © 2018 National Council,New PersonFit Statistic Lognormal Response Times,0.03424834036149082,0.8632004286048975,0.03392426463062287,0.03441610267399043,0.0342108637289983,0.029893456785962464,0.009797861194731696,0.03032307950203351,0.0,0.0
Liu C.; Kolen M.J.,A Comparison of Strategies for Smoothing Parameter Selection for Mixed-Format Tests Under the Random Groups Design,2018,55,"Smoothing techniques are designed to improve the accuracy of equating functions. The main purpose of this study is to compare seven model selection strategies for choosing the smoothing parameter (C) for polynomial loglinear presmoothing and one procedure for model selection in cubic spline postsmoothing for mixed-format pseudo tests under the random groups design. These model selection strategies were compared for four sample sizes (500, 1,000, 2,000, and 3,000) and two content areas (Advanced Placement [AP] Biology and AP Environmental Science). For polynomial loglinear presmoothing, the Akaike information criterion (AIC) was the only statistic that reduced both random equating error and total equating error in all investigated conditions. Cubic spline postsmoothing tended to produce more accurate results than any of the model selection strategies in polynomial loglinear smoothing. © 2018 by the National Council on Measurement in Education",A Comparison of Strategies for Smoothing Parameter Selection for Mixed-Format Tests Under the Random Groups Design,"Smoothing techniques are designed to improve the accuracy of equating functions. The main purpose of this study is to compare seven model selection strategies for choosing the smoothing parameter (C) for polynomial loglinear presmoothing and one procedure for model selection in cubic spline postsmoothing for mixed-format pseudo tests under the random groups design. These model selection strategies were compared for four sample sizes (500, 1,000, 2,000, and 3,000) and two content areas (Advanced Placement [AP] Biology and AP Environmental Science). For polynomial loglinear presmoothing, the Akaike information criterion (AIC) was the only statistic that reduced both random equating error and total equating error in all investigated conditions. Cubic spline postsmoothing tended to produce more accurate results than any of the model selection strategies in polynomial loglinear smoothing. © 2018 by the National Council on Measurement in Education","['smooth', 'technique', 'design', 'improve', 'accuracy', 'equate', 'function', 'main', 'purpose', 'study', 'compare', 'seven', 'selection', 'strategy', 'choose', 'smoothing', 'parameter', 'C', 'polynomial', 'loglinear', 'presmoothe', 'procedure', 'selection', 'cubic', 'spline', 'postsmoothing', 'mixedformat', 'pseudo', 'test', 'random', 'group', 'design', 'selection', 'strategy', 'compare', 'sample', 'size', '500', '1000', '2000', '3000', 'content', 'area', 'Advanced', 'Placement', 'AP', 'Biology', 'AP', 'Environmental', 'Science', 'polynomial', 'loglinear', 'presmoothe', 'Akaike', 'information', 'criterion', 'AIC', 'statistic', 'reduce', 'random', 'equating', 'error', 'total', 'equate', 'error', 'investigate', 'condition', 'cubic', 'spline', 'postsmoothing', 'tend', 'produce', 'accurate', 'result', 'selection', 'strategy', 'polynomial', 'loglinear', 'smooth', '©', '2018', 'National', 'Council']","['Comparison', 'strategy', 'Smoothing', 'Parameter', 'Selection', 'MixedFormat', 'Tests', 'Random', 'Groups', 'Design']",smooth technique design improve accuracy equate function main purpose study compare seven selection strategy choose smoothing parameter C polynomial loglinear presmoothe procedure selection cubic spline postsmoothing mixedformat pseudo test random group design selection strategy compare sample size 500 1000 2000 3000 content area Advanced Placement AP Biology AP Environmental Science polynomial loglinear presmoothe Akaike information criterion AIC statistic reduce random equating error total equate error investigate condition cubic spline postsmoothing tend produce accurate result selection strategy polynomial loglinear smooth © 2018 National Council,Comparison strategy Smoothing Parameter Selection MixedFormat Tests Random Groups Design,0.027646223395711343,0.026863461312763057,0.8916684075687866,0.02697480943570595,0.02684709828703303,0.023265940026624035,0.0,0.0,0.058443267661673924,0.0
"Wang J.; Engelhard G., Jr.",Conceptualizing Rater Judgments and Rating Processes for Rater-Mediated Assessments,2019,56,"Rater-mediated assessments exhibit scoring challenges due to the involvement of human raters. The quality of human ratings largely determines the reliability, validity, and fairness of the assessment process. Our research recommends that the evaluation of ratings should be based on two aspects: a theoretical model of human judgment and an appropriate measurement model for evaluating these judgments. In rater-mediated assessments, the underlying constructs and response processes may require the use of different rater judgment models and the application of different measurement models. We describe the use of Brunswik's lens model as an organizing theme for conceptualizing human judgments in rater-mediated assessments. The constructs vary depending on which distal variables are identified in the lens models for the underlying rater-mediated assessment. For example, one lens model can be developed to emphasize the measurement of student proficiency, while another lens model can stress the evaluation of rater accuracy. Next, we describe two measurement models that reflect different response processes (cumulative and unfolding) from raters: Rasch and hyperbolic cosine models. Future directions for the development and evaluation of rater-mediated assessments are suggested. © 2019 by the National Council on Measurement in Education",Conceptualizing Rater Judgments and Rating Processes for Rater-Mediated Assessments,"Rater-mediated assessments exhibit scoring challenges due to the involvement of human raters. The quality of human ratings largely determines the reliability, validity, and fairness of the assessment process. Our research recommends that the evaluation of ratings should be based on two aspects: a theoretical model of human judgment and an appropriate measurement model for evaluating these judgments. In rater-mediated assessments, the underlying constructs and response processes may require the use of different rater judgment models and the application of different measurement models. We describe the use of Brunswik's lens model as an organizing theme for conceptualizing human judgments in rater-mediated assessments. The constructs vary depending on which distal variables are identified in the lens models for the underlying rater-mediated assessment. For example, one lens model can be developed to emphasize the measurement of student proficiency, while another lens model can stress the evaluation of rater accuracy. Next, we describe two measurement models that reflect different response processes (cumulative and unfolding) from raters: Rasch and hyperbolic cosine models. Future directions for the development and evaluation of rater-mediated assessments are suggested. © 2019 by the National Council on Measurement in Education","['ratermediated', 'assessment', 'exhibit', 'scoring', 'challenge', 'involvement', 'human', 'rater', 'quality', 'human', 'rating', 'largely', 'determine', 'reliability', 'validity', 'fairness', 'assessment', 'process', 'research', 'recommend', 'evaluation', 'rating', 'base', 'aspect', 'theoretical', 'human', 'judgment', 'appropriate', 'evaluate', 'judgment', 'ratermediate', 'assessment', 'underlie', 'construct', 'response', 'process', 'require', 'different', 'rater', 'judgment', 'application', 'different', 'describe', 'Brunswiks', 'lens', 'organizing', 'theme', 'conceptualize', 'human', 'judgment', 'ratermediate', 'assessment', 'construct', 'vary', 'depend', 'distal', 'variable', 'identify', 'lens', 'underlie', 'ratermediate', 'assessment', 'example', 'lens', 'develop', 'emphasize', 'student', 'proficiency', 'lens', 'stress', 'evaluation', 'rater', 'accuracy', 'Next', 'describe', 'reflect', 'different', 'response', 'process', 'cumulative', 'unfold', 'rater', 'Rasch', 'hyperbolic', 'cosine', 'future', 'direction', 'development', 'evaluation', 'ratermediate', 'assessment', 'suggest', '©', '2019', 'National', 'Council']","['conceptualize', 'Rater', 'Judgments', 'Rating', 'Processes', 'RaterMediated', 'assessment']",ratermediated assessment exhibit scoring challenge involvement human rater quality human rating largely determine reliability validity fairness assessment process research recommend evaluation rating base aspect theoretical human judgment appropriate evaluate judgment ratermediate assessment underlie construct response process require different rater judgment application different describe Brunswiks lens organizing theme conceptualize human judgment ratermediate assessment construct vary depend distal variable identify lens underlie ratermediate assessment example lens develop emphasize student proficiency lens stress evaluation rater accuracy Next describe reflect different response process cumulative unfold rater Rasch hyperbolic cosine future direction development evaluation ratermediate assessment suggest © 2019 National Council,conceptualize Rater Judgments Rating Processes RaterMediated assessment,0.8861466238732534,0.0286315092328008,0.0283310097207111,0.02845488646125788,0.028435970711976826,0.0,0.0,0.05461090881680483,0.0,0.13361665384825622
Wyse A.E.; Babcock B.,Does Maximizing Information at the Cut Score Always Maximize Classification Accuracy and Consistency?,2016,53,"A common suggestion made in the psychometric literature for fixed-length classification tests is that one should design tests so that they have maximum information at the cut score. Designing tests in this way is believed to maximize the classification accuracy and consistency of the assessment. This article uses simulated examples to illustrate that one can obtain higher classification accuracy and consistency by designing tests that have maximum test information at locations other than at the cut score. We show that the location where one should maximize the test information is dependent on the length of the test, the mean of the ability distribution in comparison to the cut score, and, to a lesser degree, whether or not one wants to optimize classification accuracy or consistency. Analyses also suggested that the differences in classification performance between designing tests optimally versus maximizing information at the cut score tended to be greatest when tests were short and the mean of ability distribution was further away from the cut score. Larger differences were also found in the simulated examples that used the 3PL model compared to the examples that used the Rasch model. © 2016 by the National Council on Measurement in Education.",Does Maximizing Information at the Cut Score Always Maximize Classification Accuracy and Consistency?,"A common suggestion made in the psychometric literature for fixed-length classification tests is that one should design tests so that they have maximum information at the cut score. Designing tests in this way is believed to maximize the classification accuracy and consistency of the assessment. This article uses simulated examples to illustrate that one can obtain higher classification accuracy and consistency by designing tests that have maximum test information at locations other than at the cut score. We show that the location where one should maximize the test information is dependent on the length of the test, the mean of the ability distribution in comparison to the cut score, and, to a lesser degree, whether or not one wants to optimize classification accuracy or consistency. Analyses also suggested that the differences in classification performance between designing tests optimally versus maximizing information at the cut score tended to be greatest when tests were short and the mean of ability distribution was further away from the cut score. Larger differences were also found in the simulated examples that used the 3PL model compared to the examples that used the Rasch model. © 2016 by the National Council on Measurement in Education.","['common', 'suggestion', 'psychometric', 'literature', 'fixedlength', 'classification', 'test', 'design', 'test', 'maximum', 'information', 'cut', 'score', 'designing', 'test', 'way', 'believe', 'maximize', 'classification', 'accuracy', 'consistency', 'assessment', 'article', 'simulate', 'example', 'illustrate', 'obtain', 'high', 'classification', 'accuracy', 'consistency', 'design', 'test', 'maximum', 'test', 'information', 'location', 'cut', 'score', 'location', 'maximize', 'test', 'information', 'dependent', 'length', 'test', 'mean', 'ability', 'distribution', 'comparison', 'cut', 'score', 'degree', 'want', 'optimize', 'classification', 'accuracy', 'consistency', 'Analyses', 'suggest', 'difference', 'classification', 'performance', 'design', 'test', 'optimally', 'versus', 'maximize', 'information', 'cut', 'score', 'tend', 'great', 'test', 'short', 'mean', 'ability', 'distribution', 'far', 'away', 'cut', 'score', 'large', 'difference', 'find', 'simulate', 'example', '3pl', 'compare', 'example', 'Rasch', '©', '2016', 'National', 'Council']","['Maximizing', 'Information', 'Cut', 'Score', 'maximize', 'Classification', 'Accuracy', 'Consistency']",common suggestion psychometric literature fixedlength classification test design test maximum information cut score designing test way believe maximize classification accuracy consistency assessment article simulate example illustrate obtain high classification accuracy consistency design test maximum test information location cut score location maximize test information dependent length test mean ability distribution comparison cut score degree want optimize classification accuracy consistency Analyses suggest difference classification performance design test optimally versus maximize information cut score tend great test short mean ability distribution far away cut score large difference find simulate example 3pl compare example Rasch © 2016 National Council,Maximizing Information Cut Score maximize Classification Accuracy Consistency,0.02935036212195747,0.8831066085035175,0.02897962369609045,0.028986977546566282,0.029576428131868206,0.03470151490660511,0.051370065505378364,0.029902267913637603,0.015520946142548396,0.0
Lee W.-Y.; Cho S.-J.,Detecting Differential Item Discrimination (DID) and the Consequences of Ignoring DID in Multilevel Item Response Models,2017,54,"Cross-level invariance in a multilevel item response model can be investigated by testing whether the within-level item discriminations are equal to the between-level item discriminations. Testing the cross-level invariance assumption is important to understand constructs in multilevel data. However, in most multilevel item response model applications, the cross-level invariance is assumed without testing of the cross-level invariance assumption. In this study, the detection methods of differential item discrimination (DID) over levels and the consequences of ignoring DID are illustrated and discussed with the use of multilevel item response models. Simulation results showed that the likelihood ratio test (LRT) performed well in detecting global DID at the test level when some portion of the items exhibited DID. At the item level, the Akaike information criterion (AIC), the sample-size adjusted Bayesian information criterion (saBIC), LRT, and Wald test showed a satisfactory rejection rate (>.8) when some portion of the items exhibited DID and the items had lower intraclass correlations (or higher DID magnitudes). When DID was ignored, the accuracy of the item discrimination estimates and standard errors was mainly problematic. Implications of the findings and limitations are discussed. Copyright © 2017 by the National Council on Measurement in Education",Detecting Differential Item Discrimination (DID) and the Consequences of Ignoring DID in Multilevel Item Response Models,"Cross-level invariance in a multilevel item response model can be investigated by testing whether the within-level item discriminations are equal to the between-level item discriminations. Testing the cross-level invariance assumption is important to understand constructs in multilevel data. However, in most multilevel item response model applications, the cross-level invariance is assumed without testing of the cross-level invariance assumption. In this study, the detection methods of differential item discrimination (DID) over levels and the consequences of ignoring DID are illustrated and discussed with the use of multilevel item response models. Simulation results showed that the likelihood ratio test (LRT) performed well in detecting global DID at the test level when some portion of the items exhibited DID. At the item level, the Akaike information criterion (AIC), the sample-size adjusted Bayesian information criterion (saBIC), LRT, and Wald test showed a satisfactory rejection rate (>.8) when some portion of the items exhibited DID and the items had lower intraclass correlations (or higher DID magnitudes). When DID was ignored, the accuracy of the item discrimination estimates and standard errors was mainly problematic. Implications of the findings and limitations are discussed. Copyright © 2017 by the National Council on Measurement in Education","['crosslevel', 'invariance', 'multilevel', 'item', 'response', 'investigate', 'test', 'withinlevel', 'item', 'discrimination', 'equal', 'betweenlevel', 'item', 'discrimination', 'test', 'crosslevel', 'invariance', 'assumption', 'important', 'understand', 'construct', 'multilevel', 'datum', 'multilevel', 'item', 'response', 'application', 'crosslevel', 'invariance', 'assume', 'testing', 'crosslevel', 'invariance', 'assumption', 'study', 'detection', 'method', 'differential', 'item', 'discrimination', 'level', 'consequence', 'ignore', 'illustrate', 'discuss', 'multilevel', 'item', 'response', 'Simulation', 'result', 'likelihood', 'ratio', 'test', 'LRT', 'perform', 'detect', 'global', 'DID', 'test', 'level', 'portion', 'item', 'exhibit', 'item', 'level', 'Akaike', 'information', 'criterion', 'AIC', 'samplesize', 'adjust', 'bayesian', 'information', 'criterion', 'saBIC', 'LRT', 'Wald', 'test', 'satisfactory', 'rejection', 'rate', '8', 'portion', 'item', 'exhibit', 'item', 'low', 'intraclass', 'correlation', 'high', 'magnitude', 'ignore', 'accuracy', 'item', 'discrimination', 'estimate', 'standard', 'error', 'mainly', 'problematic', 'implication', 'finding', 'limitation', 'discuss', 'Copyright', '©', '2017', 'National', 'Council']","['detect', 'Differential', 'Item', 'Discrimination', 'Consequences', 'Ignoring', 'Multilevel', 'Item', 'Response', 'Models']",crosslevel invariance multilevel item response investigate test withinlevel item discrimination equal betweenlevel item discrimination test crosslevel invariance assumption important understand construct multilevel datum multilevel item response application crosslevel invariance assume testing crosslevel invariance assumption study detection method differential item discrimination level consequence ignore illustrate discuss multilevel item response Simulation result likelihood ratio test LRT perform detect global DID test level portion item exhibit item level Akaike information criterion AIC samplesize adjust bayesian information criterion saBIC LRT Wald test satisfactory rejection rate 8 portion item exhibit item low intraclass correlation high magnitude ignore accuracy item discrimination estimate standard error mainly problematic implication finding limitation discuss Copyright © 2017 National Council,detect Differential Item Discrimination Consequences Ignoring Multilevel Item Response Models,0.891408670425629,0.027276095635252094,0.02708697381614871,0.027194628637314092,0.027033631485656053,0.08133898059356101,0.0,0.0,0.0,0.0
Albano A.D.; Cai L.; Lease E.M.; McConnell S.R.,Computerized Adaptive Testing in Early Education: Exploring the Impact of Item Position Effects on Ability Estimation,2019,56,"Studies have shown that item difficulty can vary significantly based on the context of an item within a test form. In particular, item position may be associated with practice and fatigue effects that influence item parameter estimation. The purpose of this research was to examine the relevance of item position specifically for assessments used in early education, an area of testing that has received relatively limited psychometric attention. In an initial study, multilevel item response models fit to data from an early literacy measure revealed statistically significant increases in difficulty for items appearing later in a 20-item form. The estimated linear change in logits for an increase of 1 in position was.024, resulting in a predicted change of.46 logits for a shift from the beginning to the end of the form. A subsequent simulation study examined impacts of item position effects on person ability estimation within computerized adaptive testing. Implications and recommendations for practice are discussed. © 2019 by the National Council on Measurement in Education",Computerized Adaptive Testing in Early Education: Exploring the Impact of Item Position Effects on Ability Estimation,"Studies have shown that item difficulty can vary significantly based on the context of an item within a test form. In particular, item position may be associated with practice and fatigue effects that influence item parameter estimation. The purpose of this research was to examine the relevance of item position specifically for assessments used in early education, an area of testing that has received relatively limited psychometric attention. In an initial study, multilevel item response models fit to data from an early literacy measure revealed statistically significant increases in difficulty for items appearing later in a 20-item form. The estimated linear change in logits for an increase of 1 in position was.024, resulting in a predicted change of.46 logits for a shift from the beginning to the end of the form. A subsequent simulation study examined impacts of item position effects on person ability estimation within computerized adaptive testing. Implications and recommendations for practice are discussed. © 2019 by the National Council on Measurement in Education","['study', 'item', 'difficulty', 'vary', 'significantly', 'base', 'context', 'item', 'test', 'form', 'particular', 'item', 'position', 'associate', 'practice', 'fatigue', 'effect', 'influence', 'item', 'parameter', 'estimation', 'purpose', 'research', 'examine', 'relevance', 'item', 'position', 'specifically', 'assessment', 'early', 'area', 'testing', 'receive', 'relatively', 'limited', 'psychometric', 'attention', 'initial', 'study', 'multilevel', 'item', 'response', 'fit', 'datum', 'early', 'literacy', 'measure', 'reveal', 'statistically', 'significant', 'increase', 'difficulty', 'item', 'appear', 'later', '20item', 'form', 'estimate', 'linear', 'change', 'logit', 'increase', '1', 'position', 'was024', 'result', 'predict', 'change', 'of46', 'logit', 'shift', 'beginning', 'end', 'form', 'subsequent', 'simulation', 'study', 'examine', 'impact', 'item', 'position', 'effect', 'person', 'ability', 'estimation', 'computerized', 'adaptive', 'testing', 'Implications', 'recommendation', 'practice', 'discuss', '©', '2019', 'National', 'Council']","['Computerized', 'Adaptive', 'Testing', 'early', 'explore', 'Impact', 'Item', 'Position', 'Effects', 'Ability', 'Estimation']",study item difficulty vary significantly base context item test form particular item position associate practice fatigue effect influence item parameter estimation purpose research examine relevance item position specifically assessment early area testing receive relatively limited psychometric attention initial study multilevel item response fit datum early literacy measure reveal statistically significant increase difficulty item appear later 20item form estimate linear change logit increase 1 position was024 result predict change of46 logit shift beginning end form subsequent simulation study examine impact item position effect person ability estimation computerized adaptive testing Implications recommendation practice discuss © 2019 National Council,Computerized Adaptive Testing early explore Impact Item Position Effects Ability Estimation,0.02440452238305877,0.024237596212041822,0.024217435781335554,0.024218500129307707,0.9029219454942562,0.08477873182586133,0.0,0.0022339119533720565,0.0,0.009738097989674585
Debeer D.; Janssen R.; De Boeck P.,Modeling Skipped and Not-Reached Items Using IRTrees,2017,54,"When dealing with missing responses, two types of omissions can be discerned: items can be skipped or not reached by the test taker. When the occurrence of these omissions is related to the proficiency process the missingness is nonignorable. The purpose of this article is to present a tree-based IRT framework for modeling responses and omissions jointly, taking into account that test takers as well as items can contribute to the two types of omissions. The proposed framework covers several existing models for missing responses, and many IRTree models can be estimated using standard statistical software. Further, simulated data is used to show that ignoring missing responses is less robust than often considered. Finally, as an illustration of its applicability, the IRTree approach is applied to data from the 2009 PISA reading assessment. Copyright © 2017 by the National Council on Measurement in Education",Modeling Skipped and Not-Reached Items Using IRTrees,"When dealing with missing responses, two types of omissions can be discerned: items can be skipped or not reached by the test taker. When the occurrence of these omissions is related to the proficiency process the missingness is nonignorable. The purpose of this article is to present a tree-based IRT framework for modeling responses and omissions jointly, taking into account that test takers as well as items can contribute to the two types of omissions. The proposed framework covers several existing models for missing responses, and many IRTree models can be estimated using standard statistical software. Further, simulated data is used to show that ignoring missing responses is less robust than often considered. Finally, as an illustration of its applicability, the IRTree approach is applied to data from the 2009 PISA reading assessment. Copyright © 2017 by the National Council on Measurement in Education","['deal', 'miss', 'response', 'type', 'omission', 'discern', 'item', 'skip', 'reach', 'test', 'taker', 'occurrence', 'omission', 'relate', 'proficiency', 'process', 'missingness', 'nonignorable', 'purpose', 'article', 'present', 'treebase', 'IRT', 'framework', 'modeling', 'response', 'omission', 'jointly', 'account', 'test', 'taker', 'item', 'contribute', 'type', 'omission', 'propose', 'framework', 'cover', 'exist', 'miss', 'response', 'IRTree', 'estimate', 'standard', 'statistical', 'software', 'Further', 'simulate', 'datum', 'ignore', 'miss', 'response', 'robust', 'consider', 'finally', 'illustration', 'applicability', 'IRTree', 'approach', 'apply', 'datum', '2009', 'PISA', 'reading', 'assessment', 'Copyright', '©', '2017', 'National', 'Council']","['Skipped', 'NotReached', 'Items', 'irtree']",deal miss response type omission discern item skip reach test taker occurrence omission relate proficiency process missingness nonignorable purpose article present treebase IRT framework modeling response omission jointly account test taker item contribute type omission propose framework cover exist miss response IRTree estimate standard statistical software Further simulate datum ignore miss response robust consider finally illustration applicability IRTree approach apply datum 2009 PISA reading assessment Copyright © 2017 National Council,Skipped NotReached Items irtree,0.029223060859927985,0.029101715054553733,0.8828760712219655,0.029475480302428046,0.029323672561124822,0.040014754439684296,0.0,0.02666992737563856,0.0,0.00011255800210624151
Wesolowski B.C.; Wind S.A.,Pedagogical Considerations for Examining Rater Variability in Rater-Mediated Assessments: A Three-Model Framework,2019,56,"Rater-mediated assessments are a common methodology for measuring persons, investigating rater behavior, and/or defining latent constructs. The purpose of this article is to provide a pedagogical framework for examining rater variability in the context of rater-mediated assessments using three distinct models. The first model is the observation model, which includes ecological/environmental considerations for the evaluation system. The second model is the measurement model, which includes the transformation of observed, rater response data to linear measures using a measurement model with specific requirements of rater-invariant measurement in order to examine raters’ construct-relevant variability stemming from the evaluative system. The third model is the interaction model, which includes an interaction parameter to allow for the investigation into raters’ systematic, construct-irrelevant variability stemming from the evaluative system. Implications for measurement outcomes and validity are discussed. © 2019 by the National Council on Measurement in Education",Pedagogical Considerations for Examining Rater Variability in Rater-Mediated Assessments: A Three-Model Framework,"Rater-mediated assessments are a common methodology for measuring persons, investigating rater behavior, and/or defining latent constructs. The purpose of this article is to provide a pedagogical framework for examining rater variability in the context of rater-mediated assessments using three distinct models. The first model is the observation model, which includes ecological/environmental considerations for the evaluation system. The second model is the measurement model, which includes the transformation of observed, rater response data to linear measures using a measurement model with specific requirements of rater-invariant measurement in order to examine raters’ construct-relevant variability stemming from the evaluative system. The third model is the interaction model, which includes an interaction parameter to allow for the investigation into raters’ systematic, construct-irrelevant variability stemming from the evaluative system. Implications for measurement outcomes and validity are discussed. © 2019 by the National Council on Measurement in Education","['ratermediated', 'assessment', 'common', 'methodology', 'measure', 'person', 'investigate', 'rater', 'behavior', 'andor', 'define', 'latent', 'construct', 'purpose', 'article', 'provide', 'pedagogical', 'framework', 'examine', 'rater', 'variability', 'context', 'ratermediate', 'assessment', 'distinct', 'observation', 'include', 'ecologicalenvironmental', 'consideration', 'evaluation', 'system', 'second', 'include', 'transformation', 'observe', 'rater', 'response', 'datum', 'linear', 'measure', 'specific', 'requirement', 'raterinvariant', 'order', 'examine', 'rater', ""'"", 'constructrelevant', 'variability', 'stem', 'evaluative', 'system', 'interaction', 'include', 'interaction', 'parameter', 'allow', 'investigation', 'rater', '’', 'systematic', 'constructirrelevant', 'variability', 'stem', 'evaluative', 'system', 'Implications', 'outcome', 'validity', 'discuss', '©', '2019', 'National', 'Council']","['Pedagogical', 'Considerations', 'examine', 'Rater', 'Variability', 'RaterMediated', 'assessment', 'A', 'ThreeModel', 'Framework']",ratermediated assessment common methodology measure person investigate rater behavior andor define latent construct purpose article provide pedagogical framework examine rater variability context ratermediate assessment distinct observation include ecologicalenvironmental consideration evaluation system second include transformation observe rater response datum linear measure specific requirement raterinvariant order examine rater ' constructrelevant variability stem evaluative system interaction include interaction parameter allow investigation rater ’ systematic constructirrelevant variability stem evaluative system Implications outcome validity discuss © 2019 National Council,Pedagogical Considerations examine Rater Variability RaterMediated assessment A ThreeModel Framework,0.8865059671617208,0.028170578328154526,0.028068368323552667,0.028431891643522122,0.028823194543049945,0.0,0.0,0.026730591673745353,0.0,0.18058797591440218
Wu Q.; De Laet T.; Janssen R.,Modeling Partial Knowledge on Multiple-Choice Items Using Elimination Testing,2019,56,"Single-best answers to multiple-choice items are commonly dichotomized into correct and incorrect responses, and modeled using either a dichotomous item response theory (IRT) model or a polytomous one if differences among all response options are to be retained. The current study presents an alternative IRT-based modeling approach to multiple-choice items administered with the procedure of elimination testing, which asks test-takers to eliminate all the response options they consider to be incorrect. The partial credit model is derived for the obtained responses. By extracting more information pertaining to test-takers’ partial knowledge on the items, the proposed approach has the advantage of providing more accurate estimation of the latent ability. In addition, it may shed some light on the possible answering processes of test-takers on the items. As an illustration, the proposed approach is applied to a classroom examination of an undergraduate course in engineering science. © 2019 by the National Council on Measurement in Education",Modeling Partial Knowledge on Multiple-Choice Items Using Elimination Testing,"Single-best answers to multiple-choice items are commonly dichotomized into correct and incorrect responses, and modeled using either a dichotomous item response theory (IRT) model or a polytomous one if differences among all response options are to be retained. The current study presents an alternative IRT-based modeling approach to multiple-choice items administered with the procedure of elimination testing, which asks test-takers to eliminate all the response options they consider to be incorrect. The partial credit model is derived for the obtained responses. By extracting more information pertaining to test-takers’ partial knowledge on the items, the proposed approach has the advantage of providing more accurate estimation of the latent ability. In addition, it may shed some light on the possible answering processes of test-takers on the items. As an illustration, the proposed approach is applied to a classroom examination of an undergraduate course in engineering science. © 2019 by the National Council on Measurement in Education","['singlebest', 'answer', 'multiplechoice', 'item', 'commonly', 'dichotomize', 'correct', 'incorrect', 'response', 'dichotomous', 'item', 'response', 'theory', 'IRT', 'polytomous', 'difference', 'response', 'option', 'retain', 'current', 'study', 'present', 'alternative', 'irtbase', 'modeling', 'approach', 'multiplechoice', 'item', 'administer', 'procedure', 'elimination', 'testing', 'ask', 'testtaker', 'eliminate', 'response', 'option', 'consider', 'incorrect', 'partial', 'credit', 'derive', 'obtain', 'response', 'extract', 'information', 'pertain', 'testtaker', ""'"", 'partial', 'knowledge', 'item', 'propose', 'approach', 'advantage', 'provide', 'accurate', 'estimation', 'latent', 'ability', 'addition', 'shed', 'light', 'possible', 'answering', 'process', 'testtaker', 'item', 'illustration', 'propose', 'approach', 'apply', 'classroom', 'examination', 'undergraduate', 'course', 'engineering', 'science', '©', '2019', 'National', 'Council']","['Partial', 'Knowledge', 'MultipleChoice', 'Items', 'Elimination', 'Testing']",singlebest answer multiplechoice item commonly dichotomize correct incorrect response dichotomous item response theory IRT polytomous difference response option retain current study present alternative irtbase modeling approach multiplechoice item administer procedure elimination testing ask testtaker eliminate response option consider incorrect partial credit derive obtain response extract information pertain testtaker ' partial knowledge item propose approach advantage provide accurate estimation latent ability addition shed light possible answering process testtaker item illustration propose approach apply classroom examination undergraduate course engineering science © 2019 National Council,Partial Knowledge MultipleChoice Items Elimination Testing,0.02539112898952563,0.025252354787455304,0.8986956025889985,0.025286453803229334,0.025374459830791295,0.06565964246850518,0.0,0.007619984690463504,0.0,0.0
Svetina D.; Liaw Y.-L.; Rutkowski L.; Rutkowski D.,Routing Strategies and Optimizing Design for Multistage Testing in International Large-Scale Assessments,2019,56,"This study investigates the effect of several design and administration choices on item exposure and person/item parameter recovery under a multistage test (MST) design. In a simulation study, we examine whether number-correct (NC) or item response theory (IRT) methods are differentially effective at routing students to the correct next stage(s) and whether routing choices (optimal versus suboptimal routing) have an impact on achievement precision. Additionally, we examine the impact of testlet length on both person and item recovery. Overall, our results suggest that no single approach works best across the studied conditions. With respect to the mean person parameter recovery, IRT scoring (via either Fisher information or preliminary EAP estimates) outperformed classical NC methods, although differences in bias and root mean squared error were generally small. Item exposure rates were found to be more evenly distributed when suboptimal routing methods were used, and item recovery (both difficulty and discrimination) was most precisely observed for items with moderate difficulties. Based on the results of the simulation study, we draw conclusions and discuss implications for practice in the context of international large-scale assessments that recently introduced adaptive assessment in the form of MST. Future research directions are also discussed. © 2019 by the National Council on Measurement in Education",Routing Strategies and Optimizing Design for Multistage Testing in International Large-Scale Assessments,"This study investigates the effect of several design and administration choices on item exposure and person/item parameter recovery under a multistage test (MST) design. In a simulation study, we examine whether number-correct (NC) or item response theory (IRT) methods are differentially effective at routing students to the correct next stage(s) and whether routing choices (optimal versus suboptimal routing) have an impact on achievement precision. Additionally, we examine the impact of testlet length on both person and item recovery. Overall, our results suggest that no single approach works best across the studied conditions. With respect to the mean person parameter recovery, IRT scoring (via either Fisher information or preliminary EAP estimates) outperformed classical NC methods, although differences in bias and root mean squared error were generally small. Item exposure rates were found to be more evenly distributed when suboptimal routing methods were used, and item recovery (both difficulty and discrimination) was most precisely observed for items with moderate difficulties. Based on the results of the simulation study, we draw conclusions and discuss implications for practice in the context of international large-scale assessments that recently introduced adaptive assessment in the form of MST. Future research directions are also discussed. © 2019 by the National Council on Measurement in Education","['study', 'investigate', 'effect', 'design', 'administration', 'choice', 'item', 'exposure', 'personitem', 'parameter', 'recovery', 'multistage', 'test', 'mst', 'design', 'simulation', 'study', 'examine', 'numbercorrect', 'NC', 'item', 'response', 'theory', 'IRT', 'method', 'differentially', 'effective', 'route', 'student', 'correct', 'stage', 'route', 'choice', 'optimal', 'versus', 'suboptimal', 'routing', 'impact', 'achievement', 'precision', 'additionally', 'examine', 'impact', 'testlet', 'length', 'person', 'item', 'recovery', 'overall', 'result', 'suggest', 'single', 'approach', 'work', 'study', 'condition', 'respect', 'mean', 'person', 'parameter', 'recovery', 'IRT', 'scoring', 'Fisher', 'information', 'preliminary', 'EAP', 'estimate', 'outperform', 'classical', 'NC', 'method', 'difference', 'bias', 'root', 'mean', 'squared', 'error', 'generally', 'small', 'Item', 'exposure', 'rate', 'find', 'evenly', 'distribute', 'suboptimal', 'routing', 'method', 'item', 'recovery', 'difficulty', 'discrimination', 'precisely', 'observe', 'item', 'moderate', 'difficulty', 'base', 'result', 'simulation', 'study', 'draw', 'conclusion', 'discuss', 'implication', 'practice', 'context', 'international', 'largescale', 'assessment', 'recently', 'introduce', 'adaptive', 'assessment', 'form', 'MST', 'Future', 'research', 'direction', 'discuss', '©', '2019', 'National', 'Council']","['route', 'Strategies', 'Optimizing', 'Design', 'Multistage', 'Testing', 'International', 'LargeScale', 'assessment']",study investigate effect design administration choice item exposure personitem parameter recovery multistage test mst design simulation study examine numbercorrect NC item response theory IRT method differentially effective route student correct stage route choice optimal versus suboptimal routing impact achievement precision additionally examine impact testlet length person item recovery overall result suggest single approach work study condition respect mean person parameter recovery IRT scoring Fisher information preliminary EAP estimate outperform classical NC method difference bias root mean squared error generally small Item exposure rate find evenly distribute suboptimal routing method item recovery difficulty discrimination precisely observe item moderate difficulty base result simulation study draw conclusion discuss implication practice context international largescale assessment recently introduce adaptive assessment form MST Future research direction discuss © 2019 National Council,route Strategies Optimizing Design Multistage Testing International LargeScale assessment,0.9136137768726224,0.02172100555065846,0.021462254462876577,0.021572016117138854,0.021630946996703773,0.07752450282169474,0.0,0.014907093833093795,0.010372117907119609,0.006963141655083024
Kim H.J.; Brennan R.L.; Lee W.-C.,Structural Zeros and Their Implications With Log-Linear Bivariate Presmoothing Under the Internal-Anchor Design,2017,54,"In equating, when common items are internal and scoring is conducted in terms of the number of correct items, some pairs of total scores (X) and common-item scores (V) can never be observed in a bivariate distribution of X and V; these pairs are called structural zeros. This simulation study examines how equating results compare for different approaches to handling structural zeros. The study considers four approaches: the no-smoothing, unique-common, total-common, and adjusted total-common approaches. This study led to four main findings: (1) the total-common approach generally had the worst results; (2) for relatively small effect sizes, the unique-common approach generally had the smallest overall error; (3) for relatively large effect sizes, the adjusted total-common approach generally had the smallest overall error; and, (4) if sole interest focuses on reducing bias only, the adjusted total-common approach was generally preferable. These results suggest that, when common items are internal and log-linear bivariate presmoothing is performed, structural zeros should be maintained, even if there is some loss in the moment preservation property. Copyright © 2017 by the National Council on Measurement in Education",Structural Zeros and Their Implications With Log-Linear Bivariate Presmoothing Under the Internal-Anchor Design,"In equating, when common items are internal and scoring is conducted in terms of the number of correct items, some pairs of total scores (X) and common-item scores (V) can never be observed in a bivariate distribution of X and V; these pairs are called structural zeros. This simulation study examines how equating results compare for different approaches to handling structural zeros. The study considers four approaches: the no-smoothing, unique-common, total-common, and adjusted total-common approaches. This study led to four main findings: (1) the total-common approach generally had the worst results; (2) for relatively small effect sizes, the unique-common approach generally had the smallest overall error; (3) for relatively large effect sizes, the adjusted total-common approach generally had the smallest overall error; and, (4) if sole interest focuses on reducing bias only, the adjusted total-common approach was generally preferable. These results suggest that, when common items are internal and log-linear bivariate presmoothing is performed, structural zeros should be maintained, even if there is some loss in the moment preservation property. Copyright © 2017 by the National Council on Measurement in Education","['equate', 'common', 'item', 'internal', 'scoring', 'conduct', 'term', 'number', 'correct', 'item', 'pair', 'total', 'score', 'X', 'commonitem', 'score', 'V', 'observe', 'bivariate', 'distribution', 'x', 'v', 'pair', 'structural', 'zero', 'simulation', 'study', 'examine', 'equate', 'result', 'compare', 'different', 'approach', 'handle', 'structural', 'zero', 'study', 'consider', 'approach', 'nosmoothe', 'uniquecommon', 'totalcommon', 'adjusted', 'totalcommon', 'approach', 'study', 'lead', 'main', 'finding', '1', 'totalcommon', 'approach', 'generally', 'bad', 'result', '2', 'relatively', 'small', 'effect', 'size', 'uniquecommon', 'approach', 'generally', 'small', 'overall', 'error', '3', 'relatively', 'large', 'effect', 'size', 'adjust', 'totalcommon', 'approach', 'generally', 'small', 'overall', 'error', '4', 'sole', 'interest', 'focus', 'reduce', 'bias', 'adjust', 'totalcommon', 'approach', 'generally', 'preferable', 'result', 'suggest', 'common', 'item', 'internal', 'loglinear', 'bivariate', 'presmoothing', 'perform', 'structural', 'zero', 'maintain', 'loss', 'moment', 'preservation', 'property', 'Copyright', '©', '2017', 'National', 'Council']","['Structural', 'Zeros', 'implication', 'LogLinear', 'Bivariate', 'Presmoothing', 'InternalAnchor', 'Design']",equate common item internal scoring conduct term number correct item pair total score X commonitem score V observe bivariate distribution x v pair structural zero simulation study examine equate result compare different approach handle structural zero study consider approach nosmoothe uniquecommon totalcommon adjusted totalcommon approach study lead main finding 1 totalcommon approach generally bad result 2 relatively small effect size uniquecommon approach generally small overall error 3 relatively large effect size adjust totalcommon approach generally small overall error 4 sole interest focus reduce bias adjust totalcommon approach generally preferable result suggest common item internal loglinear bivariate presmoothing perform structural zero maintain loss moment preservation property Copyright © 2017 National Council,Structural Zeros implication LogLinear Bivariate Presmoothing InternalAnchor Design,0.027501224916904373,0.027219454298656083,0.8907395992931253,0.027253383123065793,0.02728633836824847,0.026588792790060364,0.0016115557679648664,0.016727952036900275,0.03964405369534078,0.0008207493240130419
Andrich D.; Marais I.,Controlling Bias in Both Constructed Response and Multiple-Choice Items When Analyzed With the Dichotomous Rasch Model,2018,55,"Even though guessing biases difficulty estimates as a function of item difficulty in the dichotomous Rasch model, assessment programs with tests which include multiple-choice items often construct scales using this model. Research has shown that when all items are multiple-choice, this bias can largely be eliminated. However, many assessments have a combination of multiple-choice and constructed response items. Using vertically scaled numeracy assessments from a large-scale assessment program, this article shows that eliminating the bias on estimates of the multiple-choice items also impacts on the difficulty estimates of the constructed response items. This implies that the original estimates of the constructed response items were biased by the guessing on the multiple-choice items. This bias has implications for both defining difficulties in item banks for use in adaptive testing composed of both multiple-choice and constructed response items, and for the construction of proficiency scales. Copyright © 2018 by the National Council on Measurement in Education",Controlling Bias in Both Constructed Response and Multiple-Choice Items When Analyzed With the Dichotomous Rasch Model,"Even though guessing biases difficulty estimates as a function of item difficulty in the dichotomous Rasch model, assessment programs with tests which include multiple-choice items often construct scales using this model. Research has shown that when all items are multiple-choice, this bias can largely be eliminated. However, many assessments have a combination of multiple-choice and constructed response items. Using vertically scaled numeracy assessments from a large-scale assessment program, this article shows that eliminating the bias on estimates of the multiple-choice items also impacts on the difficulty estimates of the constructed response items. This implies that the original estimates of the constructed response items were biased by the guessing on the multiple-choice items. This bias has implications for both defining difficulties in item banks for use in adaptive testing composed of both multiple-choice and constructed response items, and for the construction of proficiency scales. Copyright © 2018 by the National Council on Measurement in Education","['guess', 'bias', 'difficulty', 'estimate', 'function', 'item', 'difficulty', 'dichotomous', 'Rasch', 'assessment', 'program', 'test', 'include', 'multiplechoice', 'item', 'construct', 'scale', 'Research', 'item', 'multiplechoice', 'bias', 'largely', 'eliminate', 'assessment', 'combination', 'multiplechoice', 'construct', 'response', 'item', 'vertically', 'scale', 'numeracy', 'assessment', 'largescale', 'assessment', 'program', 'article', 'eliminate', 'bias', 'estimate', 'multiplechoice', 'item', 'impact', 'difficulty', 'estimate', 'construct', 'response', 'item', 'imply', 'original', 'estimate', 'construct', 'response', 'item', 'bias', 'guessing', 'multiplechoice', 'item', 'bias', 'implication', 'define', 'difficulty', 'item', 'bank', 'adaptive', 'testing', 'compose', 'multiplechoice', 'construct', 'response', 'item', 'construction', 'proficiency', 'scale', 'copyright', '©', '2018', 'National', 'Council']","['control', 'Bias', 'Constructed', 'Response', 'MultipleChoice', 'Items', 'analyze', 'Dichotomous', 'Rasch']",guess bias difficulty estimate function item difficulty dichotomous Rasch assessment program test include multiplechoice item construct scale Research item multiplechoice bias largely eliminate assessment combination multiplechoice construct response item vertically scale numeracy assessment largescale assessment program article eliminate bias estimate multiplechoice item impact difficulty estimate construct response item imply original estimate construct response item bias guessing multiplechoice item bias implication define difficulty item bank adaptive testing compose multiplechoice construct response item construction proficiency scale copyright © 2018 National Council,control Bias Constructed Response MultipleChoice Items analyze Dichotomous Rasch,0.8620698405503169,0.03450941384976491,0.03456711440138198,0.03422701458882945,0.034626616609706624,0.07032354749260573,0.0,0.012245236887721032,0.0,0.004931364495012373
Clauser B.E.; Baldwin P.; Margolis M.J.; Mee J.; Winward M.,An Experimental Study of the Internal Consistency of Judgments Made in Bookmark Standard Setting,2017,54,"Validating performance standards is challenging and complex. Because of the difficulties associated with collecting evidence related to external criteria, validity arguments rely heavily on evidence related to internal criteria—especially evidence that expert judgments are internally consistent. Given its importance, it is somewhat surprising that evidence of this kind has rarely been published in the context of the widely used bookmark standard-setting procedure. In this article we examined the effect of ordered item booklet difficulty on content experts’ bookmark judgments. If panelists make internally consistent judgments, their resultant cut scores should be unaffected by the difficulty of their respective booklets. This internal consistency was not observed: the results suggest that substantial systematic differences in the resultant cut scores can arise when the difficulty of the ordered item booklets varies. These findings raise questions about the ability of content experts to make the judgments required by the bookmark procedure. Copyright © 2017 by the National Council on Measurement in Education",An Experimental Study of the Internal Consistency of Judgments Made in Bookmark Standard Setting,"Validating performance standards is challenging and complex. Because of the difficulties associated with collecting evidence related to external criteria, validity arguments rely heavily on evidence related to internal criteria—especially evidence that expert judgments are internally consistent. Given its importance, it is somewhat surprising that evidence of this kind has rarely been published in the context of the widely used bookmark standard-setting procedure. In this article we examined the effect of ordered item booklet difficulty on content experts’ bookmark judgments. If panelists make internally consistent judgments, their resultant cut scores should be unaffected by the difficulty of their respective booklets. This internal consistency was not observed: the results suggest that substantial systematic differences in the resultant cut scores can arise when the difficulty of the ordered item booklets varies. These findings raise questions about the ability of content experts to make the judgments required by the bookmark procedure. Copyright © 2017 by the National Council on Measurement in Education","['validate', 'performance', 'standard', 'challenging', 'complex', 'difficulty', 'associate', 'collect', 'evidence', 'relate', 'external', 'criterion', 'validity', 'argument', 'rely', 'heavily', 'evidence', 'relate', 'internal', 'criterion', '—', 'especially', 'evidence', 'expert', 'judgment', 'internally', 'consistent', 'importance', 'somewhat', 'surprising', 'evidence', 'kind', 'rarely', 'publish', 'context', 'widely', 'bookmark', 'standardsette', 'procedure', 'article', 'examine', 'effect', 'order', 'item', 'booklet', 'difficulty', 'content', 'expert', '’', 'bookmark', 'judgment', 'panelist', 'internally', 'consistent', 'judgment', 'resultant', 'cut', 'score', 'unaffected', 'difficulty', 'respective', 'booklet', 'internal', 'consistency', 'observe', 'result', 'suggest', 'substantial', 'systematic', 'difference', 'resultant', 'cut', 'score', 'arise', 'difficulty', 'ordered', 'item', 'booklet', 'vary', 'finding', 'raise', 'question', 'ability', 'content', 'expert', 'judgment', 'require', 'bookmark', 'procedure', 'Copyright', '©', '2017', 'National', 'Council']","['Experimental', 'Study', 'Internal', 'Consistency', 'Judgments', 'Bookmark', 'Standard', 'Setting']",validate performance standard challenging complex difficulty associate collect evidence relate external criterion validity argument rely heavily evidence relate internal criterion — especially evidence expert judgment internally consistent importance somewhat surprising evidence kind rarely publish context widely bookmark standardsette procedure article examine effect order item booklet difficulty content expert ’ bookmark judgment panelist internally consistent judgment resultant cut score unaffected difficulty respective booklet internal consistency observe result suggest substantial systematic difference resultant cut score arise difficulty ordered item booklet vary finding raise question ability content expert judgment require bookmark procedure Copyright © 2017 National Council,Experimental Study Internal Consistency Judgments Bookmark Standard Setting,0.02627026169410594,0.8942400796810784,0.026298464011142803,0.026476618745575716,0.02671457586809703,0.012529595511793402,0.0018292429609490985,0.0569180227077332,0.0015434069659410734,0.009772405578468793
Wiberg M.; González J.,Statistical Assessment of Estimated Transformations in Observed-Score Equating,2016,53,"Equating methods make use of an appropriate transformation function to map the scores of one test form into the scale of another so that scores are comparable and can be used interchangeably. The equating literature shows that the ways of judging the success of an equating (i.e., the score transformation) might differ depending on the adopted framework. Rather than targeting different parts of the equating process and aiming to evaluate the process from different aspects, this article views the equating transformation as a standard statistical estimator and discusses how this estimator should be assessed in an equating framework. For the kernel equating framework, a numerical illustration shows the potentials of viewing the equating transformation as a statistical estimator as opposed to assessing it using equating-specific criteria. A discussion on how this approach can be used to compare other equating estimators from different frameworks is also included. © 2016 by the National Council on Measurement in Education.",Statistical Assessment of Estimated Transformations in Observed-Score Equating,"Equating methods make use of an appropriate transformation function to map the scores of one test form into the scale of another so that scores are comparable and can be used interchangeably. The equating literature shows that the ways of judging the success of an equating (i.e., the score transformation) might differ depending on the adopted framework. Rather than targeting different parts of the equating process and aiming to evaluate the process from different aspects, this article views the equating transformation as a standard statistical estimator and discusses how this estimator should be assessed in an equating framework. For the kernel equating framework, a numerical illustration shows the potentials of viewing the equating transformation as a statistical estimator as opposed to assessing it using equating-specific criteria. A discussion on how this approach can be used to compare other equating estimators from different frameworks is also included. © 2016 by the National Council on Measurement in Education.","['equating', 'method', 'appropriate', 'transformation', 'function', 'map', 'score', 'test', 'form', 'scale', 'score', 'comparable', 'interchangeably', 'equate', 'literature', 'way', 'judge', 'success', 'equating', 'ie', 'score', 'transformation', 'differ', 'depend', 'adopt', 'framework', 'target', 'different', 'equate', 'process', 'aim', 'evaluate', 'process', 'different', 'aspect', 'article', 'view', 'equate', 'transformation', 'standard', 'statistical', 'estimator', 'discuss', 'estimator', 'assess', 'equate', 'framework', 'kernel', 'equating', 'framework', 'numerical', 'illustration', 'potential', 'view', 'equate', 'transformation', 'statistical', 'estimator', 'oppose', 'assess', 'equatingspecific', 'criterion', 'discussion', 'approach', 'compare', 'equate', 'estimator', 'different', 'framework', 'include', '©', '2016', 'National', 'Council']","['statistical', 'Assessment', 'Estimated', 'Transformations', 'ObservedScore', 'Equating']",equating method appropriate transformation function map score test form scale score comparable interchangeably equate literature way judge success equating ie score transformation differ depend adopt framework target different equate process aim evaluate process different aspect article view equate transformation standard statistical estimator discuss estimator assess equate framework kernel equating framework numerical illustration potential view equate transformation statistical estimator oppose assess equatingspecific criterion discussion approach compare equate estimator different framework include © 2016 National Council,statistical Assessment Estimated Transformations ObservedScore Equating,0.03210043964018638,0.03129789307298735,0.03144830116318973,0.8738507377804,0.031302628343236445,0.0,0.0,0.015073654302474114,0.1631588462180912,0.0008552373668802515
Herborn K.; Mustafić M.; Greiff S.,Mapping an Experiment-Based Assessment of Collaborative Behavior Onto Collaborative Problem Solving in PISA 2015: A Cluster Analysis Approach for Collaborator Profiles,2017,54,"Collaborative problem solving (CPS) assessment is a new academic research field with a number of educational implications. In 2015, the Programme for International Student Assessment (PISA) assessed CPS with a computer-simulated human-agent (H-A) approach that claimed to measure 12 individual CPS skills for the first time. After reviewing the approach, we conceptually embedded a computer-based collaborative behavior assessment (COLBAS) into the overarching PISA 2015 CPS approach. COLBAS is an H-A CPS assessment instrument that can be used to measure certain aspects of CPS. In addition, we applied a model-based cluster analysis to the embedded COLBAS aspects. The analysis revealed three types of student collaborator profiles that differed in cognitive performance and motivation: (a) passive low-performing (non-)collaborators, (b) active high-performing collaborators, and (c) compensating collaborators. Copyright © 2017 by the National Council on Measurement in Education",Mapping an Experiment-Based Assessment of Collaborative Behavior Onto Collaborative Problem Solving in PISA 2015: A Cluster Analysis Approach for Collaborator Profiles,"Collaborative problem solving (CPS) assessment is a new academic research field with a number of educational implications. In 2015, the Programme for International Student Assessment (PISA) assessed CPS with a computer-simulated human-agent (H-A) approach that claimed to measure 12 individual CPS skills for the first time. After reviewing the approach, we conceptually embedded a computer-based collaborative behavior assessment (COLBAS) into the overarching PISA 2015 CPS approach. COLBAS is an H-A CPS assessment instrument that can be used to measure certain aspects of CPS. In addition, we applied a model-based cluster analysis to the embedded COLBAS aspects. The analysis revealed three types of student collaborator profiles that differed in cognitive performance and motivation: (a) passive low-performing (non-)collaborators, (b) active high-performing collaborators, and (c) compensating collaborators. Copyright © 2017 by the National Council on Measurement in Education","['collaborative', 'problem', 'solve', 'CPS', 'assessment', 'new', 'academic', 'research', 'field', 'number', 'educational', 'implication', '2015', 'Programme', 'International', 'Student', 'Assessment', 'PISA', 'assess', 'CPS', 'computersimulate', 'humanagent', 'ha', 'approach', 'claim', 'measure', '12', 'individual', 'CPS', 'skill', 'time', 'review', 'approach', 'conceptually', 'embed', 'computerbased', 'collaborative', 'behavior', 'assessment', 'COLBAS', 'overarching', 'PISA', '2015', 'CPS', 'approach', 'COLBAS', 'ha', 'CPS', 'assessment', 'instrument', 'measure', 'certain', 'aspect', 'CPS', 'addition', 'apply', 'modelbase', 'cluster', 'analysis', 'embedded', 'COLBAS', 'aspect', 'analysis', 'reveal', 'type', 'student', 'collaborator', 'profile', 'differ', 'cognitive', 'performance', 'motivation', 'passive', 'lowperforme', 'noncollaborator', 'b', 'active', 'highperforming', 'collaborator', 'c', 'compensating', 'collaborator', 'Copyright', '©', '2017', 'National', 'Council']","['mapping', 'ExperimentBased', 'Assessment', 'Collaborative', 'Behavior', 'Onto', 'Collaborative', 'Problem', 'Solving', 'PISA', '2015', 'Cluster', 'Analysis', 'Approach', 'Collaborator', 'Profiles']",collaborative problem solve CPS assessment new academic research field number educational implication 2015 Programme International Student Assessment PISA assess CPS computersimulate humanagent ha approach claim measure 12 individual CPS skill time review approach conceptually embed computerbased collaborative behavior assessment COLBAS overarching PISA 2015 CPS approach COLBAS ha CPS assessment instrument measure certain aspect CPS addition apply modelbase cluster analysis embedded COLBAS aspect analysis reveal type student collaborator profile differ cognitive performance motivation passive lowperforme noncollaborator b active highperforming collaborator c compensating collaborator Copyright © 2017 National Council,mapping ExperimentBased Assessment Collaborative Behavior Onto Collaborative Problem Solving PISA 2015 Cluster Analysis Approach Collaborator Profiles,0.02673862906738583,0.026766746012134236,0.026829938656969792,0.026896466071188292,0.8927682201923218,0.0,0.0,0.09882231532778421,0.0,0.0
Wind S.A.; Jones E.,Exploring the Influence of Range Restrictions on Connectivity in Sparse Assessment Networks: An Illustration and Exploration Within the Context of Classroom Observations,2018,55,"Range restrictions, or raters’ tendency to limit their ratings to a subset of available rating scale categories, are well documented in large-scale teacher evaluation systems based on principal observations. When these restrictions occur, the ratings observed during operational teacher evaluations are limited to a subset of the available categories. However, range restrictions are less common within teacher performances that are used to establish links (anchor ratings) in otherwise disconnected assessment systems. As a result, principals’ category use may be different between anchor ratings and operational ratings. The purpose of this study is to explore the consequences of discrepancies in rating scale category use across operational and anchor ratings within the context of teacher evaluation systems based on principal observations. First, we used real data to illustrate the presence of range restriction in operational ratings, and the effect of this restriction on connectivity. Then, we used simulated data to explore these effects using experimental manipulation. Results suggested that discrepancies in range restriction between anchor and operational ratings do not systematically impact the precision of teacher, principal, and teaching practice estimates. We discuss the implications of these results in terms of research and practice for teacher evaluation systems. Copyright © 2018 by the National Council on Measurement in Education",Exploring the Influence of Range Restrictions on Connectivity in Sparse Assessment Networks: An Illustration and Exploration Within the Context of Classroom Observations,"Range restrictions, or raters’ tendency to limit their ratings to a subset of available rating scale categories, are well documented in large-scale teacher evaluation systems based on principal observations. When these restrictions occur, the ratings observed during operational teacher evaluations are limited to a subset of the available categories. However, range restrictions are less common within teacher performances that are used to establish links (anchor ratings) in otherwise disconnected assessment systems. As a result, principals’ category use may be different between anchor ratings and operational ratings. The purpose of this study is to explore the consequences of discrepancies in rating scale category use across operational and anchor ratings within the context of teacher evaluation systems based on principal observations. First, we used real data to illustrate the presence of range restriction in operational ratings, and the effect of this restriction on connectivity. Then, we used simulated data to explore these effects using experimental manipulation. Results suggested that discrepancies in range restriction between anchor and operational ratings do not systematically impact the precision of teacher, principal, and teaching practice estimates. We discuss the implications of these results in terms of research and practice for teacher evaluation systems. Copyright © 2018 by the National Council on Measurement in Education","['range', 'restriction', 'rater', ""'"", 'tendency', 'limit', 'rating', 'subset', 'available', 'rating', 'scale', 'category', 'document', 'largescale', 'teacher', 'evaluation', 'system', 'base', 'principal', 'observation', 'restriction', 'occur', 'rating', 'observe', 'operational', 'teacher', 'evaluation', 'limit', 'subset', 'available', 'category', 'range', 'restriction', 'common', 'teacher', 'performance', 'establish', 'link', 'anchor', 'rating', 'disconnect', 'assessment', 'system', 'result', 'principal', ""'"", 'category', 'different', 'anchor', 'rating', 'operational', 'rating', 'purpose', 'study', 'explore', 'consequence', 'discrepancy', 'rating', 'scale', 'category', 'operational', 'anchor', 'rating', 'context', 'teacher', 'evaluation', 'system', 'base', 'principal', 'observation', 'real', 'datum', 'illustrate', 'presence', 'range', 'restriction', 'operational', 'rating', 'effect', 'restriction', 'connectivity', 'simulated', 'datum', 'explore', 'effect', 'experimental', 'manipulation', 'result', 'suggest', 'discrepancy', 'range', 'restriction', 'anchor', 'operational', 'rating', 'systematically', 'impact', 'precision', 'teacher', 'principal', 'teaching', 'practice', 'estimate', 'discuss', 'implication', 'result', 'term', 'research', 'practice', 'teacher', 'evaluation', 'system', 'Copyright', '©', '2018', 'National', 'Council']","['explore', 'Influence', 'Range', 'Restrictions', 'Connectivity', 'Sparse', 'Assessment', 'Networks', 'Illustration', 'Exploration', 'Context', 'Classroom', 'Observations']",range restriction rater ' tendency limit rating subset available rating scale category document largescale teacher evaluation system base principal observation restriction occur rating observe operational teacher evaluation limit subset available category range restriction common teacher performance establish link anchor rating disconnect assessment system result principal ' category different anchor rating operational rating purpose study explore consequence discrepancy rating scale category operational anchor rating context teacher evaluation system base principal observation real datum illustrate presence range restriction operational rating effect restriction connectivity simulated datum explore effect experimental manipulation result suggest discrepancy range restriction anchor operational rating systematically impact precision teacher principal teaching practice estimate discuss implication result term research practice teacher evaluation system Copyright © 2018 National Council,explore Influence Range Restrictions Connectivity Sparse Assessment Networks Illustration Exploration Context Classroom Observations,0.03164577445079466,0.031479492902997154,0.031074734127031222,0.03127266150019157,0.8745273370189854,0.0,0.010338048535642954,0.03346354118308789,0.007791472515782875,0.08986076959110909
Rosen Y.,Assessing Students in Human-to-Agent Settings to Inform Collaborative Problem-Solving Learning,2017,54,"In order to understand potential applications of collaborative problem-solving (CPS) assessment tasks, it is necessary to examine empirically the multifaceted student performance that may be distributed across collaboration methods and purposes of the assessment. Ideally, each student should be matched with various types of group members and must apply the skills in varied contexts and tasks. One solution to these assessment demands is to use computer-based (virtual) agents to serve as the collaborators in the interactions with students. This article proposes a human-to-agent (H-A) approach for formative CPS assessment and describes an international pilot study aimed to provide preliminary empirical findings on the use of H-A CPS assessment to inform collaborative learning. Overall, the findings showed promise in terms of using a H-A CPS assessment task as a formative tool for structuring effective groups in the context of CPS online learning. Copyright © 2017 by the National Council on Measurement in Education",Assessing Students in Human-to-Agent Settings to Inform Collaborative Problem-Solving Learning,"In order to understand potential applications of collaborative problem-solving (CPS) assessment tasks, it is necessary to examine empirically the multifaceted student performance that may be distributed across collaboration methods and purposes of the assessment. Ideally, each student should be matched with various types of group members and must apply the skills in varied contexts and tasks. One solution to these assessment demands is to use computer-based (virtual) agents to serve as the collaborators in the interactions with students. This article proposes a human-to-agent (H-A) approach for formative CPS assessment and describes an international pilot study aimed to provide preliminary empirical findings on the use of H-A CPS assessment to inform collaborative learning. Overall, the findings showed promise in terms of using a H-A CPS assessment task as a formative tool for structuring effective groups in the context of CPS online learning. Copyright © 2017 by the National Council on Measurement in Education","['order', 'understand', 'potential', 'application', 'collaborative', 'problemsolve', 'CPS', 'assessment', 'task', 'necessary', 'examine', 'empirically', 'multifaceted', 'student', 'performance', 'distribute', 'collaboration', 'method', 'purpose', 'assessment', 'ideally', 'student', 'match', 'type', 'group', 'member', 'apply', 'skill', 'varied', 'context', 'task', 'solution', 'assessment', 'demand', 'computerbase', 'virtual', 'agent', 'serve', 'collaborator', 'interaction', 'student', 'article', 'propose', 'humantoagent', 'ha', 'approach', 'formative', 'CPS', 'assessment', 'describe', 'international', 'pilot', 'study', 'aim', 'provide', 'preliminary', 'empirical', 'finding', 'ha', 'CPS', 'assessment', 'inform', 'collaborative', 'learn', 'Overall', 'finding', 'promise', 'term', 'ha', 'CPS', 'assessment', 'task', 'formative', 'tool', 'structure', 'effective', 'group', 'context', 'CPS', 'online', 'learn', 'Copyright', '©', '2017', 'National', 'Council']","['assess', 'student', 'HumantoAgent', 'Settings', 'inform', 'Collaborative', 'ProblemSolving', 'Learning']",order understand potential application collaborative problemsolve CPS assessment task necessary examine empirically multifaceted student performance distribute collaboration method purpose assessment ideally student match type group member apply skill varied context task solution assessment demand computerbase virtual agent serve collaborator interaction student article propose humantoagent ha approach formative CPS assessment describe international pilot study aim provide preliminary empirical finding ha CPS assessment inform collaborative learn Overall finding promise term ha CPS assessment task formative tool structure effective group context CPS online learn Copyright © 2017 National Council,assess student HumantoAgent Settings inform Collaborative ProblemSolving Learning,0.026261833308686925,0.026123505717347662,0.02590974806669353,0.8948382897528319,0.026866623154439932,0.0,0.0,0.13373811669905875,0.0,0.0
Jin K.-Y.; Wang W.-C.,A New Facets Model for Rater's Centrality/Extremity Response Style,2018,55,"The Rasch facets model was developed to account for facet data, such as student essays graded by raters, but it accounts for only one kind of rater effect (severity). In practice, raters may exhibit various tendencies such as using middle or extreme scores in their ratings, which is referred to as the rater centrality/extremity response style. To achieve better measurement quality in rater data, it is desirable to simultaneously consider both rater severity and rater centrality/extremity. A new facets model is thus developed by adding to the Rasch facets model a weight parameter for the item thresholds for each rater. The parameters of the new facets model can be estimated with the JAGS freeware. An empirical example is provided to illustrate the implications and applications of the new model. Two simulation studies were conducted. The first simulation was to evaluate the parameter recovery of the new facets model and the consequences of ignoring the effects of rater centrality/extremity on parameter estimation. The second simulation was to illustrate how rater severity affects the relationship between rater centrality and the standard deviation of raw rating scores. © 2018 by the National Council on Measurement in Education",A New Facets Model for Rater's Centrality/Extremity Response Style,"The Rasch facets model was developed to account for facet data, such as student essays graded by raters, but it accounts for only one kind of rater effect (severity). In practice, raters may exhibit various tendencies such as using middle or extreme scores in their ratings, which is referred to as the rater centrality/extremity response style. To achieve better measurement quality in rater data, it is desirable to simultaneously consider both rater severity and rater centrality/extremity. A new facets model is thus developed by adding to the Rasch facets model a weight parameter for the item thresholds for each rater. The parameters of the new facets model can be estimated with the JAGS freeware. An empirical example is provided to illustrate the implications and applications of the new model. Two simulation studies were conducted. The first simulation was to evaluate the parameter recovery of the new facets model and the consequences of ignoring the effects of rater centrality/extremity on parameter estimation. The second simulation was to illustrate how rater severity affects the relationship between rater centrality and the standard deviation of raw rating scores. © 2018 by the National Council on Measurement in Education","['Rasch', 'facet', 'develop', 'account', 'facet', 'datum', 'student', 'essay', 'grade', 'rater', 'account', 'kind', 'rater', 'effect', 'severity', 'practice', 'rater', 'exhibit', 'tendency', 'middle', 'extreme', 'score', 'rating', 'refer', 'rater', 'centralityextremity', 'response', 'style', 'achieve', 'quality', 'rater', 'datum', 'desirable', 'simultaneously', 'consider', 'rater', 'severity', 'rater', 'centralityextremity', 'new', 'facet', 'develop', 'add', 'Rasch', 'facet', 'weight', 'parameter', 'item', 'threshold', 'rater', 'parameter', 'new', 'facet', 'estimate', 'JAGS', 'freeware', 'empirical', 'example', 'provide', 'illustrate', 'implication', 'application', 'new', 'simulation', 'study', 'conduct', 'simulation', 'evaluate', 'parameter', 'recovery', 'new', 'facet', 'consequence', 'ignore', 'effect', 'rater', 'centralityextremity', 'parameter', 'estimation', 'second', 'simulation', 'illustrate', 'rater', 'severity', 'affect', 'relationship', 'rater', 'centrality', 'standard', 'deviation', 'raw', 'rating', 'score', '©', '2018', 'National', 'Council']","['New', 'Facets', 'Raters', 'CentralityExtremity', 'Response', 'Style']",Rasch facet develop account facet datum student essay grade rater account kind rater effect severity practice rater exhibit tendency middle extreme score rating refer rater centralityextremity response style achieve quality rater datum desirable simultaneously consider rater severity rater centralityextremity new facet develop add Rasch facet weight parameter item threshold rater parameter new facet estimate JAGS freeware empirical example provide illustrate implication application new simulation study conduct simulation evaluate parameter recovery new facet consequence ignore effect rater centralityextremity parameter estimation second simulation illustrate rater severity affect relationship rater centrality standard deviation raw rating score © 2018 National Council,New Facets Raters CentralityExtremity Response Style,0.8692367386421732,0.032692324186702204,0.03242447844401855,0.03243874751122266,0.03320771121588329,0.001978237959026071,0.0021092695612704225,0.0,0.0,0.26990737545290633
Wolkowitz A.A.; Wright K.D.,Effectiveness of Equating at the Passing Score for Exams With Small Sample Sizes,2019,56,"This article explores the amount of equating error at a passing score when equating scores from exams with small samples sizes. This article focuses on equating using classical test theory methods of Tucker linear, Levine linear, frequency estimation, and chained equipercentile equating. Both simulation and real data studies were used in the investigation. The results of the study supported past findings that as the sample sizes increase, the amount of bias in the equating at the passing score decreases. The research also highlights the importance for practitioners to understand the data, to have an informed expectation of the results, and to have a documented rationale for an acceptable amount of equating error. © 2019 by the National Council on Measurement in Education",Effectiveness of Equating at the Passing Score for Exams With Small Sample Sizes,"This article explores the amount of equating error at a passing score when equating scores from exams with small samples sizes. This article focuses on equating using classical test theory methods of Tucker linear, Levine linear, frequency estimation, and chained equipercentile equating. Both simulation and real data studies were used in the investigation. The results of the study supported past findings that as the sample sizes increase, the amount of bias in the equating at the passing score decreases. The research also highlights the importance for practitioners to understand the data, to have an informed expectation of the results, and to have a documented rationale for an acceptable amount of equating error. © 2019 by the National Council on Measurement in Education","['article', 'explore', 'equate', 'error', 'pass', 'score', 'equate', 'score', 'exam', 'small', 'sample', 'size', 'article', 'focus', 'equate', 'classical', 'test', 'theory', 'method', 'Tucker', 'linear', 'Levine', 'linear', 'frequency', 'estimation', 'chain', 'equipercentile', 'equate', 'simulation', 'real', 'datum', 'study', 'investigation', 'result', 'study', 'support', 'past', 'finding', 'sample', 'size', 'increase', 'bias', 'equating', 'pass', 'score', 'decrease', 'research', 'highlight', 'importance', 'practitioner', 'understand', 'datum', 'informed', 'expectation', 'result', 'document', 'rationale', 'acceptable', 'equate', 'error', '©', '2019', 'National', 'Council']","['effectiveness', 'Equating', 'Passing', 'Score', 'exam', 'Small', 'Sample', 'size']",article explore equate error pass score equate score exam small sample size article focus equate classical test theory method Tucker linear Levine linear frequency estimation chain equipercentile equate simulation real datum study investigation result study support past finding sample size increase bias equating pass score decrease research highlight importance practitioner understand datum informed expectation result document rationale acceptable equate error © 2019 National Council,effectiveness Equating Passing Score exam Small Sample size,0.02998247194431553,0.028872877021208623,0.8829933106684491,0.029107697116230492,0.029043643249796195,0.0,0.0034895784736274035,0.010614718915898012,0.17808790005955982,0.002768915414091878
Peabody M.R.; Wind S.A.,Exploring the Influence of Judge Proficiency on Standard-Setting Judgments,2019,56,"Setting performance standards is a judgmental process involving human opinions and values as well as technical and empirical considerations. Although all cut score decisions are by nature somewhat arbitrary, they should not be capricious. Judges selected for standard-setting panels should have the proper qualifications to make the judgments asked of them; however, even qualified judges vary in expertise and in some cases, such as highly specialized areas or when members of the public are involved, it may be difficult to ensure that each member of a standard-setting panel has the requisite expertise to make qualified judgments. Given the subjective nature of these types of judgments, and that a large part of the validity argument for an exam lies in the robustness of its passing standard, an examination of the influence of judge proficiency on the judgments is warranted. This study explores the use of the many-facet Rasch model as a method for adjusting modified Angoff standard-setting ratings based on judges’ proficiency levels. The results suggest differences in the severity and quality of standard-setting judgments across levels of judge proficiency, such that judges who answered easy items incorrectly tended to perceive them as easier, but those who answered correctly tended to provide ratings within normal stochastic limits. © 2019 by the National Council on Measurement in Education",Exploring the Influence of Judge Proficiency on Standard-Setting Judgments,"Setting performance standards is a judgmental process involving human opinions and values as well as technical and empirical considerations. Although all cut score decisions are by nature somewhat arbitrary, they should not be capricious. Judges selected for standard-setting panels should have the proper qualifications to make the judgments asked of them; however, even qualified judges vary in expertise and in some cases, such as highly specialized areas or when members of the public are involved, it may be difficult to ensure that each member of a standard-setting panel has the requisite expertise to make qualified judgments. Given the subjective nature of these types of judgments, and that a large part of the validity argument for an exam lies in the robustness of its passing standard, an examination of the influence of judge proficiency on the judgments is warranted. This study explores the use of the many-facet Rasch model as a method for adjusting modified Angoff standard-setting ratings based on judges’ proficiency levels. The results suggest differences in the severity and quality of standard-setting judgments across levels of judge proficiency, such that judges who answered easy items incorrectly tended to perceive them as easier, but those who answered correctly tended to provide ratings within normal stochastic limits. © 2019 by the National Council on Measurement in Education","['set', 'performance', 'standard', 'judgmental', 'process', 'involve', 'human', 'opinion', 'value', 'technical', 'empirical', 'consideration', 'cut', 'score', 'decision', 'nature', 'somewhat', 'arbitrary', 'capricious', 'Judges', 'select', 'standardsetting', 'panel', 'proper', 'qualification', 'judgment', 'ask', 'qualified', 'judge', 'vary', 'expertise', 'case', 'highly', 'specialized', 'area', 'member', 'public', 'involve', 'difficult', 'ensure', 'member', 'standardsette', 'panel', 'requisite', 'expertise', 'qualified', 'judgment', 'subjective', 'nature', 'type', 'judgment', 'large', 'validity', 'argument', 'exam', 'lie', 'robustness', 'pass', 'standard', 'examination', 'influence', 'judge', 'proficiency', 'judgment', 'warrant', 'study', 'explore', 'manyfacet', 'Rasch', 'method', 'adjust', 'modify', 'Angoff', 'standardsette', 'rating', 'base', 'judge', '’', 'proficiency', 'level', 'result', 'suggest', 'difference', 'severity', 'quality', 'standardsette', 'judgment', 'level', 'judge', 'proficiency', 'judge', 'answer', 'easy', 'item', 'incorrectly', 'tend', 'perceive', 'easy', 'answer', 'correctly', 'tend', 'provide', 'rating', 'normal', 'stochastic', 'limit', '©', '2019', 'National', 'Council']","['explore', 'Influence', 'Judge', 'Proficiency', 'StandardSetting', 'Judgments']",set performance standard judgmental process involve human opinion value technical empirical consideration cut score decision nature somewhat arbitrary capricious Judges select standardsetting panel proper qualification judgment ask qualified judge vary expertise case highly specialized area member public involve difficult ensure member standardsette panel requisite expertise qualified judgment subjective nature type judgment large validity argument exam lie robustness pass standard examination influence judge proficiency judgment warrant study explore manyfacet Rasch method adjust modify Angoff standardsette rating base judge ’ proficiency level result suggest difference severity quality standardsette judgment level judge proficiency judge answer easy item incorrectly tend perceive easy answer correctly tend provide rating normal stochastic limit © 2019 National Council,explore Influence Judge Proficiency StandardSetting Judgments,0.024838186944771124,0.90068826478844,0.024863975914735253,0.024765449930711003,0.02484412242134258,0.004610099215367188,0.008308281154547691,0.043395564261298196,0.0030797521919125087,0.02604020452610761
McNeish D.; Dumas D.,Calculating Conditional Reliability for Dynamic Measurement Model Capacity Estimates,2018,55,"Dynamic measurement modeling (DMM) is a recent framework for measuring developing constructs whose manifestation occurs after an assessment is administered (e.g., learning capacity). Empirical studies have suggested that DMM may improve consequential validity of test scores because DMM learning capacity estimates were shown to be much less related to demographic factors like examinees’ socioeconomic status compared to traditional single-administration item response theory (IRT)–based estimates. Though promotion of DMM has hinged on improved validity, no methods for computing reliability (a prerequisite for validity) have been advanced and DMM is sufficiently different from classical test theory (CTT) and IRT that known methods cannot be directly imported. This article advances one method for computing conditional reliability for DMM so that precision of the estimates can be assessed. © 2018 by the National Council on Measurement in Education",Calculating Conditional Reliability for Dynamic Measurement Model Capacity Estimates,"Dynamic measurement modeling (DMM) is a recent framework for measuring developing constructs whose manifestation occurs after an assessment is administered (e.g., learning capacity). Empirical studies have suggested that DMM may improve consequential validity of test scores because DMM learning capacity estimates were shown to be much less related to demographic factors like examinees’ socioeconomic status compared to traditional single-administration item response theory (IRT)–based estimates. Though promotion of DMM has hinged on improved validity, no methods for computing reliability (a prerequisite for validity) have been advanced and DMM is sufficiently different from classical test theory (CTT) and IRT that known methods cannot be directly imported. This article advances one method for computing conditional reliability for DMM so that precision of the estimates can be assessed. © 2018 by the National Council on Measurement in Education","['dynamic', 'DMM', 'recent', 'framework', 'measure', 'develop', 'construct', 'manifestation', 'occur', 'assessment', 'administer', 'eg', 'learning', 'capacity', 'empirical', 'study', 'suggest', 'DMM', 'improve', 'consequential', 'validity', 'test', 'score', 'DMM', 'learn', 'capacity', 'estimate', 'related', 'demographic', 'factor', 'like', 'examinee', ""'"", 'socioeconomic', 'status', 'compare', 'traditional', 'singleadministration', 'item', 'response', 'theory', 'IRT', '–', 'base', 'estimate', 'promotion', 'DMM', 'hinge', 'improve', 'validity', 'method', 'compute', 'reliability', 'prerequisite', 'validity', 'advance', 'DMM', 'sufficiently', 'different', 'classical', 'test', 'theory', 'CTT', 'IRT', 'know', 'method', 'directly', 'import', 'article', 'advance', 'method', 'compute', 'conditional', 'reliability', 'DMM', 'precision', 'estimate', 'assess', '©', '2018', 'National', 'Council']","['calculate', 'Conditional', 'Reliability', 'Dynamic', 'Capacity', 'estimate']",dynamic DMM recent framework measure develop construct manifestation occur assessment administer eg learning capacity empirical study suggest DMM improve consequential validity test score DMM learn capacity estimate related demographic factor like examinee ' socioeconomic status compare traditional singleadministration item response theory IRT – base estimate promotion DMM hinge improve validity method compute reliability prerequisite validity advance DMM sufficiently different classical test theory CTT IRT know method directly import article advance method compute conditional reliability DMM precision estimate assess © 2018 National Council,calculate Conditional Reliability Dynamic Capacity estimate,0.03013564227086131,0.030029915915952072,0.029969422701646282,0.8797265408553572,0.03013847825618313,0.02358748083203861,0.014437059465350023,0.044041413968543774,0.005160676236795482,0.0
Johnson M.S.; Sinharay S.,Measures of Agreement to Assess Attribute-Level Classification Accuracy and Consistency for Cognitive Diagnostic Assessments,2018,55,"One of the proposed uses of cognitive diagnostic assessments is to classify the examinees as either masters or nonmasters on each of a number of skills being assessed. As with any test, it is important to report the quality of these binary classifications with measures of their reliability. Cui et al. and Wang et al. have suggested reliability measures that can be calculated from the model parameters of cognitive diagnosis models; these previously suggested indices are measures of agreement between either the estimated and true mastery classifications, or between the estimated classifications from parallel assessments. This article discusses the limitations of these existing methods and suggests the use of other measures of agreement. A simulation study demonstrates that the proposed measures are related to factors that would be expected to be associated with reliability; for example, reliability increases with variability in the population and with item discrimination, whereas the previously suggested measures do not show the same pattern. A real data example is also included. © 2018 by the National Council on Measurement in Education",Measures of Agreement to Assess Attribute-Level Classification Accuracy and Consistency for Cognitive Diagnostic Assessments,"One of the proposed uses of cognitive diagnostic assessments is to classify the examinees as either masters or nonmasters on each of a number of skills being assessed. As with any test, it is important to report the quality of these binary classifications with measures of their reliability. Cui et al. and Wang et al. have suggested reliability measures that can be calculated from the model parameters of cognitive diagnosis models; these previously suggested indices are measures of agreement between either the estimated and true mastery classifications, or between the estimated classifications from parallel assessments. This article discusses the limitations of these existing methods and suggests the use of other measures of agreement. A simulation study demonstrates that the proposed measures are related to factors that would be expected to be associated with reliability; for example, reliability increases with variability in the population and with item discrimination, whereas the previously suggested measures do not show the same pattern. A real data example is also included. © 2018 by the National Council on Measurement in Education","['propose', 'cognitive', 'diagnostic', 'assessment', 'classify', 'examinee', 'master', 'nonmaster', 'number', 'skill', 'assess', 'test', 'important', 'report', 'quality', 'binary', 'classification', 'measure', 'reliability', 'Cui', 'et', 'al', 'Wang', 'et', 'al', 'suggest', 'reliability', 'measure', 'calculate', 'parameter', 'cognitive', 'diagnosis', 'previously', 'suggest', 'index', 'measure', 'agreement', 'estimated', 'true', 'mastery', 'classification', 'estimate', 'classification', 'parallel', 'assessment', 'article', 'discuss', 'limitation', 'exist', 'method', 'suggest', 'measure', 'agreement', 'simulation', 'study', 'demonstrate', 'propose', 'measure', 'relate', 'factor', 'expect', 'associate', 'reliability', 'example', 'reliability', 'increase', 'variability', 'population', 'item', 'discrimination', 'previously', 'suggest', 'measure', 'pattern', 'real', 'datum', 'example', 'include', '©', '2018', 'National', 'Council']","['measure', 'Agreement', 'Assess', 'AttributeLevel', 'Classification', 'Accuracy', 'Consistency', 'Cognitive', 'Diagnostic', 'assessment']",propose cognitive diagnostic assessment classify examinee master nonmaster number skill assess test important report quality binary classification measure reliability Cui et al Wang et al suggest reliability measure calculate parameter cognitive diagnosis previously suggest index measure agreement estimated true mastery classification estimate classification parallel assessment article discuss limitation exist method suggest measure agreement simulation study demonstrate propose measure relate factor expect associate reliability example reliability increase variability population item discrimination previously suggest measure pattern real datum example include © 2018 National Council,measure Agreement Assess AttributeLevel Classification Accuracy Consistency Cognitive Diagnostic assessment,0.02745375613722888,0.027300790229000532,0.02726482494739732,0.027558086383359857,0.8904225423030134,0.0366086935424751,0.05730285282594372,0.0394643252249795,0.0,0.004797998160544433
Berger S.; Verschoor A.J.; Eggen T.J.H.M.; Moser U.,Efficiency of Targeted Multistage Calibration Designs Under Practical Constraints: A Simulation Study,2019,56,"Calibration of an item bank for computer adaptive testing requires substantial resources. In this study, we investigated whether the efficiency of calibration under the Rasch model could be enhanced by improving the match between item difficulty and student ability. We introduced targeted multistage calibration designs, a design type that considers ability-related background variables and performance for assigning students to suitable items. Furthermore, we investigated whether uncertainty about item difficulty could impair the assembling of efficient designs. The results indicated that targeted multistage calibration designs were more efficient than ordinary targeted designs under optimal conditions. Limited knowledge about item difficulty reduced the efficiency of one of the two investigated targeted multistage calibration designs, whereas targeted designs were more robust. © 2019 by the National Council on Measurement in Education",Efficiency of Targeted Multistage Calibration Designs Under Practical Constraints: A Simulation Study,"Calibration of an item bank for computer adaptive testing requires substantial resources. In this study, we investigated whether the efficiency of calibration under the Rasch model could be enhanced by improving the match between item difficulty and student ability. We introduced targeted multistage calibration designs, a design type that considers ability-related background variables and performance for assigning students to suitable items. Furthermore, we investigated whether uncertainty about item difficulty could impair the assembling of efficient designs. The results indicated that targeted multistage calibration designs were more efficient than ordinary targeted designs under optimal conditions. Limited knowledge about item difficulty reduced the efficiency of one of the two investigated targeted multistage calibration designs, whereas targeted designs were more robust. © 2019 by the National Council on Measurement in Education","['calibration', 'item', 'bank', 'computer', 'adaptive', 'testing', 'require', 'substantial', 'resource', 'study', 'investigate', 'efficiency', 'calibration', 'Rasch', 'enhance', 'improve', 'match', 'item', 'difficulty', 'student', 'ability', 'introduce', 'target', 'multistage', 'calibration', 'design', 'design', 'type', 'consider', 'abilityrelate', 'background', 'variable', 'performance', 'assign', 'student', 'suitable', 'item', 'furthermore', 'investigate', 'uncertainty', 'item', 'difficulty', 'impair', 'assembling', 'efficient', 'design', 'result', 'indicate', 'target', 'multistage', 'calibration', 'design', 'efficient', 'ordinary', 'target', 'design', 'optimal', 'condition', 'limited', 'knowledge', 'item', 'difficulty', 'reduce', 'efficiency', 'investigate', 'target', 'multistage', 'calibration', 'design', 'target', 'design', 'robust', '©', '2019', 'National', 'Council']","['efficiency', 'Targeted', 'Multistage', 'calibration', 'design', 'practical', 'Constraints', 'A', 'Simulation', 'Study']",calibration item bank computer adaptive testing require substantial resource study investigate efficiency calibration Rasch enhance improve match item difficulty student ability introduce target multistage calibration design design type consider abilityrelate background variable performance assign student suitable item furthermore investigate uncertainty item difficulty impair assembling efficient design result indicate target multistage calibration design efficient ordinary target design optimal condition limited knowledge item difficulty reduce efficiency investigate target multistage calibration design target design robust © 2019 National Council,efficiency Targeted Multistage calibration design practical Constraints A Simulation Study,0.03227083242180223,0.031958836723717056,0.8718663680433674,0.03184595072186337,0.03205801208924999,0.04133010169638613,0.0,0.009636554126507216,0.012620103359306408,0.0
Shin H.J.; Wilson M.; Choi I.-H.,Structured Constructs Models Based on Change-Point Analysis,2017,54,"This study proposes a structured constructs model (SCM) to examine measurement in the context of a multidimensional learning progression (LP). The LP is assumed to have features that go beyond a typical multidimentional IRT model, in that there are hypothesized to be certain cross-dimensional linkages that correspond to requirements between the levels of the different dimensions. The new model builds on multidimensional item response theory models and change-point analysis to add cut-score and discontinuity parameters that embody these substantive requirements. This modeling strategy allows us to place the examinees in the appropriate LP level and simultaneously to model the hypothesized requirement relations. Results from a simulation study indicate that the proposed change-point SCM recovers the generating parameters well. When the hypothesized requirement relations are ignored, the model fit tends to become worse, and the model parameters appear to be more biased. Moreover, the proposed model can be used to find validity evidence to support or disprove initial theoretical hypothesized links in the LP through empirical data. We illustrate the technique with data from an assessment system designed to measure student progress in a middle-school statistics and modeling curriculum. Copyright © 2017 by the National Council on Measurement in Education",Structured Constructs Models Based on Change-Point Analysis,"This study proposes a structured constructs model (SCM) to examine measurement in the context of a multidimensional learning progression (LP). The LP is assumed to have features that go beyond a typical multidimentional IRT model, in that there are hypothesized to be certain cross-dimensional linkages that correspond to requirements between the levels of the different dimensions. The new model builds on multidimensional item response theory models and change-point analysis to add cut-score and discontinuity parameters that embody these substantive requirements. This modeling strategy allows us to place the examinees in the appropriate LP level and simultaneously to model the hypothesized requirement relations. Results from a simulation study indicate that the proposed change-point SCM recovers the generating parameters well. When the hypothesized requirement relations are ignored, the model fit tends to become worse, and the model parameters appear to be more biased. Moreover, the proposed model can be used to find validity evidence to support or disprove initial theoretical hypothesized links in the LP through empirical data. We illustrate the technique with data from an assessment system designed to measure student progress in a middle-school statistics and modeling curriculum. Copyright © 2017 by the National Council on Measurement in Education","['study', 'propose', 'structured', 'constructs', 'SCM', 'examine', 'context', 'multidimensional', 'learning', 'progression', 'LP', 'LP', 'assume', 'feature', 'typical', 'multidimentional', 'IRT', 'hypothesize', 'certain', 'crossdimensional', 'linkage', 'correspond', 'requirement', 'level', 'different', 'dimension', 'new', 'build', 'multidimensional', 'item', 'response', 'theory', 'changepoint', 'analysis', 'add', 'cutscore', 'discontinuity', 'parameter', 'embody', 'substantive', 'requirement', 'modeling', 'strategy', 'allow', 'place', 'examinee', 'appropriate', 'lp', 'level', 'simultaneously', 'hypothesize', 'requirement', 'relation', 'result', 'simulation', 'study', 'indicate', 'propose', 'changepoint', 'SCM', 'recover', 'generate', 'parameter', 'hypothesized', 'requirement', 'relation', 'ignore', 'fit', 'tend', 'bad', 'parameter', 'appear', 'biased', 'propose', 'find', 'validity', 'evidence', 'support', 'disprove', 'initial', 'theoretical', 'hypothesize', 'link', 'LP', 'empirical', 'datum', 'illustrate', 'technique', 'datum', 'assessment', 'system', 'design', 'measure', 'student', 'progress', 'middleschool', 'statistic', 'curriculum', 'Copyright', '©', '2017', 'National', 'Council']","['structure', 'Constructs', 'Models', 'base', 'ChangePoint', 'Analysis']",study propose structured constructs SCM examine context multidimensional learning progression LP LP assume feature typical multidimentional IRT hypothesize certain crossdimensional linkage correspond requirement level different dimension new build multidimensional item response theory changepoint analysis add cutscore discontinuity parameter embody substantive requirement modeling strategy allow place examinee appropriate lp level simultaneously hypothesize requirement relation result simulation study indicate propose changepoint SCM recover generate parameter hypothesized requirement relation ignore fit tend bad parameter appear biased propose find validity evidence support disprove initial theoretical hypothesize link LP empirical datum illustrate technique datum assessment system design measure student progress middleschool statistic curriculum Copyright © 2017 National Council,structure Constructs Models base ChangePoint Analysis,0.025321438230403618,0.025249691840311363,0.02515332800171364,0.02525538808261295,0.8990201538449584,0.03220881575719109,0.0031163818490757677,0.03428359277347216,0.0,0.01309442990924831
Olsen J.; Aleven V.; Rummel N.,Statistically Modeling Individual Students’ Learning Over Successive Collaborative Practice Opportunities,2017,54,"Within educational data mining, many statistical models capture the learning of students working individually. However, not much work has been done to extend these statistical models of individual learning to a collaborative setting, despite the effectiveness of collaborative learning activities. We extend a widely used model (the additive factors model) to account for the effect of collaboration on individual learning, including having the help of a partner and getting to observe/help a partner. We find evidence that models that include these collaborative features have a better fit than the original models for performance data and that learning rates estimated using the extended models provide insights into how collaboration benefits individual students’ learning outcomes. Copyright © 2017 by the National Council on Measurement in Education",Statistically Modeling Individual Students’ Learning Over Successive Collaborative Practice Opportunities,"Within educational data mining, many statistical models capture the learning of students working individually. However, not much work has been done to extend these statistical models of individual learning to a collaborative setting, despite the effectiveness of collaborative learning activities. We extend a widely used model (the additive factors model) to account for the effect of collaboration on individual learning, including having the help of a partner and getting to observe/help a partner. We find evidence that models that include these collaborative features have a better fit than the original models for performance data and that learning rates estimated using the extended models provide insights into how collaboration benefits individual students’ learning outcomes. Copyright © 2017 by the National Council on Measurement in Education","['educational', 'datum', 'statistical', 'capture', 'learning', 'student', 'work', 'individually', 'work', 'extend', 'statistical', 'individual', 'learning', 'collaborative', 'setting', 'despite', 'effectiveness', 'collaborative', 'learning', 'activity', 'extend', 'widely', 'additive', 'factor', 'account', 'effect', 'collaboration', 'individual', 'learning', 'include', 'help', 'partner', 'observehelp', 'partner', 'find', 'evidence', 'include', 'collaborative', 'feature', 'fit', 'original', 'performance', 'datum', 'learn', 'rate', 'estimate', 'extend', 'provide', 'insight', 'collaboration', 'benefit', 'individual', 'student', '’', 'learn', 'outcome', 'copyright', '©', '2017', 'National', 'Council']","['statistically', 'Individual', 'Students', ""'"", 'learning', 'Successive', 'Collaborative', 'Practice', 'opportunity']",educational datum statistical capture learning student work individually work extend statistical individual learning collaborative setting despite effectiveness collaborative learning activity extend widely additive factor account effect collaboration individual learning include help partner observehelp partner find evidence include collaborative feature fit original performance datum learn rate estimate extend provide insight collaboration benefit individual student ’ learn outcome copyright © 2017 National Council,statistically Individual Students ' learning Successive Collaborative Practice opportunity,0.03055183711237002,0.030620054726904693,0.030466097867384592,0.030653797721358532,0.8777082125719822,0.0,0.0,0.09710558871422685,0.0,0.0
Madison M.J.; Bradshaw L.,Evaluating Intervention Effects in a Diagnostic Classification Model Framework,2018,55,"The evaluation of intervention effects is an important objective of educational research. One way to evaluate the effectiveness of an intervention is to conduct an experiment that assigns individuals to control and treatment groups. In the context of pretest/posttest designed studies, this is referred to as a control-group pretest/posttest design. The transition diagnostic classification model (TDCM) was recently developed to assess growth, defined as change in attribute mastery status over time, in a diagnostic classification model framework. The TDCM, however, does not model multiple groups, and therefore is not able to analyze data from a control-group pretest/posttest designed experiment. In this study, we extend the TDCM to model multiple groups, thereby enabling the examination of group-differential growth in attribute mastery and the evaluation of intervention effects. The utility of the multigroup TDCM is demonstrated in the evaluation of an innovative instructional method in mathematics education. Copyright © 2018 by the National Council on Measurement in Education",Evaluating Intervention Effects in a Diagnostic Classification Model Framework,"The evaluation of intervention effects is an important objective of educational research. One way to evaluate the effectiveness of an intervention is to conduct an experiment that assigns individuals to control and treatment groups. In the context of pretest/posttest designed studies, this is referred to as a control-group pretest/posttest design. The transition diagnostic classification model (TDCM) was recently developed to assess growth, defined as change in attribute mastery status over time, in a diagnostic classification model framework. The TDCM, however, does not model multiple groups, and therefore is not able to analyze data from a control-group pretest/posttest designed experiment. In this study, we extend the TDCM to model multiple groups, thereby enabling the examination of group-differential growth in attribute mastery and the evaluation of intervention effects. The utility of the multigroup TDCM is demonstrated in the evaluation of an innovative instructional method in mathematics education. Copyright © 2018 by the National Council on Measurement in Education","['evaluation', 'intervention', 'effect', 'important', 'objective', 'educational', 'research', 'way', 'evaluate', 'effectiveness', 'intervention', 'conduct', 'experiment', 'assign', 'individual', 'control', 'treatment', 'group', 'context', 'pretestposttest', 'design', 'study', 'refer', 'controlgroup', 'pretestpostt', 'design', 'transition', 'diagnostic', 'classification', 'TDCM', 'recently', 'develop', 'assess', 'growth', 'define', 'change', 'attribute', 'mastery', 'status', 'time', 'diagnostic', 'classification', 'framework', 'TDCM', 'multiple', 'group', 'able', 'analyze', 'datum', 'controlgroup', 'pretestposttest', 'design', 'experiment', 'study', 'extend', 'TDCM', 'multiple', 'group', 'enable', 'examination', 'groupdifferential', 'growth', 'attribute', 'mastery', 'evaluation', 'intervention', 'effect', 'utility', 'multigroup', 'TDCM', 'demonstrate', 'evaluation', 'innovative', 'instructional', 'method', 'mathematics', 'Copyright', '©', '2018', 'National', 'Council']","['evaluate', 'Intervention', 'Effects', 'Diagnostic', 'Classification', 'Framework']",evaluation intervention effect important objective educational research way evaluate effectiveness intervention conduct experiment assign individual control treatment group context pretestposttest design study refer controlgroup pretestpostt design transition diagnostic classification TDCM recently develop assess growth define change attribute mastery status time diagnostic classification framework TDCM multiple group able analyze datum controlgroup pretestposttest design experiment study extend TDCM multiple group enable examination groupdifferential growth attribute mastery evaluation intervention effect utility multigroup TDCM demonstrate evaluation innovative instructional method mathematics Copyright © 2018 National Council,evaluate Intervention Effects Diagnostic Classification Framework,0.8878180737368566,0.02796072870557639,0.027721589195794184,0.02809129251777611,0.028408315843996616,0.013338484034989996,0.01592384114269807,0.04464869460507305,0.0060925133449398075,0.008720394256304078
Zhang X.; Tao J.; Wang C.; Shi N.-Z.,Bayesian Model Selection Methods for Multilevel IRT Models: A Comparison of Five DIC-Based Indices,2019,56,"Model selection is important in any statistical analysis, and the primary goal is to find the preferred (or most parsimonious) model, based on certain criteria, from a set of candidate models given data. Several recent publications have employed the deviance information criterion (DIC) to do model selection among different forms of multilevel item response theory models (MLIRT). The majority of the practitioners use WinBUGS for implementing MCMC algorithms for MLIRT models, and the default version of DIC provided by WinBUGS focused on the measurement-level parameters only. The results herein show that this version of DIC is inappropriate. This study introduces five variants of DIC as a model selection index for MLIRT models with dichotomous outcomes. Considering a multilevel IRT model with three levels, five forms of DIC are formed: first-level conditional DIC computed from the measurement model only, which is the index given by many software packages such as WinBUGS; second-level marginalized DIC and second-level joint DIC computed from the second-level model; and top-level marginalized DIC and top-level joint DIC computed from the entire model. We evaluate the performance of the five model selection indices via simulation studies. The manipulated factors include the number of groups, the number of second-level covariates, the number of top-level covariates, and the types of measurement models (one-parameter vs. two-parameter). Considering the computational viability and interpretability, the second-level joint DIC is recommended for MLIRT models under our simulated conditions. © 2019 by the National Council on Measurement in Education",Bayesian Model Selection Methods for Multilevel IRT Models: A Comparison of Five DIC-Based Indices,"Model selection is important in any statistical analysis, and the primary goal is to find the preferred (or most parsimonious) model, based on certain criteria, from a set of candidate models given data. Several recent publications have employed the deviance information criterion (DIC) to do model selection among different forms of multilevel item response theory models (MLIRT). The majority of the practitioners use WinBUGS for implementing MCMC algorithms for MLIRT models, and the default version of DIC provided by WinBUGS focused on the measurement-level parameters only. The results herein show that this version of DIC is inappropriate. This study introduces five variants of DIC as a model selection index for MLIRT models with dichotomous outcomes. Considering a multilevel IRT model with three levels, five forms of DIC are formed: first-level conditional DIC computed from the measurement model only, which is the index given by many software packages such as WinBUGS; second-level marginalized DIC and second-level joint DIC computed from the second-level model; and top-level marginalized DIC and top-level joint DIC computed from the entire model. We evaluate the performance of the five model selection indices via simulation studies. The manipulated factors include the number of groups, the number of second-level covariates, the number of top-level covariates, and the types of measurement models (one-parameter vs. two-parameter). Considering the computational viability and interpretability, the second-level joint DIC is recommended for MLIRT models under our simulated conditions. © 2019 by the National Council on Measurement in Education","['selection', 'important', 'statistical', 'analysis', 'primary', 'goal', 'find', 'preferred', 'parsimonious', 'base', 'certain', 'criterion', 'set', 'candidate', 'datum', 'recent', 'publication', 'employ', 'deviance', 'information', 'criterion', 'DIC', 'selection', 'different', 'form', 'multilevel', 'item', 'response', 'theory', 'MLIRT', 'majority', 'practitioner', 'WinBUGS', 'implement', 'MCMC', 'algorithm', 'MLIRT', 'default', 'version', 'DIC', 'provide', 'WinBUGS', 'focus', 'measurementlevel', 'parameter', 'result', 'version', 'DIC', 'inappropriate', 'study', 'introduce', 'variant', 'DIC', 'selection', 'index', 'MLIRT', 'dichotomous', 'outcome', 'consider', 'multilevel', 'IRT', 'level', 'form', 'DIC', 'form', 'firstlevel', 'conditional', 'DIC', 'compute', 'index', 'software', 'package', 'WinBUGS', 'secondlevel', 'marginalize', 'DIC', 'secondlevel', 'joint', 'DIC', 'compute', 'secondlevel', 'toplevel', 'marginalize', 'DIC', 'toplevel', 'joint', 'DIC', 'compute', 'entire', 'evaluate', 'performance', 'selection', 'indice', 'simulation', 'study', 'manipulate', 'factor', 'include', 'number', 'group', 'number', 'secondlevel', 'covariate', 'number', 'toplevel', 'covariate', 'type', 'oneparameter', 'vs', 'twoparameter', 'consider', 'computational', 'viability', 'interpretability', 'secondlevel', 'joint', 'DIC', 'recommend', 'MLIRT', 'simulate', 'condition', '©', '2019', 'National', 'Council']","['Bayesian', 'Selection', 'Methods', 'Multilevel', 'IRT', 'Models', 'Comparison', 'Five', 'DICBased', 'index']",selection important statistical analysis primary goal find preferred parsimonious base certain criterion set candidate datum recent publication employ deviance information criterion DIC selection different form multilevel item response theory MLIRT majority practitioner WinBUGS implement MCMC algorithm MLIRT default version DIC provide WinBUGS focus measurementlevel parameter result version DIC inappropriate study introduce variant DIC selection index MLIRT dichotomous outcome consider multilevel IRT level form DIC form firstlevel conditional DIC compute index software package WinBUGS secondlevel marginalize DIC secondlevel joint DIC compute secondlevel toplevel marginalize DIC toplevel joint DIC compute entire evaluate performance selection indice simulation study manipulate factor include number group number secondlevel covariate number toplevel covariate type oneparameter vs twoparameter consider computational viability interpretability secondlevel joint DIC recommend MLIRT simulate condition © 2019 National Council,Bayesian Selection Methods Multilevel IRT Models Comparison Five DICBased index,0.8803874813573462,0.030114542476256674,0.029872667374762525,0.02977991323965775,0.02984539555197677,0.02822364348236218,0.0044408448383770754,0.005034462887157821,0.00297613643049745,0.0
Luo X.; Kim D.,A Top-Down Approach to Designing the Computerized Adaptive Multistage Test,2018,55,"The top-down approach to designing a multistage test is relatively understudied in the literature and underused in research and practice. This study introduced a route-based top-down design approach that directly sets design parameters at the test level and utilizes the advanced automated test assembly algorithm seeking global optimality. The design process in this approach consists of five sub-processes: (1) route mapping, (2) setting objectives, (3) setting constraints, (4) routing error control, and (5) test assembly. Results from a simulation study confirmed that the assembly, measurement and routing results of the top-down design eclipsed those of the bottom-up design. Additionally, the top-down design approach provided unique insights into design decisions that could be used to refine the test. Regardless of these advantages, it is recommended applying both top-down and bottom-up approaches in a complementary manner in practice. Copyright © 2018 by the National Council on Measurement in Education",A Top-Down Approach to Designing the Computerized Adaptive Multistage Test,"The top-down approach to designing a multistage test is relatively understudied in the literature and underused in research and practice. This study introduced a route-based top-down design approach that directly sets design parameters at the test level and utilizes the advanced automated test assembly algorithm seeking global optimality. The design process in this approach consists of five sub-processes: (1) route mapping, (2) setting objectives, (3) setting constraints, (4) routing error control, and (5) test assembly. Results from a simulation study confirmed that the assembly, measurement and routing results of the top-down design eclipsed those of the bottom-up design. Additionally, the top-down design approach provided unique insights into design decisions that could be used to refine the test. Regardless of these advantages, it is recommended applying both top-down and bottom-up approaches in a complementary manner in practice. Copyright © 2018 by the National Council on Measurement in Education","['topdown', 'approach', 'design', 'multistage', 'test', 'relatively', 'understudied', 'literature', 'underuse', 'research', 'practice', 'study', 'introduce', 'routebase', 'topdown', 'design', 'approach', 'directly', 'set', 'design', 'parameter', 'test', 'level', 'utilize', 'advanced', 'automate', 'test', 'assembly', 'algorithm', 'seek', 'global', 'optimality', 'design', 'process', 'approach', 'consist', 'subprocesse', '1', 'route', 'mapping', '2', 'set', 'objective', '3', 'setting', 'constraint', '4', 'route', 'error', 'control', '5', 'test', 'assembly', 'result', 'simulation', 'study', 'confirm', 'assembly', 'route', 'result', 'topdown', 'design', 'eclipse', 'bottomup', 'design', 'additionally', 'topdown', 'design', 'approach', 'provide', 'unique', 'insight', 'design', 'decision', 'refine', 'test', 'regardless', 'advantage', 'recommend', 'apply', 'topdown', 'bottomup', 'approach', 'complementary', 'manner', 'practice', 'Copyright', '©', '2018', 'National', 'Council']","['TopDown', 'Approach', 'design', 'Computerized', 'Adaptive', 'Multistage', 'test']",topdown approach design multistage test relatively understudied literature underuse research practice study introduce routebase topdown design approach directly set design parameter test level utilize advanced automate test assembly algorithm seek global optimality design process approach consist subprocesse 1 route mapping 2 set objective 3 setting constraint 4 route error control 5 test assembly result simulation study confirm assembly route result topdown design eclipse bottomup design additionally topdown design approach provide unique insight design decision refine test regardless advantage recommend apply topdown bottomup approach complementary manner practice Copyright © 2018 National Council,TopDown Approach design Computerized Adaptive Multistage test,0.02865981180880573,0.8861967528311406,0.028387367234504895,0.02826720020923533,0.028488867916313512,0.03142732375873549,0.0,0.023686539769450184,0.011519932768413923,0.0
Liu S.; Cai Y.; Tu D.,On-the-Fly Constraint-Controlled Assembly Methods for Multistage Adaptive Testing for Cognitive Diagnosis,2018,55,"This study applied the mode of on-the-fly assembled multistage adaptive testing to cognitive diagnosis (CD-OMST). Several and several module assembly methods for CD-OMST were proposed and compared in terms of measurement precision, test security, and constrain management. The module assembly methods in the study included the maximum priority index method (MPI), the revised maximum priority index (RMPI), the weighted deviation model (WDM), and the two revised Monte Carlo methods (R1-MC, R2-MC). Simulation results showed that on the whole the CD-OMST performs well in that it not only has acceptable attribute pattern correct classification rates but also satisfies both statistical and nonstatistical constraints; the RMPI method was generally better than the MPI method, the R2-MC method was generally better than the R1-MC method, and the two revised Monte Carlo methods performed best in terms of test security and constraint management, whereas the RMPI and WDM methods worked best in terms of measurement precision. The study is not only expected to provide information about how to combine MST and CD using an on-the-fly method and how do these assembled methods in CD-OMST perform relative to each other but also offer guidance for practitioners to assemble modules in CD-OMST with both statistical and nonstatistical constraints. © 2018 by the National Council on Measurement in Education",On-the-Fly Constraint-Controlled Assembly Methods for Multistage Adaptive Testing for Cognitive Diagnosis,"This study applied the mode of on-the-fly assembled multistage adaptive testing to cognitive diagnosis (CD-OMST). Several and several module assembly methods for CD-OMST were proposed and compared in terms of measurement precision, test security, and constrain management. The module assembly methods in the study included the maximum priority index method (MPI), the revised maximum priority index (RMPI), the weighted deviation model (WDM), and the two revised Monte Carlo methods (R1-MC, R2-MC). Simulation results showed that on the whole the CD-OMST performs well in that it not only has acceptable attribute pattern correct classification rates but also satisfies both statistical and nonstatistical constraints; the RMPI method was generally better than the MPI method, the R2-MC method was generally better than the R1-MC method, and the two revised Monte Carlo methods performed best in terms of test security and constraint management, whereas the RMPI and WDM methods worked best in terms of measurement precision. The study is not only expected to provide information about how to combine MST and CD using an on-the-fly method and how do these assembled methods in CD-OMST perform relative to each other but also offer guidance for practitioners to assemble modules in CD-OMST with both statistical and nonstatistical constraints. © 2018 by the National Council on Measurement in Education","['study', 'apply', 'mode', 'onthefly', 'assemble', 'multistage', 'adaptive', 'testing', 'cognitive', 'diagnosis', 'CDOMST', 'module', 'assembly', 'method', 'cdomst', 'propose', 'compare', 'term', 'precision', 'test', 'security', 'constrain', 'management', 'module', 'assembly', 'method', 'study', 'include', 'maximum', 'priority', 'index', 'method', 'MPI', 'revise', 'maximum', 'priority', 'index', 'rmpi', 'weight', 'deviation', 'WDM', 'revise', 'Monte', 'Carlo', 'method', 'R1MC', 'R2MC', 'Simulation', 'result', 'cdomst', 'perform', 'acceptable', 'attribute', 'pattern', 'correct', 'classification', 'rate', 'satisfy', 'statistical', 'nonstatistical', 'constraint', 'rmpi', 'method', 'generally', 'MPI', 'method', 'R2MC', 'method', 'generally', 'R1MC', 'method', 'revise', 'Monte', 'Carlo', 'method', 'perform', 'term', 'test', 'security', 'constraint', 'management', 'RMPI', 'WDM', 'method', 'work', 'term', 'precision', 'study', 'expect', 'provide', 'information', 'combine', 'MST', 'CD', 'onthefly', 'method', 'assemble', 'method', 'cdomst', 'perform', 'relative', 'offer', 'guidance', 'practitioner', 'assemble', 'module', 'cdomst', 'statistical', 'nonstatistical', 'constraint', '©', '2018', 'National', 'Council']","['OntheFly', 'ConstraintControlled', 'Assembly', 'Methods', 'Multistage', 'Adaptive', 'Testing', 'Cognitive', 'diagnosis']",study apply mode onthefly assemble multistage adaptive testing cognitive diagnosis CDOMST module assembly method cdomst propose compare term precision test security constrain management module assembly method study include maximum priority index method MPI revise maximum priority index rmpi weight deviation WDM revise Monte Carlo method R1MC R2MC Simulation result cdomst perform acceptable attribute pattern correct classification rate satisfy statistical nonstatistical constraint rmpi method generally MPI method R2MC method generally R1MC method revise Monte Carlo method perform term test security constraint management RMPI WDM method work term precision study expect provide information combine MST CD onthefly method assemble method cdomst perform relative offer guidance practitioner assemble module cdomst statistical nonstatistical constraint © 2018 National Council,OntheFly ConstraintControlled Assembly Methods Multistage Adaptive Testing Cognitive diagnosis,0.0277221074013282,0.8889367808441344,0.027482695609862214,0.027727713083913103,0.028130703060762152,0.03685335577718252,0.011294000678127707,0.0027795957170139193,0.017830583710300444,0.0
Scoular C.; Care E.; Hesse F.W.,Designs for Operationalizing Collaborative Problem Solving for Automated Assessment,2017,54,"Collaborative problem solving is a complex skill set that draws on social and cognitive factors. The construct remains in its infancy due to lack of empirical evidence that can be drawn upon for validation. The differences and similarities between two large-scale initiatives that reflect this state of the art, in terms of underlying assumptions about the construct and approach to task development, are outlined. The goal is to clarify how definitions of the nature of the construct impact the approach to design of assessment tasks. Illustrations of two different approaches to the development of a task designed to elicit behaviors that manifest the construct are presented. The method highlights the degree to which these approaches might constrain a comprehensive assessment of the construct. Copyright © 2017 by the National Council on Measurement in Education",Designs for Operationalizing Collaborative Problem Solving for Automated Assessment,"Collaborative problem solving is a complex skill set that draws on social and cognitive factors. The construct remains in its infancy due to lack of empirical evidence that can be drawn upon for validation. The differences and similarities between two large-scale initiatives that reflect this state of the art, in terms of underlying assumptions about the construct and approach to task development, are outlined. The goal is to clarify how definitions of the nature of the construct impact the approach to design of assessment tasks. Illustrations of two different approaches to the development of a task designed to elicit behaviors that manifest the construct are presented. The method highlights the degree to which these approaches might constrain a comprehensive assessment of the construct. Copyright © 2017 by the National Council on Measurement in Education","['collaborative', 'problem', 'solve', 'complex', 'skill', 'set', 'draw', 'social', 'cognitive', 'factor', 'construct', 'remain', 'infancy', 'lack', 'empirical', 'evidence', 'draw', 'validation', 'difference', 'similarity', 'largescale', 'initiative', 'reflect', 'state', 'art', 'term', 'underlie', 'assumption', 'construct', 'approach', 'task', 'development', 'outline', 'goal', 'clarify', 'definition', 'nature', 'construct', 'impact', 'approach', 'design', 'assessment', 'task', 'illustration', 'different', 'approach', 'development', 'task', 'design', 'elicit', 'behavior', 'manifest', 'construct', 'present', 'method', 'highlight', 'degree', 'approach', 'constrain', 'comprehensive', 'assessment', 'construct', 'Copyright', '©', '2017', 'National', 'Council']","['design', 'Operationalizing', 'Collaborative', 'Problem', 'Solving', 'Automated', 'Assessment']",collaborative problem solve complex skill set draw social cognitive factor construct remain infancy lack empirical evidence draw validation difference similarity largescale initiative reflect state art term underlie assumption construct approach task development outline goal clarify definition nature construct impact approach design assessment task illustration different approach development task design elicit behavior manifest construct present method highlight degree approach constrain comprehensive assessment construct Copyright © 2017 National Council,design Operationalizing Collaborative Problem Solving Automated Assessment,0.028143855453626822,0.8894841166761931,0.027221008352925527,0.027392586258710723,0.02775843325854391,0.004561452218476376,0.0,0.0974128119627593,0.0,0.0
Humphry S.M.; Heldsinger S.,A Two-Stage Method for Classroom Assessments of Essay Writing,2019,56,"To capitalize on professional expertise in educational assessment, it is desirable to develop and test methods of rater-mediated assessment that enable classroom teachers to make reliable and informative judgments. Accordingly, this article investigates the reliability of a two-stage method used by classroom teachers to assess primary school students’ persuasive writing. Stage 1 involves pairwise comparisons and stage 2 involves rating against calibrated exemplars from stage 1 plus performance descriptors. A high level of interrater reliability among teachers was obtained. This is consistent with previous evidence that the two-stage method is a viable classroom assessment method without extensive training and moderation. Implications for assessment practices in education are discussed with a focus on the widely expressed desire to value professional expertise. © 2019 by the National Council on Measurement in Education",A Two-Stage Method for Classroom Assessments of Essay Writing,"To capitalize on professional expertise in educational assessment, it is desirable to develop and test methods of rater-mediated assessment that enable classroom teachers to make reliable and informative judgments. Accordingly, this article investigates the reliability of a two-stage method used by classroom teachers to assess primary school students’ persuasive writing. Stage 1 involves pairwise comparisons and stage 2 involves rating against calibrated exemplars from stage 1 plus performance descriptors. A high level of interrater reliability among teachers was obtained. This is consistent with previous evidence that the two-stage method is a viable classroom assessment method without extensive training and moderation. Implications for assessment practices in education are discussed with a focus on the widely expressed desire to value professional expertise. © 2019 by the National Council on Measurement in Education","['capitalize', 'professional', 'expertise', 'educational', 'assessment', 'desirable', 'develop', 'test', 'method', 'ratermediate', 'assessment', 'enable', 'classroom', 'teacher', 'reliable', 'informative', 'judgment', 'accordingly', 'article', 'investigate', 'reliability', 'twostage', 'method', 'classroom', 'teacher', 'assess', 'primary', 'school', 'student', ""'"", 'persuasive', 'write', 'stage', '1', 'involve', 'pairwise', 'comparison', 'stage', '2', 'involve', 'rating', 'calibrate', 'exemplar', 'stage', '1', 'plus', 'performance', 'descriptor', 'high', 'level', 'interrater', 'reliability', 'teacher', 'obtain', 'consistent', 'previous', 'evidence', 'twostage', 'method', 'viable', 'classroom', 'assessment', 'method', 'extensive', 'training', 'moderation', 'Implications', 'assessment', 'practice', 'discuss', 'focus', 'widely', 'express', 'desire', 'value', 'professional', 'expertise', '©', '2019', 'National', 'Council']","['TwoStage', 'Method', 'Classroom', 'assessment', 'Essay', 'writing']",capitalize professional expertise educational assessment desirable develop test method ratermediate assessment enable classroom teacher reliable informative judgment accordingly article investigate reliability twostage method classroom teacher assess primary school student ' persuasive write stage 1 involve pairwise comparison stage 2 involve rating calibrate exemplar stage 1 plus performance descriptor high level interrater reliability teacher obtain consistent previous evidence twostage method viable classroom assessment method extensive training moderation Implications assessment practice discuss focus widely express desire value professional expertise © 2019 National Council,TwoStage Method Classroom assessment Essay writing,0.02604487728939138,0.02645558153407332,0.8955446294286789,0.025877942283789715,0.026076969464066766,0.0,0.003005593470095062,0.099466984474227,0.0,0.01370295028597853
Trierweiler T.J.; Lewis C.; Smith R.L.,Further Study of the Choice of Anchor Tests in Equating,2016,53,"In this study, we describe what factors influence the observed score correlation between an (external) anchor test and a total test. We show that the anchor to full-test observed score correlation is based on two components: the true score correlation between the anchor and total test, and the reliability of the anchor test. Findings using an analytical approach suggest that making an anchor test a miditest does not generally maximize the anchor to total test correlation. Results are discussed in the context of what conditions maximize the correlations between the anchor and total test. Copyright © 2016 by the National Council on Measurement in Education",Further Study of the Choice of Anchor Tests in Equating,"In this study, we describe what factors influence the observed score correlation between an (external) anchor test and a total test. We show that the anchor to full-test observed score correlation is based on two components: the true score correlation between the anchor and total test, and the reliability of the anchor test. Findings using an analytical approach suggest that making an anchor test a miditest does not generally maximize the anchor to total test correlation. Results are discussed in the context of what conditions maximize the correlations between the anchor and total test. Copyright © 2016 by the National Council on Measurement in Education","['study', 'describe', 'factor', 'influence', 'observe', 'score', 'correlation', 'external', 'anchor', 'test', 'total', 'test', 'anchor', 'fulltest', 'observe', 'score', 'correlation', 'base', 'component', 'true', 'score', 'correlation', 'anchor', 'total', 'test', 'reliability', 'anchor', 'test', 'Findings', 'analytical', 'approach', 'suggest', 'anchor', 'test', 'miditest', 'generally', 'maximize', 'anchor', 'total', 'test', 'correlation', 'result', 'discuss', 'context', 'condition', 'maximize', 'correlation', 'anchor', 'total', 'test', 'Copyright', '©', '2016', 'National', 'Council']","['Study', 'Choice', 'Anchor', 'Tests', 'Equating']",study describe factor influence observe score correlation external anchor test total test anchor fulltest observe score correlation base component true score correlation anchor total test reliability anchor test Findings analytical approach suggest anchor test miditest generally maximize anchor total test correlation result discuss context condition maximize correlation anchor total test Copyright © 2016 National Council,Study Choice Anchor Tests Equating,0.03976011008536465,0.8407097786080882,0.039802139020035966,0.040042859609743416,0.039685112676767845,0.010383964820844898,0.0745159817409332,0.018874301352417872,0.045013032633670416,0.0
Wise S.L.; Kingsbury G.G.,Modeling Student Test-Taking Motivation in the Context of an Adaptive Achievement Test,2016,53,"This study examined the utility of response time-based analyses in understanding the behavior of unmotivated test takers. For the data from an adaptive achievement test, patterns of observed rapid-guessing behavior and item response accuracy were compared to the behavior expected under several types of models that have been proposed to represent unmotivated test taking behavior. Test taker behavior was found to be inconsistent with these models, with the exception of the effort-moderated model. Effort-moderated scoring was found to both yield scores that were more accurate than those found under traditional scoring, and exhibit improved person fit statistics. In addition, an effort-guided adaptive test was proposed and shown by a simulation study to alleviate item difficulty mistargeting caused by unmotivated test taking. © 2016 by the National Council on Measurement in Education.",Modeling Student Test-Taking Motivation in the Context of an Adaptive Achievement Test,"This study examined the utility of response time-based analyses in understanding the behavior of unmotivated test takers. For the data from an adaptive achievement test, patterns of observed rapid-guessing behavior and item response accuracy were compared to the behavior expected under several types of models that have been proposed to represent unmotivated test taking behavior. Test taker behavior was found to be inconsistent with these models, with the exception of the effort-moderated model. Effort-moderated scoring was found to both yield scores that were more accurate than those found under traditional scoring, and exhibit improved person fit statistics. In addition, an effort-guided adaptive test was proposed and shown by a simulation study to alleviate item difficulty mistargeting caused by unmotivated test taking. © 2016 by the National Council on Measurement in Education.","['study', 'examine', 'utility', 'response', 'timebase', 'analysis', 'understand', 'behavior', 'unmotivated', 'test', 'taker', 'datum', 'adaptive', 'achievement', 'test', 'pattern', 'observe', 'rapidguesse', 'behavior', 'item', 'response', 'accuracy', 'compare', 'behavior', 'expect', 'type', 'propose', 'represent', 'unmotivated', 'test', 'behavior', 'Test', 'taker', 'behavior', 'find', 'inconsistent', 'exception', 'effortmoderated', 'Effortmoderated', 'scoring', 'find', 'yield', 'score', 'accurate', 'find', 'traditional', 'scoring', 'exhibit', 'improved', 'person', 'fit', 'statistic', 'addition', 'effortguided', 'adaptive', 'test', 'propose', 'simulation', 'study', 'alleviate', 'item', 'difficulty', 'mistargeting', 'cause', 'unmotivated', 'test', '©', '2016', 'National', 'Council']","['student', 'TestTaking', 'Motivation', 'Context', 'Adaptive', 'Achievement', 'test']",study examine utility response timebase analysis understand behavior unmotivated test taker datum adaptive achievement test pattern observe rapidguesse behavior item response accuracy compare behavior expect type propose represent unmotivated test behavior Test taker behavior find inconsistent exception effortmoderated Effortmoderated scoring find yield score accurate find traditional scoring exhibit improved person fit statistic addition effortguided adaptive test propose simulation study alleviate item difficulty mistargeting cause unmotivated test © 2016 National Council,student TestTaking Motivation Context Adaptive Achievement test,0.8801961798459418,0.02983430141331959,0.0298421964142517,0.029898939788023254,0.030228382538463678,0.04959124628194864,0.002185584173887287,0.013534780852757871,0.0023063463531421123,0.004741491502085471
Lee Y.-H.; Haberman S.J.; Dorans N.J.,Use of Adjustment by Minimum Discriminant Information in Linking Constructed-Response Test Scores in the Absence of Common Items,2019,56,"In many educational tests, both multiple-choice (MC) and constructed-response (CR) sections are used to measure different constructs. In many common cases, security concerns lead to the use of form-specific CR items that cannot be used for equating test scores, along with MC sections that can be linked to previous test forms via common items. In such cases, adjustment by minimum discriminant information may be used to link CR section scores and composite scores based on both MC and CR sections. This approach is an innovative extension that addresses the long-standing issue of linking CR test scores across test forms in the absence of common items in educational measurement. It is applied to a series of administrations from an international language assessment with MC sections for receptive skills and CR sections for productive skills. To assess the linking results, harmonic regression is applied to examine the effects of the proposed linking method on score stability, among several analyses for evaluation. © 2019 by the National Council on Measurement in Education",Use of Adjustment by Minimum Discriminant Information in Linking Constructed-Response Test Scores in the Absence of Common Items,"In many educational tests, both multiple-choice (MC) and constructed-response (CR) sections are used to measure different constructs. In many common cases, security concerns lead to the use of form-specific CR items that cannot be used for equating test scores, along with MC sections that can be linked to previous test forms via common items. In such cases, adjustment by minimum discriminant information may be used to link CR section scores and composite scores based on both MC and CR sections. This approach is an innovative extension that addresses the long-standing issue of linking CR test scores across test forms in the absence of common items in educational measurement. It is applied to a series of administrations from an international language assessment with MC sections for receptive skills and CR sections for productive skills. To assess the linking results, harmonic regression is applied to examine the effects of the proposed linking method on score stability, among several analyses for evaluation. © 2019 by the National Council on Measurement in Education","['educational', 'test', 'multiplechoice', 'MC', 'constructedresponse', 'CR', 'section', 'measure', 'different', 'construct', 'common', 'case', 'security', 'concern', 'lead', 'formspecific', 'CR', 'item', 'equate', 'test', 'score', 'MC', 'section', 'link', 'previous', 'test', 'form', 'common', 'item', 'case', 'adjustment', 'minimum', 'discriminant', 'information', 'link', 'CR', 'section', 'score', 'composite', 'score', 'base', 'MC', 'CR', 'section', 'approach', 'innovative', 'extension', 'address', 'longstanding', 'issue', 'link', 'CR', 'test', 'score', 'test', 'form', 'absence', 'common', 'item', 'educational', 'apply', 'series', 'administration', 'international', 'language', 'assessment', 'MC', 'section', 'receptive', 'skill', 'cr', 'section', 'productive', 'skill', 'assess', 'link', 'result', 'harmonic', 'regression', 'apply', 'examine', 'effect', 'propose', 'link', 'method', 'score', 'stability', 'analysis', 'evaluation', '©', '2019', 'National', 'Council']","['Use', 'Adjustment', 'Minimum', 'Discriminant', 'Information', 'Linking', 'ConstructedResponse', 'Test', 'Scores', 'Absence', 'Common', 'Items']",educational test multiplechoice MC constructedresponse CR section measure different construct common case security concern lead formspecific CR item equate test score MC section link previous test form common item case adjustment minimum discriminant information link CR section score composite score base MC CR section approach innovative extension address longstanding issue link CR test score test form absence common item educational apply series administration international language assessment MC section receptive skill cr section productive skill assess link result harmonic regression apply examine effect propose link method score stability analysis evaluation © 2019 National Council,Use Adjustment Minimum Discriminant Information Linking ConstructedResponse Test Scores Absence Common Items,0.03297895327238375,0.03177836026172378,0.8717163071330978,0.03182725396276917,0.03169912537002543,0.024723062647091183,0.006269053614986798,0.037753172637269974,0.03088410321220652,0.0029243154624505763
Joo S.-H.; Lee P.; Stark S.,Development of Information Functions and Indices for the GGUM-RANK Multidimensional Forced Choice IRT Model,2018,55,"This research derived information functions and proposed new scalar information indices to examine the quality of multidimensional forced choice (MFC) items based on the RANK model. We also explored how GGUM-RANK information, latent trait recovery, and reliability varied across three MFC formats: pairs (two response alternatives), triplets (three alternatives), and tetrads (four alternatives). As expected, tetrad and triplet measures provided substantially more information than pairs, and MFC items composed of statements with high discrimination parameters were most informative. The methods and findings of this study will help practitioners to construct better MFC items, make informed projections about reliability with different MFC formats, and facilitate the development of MFC triplet- and tetrad-based computerized adaptive tests. © 2018 by the National Council on Measurement in Education",Development of Information Functions and Indices for the GGUM-RANK Multidimensional Forced Choice IRT Model,"This research derived information functions and proposed new scalar information indices to examine the quality of multidimensional forced choice (MFC) items based on the RANK model. We also explored how GGUM-RANK information, latent trait recovery, and reliability varied across three MFC formats: pairs (two response alternatives), triplets (three alternatives), and tetrads (four alternatives). As expected, tetrad and triplet measures provided substantially more information than pairs, and MFC items composed of statements with high discrimination parameters were most informative. The methods and findings of this study will help practitioners to construct better MFC items, make informed projections about reliability with different MFC formats, and facilitate the development of MFC triplet- and tetrad-based computerized adaptive tests. © 2018 by the National Council on Measurement in Education","['research', 'derive', 'information', 'function', 'propose', 'new', 'scalar', 'information', 'indice', 'examine', 'quality', 'multidimensional', 'force', 'choice', 'MFC', 'item', 'base', 'rank', 'explore', 'GGUMRANK', 'information', 'latent', 'trait', 'recovery', 'reliability', 'vary', 'MFC', 'format', 'pair', 'response', 'alternative', 'triplet', 'alternative', 'tetrad', 'alternative', 'expect', 'tetrad', 'triplet', 'measure', 'provide', 'substantially', 'information', 'pair', 'MFC', 'item', 'compose', 'statement', 'high', 'discrimination', 'parameter', 'informative', 'method', 'finding', 'study', 'help', 'practitioner', 'construct', 'mfc', 'item', 'informed', 'projection', 'reliability', 'different', 'MFC', 'format', 'facilitate', 'development', 'MFC', 'triplet', 'tetradbase', 'computerized', 'adaptive', 'test', '©', '2018', 'National', 'Council']","['development', 'Information', 'Functions', 'Indices', 'GGUMRANK', 'Multidimensional', 'Forced', 'Choice', 'IRT']",research derive information function propose new scalar information indice examine quality multidimensional force choice MFC item base rank explore GGUMRANK information latent trait recovery reliability vary MFC format pair response alternative triplet alternative tetrad alternative expect tetrad triplet measure provide substantially information pair MFC item compose statement high discrimination parameter informative method finding study help practitioner construct mfc item informed projection reliability different MFC format facilitate development MFC triplet tetradbase computerized adaptive test © 2018 National Council,development Information Functions Indices GGUMRANK Multidimensional Forced Choice IRT,0.03226276689876929,0.031784750798214,0.03164782388374112,0.8724177803048694,0.03188687811440632,0.0395153847861714,0.004286819314098527,0.00682968132753665,0.0,0.00345213541159018
Cheng Y.; Liu C.,A Short Note on the Relationship Between Pass Rate and Multiple Attempts,2016,53,"For a certification, licensure, or placement exam, allowing examinees to take multiple attempts at the test could effectively change the pass rate. Change in the pass rate can occur without any change in the underlying latent trait, and can be an artifact of multiple attempts and imperfect reliability of the test. By deriving formulae to compute the pass rate under two definitions, this article provides tools for testing practitioners to compute and evaluate the change in the expected pass rate when a certain (maximum) number of attempts are allowed without any change in the latent trait. This article also includes a simulation study that considers change in ability and differential motivation of examinees to retake the test. Results indicate that the general trend shown by the analytical results is maintained—that is, the marginal expected pass rate increases with more attempts when the testing volume is defined as the total number of test takers, and decreases with more attempts when the testing volume is defined as the total number of test attempts. Copyright © 2016 by the National Council on Measurement in Education",A Short Note on the Relationship Between Pass Rate and Multiple Attempts,"For a certification, licensure, or placement exam, allowing examinees to take multiple attempts at the test could effectively change the pass rate. Change in the pass rate can occur without any change in the underlying latent trait, and can be an artifact of multiple attempts and imperfect reliability of the test. By deriving formulae to compute the pass rate under two definitions, this article provides tools for testing practitioners to compute and evaluate the change in the expected pass rate when a certain (maximum) number of attempts are allowed without any change in the latent trait. This article also includes a simulation study that considers change in ability and differential motivation of examinees to retake the test. Results indicate that the general trend shown by the analytical results is maintained—that is, the marginal expected pass rate increases with more attempts when the testing volume is defined as the total number of test takers, and decreases with more attempts when the testing volume is defined as the total number of test attempts. Copyright © 2016 by the National Council on Measurement in Education","['certification', 'licensure', 'placement', 'exam', 'allow', 'examine', 'multiple', 'attempt', 'test', 'effectively', 'change', 'pass', 'rate', 'Change', 'pass', 'rate', 'occur', 'change', 'underlie', 'latent', 'trait', 'artifact', 'multiple', 'attempt', 'imperfect', 'reliability', 'test', 'derive', 'formulae', 'compute', 'pass', 'rate', 'definition', 'article', 'provide', 'tool', 'testing', 'practitioner', 'compute', 'evaluate', 'change', 'expect', 'pass', 'rate', 'certain', 'maximum', 'number', 'attempt', 'allow', 'change', 'latent', 'trait', 'article', 'include', 'simulation', 'study', 'consider', 'change', 'ability', 'differential', 'motivation', 'examinee', 'retake', 'test', 'result', 'indicate', 'general', 'trend', 'analytical', 'result', 'maintain', '—', 'marginal', 'expect', 'pass', 'rate', 'increase', 'attempt', 'testing', 'volume', 'define', 'total', 'number', 'test', 'taker', 'decrease', 'attempt', 'testing', 'volume', 'define', 'total', 'number', 'test', 'attempt', 'Copyright', '©', '2016', 'National', 'Council']","['Short', 'Note', 'relationship', 'Pass', 'Rate', 'Multiple', 'attempt']",certification licensure placement exam allow examine multiple attempt test effectively change pass rate Change pass rate occur change underlie latent trait artifact multiple attempt imperfect reliability test derive formulae compute pass rate definition article provide tool testing practitioner compute evaluate change expect pass rate certain maximum number attempt allow change latent trait article include simulation study consider change ability differential motivation examinee retake test result indicate general trend analytical result maintain — marginal expect pass rate increase attempt testing volume define total number test taker decrease attempt testing volume define total number test attempt Copyright © 2016 National Council,Short Note relationship Pass Rate Multiple attempt,0.8825396617808556,0.029305222573058915,0.029590353152407437,0.029242436778601333,0.02932232571507664,0.038250076388460154,0.02126823419958835,0.015780616591132127,0.000991574108838034,0.0
Cizek G.J.; Kosh A.E.; Toutkoushian E.K.,Gathering and Evaluating Validity Evidence: The Generalized Assessment Alignment Tool,2018,55,"Alignment is an essential piece of validity evidence for both educational (K-12) and credentialing (licensure and certification) assessments. In this article, a comprehensive review of commonly used contemporary alignment procedures is provided; some key weaknesses in current alignment approaches are identified; principles for evaluating alignment methods are distilled; and a new approach to investigating alignment is proposed and illustrated. The article concludes with suggestions for alignment research and practice. © 2018 by the National Council on Measurement in Education",Gathering and Evaluating Validity Evidence: The Generalized Assessment Alignment Tool,"Alignment is an essential piece of validity evidence for both educational (K-12) and credentialing (licensure and certification) assessments. In this article, a comprehensive review of commonly used contemporary alignment procedures is provided; some key weaknesses in current alignment approaches are identified; principles for evaluating alignment methods are distilled; and a new approach to investigating alignment is proposed and illustrated. The article concludes with suggestions for alignment research and practice. © 2018 by the National Council on Measurement in Education","['alignment', 'essential', 'piece', 'validity', 'evidence', 'educational', 'K12', 'credentiale', 'licensure', 'certification', 'assessment', 'article', 'comprehensive', 'review', 'commonly', 'contemporary', 'alignment', 'procedure', 'provide', 'key', 'weakness', 'current', 'alignment', 'approach', 'identify', 'principle', 'evaluate', 'alignment', 'method', 'distil', 'new', 'approach', 'investigate', 'alignment', 'propose', 'illustrate', 'article', 'conclude', 'suggestion', 'alignment', 'research', 'practice', '©', '2018', 'National', 'Council']","['gather', 'evaluate', 'Validity', 'Evidence', 'Generalized', 'Assessment', 'Alignment', 'Tool']",alignment essential piece validity evidence educational K12 credentiale licensure certification assessment article comprehensive review commonly contemporary alignment procedure provide key weakness current alignment approach identify principle evaluate alignment method distil new approach investigate alignment propose illustrate article conclude suggestion alignment research practice © 2018 National Council,gather evaluate Validity Evidence Generalized Assessment Alignment Tool,0.03953143987947229,0.03917972768961518,0.03879299712403854,0.8436240438601452,0.038871791446728754,0.007649552964839823,0.0,0.04631005269985086,0.0,0.0
Zwick R.; Ye L.; Isham S.,Aggregating Polytomous DIF Results Over Multiple Test Administrations,2018,55,"In typical differential item functioning (DIF) assessments, an item's DIF status is not influenced by its status in previous test administrations. An item that has shown DIF at multiple administrations may be treated the same way as an item that has shown DIF in only the most recent administration. Therefore, much useful information about the item's functioning is ignored. In earlier work, we developed the Bayesian updating (BU) DIF procedure for dichotomous items and showed how it could be used to formally aggregate DIF results over administrations. More recently, we extended the BU method to the case of polytomously scored items. We conducted an extensive simulation study that included four “administrations” of a test. For the single-administration case, we compared the Bayesian approach to an existing polytomous-DIF procedure. For the multiple-administration case, we compared BU to two non-Bayesian methods of aggregating the polytomous-DIF results over administrations. We concluded that both the BU approach and a simple non-Bayesian method show promise as methods of aggregating polytomous DIF results over administrations. Copyright © 2018 by the National Council on Measurement in Education",Aggregating Polytomous DIF Results Over Multiple Test Administrations,"In typical differential item functioning (DIF) assessments, an item's DIF status is not influenced by its status in previous test administrations. An item that has shown DIF at multiple administrations may be treated the same way as an item that has shown DIF in only the most recent administration. Therefore, much useful information about the item's functioning is ignored. In earlier work, we developed the Bayesian updating (BU) DIF procedure for dichotomous items and showed how it could be used to formally aggregate DIF results over administrations. More recently, we extended the BU method to the case of polytomously scored items. We conducted an extensive simulation study that included four “administrations” of a test. For the single-administration case, we compared the Bayesian approach to an existing polytomous-DIF procedure. For the multiple-administration case, we compared BU to two non-Bayesian methods of aggregating the polytomous-DIF results over administrations. We concluded that both the BU approach and a simple non-Bayesian method show promise as methods of aggregating polytomous DIF results over administrations. Copyright © 2018 by the National Council on Measurement in Education","['typical', 'differential', 'item', 'function', 'DIF', 'assessment', 'item', 'DIF', 'status', 'influence', 'status', 'previous', 'test', 'administration', 'item', 'DIF', 'multiple', 'administration', 'treat', 'way', 'item', 'DIF', 'recent', 'administration', 'useful', 'information', 'item', 'function', 'ignore', 'early', 'work', 'develop', 'Bayesian', 'update', 'bu', 'dif', 'procedure', 'dichotomous', 'item', 'formally', 'aggregate', 'dif', 'result', 'administration', 'recently', 'extend', 'BU', 'method', 'case', 'polytomously', 'score', 'item', 'conduct', 'extensive', 'simulation', 'study', 'include', '""', 'administration', '""', 'test', 'singleadministration', 'case', 'compare', 'bayesian', 'approach', 'exist', 'polytomousDIF', 'procedure', 'multipleadministration', 'case', 'compare', 'BU', 'nonbayesian', 'method', 'aggregate', 'polytomousDIF', 'result', 'administration', 'conclude', 'BU', 'approach', 'simple', 'nonbayesian', 'method', 'promise', 'method', 'aggregate', 'polytomous', 'DIF', 'result', 'administration', 'copyright', '©', '2018', 'National', 'Council']","['aggregate', 'polytomous', 'DIF', 'result', 'Multiple', 'Test', 'Administrations']","typical differential item function DIF assessment item DIF status influence status previous test administration item DIF multiple administration treat way item DIF recent administration useful information item function ignore early work develop Bayesian update bu dif procedure dichotomous item formally aggregate dif result administration recently extend BU method case polytomously score item conduct extensive simulation study include "" administration "" test singleadministration case compare bayesian approach exist polytomousDIF procedure multipleadministration case compare BU nonbayesian method aggregate polytomousDIF result administration conclude BU approach simple nonbayesian method promise method aggregate polytomous DIF result administration copyright © 2018 National Council",aggregate polytomous DIF result Multiple Test Administrations,0.8745729993157402,0.03120775076226531,0.031491236567631364,0.03125517822531031,0.031472835129052815,0.07779915575263556,0.0,0.0,0.0,0.0
Duckor B.; Holmberg C.,Exploring How to Model Formative Assessment Trajectories of Posing-Pausing-Probing Practices: Toward a Teacher Learning Progressions Framework for the Study of Novice Teachers,2019,56,"A robust body of evidence supports the finding that particular teaching and assessment strategies in the K-12 classroom can improve student achievement. While experts have identified many effective teaching and learning practices in the assessment for learning literature, teachers’ knowledge and use of “high leverage” formative assessment (FA) practices are difficult to model in novice populations. By employing advances in construct modeling, the theoretical underpinnings of learning progressions research, and four principles of evidence-centered design, teacher educators along with psychometricians can test hypotheses about teacher learning progressions. Utilizing an FA moves-based framework, the article examines how beginning teachers’ posing, pausing, and probing practices align with five key strategies of FA. Examples of construct maps, instructional tasks, and turns of talk analysis using scoring guides are provided from an empirical study of novice science preservice teachers in a high-needs school district. © 2019 by the National Council on Measurement in Education",Exploring How to Model Formative Assessment Trajectories of Posing-Pausing-Probing Practices: Toward a Teacher Learning Progressions Framework for the Study of Novice Teachers,"A robust body of evidence supports the finding that particular teaching and assessment strategies in the K-12 classroom can improve student achievement. While experts have identified many effective teaching and learning practices in the assessment for learning literature, teachers’ knowledge and use of “high leverage” formative assessment (FA) practices are difficult to model in novice populations. By employing advances in construct modeling, the theoretical underpinnings of learning progressions research, and four principles of evidence-centered design, teacher educators along with psychometricians can test hypotheses about teacher learning progressions. Utilizing an FA moves-based framework, the article examines how beginning teachers’ posing, pausing, and probing practices align with five key strategies of FA. Examples of construct maps, instructional tasks, and turns of talk analysis using scoring guides are provided from an empirical study of novice science preservice teachers in a high-needs school district. © 2019 by the National Council on Measurement in Education","['robust', 'body', 'evidence', 'support', 'finding', 'particular', 'teaching', 'assessment', 'strategy', 'K12', 'classroom', 'improve', 'student', 'achievement', 'expert', 'identify', 'effective', 'teaching', 'learn', 'practice', 'assessment', 'learn', 'literature', 'teacher', '’', 'knowledge', '""', 'high', 'leverage', '""', 'formative', 'assessment', 'FA', 'practice', 'difficult', 'novice', 'population', 'employ', 'advance', 'construct', 'theoretical', 'underpinning', 'learn', 'progression', 'research', 'principle', 'evidencecentere', 'design', 'teacher', 'educator', 'psychometrician', 'test', 'hypothesis', 'teacher', 'learn', 'progression', 'utilize', 'fa', 'movesbase', 'framework', 'article', 'examine', 'begin', 'teacher', '’', 'pose', 'pausing', 'probe', 'practice', 'align', 'key', 'strategy', 'FA', 'Examples', 'construct', 'map', 'instructional', 'task', 'turn', 'talk', 'analysis', 'scoring', 'guide', 'provide', 'empirical', 'study', 'novice', 'science', 'preservice', 'teacher', 'highneed', 'school', 'district', '©', '2019', 'National', 'Council']","['explore', 'Formative', 'Assessment', 'Trajectories', 'posingpausingprobe', 'Practices', 'teacher', 'Learning', 'Progressions', 'Framework', 'Study', 'Novice', 'Teachers']","robust body evidence support finding particular teaching assessment strategy K12 classroom improve student achievement expert identify effective teaching learn practice assessment learn literature teacher ’ knowledge "" high leverage "" formative assessment FA practice difficult novice population employ advance construct theoretical underpinning learn progression research principle evidencecentere design teacher educator psychometrician test hypothesis teacher learn progression utilize fa movesbase framework article examine begin teacher ’ pose pausing probe practice align key strategy FA Examples construct map instructional task turn talk analysis scoring guide provide empirical study novice science preservice teacher highneed school district © 2019 National Council",explore Formative Assessment Trajectories posingpausingprobe Practices teacher Learning Progressions Framework Study Novice Teachers,0.9001263984433063,0.025212979728762804,0.024732602917625066,0.02492385807742035,0.02500416083288544,0.0,0.0,0.10867341876110843,0.0,0.0
Suh Y.,Effect Size Measures for Differential Item Functioning in a Multidimensional IRT Model,2016,53,"This study adapted an effect size measure used for studying differential item functioning (DIF) in unidimensional tests and extended the measure to multidimensional tests. Two effect size measures were considered in a multidimensional item response theory model: signed weighted P-difference and unsigned weighted P-difference. The performance of the effect size measures was investigated under various simulation conditions including different sample sizes and DIF magnitudes. As another way of studying DIF, the χ2 difference test was included to compare the result of statistical significance (statistical tests) with that of practical significance (effect size measures). The adequacy of existing effect size criteria used in unidimensional tests was also evaluated. Both effect size measures worked well in estimating true effect sizes, identifying DIF types, and classifying effect size categories. Finally, a real data analysis was conducted to support the simulation results. Copyright © 2016 by the National Council on Measurement in Education",Effect Size Measures for Differential Item Functioning in a Multidimensional IRT Model,"This study adapted an effect size measure used for studying differential item functioning (DIF) in unidimensional tests and extended the measure to multidimensional tests. Two effect size measures were considered in a multidimensional item response theory model: signed weighted P-difference and unsigned weighted P-difference. The performance of the effect size measures was investigated under various simulation conditions including different sample sizes and DIF magnitudes. As another way of studying DIF, the χ2 difference test was included to compare the result of statistical significance (statistical tests) with that of practical significance (effect size measures). The adequacy of existing effect size criteria used in unidimensional tests was also evaluated. Both effect size measures worked well in estimating true effect sizes, identifying DIF types, and classifying effect size categories. Finally, a real data analysis was conducted to support the simulation results. Copyright © 2016 by the National Council on Measurement in Education","['study', 'adapt', 'effect', 'size', 'measure', 'study', 'differential', 'item', 'function', 'DIF', 'unidimensional', 'test', 'extend', 'measure', 'multidimensional', 'test', 'effect', 'size', 'measure', 'consider', 'multidimensional', 'item', 'response', 'theory', 'sign', 'weight', 'Pdifference', 'unsigned', 'weight', 'Pdifference', 'performance', 'effect', 'size', 'measure', 'investigate', 'simulation', 'condition', 'include', 'different', 'sample', 'size', 'dif', 'magnitude', 'way', 'study', 'DIF', 'χ2', 'difference', 'test', 'include', 'compare', 'result', 'statistical', 'significance', 'statistical', 'test', 'practical', 'significance', 'effect', 'size', 'measure', 'adequacy', 'exist', 'effect', 'size', 'criterion', 'unidimensional', 'test', 'evaluate', 'effect', 'size', 'measure', 'work', 'estimate', 'true', 'effect', 'size', 'identify', 'DIF', 'type', 'classify', 'effect', 'size', 'category', 'finally', 'real', 'datum', 'analysis', 'conduct', 'support', 'simulation', 'result', 'copyright', '©', '2016', 'National', 'Council']","['effect', 'Size', 'Measures', 'Differential', 'Item', 'Functioning', 'Multidimensional', 'IRT']",study adapt effect size measure study differential item function DIF unidimensional test extend measure multidimensional test effect size measure consider multidimensional item response theory sign weight Pdifference unsigned weight Pdifference performance effect size measure investigate simulation condition include different sample size dif magnitude way study DIF χ2 difference test include compare result statistical significance statistical test practical significance effect size measure adequacy exist effect size criterion unidimensional test evaluate effect size measure work estimate true effect size identify DIF type classify effect size category finally real datum analysis conduct support simulation result copyright © 2016 National Council,effect Size Measures Differential Item Functioning Multidimensional IRT,0.03170944848787949,0.8740571136146879,0.031213703252754636,0.03144686895844767,0.031572865686230335,0.07548746168766221,0.0,0.0,0.0005813610927023916,0.020536374093538895
Ke X.; Zeng Y.; Luo H.,Autoscoring Essays Based on Complex Networks,2016,53,"This article presents a novel method, the Complex Dynamics Essay Scorer (CDES), for automated essay scoring using complex network features. Texts produced by college students in China were represented as scale-free networks (e.g., a word adjacency model) from which typical network features, such as the in-/out-degrees, clustering coefficient (CC), and dynamic networks, were obtained. The CDES integrates the classical concepts of network feature representation and essay score series variation. Several experiments indicated that the network measures different essay qualities and can be clearly demonstrated to develop complex networks for autoscoring tasks. The average agreement of the CDES and human rater scores was 86.5%, and the average Pearson correlation was.77. The results indicate that the CDES produced functional complex systems and autoscored Chinese essays in a method consistent with human raters. Our research suggests potential applications in other areas of educational assessment. Copyright © 2016 by the National Council on Measurement in Education",,"This article presents a novel method, the Complex Dynamics Essay Scorer (CDES), for automated essay scoring using complex network features. Texts produced by college students in China were represented as scale-free networks (e.g., a word adjacency model) from which typical network features, such as the in-/out-degrees, clustering coefficient (CC), and dynamic networks, were obtained. The CDES integrates the classical concepts of network feature representation and essay score series variation. Several experiments indicated that the network measures different essay qualities and can be clearly demonstrated to develop complex networks for autoscoring tasks. The average agreement of the CDES and human rater scores was 86.5%, and the average Pearson correlation was.77. The results indicate that the CDES produced functional complex systems and autoscored Chinese essays in a method consistent with human raters. Our research suggests potential applications in other areas of educational assessment. Copyright © 2016 by the National Council on Measurement in Education","['article', 'present', 'novel', 'method', 'Complex', 'Dynamics', 'Essay', 'Scorer', 'CDES', 'automate', 'essay', 'scoring', 'complex', 'network', 'feature', 'Texts', 'produce', 'college', 'student', 'China', 'represent', 'scalefree', 'network', 'eg', 'word', 'adjacency', 'typical', 'network', 'feature', 'inoutdegree', 'cluster', 'coefficient', 'cc', 'dynamic', 'network', 'obtain', 'CDES', 'integrate', 'classical', 'concept', 'network', 'feature', 'representation', 'essay', 'score', 'series', 'variation', 'experiment', 'indicate', 'network', 'measure', 'different', 'essay', 'quality', 'clearly', 'demonstrate', 'develop', 'complex', 'network', 'autoscore', 'task', 'average', 'agreement', 'CDES', 'human', 'rater', 'score', '865', 'average', 'Pearson', 'correlation', 'was77', 'result', 'indicate', 'CDES', 'produce', 'functional', 'complex', 'system', 'autoscore', 'chinese', 'essay', 'method', 'consistent', 'human', 'rater', 'research', 'suggest', 'potential', 'application', 'area', 'educational', 'assessment', 'Copyright', '©', '2016', 'National', 'Council']",,article present novel method Complex Dynamics Essay Scorer CDES automate essay scoring complex network feature Texts produce college student China represent scalefree network eg word adjacency typical network feature inoutdegree cluster coefficient cc dynamic network obtain CDES integrate classical concept network feature representation essay score series variation experiment indicate network measure different essay quality clearly demonstrate develop complex network autoscore task average agreement CDES human rater score 865 average Pearson correlation was77 result indicate CDES produce functional complex system autoscore chinese essay method consistent human rater research suggest potential application area educational assessment Copyright © 2016 National Council,,0.028403057553356585,0.028432582727145786,0.028163075807199428,0.028271982339142798,0.8867293015731554,0.0,0.007598545996215301,0.04101036219434215,0.0029712702113188,0.060033182312406945
Andrews J.J.; Kerr D.; Mislevy R.J.; von Davier A.; Hao J.; Liu L.,Modeling Collaborative Interaction Patterns in a Simulation-Based Task,2017,54,"Simulations and games offer interactive tasks that can elicit rich data, providing evidence of complex skills that are difficult to measure with more conventional items and tests. However, one notable challenge in using such technologies is making sense of the data generated in order to make claims about individuals or groups. This article presents a novel methodological approach that uses the process data and performance outcomes from a simulation-based collaborative science assessment to explore the propensities of dyads to interact in accordance with certain interaction patterns. Further exploratory analyses examine how the approach can be used to answer important questions in collaboration research regarding gender and cultural differences in collaborative behavior and how interaction patterns relate to performance outcomes. Copyright © 2017 by the National Council on Measurement in Education",Modeling Collaborative Interaction Patterns in a Simulation-Based Task,"Simulations and games offer interactive tasks that can elicit rich data, providing evidence of complex skills that are difficult to measure with more conventional items and tests. However, one notable challenge in using such technologies is making sense of the data generated in order to make claims about individuals or groups. This article presents a novel methodological approach that uses the process data and performance outcomes from a simulation-based collaborative science assessment to explore the propensities of dyads to interact in accordance with certain interaction patterns. Further exploratory analyses examine how the approach can be used to answer important questions in collaboration research regarding gender and cultural differences in collaborative behavior and how interaction patterns relate to performance outcomes. Copyright © 2017 by the National Council on Measurement in Education","['simulation', 'game', 'offer', 'interactive', 'task', 'elicit', 'rich', 'datum', 'provide', 'evidence', 'complex', 'skill', 'difficult', 'measure', 'conventional', 'item', 'test', 'notable', 'challenge', 'technology', 'sense', 'datum', 'generate', 'order', 'claim', 'individual', 'group', 'article', 'present', 'novel', 'methodological', 'approach', 'process', 'datum', 'performance', 'outcome', 'simulationbased', 'collaborative', 'science', 'assessment', 'explore', 'propensity', 'dyad', 'interact', 'accordance', 'certain', 'interaction', 'pattern', 'exploratory', 'analysis', 'examine', 'approach', 'answer', 'important', 'question', 'collaboration', 'research', 'regard', 'gender', 'cultural', 'difference', 'collaborative', 'behavior', 'interaction', 'pattern', 'relate', 'performance', 'outcome', 'copyright', '©', '2017', 'National', 'Council']","['Collaborative', 'Interaction', 'Patterns', 'SimulationBased', 'Task']",simulation game offer interactive task elicit rich datum provide evidence complex skill difficult measure conventional item test notable challenge technology sense datum generate order claim individual group article present novel methodological approach process datum performance outcome simulationbased collaborative science assessment explore propensity dyad interact accordance certain interaction pattern exploratory analysis examine approach answer important question collaboration research regard gender cultural difference collaborative behavior interaction pattern relate performance outcome copyright © 2017 National Council,Collaborative Interaction Patterns SimulationBased Task,0.02401566889965269,0.02406972278924556,0.023910803436737564,0.024019230615632072,0.9039845742587321,0.009280565578243942,0.0,0.09860003829964309,0.0,4.256316672476573e-05
Heritage M.; Kingston N.M.,Classroom Assessment and Large-Scale Psychometrics: Shall the Twain Meet? (A Conversation With Margaret Heritage and Neal Kingston),2019,56,"Classroom assessment and large-scale assessment have, for the most part, existed in mutual isolation. Some experts have felt this is for the best and others have been concerned that the schism limits the potential contribution of both forms of assessment. Margaret Heritage has long been a champion of best practices in classroom assessment. Neal Kingston has been involved with the application of psychometrics to large-scale assessments for four decades. Together they discuss what commonalities and differences exist between these two assessment contexts, whether the twain should meet, what impediments or concerns exist, and whether they expect the status quo will change at all in the near future. Based on their joint keynote address at the NCME Special Conference on Classroom Assessment and Large-Scale Psychometrics, they have expanded and constructed this discussion piece. © 2019 by the National Council on Measurement in Education",Classroom Assessment and Large-Scale Psychometrics: Shall the Twain Meet? (A Conversation With Margaret Heritage and Neal Kingston),"Classroom assessment and large-scale assessment have, for the most part, existed in mutual isolation. Some experts have felt this is for the best and others have been concerned that the schism limits the potential contribution of both forms of assessment. Margaret Heritage has long been a champion of best practices in classroom assessment. Neal Kingston has been involved with the application of psychometrics to large-scale assessments for four decades. Together they discuss what commonalities and differences exist between these two assessment contexts, whether the twain should meet, what impediments or concerns exist, and whether they expect the status quo will change at all in the near future. Based on their joint keynote address at the NCME Special Conference on Classroom Assessment and Large-Scale Psychometrics, they have expanded and constructed this discussion piece. © 2019 by the National Council on Measurement in Education","['classroom', 'assessment', 'largescale', 'assessment', 'exist', 'mutual', 'isolation', 'expert', 'feel', 'good', 'concern', 'schism', 'limit', 'potential', 'contribution', 'form', 'assessment', 'Margaret', 'Heritage', 'long', 'champion', 'good', 'practice', 'classroom', 'assessment', 'Neal', 'Kingston', 'involve', 'application', 'psychometric', 'largescale', 'assessment', 'decade', 'discuss', 'commonality', 'difference', 'exist', 'assessment', 'context', 'twain', 'meet', 'impediment', 'concern', 'exist', 'expect', 'status', 'quo', 'change', 'near', 'future', 'base', 'joint', 'keynote', 'address', 'NCME', 'Special', 'Conference', 'Classroom', 'Assessment', 'LargeScale', 'Psychometrics', 'expand', 'construct', 'discussion', 'piece', '©', '2019', 'National', 'Council']","['Classroom', 'Assessment', 'LargeScale', 'Psychometrics', 'shall', 'Twain', 'Meet', 'A', 'Conversation', 'Margaret', 'Heritage', 'Neal', 'Kingston']",classroom assessment largescale assessment exist mutual isolation expert feel good concern schism limit potential contribution form assessment Margaret Heritage long champion good practice classroom assessment Neal Kingston involve application psychometric largescale assessment decade discuss commonality difference exist assessment context twain meet impediment concern exist expect status quo change near future base joint keynote address NCME Special Conference Classroom Assessment LargeScale Psychometrics expand construct discussion piece © 2019 National Council,Classroom Assessment LargeScale Psychometrics shall Twain Meet A Conversation Margaret Heritage Neal Kingston,0.027099033008875097,0.0270082637388013,0.8921136350419905,0.026796676854786376,0.026982391355546698,0.0,0.0,0.08689203341025674,0.0,0.0016597419270649538
Palermo C.; Bunch M.B.; Ridge K.,Scoring Stability in a Large-Scale Assessment Program: A Longitudinal Analysis of Leniency/Severity Effects,2019,56,"Although much attention has been given to rater effects in rater-mediated assessment contexts, little research has examined the overall stability of leniency and severity effects over time. This study examined longitudinal scoring data collected during three consecutive administrations of a large-scale, multi-state summative assessment program. Multilevel models were used to assess the overall extent of rater leniency/severity during scoring and examine the extent to which leniency/severity effects were stable across the three administrations. Model results were then applied to scaled scores to estimate the impact of the stability of leniency/severity effects on students’ scores. Results showed relative scoring stability across administrations in mathematics. In English language arts, short constructed response items showed evidence of slightly increasing severity across administrations, while essays showed mixed results: evidence of both slightly increasing severity and moderately increasing leniency over time, depending on trait. However, when model results were applied to scaled scores, results revealed rater effects had minimal impact on students’ scores. © 2019 by the National Council on Measurement in Education",Scoring Stability in a Large-Scale Assessment Program: A Longitudinal Analysis of Leniency/Severity Effects,"Although much attention has been given to rater effects in rater-mediated assessment contexts, little research has examined the overall stability of leniency and severity effects over time. This study examined longitudinal scoring data collected during three consecutive administrations of a large-scale, multi-state summative assessment program. Multilevel models were used to assess the overall extent of rater leniency/severity during scoring and examine the extent to which leniency/severity effects were stable across the three administrations. Model results were then applied to scaled scores to estimate the impact of the stability of leniency/severity effects on students’ scores. Results showed relative scoring stability across administrations in mathematics. In English language arts, short constructed response items showed evidence of slightly increasing severity across administrations, while essays showed mixed results: evidence of both slightly increasing severity and moderately increasing leniency over time, depending on trait. However, when model results were applied to scaled scores, results revealed rater effects had minimal impact on students’ scores. © 2019 by the National Council on Measurement in Education","['attention', 'rater', 'effect', 'ratermediate', 'assessment', 'context', 'little', 'research', 'examine', 'overall', 'stability', 'leniency', 'severity', 'effect', 'time', 'study', 'examine', 'longitudinal', 'scoring', 'datum', 'collect', 'consecutive', 'administration', 'largescale', 'multistate', 'summative', 'assessment', 'program', 'Multilevel', 'assess', 'overall', 'extent', 'rater', 'leniencyseverity', 'scoring', 'examine', 'extent', 'leniencyseverity', 'effect', 'stable', 'administration', 'result', 'apply', 'scale', 'score', 'estimate', 'impact', 'stability', 'leniencyseverity', 'effect', 'student', '’', 'score', 'result', 'relative', 'scoring', 'stability', 'administration', 'mathematic', 'english', 'language', 'art', 'short', 'construct', 'response', 'item', 'evidence', 'slightly', 'increase', 'severity', 'administration', 'essay', 'mixed', 'result', 'evidence', 'slightly', 'increase', 'severity', 'moderately', 'increase', 'leniency', 'time', 'depend', 'trait', 'result', 'apply', 'scale', 'score', 'result', 'reveal', 'rater', 'effect', 'minimal', 'impact', 'student', '’', 'score', '©', '2019', 'National', 'Council']","['Scoring', 'Stability', 'LargeScale', 'Assessment', 'Program', 'A', 'Longitudinal', 'Analysis', 'LeniencySeverity', 'effect']",attention rater effect ratermediate assessment context little research examine overall stability leniency severity effect time study examine longitudinal scoring datum collect consecutive administration largescale multistate summative assessment program Multilevel assess overall extent rater leniencyseverity scoring examine extent leniencyseverity effect stable administration result apply scale score estimate impact stability leniencyseverity effect student ’ score result relative scoring stability administration mathematic english language art short construct response item evidence slightly increase severity administration essay mixed result evidence slightly increase severity moderately increase leniency time depend trait result apply scale score result reveal rater effect minimal impact student ’ score © 2019 National Council,Scoring Stability LargeScale Assessment Program A Longitudinal Analysis LeniencySeverity effect,0.8935824322027346,0.0266113858209647,0.026378844455857604,0.02665186810640008,0.02677546941404317,0.008195203817332157,0.0,0.047874578640413244,0.0012629784690576977,0.13806845859445044
Jang Y.; Kim S.-H.; Cohen A.S.,The Impact of Multidimensionality on Extraction of Latent Classes in Mixture Rasch Models,2018,55,"This study investigates the effect of multidimensionality on extraction of latent classes in mixture Rasch models. In this study, two-dimensional data were generated under varying conditions. The two-dimensional data sets were analyzed with one- to five-class mixture Rasch models. Results of the simulation study indicate the mixture Rasch model tended to extract more latent classes than the number of dimensions simulated, particularly when the multidimensional structure of the data was more complex. In addition, the number of extracted latent classes decreased as the dimensions were more highly correlated regardless of multidimensional structure. An analysis of the empirical multidimensional data also shows that the number of latent classes extracted by the mixture Rasch model is larger than the number of dimensions measured by the test. © 2018 by the National Council on Measurement in Education",The Impact of Multidimensionality on Extraction of Latent Classes in Mixture Rasch Models,"This study investigates the effect of multidimensionality on extraction of latent classes in mixture Rasch models. In this study, two-dimensional data were generated under varying conditions. The two-dimensional data sets were analyzed with one- to five-class mixture Rasch models. Results of the simulation study indicate the mixture Rasch model tended to extract more latent classes than the number of dimensions simulated, particularly when the multidimensional structure of the data was more complex. In addition, the number of extracted latent classes decreased as the dimensions were more highly correlated regardless of multidimensional structure. An analysis of the empirical multidimensional data also shows that the number of latent classes extracted by the mixture Rasch model is larger than the number of dimensions measured by the test. © 2018 by the National Council on Measurement in Education","['study', 'investigate', 'effect', 'multidimensionality', 'extraction', 'latent', 'class', 'mixture', 'Rasch', 'study', 'twodimensional', 'datum', 'generate', 'vary', 'condition', 'twodimensional', 'data', 'set', 'analyze', 'fiveclass', 'mixture', 'Rasch', 'result', 'simulation', 'study', 'indicate', 'mixture', 'Rasch', 'tend', 'extract', 'latent', 'class', 'number', 'dimension', 'simulate', 'particularly', 'multidimensional', 'structure', 'datum', 'complex', 'addition', 'number', 'extract', 'latent', 'class', 'decrease', 'dimension', 'highly', 'correlate', 'regardless', 'multidimensional', 'structure', 'analysis', 'empirical', 'multidimensional', 'datum', 'number', 'latent', 'class', 'extract', 'mixture', 'Rasch', 'large', 'number', 'dimension', 'measure', 'test', '©', '2018', 'National', 'Council']","['Impact', 'Multidimensionality', 'Extraction', 'Latent', 'Classes', 'Mixture', 'Rasch', 'Models']",study investigate effect multidimensionality extraction latent class mixture Rasch study twodimensional datum generate vary condition twodimensional data set analyze fiveclass mixture Rasch result simulation study indicate mixture Rasch tend extract latent class number dimension simulate particularly multidimensional structure datum complex addition number extract latent class decrease dimension highly correlate regardless multidimensional structure analysis empirical multidimensional datum number latent class extract mixture Rasch large number dimension measure test © 2018 National Council,Impact Multidimensionality Extraction Latent Classes Mixture Rasch Models,0.03334419887324161,0.03333689508848551,0.8663669279932978,0.03333356617679371,0.03361841186818117,0.04562587571677577,0.0,0.0,0.0,0.026647812261840962
Wilson M.; Gochyyev P.; Scalise K.,Modeling Data From Collaborative Assessments: Learning in Digital Interactive Social Networks,2017,54,"This article summarizes assessment of cognitive skills through collaborative tasks, using field test results from the Assessment and Teaching of 21st Century Skills (ATC21S) project. This project, sponsored by Cisco, Intel, and Microsoft, aims to help educators around the world enable students with the skills to succeed in future career and college goals. In this article, ATC21S collaborative assessments focus on the project's “ICT Literacy—Learning in digital networks” learning progression. The article includes a description of the development of the learning progression, as well as examples and the logic behind the instrument construction. Assessments took place in random pairs of students in a demonstration digital environment. Modeling of results employed unidimensional and multidimensional item response models, with and without random effects for groups. The results indicated that, based on this data set, the models that take group into consideration in both the unidimensional and the multidimensional analyses fit better. However, the group-level variances were substantially higher than the individual-level variances. This indicates that a total individual estimate of group plus individual is likely a more informative estimate than individual alone but also that the performances of the pairs dominated the performances of the individuals. Implications are discussed in the results and conclusions. Copyright © 2017 by the National Council on Measurement in Education",Modeling Data From Collaborative Assessments: Learning in Digital Interactive Social Networks,"This article summarizes assessment of cognitive skills through collaborative tasks, using field test results from the Assessment and Teaching of 21st Century Skills (ATC21S) project. This project, sponsored by Cisco, Intel, and Microsoft, aims to help educators around the world enable students with the skills to succeed in future career and college goals. In this article, ATC21S collaborative assessments focus on the project's “ICT Literacy—Learning in digital networks” learning progression. The article includes a description of the development of the learning progression, as well as examples and the logic behind the instrument construction. Assessments took place in random pairs of students in a demonstration digital environment. Modeling of results employed unidimensional and multidimensional item response models, with and without random effects for groups. The results indicated that, based on this data set, the models that take group into consideration in both the unidimensional and the multidimensional analyses fit better. However, the group-level variances were substantially higher than the individual-level variances. This indicates that a total individual estimate of group plus individual is likely a more informative estimate than individual alone but also that the performances of the pairs dominated the performances of the individuals. Implications are discussed in the results and conclusions. Copyright © 2017 by the National Council on Measurement in Education","['article', 'summarize', 'assessment', 'cognitive', 'skill', 'collaborative', 'task', 'field', 'test', 'result', 'Assessment', 'teaching', '21st', 'Century', 'Skills', 'ATC21S', 'project', 'project', 'sponsor', 'Cisco', 'Intel', 'Microsoft', 'aim', 'help', 'educator', 'world', 'enable', 'student', 'skill', 'succeed', 'future', 'career', 'college', 'goal', 'article', 'ATC21S', 'collaborative', 'assessment', 'focus', 'project', '""', 'ICT', 'Literacy', '—', 'learning', 'digital', 'network', '""', 'learn', 'progression', 'article', 'include', 'description', 'development', 'learning', 'progression', 'example', 'logic', 'instrument', 'construction', 'assessment', 'place', 'random', 'pair', 'student', 'demonstration', 'digital', 'environment', 'modeling', 'result', 'employ', 'unidimensional', 'multidimensional', 'item', 'response', 'random', 'effect', 'group', 'result', 'indicate', 'base', 'datum', 'set', 'group', 'consideration', 'unidimensional', 'multidimensional', 'analysis', 'fit', 'grouplevel', 'variance', 'substantially', 'high', 'individuallevel', 'variance', 'indicate', 'total', 'individual', 'estimate', 'group', 'plus', 'individual', 'likely', 'informative', 'estimate', 'individual', 'performance', 'pair', 'dominate', 'performance', 'individual', 'implication', 'discuss', 'result', 'conclusion', 'Copyright', '©', '2017', 'National', 'Council']","['datum', 'collaborative', 'assessment', 'Learning', 'Digital', 'Interactive', 'Social', 'Networks']","article summarize assessment cognitive skill collaborative task field test result Assessment teaching 21st Century Skills ATC21S project project sponsor Cisco Intel Microsoft aim help educator world enable student skill succeed future career college goal article ATC21S collaborative assessment focus project "" ICT Literacy — learning digital network "" learn progression article include description development learning progression example logic instrument construction assessment place random pair student demonstration digital environment modeling result employ unidimensional multidimensional item response random effect group result indicate base datum set group consideration unidimensional multidimensional analysis fit grouplevel variance substantially high individuallevel variance indicate total individual estimate group plus individual likely informative estimate individual performance pair dominate performance individual implication discuss result conclusion Copyright © 2017 National Council",datum collaborative assessment Learning Digital Interactive Social Networks,0.02205134790587943,0.021928529178896303,0.021848708471939857,0.021997525097691772,0.9121738893455926,0.00730322990315397,0.0,0.10965241226725529,0.0,0.0
Wind S.A.; Jones E.,The Effects of Incomplete Rating Designs in Combination With Rater Effects,2019,56,"Researchers have explored a variety of topics related to identifying and distinguishing among specific types of rater effects, as well as the implications of different types of incomplete data collection designs for rater-mediated assessments. In this study, we used simulated data to examine the sensitivity of latent trait model indicators of three rater effects (leniency, central tendency, and severity) in combination with different types of incomplete rating designs (systematic links, anchor performances, and spiral). We used the rating scale model and the partial credit model to calculate rater location estimates, standard errors of rater estimates, model–data fit statistics, and the standard deviation of rating scale category thresholds as indicators of rater effects and we explored the sensitivity of these indicators to rater effects under different conditions. Our results suggest that it is possible to detect rater effects when each of the three types of rating designs is used. However, there are differences in the sensitivity of each indicator related to type of rater effect, type of rating design, and the overall proportion of effect raters. We discuss implications for research and practice related to rater-mediated assessments. © 2019 by the National Council on Measurement in Education",The Effects of Incomplete Rating Designs in Combination With Rater Effects,"Researchers have explored a variety of topics related to identifying and distinguishing among specific types of rater effects, as well as the implications of different types of incomplete data collection designs for rater-mediated assessments. In this study, we used simulated data to examine the sensitivity of latent trait model indicators of three rater effects (leniency, central tendency, and severity) in combination with different types of incomplete rating designs (systematic links, anchor performances, and spiral). We used the rating scale model and the partial credit model to calculate rater location estimates, standard errors of rater estimates, model–data fit statistics, and the standard deviation of rating scale category thresholds as indicators of rater effects and we explored the sensitivity of these indicators to rater effects under different conditions. Our results suggest that it is possible to detect rater effects when each of the three types of rating designs is used. However, there are differences in the sensitivity of each indicator related to type of rater effect, type of rating design, and the overall proportion of effect raters. We discuss implications for research and practice related to rater-mediated assessments. © 2019 by the National Council on Measurement in Education","['researcher', 'explore', 'variety', 'topic', 'relate', 'identify', 'distinguish', 'specific', 'type', 'rater', 'effect', 'implication', 'different', 'type', 'incomplete', 'datum', 'collection', 'design', 'ratermediate', 'assessment', 'study', 'simulated', 'datum', 'examine', 'sensitivity', 'latent', 'trait', 'indicator', 'rater', 'effect', 'leniency', 'central', 'tendency', 'severity', 'combination', 'different', 'type', 'incomplete', 'rating', 'design', 'systematic', 'link', 'anchor', 'performance', 'spiral', 'rating', 'scale', 'partial', 'credit', 'calculate', 'rater', 'location', 'estimate', 'standard', 'error', 'rater', 'estimate', '–', 'datum', 'fit', 'statistic', 'standard', 'deviation', 'rating', 'scale', 'category', 'threshold', 'indicator', 'rater', 'effect', 'explore', 'sensitivity', 'indicator', 'rater', 'effect', 'different', 'condition', 'result', 'suggest', 'possible', 'detect', 'rater', 'effect', 'type', 'rating', 'design', 'difference', 'sensitivity', 'indicator', 'relate', 'type', 'rater', 'effect', 'type', 'rating', 'design', 'overall', 'proportion', 'effect', 'rater', 'discuss', 'implication', 'research', 'practice', 'relate', 'ratermediate', 'assessment', '©', '2019', 'National', 'Council']","['Effects', 'Incomplete', 'Rating', 'design', 'Combination', 'Rater', 'effect']",researcher explore variety topic relate identify distinguish specific type rater effect implication different type incomplete datum collection design ratermediate assessment study simulated datum examine sensitivity latent trait indicator rater effect leniency central tendency severity combination different type incomplete rating design systematic link anchor performance spiral rating scale partial credit calculate rater location estimate standard error rater estimate – datum fit statistic standard deviation rating scale category threshold indicator rater effect explore sensitivity indicator rater effect different condition result suggest possible detect rater effect type rating design difference sensitivity indicator relate type rater effect type rating design overall proportion effect rater discuss implication research practice relate ratermediate assessment © 2019 National Council,Effects Incomplete Rating design Combination Rater effect,0.029495417446214017,0.8824940231666935,0.029122005641241403,0.029457702798545232,0.02943085094730601,0.002449967868176698,0.0,0.0,0.0043879949900028,0.2695826789929362
Halpin P.F.; von Davier A.A.; Hao J.; Liu L.,Measuring Student Engagement During Collaboration,2017,54,"This article addresses performance assessments that involve collaboration among students. We apply the Hawkes process to infer whether the actions of one student are associated with increased probability of further actions by his/her partner(s) in the near future. This leads to an intuitive notion of engagement among collaborators, and we consider a model-based index that can be used to quantify this notion. The approach is illustrated using a simulation-based task designed for science education, in which pairs of collaborators interact using online chat. We also consider the empirical relationship between chat engagement and task performance, finding that less engaged collaborators were less likely to revise their responses after being given an opportunity to share their work with their partner. Copyright © 2017 by the National Council on Measurement in Education",Measuring Student Engagement During Collaboration,"This article addresses performance assessments that involve collaboration among students. We apply the Hawkes process to infer whether the actions of one student are associated with increased probability of further actions by his/her partner(s) in the near future. This leads to an intuitive notion of engagement among collaborators, and we consider a model-based index that can be used to quantify this notion. The approach is illustrated using a simulation-based task designed for science education, in which pairs of collaborators interact using online chat. We also consider the empirical relationship between chat engagement and task performance, finding that less engaged collaborators were less likely to revise their responses after being given an opportunity to share their work with their partner. Copyright © 2017 by the National Council on Measurement in Education","['article', 'address', 'performance', 'assessment', 'involve', 'collaboration', 'student', 'apply', 'Hawkes', 'process', 'infer', 'action', 'student', 'associate', 'increase', 'probability', 'action', 'hisher', 'partner', 'near', 'future', 'lead', 'intuitive', 'notion', 'engagement', 'collaborator', 'consider', 'modelbase', 'index', 'quantify', 'notion', 'approach', 'illustrate', 'simulationbased', 'task', 'design', 'science', 'pair', 'collaborator', 'interact', 'online', 'chat', 'consider', 'empirical', 'relationship', 'chat', 'engagement', 'task', 'performance', 'find', 'engaged', 'collaborator', 'likely', 'revise', 'response', 'opportunity', 'share', 'work', 'partner', 'Copyright', '©', '2017', 'National', 'Council']","['measure', 'Student', 'Engagement', 'collaboration']",article address performance assessment involve collaboration student apply Hawkes process infer action student associate increase probability action hisher partner near future lead intuitive notion engagement collaborator consider modelbase index quantify notion approach illustrate simulationbased task design science pair collaborator interact online chat consider empirical relationship chat engagement task performance find engaged collaborator likely revise response opportunity share work partner Copyright © 2017 National Council,measure Student Engagement collaboration,0.027910275558886345,0.027861532894930187,0.0277840144901497,0.02792349094639545,0.8885206861096383,0.0,0.0,0.08518351079541217,0.0,0.0
Moses T.; Kim Y.,Stabilizing Conditional Standard Errors of Measurement in Scale Score Transformations,2017,54,"The focus of this article is on scale score transformations that can be used to stabilize conditional standard errors of measurement (CSEMs). Three transformations for stabilizing the estimated CSEMs are reviewed, including the traditional arcsine transformation, a recently developed general variance stabilization transformation, and a new method proposed in this article involving cubic transformations. Two examples are provided and the three scale score transformations are compared in terms of how well they stabilize CSEMs estimated from compound binomial and item response theory (IRT) models. Advantages of the cubic transformation are demonstrated with respect to CSEM stabilization and other scaling criteria (e.g., scale score distributions that are more symmetric). Copyright © 2017 by the National Council on Measurement in Education",Stabilizing Conditional Standard Errors of Measurement in Scale Score Transformations,"The focus of this article is on scale score transformations that can be used to stabilize conditional standard errors of measurement (CSEMs). Three transformations for stabilizing the estimated CSEMs are reviewed, including the traditional arcsine transformation, a recently developed general variance stabilization transformation, and a new method proposed in this article involving cubic transformations. Two examples are provided and the three scale score transformations are compared in terms of how well they stabilize CSEMs estimated from compound binomial and item response theory (IRT) models. Advantages of the cubic transformation are demonstrated with respect to CSEM stabilization and other scaling criteria (e.g., scale score distributions that are more symmetric). Copyright © 2017 by the National Council on Measurement in Education","['focus', 'article', 'scale', 'score', 'transformation', 'stabilize', 'conditional', 'standard', 'error', 'csem', 'transformation', 'stabilize', 'estimate', 'csem', 'review', 'include', 'traditional', 'arcsine', 'transformation', 'recently', 'develop', 'general', 'variance', 'stabilization', 'transformation', 'new', 'method', 'propose', 'article', 'involve', 'cubic', 'transformation', 'example', 'provide', 'scale', 'score', 'transformation', 'compare', 'term', 'stabilize', 'csem', 'estimate', 'compound', 'binomial', 'item', 'response', 'theory', 'IRT', 'advantage', 'cubic', 'transformation', 'demonstrate', 'respect', 'CSEM', 'stabilization', 'scaling', 'criterion', 'eg', 'scale', 'score', 'distribution', 'symmetric', 'copyright', '©', '2017', 'National', 'Council']","['stabilize', 'Conditional', 'Standard', 'Errors', 'Scale', 'Score', 'Transformations']",focus article scale score transformation stabilize conditional standard error csem transformation stabilize estimate csem review include traditional arcsine transformation recently develop general variance stabilization transformation new method propose article involve cubic transformation example provide scale score transformation compare term stabilize csem estimate compound binomial item response theory IRT advantage cubic transformation demonstrate respect CSEM stabilization scaling criterion eg scale score distribution symmetric copyright © 2017 National Council,stabilize Conditional Standard Errors Scale Score Transformations,0.03631664520901487,0.03613800443341647,0.03608229548762295,0.03667977089428023,0.8547832839756654,0.0163469983804333,0.009710530954643892,0.018772620684843665,0.033438087867271996,0.004425877603783056
Chen Y.-H.; Senk S.L.; Thompson D.R.; Voogt K.,Examining Psychometric Properties and Level Classification of the van Hiele Geometry Test Using CTT and CDM Frameworks,2019,56,"The van Hiele theory and van Hiele Geometry Test have been extensively used in mathematics assessments across countries. The purpose of this study is to use classical test theory (CTT) and cognitive diagnostic modeling (CDM) frameworks to examine psychometric properties of the van Hiele Geometry Test and to compare how various classification criteria assign van Hiele levels to students. The findings support the hierarchical property of the van Hiele theory and levels. Using conventional and combined criteria to determine mastery of a level, the percentages of students classified into an overall level were relatively high. Although some items had aberrant difficulties and low item discrimination, varied selection of the criteria across levels improved item discrimination power, especially for those items with low item discrimination index (IDI) estimates. Based on the findings, we identify items on the van Hiele Geometry Test that might be revised and we suggest changes to classification criteria to increase the number of students who can be assigned an overall level of geometry thinking according to the theory. As a result, practitioners and researchers may be better positioned to use the van Hiele Geometry Test for classroom assessment. © 2019 by the National Council on Measurement in Education",Examining Psychometric Properties and Level Classification of the van Hiele Geometry Test Using CTT and CDM Frameworks,"The van Hiele theory and van Hiele Geometry Test have been extensively used in mathematics assessments across countries. The purpose of this study is to use classical test theory (CTT) and cognitive diagnostic modeling (CDM) frameworks to examine psychometric properties of the van Hiele Geometry Test and to compare how various classification criteria assign van Hiele levels to students. The findings support the hierarchical property of the van Hiele theory and levels. Using conventional and combined criteria to determine mastery of a level, the percentages of students classified into an overall level were relatively high. Although some items had aberrant difficulties and low item discrimination, varied selection of the criteria across levels improved item discrimination power, especially for those items with low item discrimination index (IDI) estimates. Based on the findings, we identify items on the van Hiele Geometry Test that might be revised and we suggest changes to classification criteria to increase the number of students who can be assigned an overall level of geometry thinking according to the theory. As a result, practitioners and researchers may be better positioned to use the van Hiele Geometry Test for classroom assessment. © 2019 by the National Council on Measurement in Education","['van', 'Hiele', 'theory', 'van', 'Hiele', 'Geometry', 'Test', 'extensively', 'mathematic', 'assessment', 'country', 'purpose', 'study', 'classical', 'test', 'theory', 'CTT', 'cognitive', 'diagnostic', 'modeling', 'cdm', 'framework', 'examine', 'psychometric', 'property', 'van', 'Hiele', 'Geometry', 'Test', 'compare', 'classification', 'criterion', 'assign', 'van', 'Hiele', 'level', 'student', 'finding', 'support', 'hierarchical', 'property', 'van', 'Hiele', 'theory', 'level', 'conventional', 'combined', 'criterion', 'determine', 'mastery', 'level', 'percentage', 'student', 'classify', 'overall', 'level', 'relatively', 'high', 'item', 'aberrant', 'difficulty', 'low', 'item', 'discrimination', 'varied', 'selection', 'criterion', 'level', 'improve', 'item', 'discrimination', 'power', 'especially', 'item', 'low', 'item', 'discrimination', 'index', 'IDI', 'estimate', 'base', 'finding', 'identify', 'item', 'van', 'Hiele', 'Geometry', 'Test', 'revise', 'suggest', 'change', 'classification', 'criterion', 'increase', 'number', 'student', 'assign', 'overall', 'level', 'geometry', 'think', 'accord', 'theory', 'result', 'practitioner', 'researcher', 'position', 'van', 'Hiele', 'Geometry', 'Test', 'classroom', 'assessment', '©', '2019', 'National', 'Council']","['examine', 'Psychometric', 'Properties', 'Level', 'Classification', 'van', 'Hiele', 'Geometry', 'Test', 'CTT', 'CDM', 'framework']",van Hiele theory van Hiele Geometry Test extensively mathematic assessment country purpose study classical test theory CTT cognitive diagnostic modeling cdm framework examine psychometric property van Hiele Geometry Test compare classification criterion assign van Hiele level student finding support hierarchical property van Hiele theory level conventional combined criterion determine mastery level percentage student classify overall level relatively high item aberrant difficulty low item discrimination varied selection criterion level improve item discrimination power especially item low item discrimination index IDI estimate base finding identify item van Hiele Geometry Test revise suggest change classification criterion increase number student assign overall level geometry think accord theory result practitioner researcher position van Hiele Geometry Test classroom assessment © 2019 National Council,examine Psychometric Properties Level Classification van Hiele Geometry Test CTT CDM framework,0.03100601991133867,0.8762384418710342,0.030564362022994843,0.03128250872371333,0.030908667470918902,0.037214915660151,0.012536838197770817,0.03174315522182249,0.0012071013891543374,0.0
Lee S.; Suh Y.,Lord's Wald Test for Detecting DIF in Multidimensional IRT Models: A Comparison of Two Estimation Approaches,2018,55,"Lord's Wald test for differential item functioning (DIF) has not been studied extensively in the context of the multidimensional item response theory (MIRT) framework. In this article, Lord's Wald test was implemented using two estimation approaches, marginal maximum likelihood estimation and Bayesian Markov chain Monte Carlo estimation, to detect uniform and nonuniform DIF under MIRT models. The Type I error and power rates for Lord's Wald test were investigated under various simulation conditions, including different DIF types and magnitudes, different means and correlations of two ability parameters, and different sample sizes. Furthermore, English usage data were analyzed to illustrate the use of Lord's Wald test with the two estimation approaches. Copyright © 2018 by the National Council on Measurement in Education",Lord's Wald Test for Detecting DIF in Multidimensional IRT Models: A Comparison of Two Estimation Approaches,"Lord's Wald test for differential item functioning (DIF) has not been studied extensively in the context of the multidimensional item response theory (MIRT) framework. In this article, Lord's Wald test was implemented using two estimation approaches, marginal maximum likelihood estimation and Bayesian Markov chain Monte Carlo estimation, to detect uniform and nonuniform DIF under MIRT models. The Type I error and power rates for Lord's Wald test were investigated under various simulation conditions, including different DIF types and magnitudes, different means and correlations of two ability parameters, and different sample sizes. Furthermore, English usage data were analyzed to illustrate the use of Lord's Wald test with the two estimation approaches. Copyright © 2018 by the National Council on Measurement in Education","['Lords', 'Wald', 'test', 'differential', 'item', 'function', 'DIF', 'study', 'extensively', 'context', 'multidimensional', 'item', 'response', 'theory', 'MIRT', 'framework', 'article', 'Lords', 'Wald', 'test', 'implement', 'estimation', 'approach', 'marginal', 'maximum', 'likelihood', 'estimation', 'Bayesian', 'Markov', 'chain', 'Monte', 'Carlo', 'estimation', 'detect', 'uniform', 'nonuniform', 'DIF', 'MIRT', 'type', 'I', 'error', 'power', 'rate', 'Lords', 'Wald', 'test', 'investigate', 'simulation', 'condition', 'include', 'different', 'DIF', 'type', 'magnitude', 'different', 'mean', 'correlation', 'ability', 'parameter', 'different', 'sample', 'size', 'furthermore', 'english', 'usage', 'datum', 'analyze', 'illustrate', 'Lords', 'Wald', 'test', 'estimation', 'approach', 'copyright', '©', '2018', 'National', 'Council']","['Lords', 'Wald', 'Test', 'detect', 'DIF', 'Multidimensional', 'IRT', 'Models', 'Comparison', 'Estimation', 'Approaches']",Lords Wald test differential item function DIF study extensively context multidimensional item response theory MIRT framework article Lords Wald test implement estimation approach marginal maximum likelihood estimation Bayesian Markov chain Monte Carlo estimation detect uniform nonuniform DIF MIRT type I error power rate Lords Wald test investigate simulation condition include different DIF type magnitude different mean correlation ability parameter different sample size furthermore english usage datum analyze illustrate Lords Wald test estimation approach copyright © 2018 National Council,Lords Wald Test detect DIF Multidimensional IRT Models Comparison Estimation Approaches,0.02997793846506708,0.8822447469336584,0.02915971395134219,0.02932150861123328,0.0292960920386989,0.08648585223160342,0.0,0.0,0.0,0.0
Andersson B.,Asymptotic Standard Errors of Observed-Score Equating With Polytomous IRT Models,2016,53,"In observed-score equipercentile equating, the goal is to make scores on two scales or tests measuring the same construct comparable by matching the percentiles of the respective score distributions. If the tests consist of different items with multiple categories for each item, a suitable model for the responses is a polytomous item response theory (IRT) model. The parameters from such a model can be utilized to derive the score probabilities for the tests and these score probabilities may then be used in observed-score equating. In this study, the asymptotic standard errors of observed-score equating using score probability vectors from polytomous IRT models are derived using the delta method. The results are applied to the equivalent groups design and the nonequivalent groups design with either chain equating or poststratification equating within the framework of kernel equating. The derivations are presented in a general form and specific formulas for the graded response model and the generalized partial credit model are provided. The asymptotic standard errors are accurate under several simulation conditions relating to sample size, distributional misspecification and, for the nonequivalent groups design, anchor test length. Copyright © 2016 by the National Council on Measurement in Education",Asymptotic Standard Errors of Observed-Score Equating With Polytomous IRT Models,"In observed-score equipercentile equating, the goal is to make scores on two scales or tests measuring the same construct comparable by matching the percentiles of the respective score distributions. If the tests consist of different items with multiple categories for each item, a suitable model for the responses is a polytomous item response theory (IRT) model. The parameters from such a model can be utilized to derive the score probabilities for the tests and these score probabilities may then be used in observed-score equating. In this study, the asymptotic standard errors of observed-score equating using score probability vectors from polytomous IRT models are derived using the delta method. The results are applied to the equivalent groups design and the nonequivalent groups design with either chain equating or poststratification equating within the framework of kernel equating. The derivations are presented in a general form and specific formulas for the graded response model and the generalized partial credit model are provided. The asymptotic standard errors are accurate under several simulation conditions relating to sample size, distributional misspecification and, for the nonequivalent groups design, anchor test length. Copyright © 2016 by the National Council on Measurement in Education","['observedscore', 'equipercentile', 'equate', 'goal', 'score', 'scale', 'test', 'measure', 'construct', 'comparable', 'match', 'percentile', 'respective', 'score', 'distribution', 'test', 'consist', 'different', 'item', 'multiple', 'category', 'item', 'suitable', 'response', 'polytomous', 'item', 'response', 'theory', 'IRT', 'parameter', 'utilize', 'derive', 'score', 'probability', 'test', 'score', 'probability', 'observedscore', 'equate', 'study', 'asymptotic', 'standard', 'error', 'observedscore', 'equate', 'score', 'probability', 'vector', 'polytomous', 'IRT', 'derive', 'delta', 'method', 'result', 'apply', 'equivalent', 'group', 'design', 'nonequivalent', 'group', 'design', 'chain', 'equating', 'poststratification', 'equating', 'framework', 'kernel', 'equate', 'derivation', 'present', 'general', 'form', 'specific', 'formula', 'grade', 'response', 'generalized', 'partial', 'credit', 'provide', 'asymptotic', 'standard', 'error', 'accurate', 'simulation', 'condition', 'relate', 'sample', 'size', 'distributional', 'misspecification', 'nonequivalent', 'group', 'design', 'anchor', 'test', 'length', 'copyright', '©', '2016', 'National', 'Council']","['Asymptotic', 'Standard', 'Errors', 'ObservedScore', 'Equating', 'Polytomous', 'IRT']",observedscore equipercentile equate goal score scale test measure construct comparable match percentile respective score distribution test consist different item multiple category item suitable response polytomous item response theory IRT parameter utilize derive score probability test score probability observedscore equate study asymptotic standard error observedscore equate score probability vector polytomous IRT derive delta method result apply equivalent group design nonequivalent group design chain equating poststratification equating framework kernel equate derivation present general form specific formula grade response generalized partial credit provide asymptotic standard error accurate simulation condition relate sample size distributional misspecification nonequivalent group design anchor test length copyright © 2016 National Council,Asymptotic Standard Errors ObservedScore Equating Polytomous IRT,0.9011888664714245,0.024689502966960594,0.024774128269851068,0.02471241631895826,0.024635085972805837,0.026973796087220784,0.0,0.01120315297459479,0.16170993260070732,0.005949248168994943
Sinharay S.; Duong M.Q.; Wood S.W.,A New Statistic for Detection of Aberrant Answer Changes,2017,54,"As noted by Fremer and Olson, analysis of answer changes is often used to investigate testing irregularities because the analysis is readily performed and has proven its value in practice. Researchers such as Belov, Sinharay and Johnson, van der Linden and Jeon, van der Linden and Lewis, and Wollack, Cohen, and Eckerly have suggested several statistics for detection of aberrant answer changes. This article suggests a new statistic that is based on the likelihood ratio test. An advantage of the new statistic is that it follows the standard normal distribution under the null hypothesis of no aberrant answer changes. It is demonstrated in a detailed simulation study that the Type I error rate of the new statistic is very close to the nominal level and the power of the new statistic is satisfactory in comparison to those of several existing statistics for detecting aberrant answer changes. The new statistic and several existing statistics were shown to provide useful information for a real data set. Given the increasing interest in analysis of answer changes, the new statistic promises to be useful to measurement practitioners. Copyright © 2017 by the National Council on Measurement in Education",A New Statistic for Detection of Aberrant Answer Changes,"As noted by Fremer and Olson, analysis of answer changes is often used to investigate testing irregularities because the analysis is readily performed and has proven its value in practice. Researchers such as Belov, Sinharay and Johnson, van der Linden and Jeon, van der Linden and Lewis, and Wollack, Cohen, and Eckerly have suggested several statistics for detection of aberrant answer changes. This article suggests a new statistic that is based on the likelihood ratio test. An advantage of the new statistic is that it follows the standard normal distribution under the null hypothesis of no aberrant answer changes. It is demonstrated in a detailed simulation study that the Type I error rate of the new statistic is very close to the nominal level and the power of the new statistic is satisfactory in comparison to those of several existing statistics for detecting aberrant answer changes. The new statistic and several existing statistics were shown to provide useful information for a real data set. Given the increasing interest in analysis of answer changes, the new statistic promises to be useful to measurement practitioners. Copyright © 2017 by the National Council on Measurement in Education","['note', 'Fremer', 'Olson', 'analysis', 'answer', 'change', 'investigate', 'testing', 'irregularity', 'analysis', 'readily', 'perform', 'prove', 'value', 'practice', 'Researchers', 'Belov', 'Sinharay', 'Johnson', 'van', 'der', 'Linden', 'Jeon', 'van', 'der', 'Linden', 'Lewis', 'Wollack', 'Cohen', 'eckerly', 'suggest', 'statistic', 'detection', 'aberrant', 'answer', 'change', 'article', 'suggest', 'new', 'statistic', 'base', 'likelihood', 'ratio', 'test', 'advantage', 'new', 'statistic', 'follow', 'standard', 'normal', 'distribution', 'null', 'hypothesis', 'aberrant', 'answer', 'change', 'demonstrate', 'detailed', 'simulation', 'study', 'Type', 'I', 'error', 'rate', 'new', 'statistic', 'close', 'nominal', 'level', 'power', 'new', 'statistic', 'satisfactory', 'comparison', 'exist', 'statistic', 'detect', 'aberrant', 'answer', 'change', 'new', 'statistic', 'exist', 'statistic', 'provide', 'useful', 'information', 'real', 'datum', 'set', 'increase', 'interest', 'analysis', 'answer', 'change', 'new', 'statistic', 'promise', 'useful', 'practitioner', 'Copyright', '©', '2017', 'National', 'Council']","['New', 'Statistic', 'Detection', 'Aberrant', 'Answer', 'change']",note Fremer Olson analysis answer change investigate testing irregularity analysis readily perform prove value practice Researchers Belov Sinharay Johnson van der Linden Jeon van der Linden Lewis Wollack Cohen eckerly suggest statistic detection aberrant answer change article suggest new statistic base likelihood ratio test advantage new statistic follow standard normal distribution null hypothesis aberrant answer change demonstrate detailed simulation study Type I error rate new statistic close nominal level power new statistic satisfactory comparison exist statistic detect aberrant answer change new statistic exist statistic provide useful information real datum set increase interest analysis answer change new statistic promise useful practitioner Copyright © 2017 National Council,New Statistic Detection Aberrant Answer change,0.027295653899536847,0.027330988451529977,0.027140091394742473,0.8907563164326806,0.02747694982151006,0.04310320909328625,0.014560730790209503,0.017518219672769786,0.0019797362162657225,0.0
Keuning T.; van Geel M.; Visscher A.; Fox J.-P.,Assessing and Validating Effects of a Data-Based Decision-Making Intervention on Student Growth for Mathematics and Spelling,2019,56,"Data-based decision making (DBDM) is presumed to improve student performance in elementary schools in all subjects. The majority of studies in which DBDM effects have been evaluated have focused on mathematics. A hierarchical multiple single-subject design was used to measure effects of a 2-year training, in which entire school teams learned how to implement and sustain DBDM, in 39 elementary schools. In a multilevel modeling approach, student achievement in mathematics and spelling was analyzed to broaden our understanding of the effects of DBDM interventions. Student achievement data covering the period from August 2010 to July 2014 were retrieved from schools’ student monitoring systems. Student performance on standardized tests was scored on a vertical ability scale per subject for Grades 1 to 6. To investigate intervention effects, linear mixed effect analysis was conducted. Findings revealed a positive intervention effect for both mathematics and spelling. Furthermore, low-SES students and low-SES schools benefitted most from the intervention for mathematics. © 2019 by the National Council on Measurement in Education",Assessing and Validating Effects of a Data-Based Decision-Making Intervention on Student Growth for Mathematics and Spelling,"Data-based decision making (DBDM) is presumed to improve student performance in elementary schools in all subjects. The majority of studies in which DBDM effects have been evaluated have focused on mathematics. A hierarchical multiple single-subject design was used to measure effects of a 2-year training, in which entire school teams learned how to implement and sustain DBDM, in 39 elementary schools. In a multilevel modeling approach, student achievement in mathematics and spelling was analyzed to broaden our understanding of the effects of DBDM interventions. Student achievement data covering the period from August 2010 to July 2014 were retrieved from schools’ student monitoring systems. Student performance on standardized tests was scored on a vertical ability scale per subject for Grades 1 to 6. To investigate intervention effects, linear mixed effect analysis was conducted. Findings revealed a positive intervention effect for both mathematics and spelling. Furthermore, low-SES students and low-SES schools benefitted most from the intervention for mathematics. © 2019 by the National Council on Measurement in Education","['databased', 'decision', 'DBDM', 'presume', 'improve', 'student', 'performance', 'elementary', 'school', 'subject', 'majority', 'study', 'DBDM', 'effect', 'evaluate', 'focus', 'mathematic', 'hierarchical', 'multiple', 'singlesubject', 'design', 'measure', 'effect', '2year', 'training', 'entire', 'school', 'team', 'learn', 'implement', 'sustain', 'DBDM', '39', 'elementary', 'school', 'multilevel', 'modeling', 'approach', 'student', 'achievement', 'mathematic', 'spelling', 'analyze', 'broaden', 'understanding', 'effect', 'DBDM', 'intervention', 'Student', 'achievement', 'datum', 'cover', 'period', 'August', '2010', 'July', '2014', 'retrieve', 'school', '’', 'student', 'monitoring', 'system', 'student', 'performance', 'standardized', 'test', 'score', 'vertical', 'ability', 'scale', 'subject', 'Grades', '1', '6', 'investigate', 'intervention', 'effect', 'linear', 'mixed', 'effect', 'analysis', 'conduct', 'Findings', 'reveal', 'positive', 'intervention', 'effect', 'mathematic', 'spelling', 'Furthermore', 'lowSES', 'student', 'lowSES', 'school', 'benefit', 'intervention', 'mathematic', '©', '2019', 'National', 'Council']","['assess', 'validate', 'effect', 'DataBased', 'DecisionMaking', 'Intervention', 'student', 'Growth', 'Mathematics', 'Spelling']",databased decision DBDM presume improve student performance elementary school subject majority study DBDM effect evaluate focus mathematic hierarchical multiple singlesubject design measure effect 2year training entire school team learn implement sustain DBDM 39 elementary school multilevel modeling approach student achievement mathematic spelling analyze broaden understanding effect DBDM intervention Student achievement datum cover period August 2010 July 2014 retrieve school ’ student monitoring system student performance standardized test score vertical ability scale subject Grades 1 6 investigate intervention effect linear mixed effect analysis conduct Findings reveal positive intervention effect mathematic spelling Furthermore lowSES student lowSES school benefit intervention mathematic © 2019 National Council,assess validate effect DataBased DecisionMaking Intervention student Growth Mathematics Spelling,0.02677245301653292,0.02669191676497753,0.026387546587647647,0.8930354566997636,0.02711262693107833,0.00039640865700793466,0.0,0.07543707810492731,0.0,0.017695663096671303
Wind S.A.; Sebok-Syer S.S.,Examining Differential Rater Functioning Using a Between-Subgroup Outfit Approach,2019,56,"When practitioners use modern measurement models to evaluate rating quality, they commonly examine rater fit statistics that summarize how well each rater's ratings fit the expectations of the measurement model. Essentially, this approach involves examining the unexpected ratings that each misfitting rater assigned (i.e., carrying out analyses of standardized residuals). One can create plots of the standardized residuals, isolating those that resulted from raters’ ratings of particular subgroups. Practitioners can then examine the plots to identify raters who did not maintain a uniform level of severity when they assessed various subgroups (i.e., exhibited evidence of differential rater functioning). In this study, we analyzed simulated and real data to explore the utility of this between-subgroup fit approach. We used standardized between-subgroup outfit statistics to identify misfitting raters and the corresponding plots of their standardized residuals to determine whether there were any identifiable patterns in each rater's misfitting ratings related to subgroups. © 2019 by the National Council on Measurement in Education",Examining Differential Rater Functioning Using a Between-Subgroup Outfit Approach,"When practitioners use modern measurement models to evaluate rating quality, they commonly examine rater fit statistics that summarize how well each rater's ratings fit the expectations of the measurement model. Essentially, this approach involves examining the unexpected ratings that each misfitting rater assigned (i.e., carrying out analyses of standardized residuals). One can create plots of the standardized residuals, isolating those that resulted from raters’ ratings of particular subgroups. Practitioners can then examine the plots to identify raters who did not maintain a uniform level of severity when they assessed various subgroups (i.e., exhibited evidence of differential rater functioning). In this study, we analyzed simulated and real data to explore the utility of this between-subgroup fit approach. We used standardized between-subgroup outfit statistics to identify misfitting raters and the corresponding plots of their standardized residuals to determine whether there were any identifiable patterns in each rater's misfitting ratings related to subgroups. © 2019 by the National Council on Measurement in Education","['practitioner', 'modern', 'evaluate', 'rating', 'quality', 'commonly', 'examine', 'rater', 'fit', 'statistic', 'summarize', 'rater', 'rating', 'fit', 'expectation', 'essentially', 'approach', 'involve', 'examine', 'unexpected', 'rating', 'misfitting', 'rater', 'assign', 'ie', 'carry', 'analysis', 'standardized', 'residual', 'create', 'plot', 'standardized', 'residual', 'isolate', 'result', 'rater', ""'"", 'rating', 'particular', 'subgroup', 'practitioner', 'examine', 'plot', 'identify', 'rater', 'maintain', 'uniform', 'level', 'severity', 'assess', 'subgroup', 'ie', 'exhibit', 'evidence', 'differential', 'rater', 'function', 'study', 'analyze', 'simulated', 'real', 'datum', 'explore', 'utility', 'betweensubgroup', 'fit', 'approach', 'standardized', 'betweensubgroup', 'outfit', 'statistic', 'identify', 'misfitting', 'rater', 'correspond', 'plot', 'standardized', 'residual', 'determine', 'identifiable', 'pattern', 'rater', 'misfit', 'rating', 'relate', 'subgroup', '©', '2019', 'National', 'Council']","['examine', 'Differential', 'Rater', 'Functioning', 'BetweenSubgroup', 'Outfit', 'Approach']",practitioner modern evaluate rating quality commonly examine rater fit statistic summarize rater rating fit expectation essentially approach involve examine unexpected rating misfitting rater assign ie carry analysis standardized residual create plot standardized residual isolate result rater ' rating particular subgroup practitioner examine plot identify rater maintain uniform level severity assess subgroup ie exhibit evidence differential rater function study analyze simulated real datum explore utility betweensubgroup fit approach standardized betweensubgroup outfit statistic identify misfitting rater correspond plot standardized residual determine identifiable pattern rater misfit rating relate subgroup © 2019 National Council,examine Differential Rater Functioning BetweenSubgroup Outfit Approach,0.8813414480442067,0.02979710248791757,0.029437772535109846,0.029702098045473045,0.029721578887292915,0.0,0.0,0.0,0.0,0.22920119380042256
Vonkova H.; Zamarro G.; Hitt C.,Cross-Country Heterogeneity in Students’ Reporting Behavior: The Use of the Anchoring Vignette Method,2018,55,Self-reports are an indispensable source of information in education research but they are often affected by heterogeneity in reporting behavior. Failing to correct for this heterogeneity can lead to invalid comparisons across groups. The researchers use the parametric anchoring vignette method to correct for cross-country incomparability of students’ reports on teacher's classroom management. Their analysis is based on the data from the Programme for International Student Assessment 2012. The results show significant variation in implicit standards across countries. Correlations between countries’ average teacher classroom management levels and external variables like students test scores and public expenditure per pupil change substantially after vignette adjustment. The researchers conclude that the anchoring vignettes method shows potential to enhance the comparability of self-reported measures in education. Copyright © 2018 by the National Council on Measurement in Education,Cross-Country Heterogeneity in Students’ Reporting Behavior: The Use of the Anchoring Vignette Method,Self-reports are an indispensable source of information in education research but they are often affected by heterogeneity in reporting behavior. Failing to correct for this heterogeneity can lead to invalid comparisons across groups. The researchers use the parametric anchoring vignette method to correct for cross-country incomparability of students’ reports on teacher's classroom management. Their analysis is based on the data from the Programme for International Student Assessment 2012. The results show significant variation in implicit standards across countries. Correlations between countries’ average teacher classroom management levels and external variables like students test scores and public expenditure per pupil change substantially after vignette adjustment. The researchers conclude that the anchoring vignettes method shows potential to enhance the comparability of self-reported measures in education. Copyright © 2018 by the National Council on Measurement in Education,"['selfreport', 'indispensable', 'source', 'information', 'research', 'affect', 'heterogeneity', 'report', 'behavior', 'fail', 'correct', 'heterogeneity', 'lead', 'invalid', 'comparison', 'group', 'researcher', 'parametric', 'anchor', 'vignette', 'method', 'correct', 'crosscountry', 'incomparability', 'student', '’', 'report', 'teacher', 'classroom', 'management', 'analysis', 'base', 'datum', 'Programme', 'International', 'Student', 'Assessment', '2012', 'result', 'significant', 'variation', 'implicit', 'standard', 'country', 'Correlations', 'country', '’', 'average', 'teacher', 'classroom', 'management', 'level', 'external', 'variable', 'like', 'student', 'test', 'score', 'public', 'expenditure', 'pupil', 'change', 'substantially', 'vignette', 'adjustment', 'researcher', 'conclude', 'anchor', 'vignette', 'method', 'potential', 'enhance', 'comparability', 'selfreporte', 'measure', 'Copyright', '©', '2018', 'National', 'Council']","['CrossCountry', 'Heterogeneity', 'Students', ""'"", 'report', 'Behavior', 'Use', 'Anchoring', 'Vignette', 'Method']",selfreport indispensable source information research affect heterogeneity report behavior fail correct heterogeneity lead invalid comparison group researcher parametric anchor vignette method correct crosscountry incomparability student ’ report teacher classroom management analysis base datum Programme International Student Assessment 2012 result significant variation implicit standard country Correlations country ’ average teacher classroom management level external variable like student test score public expenditure pupil change substantially vignette adjustment researcher conclude anchor vignette method potential enhance comparability selfreporte measure Copyright © 2018 National Council,CrossCountry Heterogeneity Students ' report Behavior Use Anchoring Vignette Method,0.025555346021474707,0.8973992692367254,0.025650894175048795,0.025596372481445133,0.025798118085306035,0.0,0.013573879351037243,0.08296189461945502,0.011095914345625879,0.0
Kim K.Y.; Lee W.-C.,Confidence Intervals for Weighted Composite Scores Under the Compound Binomial Error Model,2018,55,"Reporting confidence intervals with test scores helps test users make important decisions about examinees by providing information about the precision of test scores. Although a variety of estimation procedures based on the binomial error model are available for computing intervals for test scores, these procedures assume that items are randomly drawn from a undifferentiated universe of items, and therefore might not be suitable for tests developed according to a table of specifications. To address this issue, four interval estimation procedures that use category subscores for the computation of confidence intervals are presented in this article. All four estimation procedures assume that subscores instead of test scores follow a binomial distribution (i.e., compound binomial error model). The relative performance of the four compound binomial–based interval estimation procedures is compared to each other and to the better known normal approximation and Wilson score procedures based on the binomial error model. Copyright © 2018 by the National Council on Measurement in Education",Confidence Intervals for Weighted Composite Scores Under the Compound Binomial Error Model,"Reporting confidence intervals with test scores helps test users make important decisions about examinees by providing information about the precision of test scores. Although a variety of estimation procedures based on the binomial error model are available for computing intervals for test scores, these procedures assume that items are randomly drawn from a undifferentiated universe of items, and therefore might not be suitable for tests developed according to a table of specifications. To address this issue, four interval estimation procedures that use category subscores for the computation of confidence intervals are presented in this article. All four estimation procedures assume that subscores instead of test scores follow a binomial distribution (i.e., compound binomial error model). The relative performance of the four compound binomial–based interval estimation procedures is compared to each other and to the better known normal approximation and Wilson score procedures based on the binomial error model. Copyright © 2018 by the National Council on Measurement in Education","['report', 'confidence', 'interval', 'test', 'score', 'help', 'test', 'user', 'important', 'decision', 'examinee', 'provide', 'information', 'precision', 'test', 'score', 'variety', 'estimation', 'procedure', 'base', 'binomial', 'error', 'available', 'compute', 'interval', 'test', 'score', 'procedure', 'assume', 'item', 'randomly', 'draw', 'undifferentiated', 'universe', 'item', 'suitable', 'test', 'develop', 'accord', 'table', 'specification', 'address', 'issue', 'interval', 'estimation', 'procedure', 'category', 'subscore', 'computation', 'confidence', 'interval', 'present', 'article', 'estimation', 'procedure', 'assume', 'subscore', 'instead', 'test', 'score', 'follow', 'binomial', 'distribution', 'ie', 'compound', 'binomial', 'error', 'relative', 'performance', 'compound', 'binomial', '–', 'base', 'interval', 'estimation', 'procedure', 'compare', 'know', 'normal', 'approximation', 'Wilson', 'score', 'procedure', 'base', 'binomial', 'error', 'Copyright', '©', '2018', 'National', 'Council']","['confidence', 'Intervals', 'Weighted', 'Composite', 'Scores', 'Compound', 'Binomial', 'Error']",report confidence interval test score help test user important decision examinee provide information precision test score variety estimation procedure base binomial error available compute interval test score procedure assume item randomly draw undifferentiated universe item suitable test develop accord table specification address issue interval estimation procedure category subscore computation confidence interval present article estimation procedure assume subscore instead test score follow binomial distribution ie compound binomial error relative performance compound binomial – base interval estimation procedure compare know normal approximation Wilson score procedure base binomial error Copyright © 2018 National Council,confidence Intervals Weighted Composite Scores Compound Binomial Error,0.030640575527682897,0.030694108023369974,0.030731615615586867,0.8769652535769518,0.030968447256408425,0.03408938495316865,0.09380961548915996,0.00794253654800091,0.004247402317469008,0.0
Ames A.; Smith E.,Subjective Priors for Item Response Models: Application of Elicitation by Design,2018,55,"Bayesian methods incorporate model parameter information prior to data collection. Eliciting information from content experts is an option, but has seen little implementation in Bayesian item response theory (IRT) modeling. This study aims to use ethical reasoning content experts to elicit prior information and incorporate this information into Markov Chain Monte Carlo (MCMC) estimation. A six-step elicitation approach is followed, with relevant details at each stage for two IRT items parameters: difficulty and guessing. Results indicate that using content experts is the preferred approach, rather than noninformative priors, for both parameter types. The use of a noninformative prior for small samples provided dramatically different results when compared to results from content expert–elicited priors. The WAMBS (When to worry and how to Avoid the Misuse of Bayesian Statistics) checklist is used to aid in comparisons. © 2018 by the National Council on Measurement in Education",Subjective Priors for Item Response Models: Application of Elicitation by Design,"Bayesian methods incorporate model parameter information prior to data collection. Eliciting information from content experts is an option, but has seen little implementation in Bayesian item response theory (IRT) modeling. This study aims to use ethical reasoning content experts to elicit prior information and incorporate this information into Markov Chain Monte Carlo (MCMC) estimation. A six-step elicitation approach is followed, with relevant details at each stage for two IRT items parameters: difficulty and guessing. Results indicate that using content experts is the preferred approach, rather than noninformative priors, for both parameter types. The use of a noninformative prior for small samples provided dramatically different results when compared to results from content expert–elicited priors. The WAMBS (When to worry and how to Avoid the Misuse of Bayesian Statistics) checklist is used to aid in comparisons. © 2018 by the National Council on Measurement in Education","['bayesian', 'method', 'incorporate', 'parameter', 'information', 'prior', 'data', 'collection', 'Eliciting', 'information', 'content', 'expert', 'option', 'little', 'implementation', 'bayesian', 'item', 'response', 'theory', 'IRT', 'study', 'aim', 'ethical', 'reasoning', 'content', 'expert', 'elicit', 'prior', 'information', 'incorporate', 'information', 'Markov', 'Chain', 'Monte', 'Carlo', 'MCMC', 'estimation', 'sixstep', 'elicitation', 'approach', 'follow', 'relevant', 'detail', 'stage', 'IRT', 'item', 'parameter', 'difficulty', 'guess', 'result', 'indicate', 'content', 'expert', 'preferred', 'approach', 'noninformative', 'prior', 'parameter', 'type', 'noninformative', 'prior', 'small', 'sample', 'provide', 'dramatically', 'different', 'result', 'compare', 'result', 'content', 'expert', '–', 'elicit', 'prior', 'WAMBS', 'worry', 'avoid', 'Misuse', 'Bayesian', 'Statistics', 'checklist', 'aid', 'comparison', '©', '2018', 'National', 'Council']","['subjective', 'Priors', 'Item', 'Response', 'Models', 'Application', 'Elicitation', 'Design']",bayesian method incorporate parameter information prior data collection Eliciting information content expert option little implementation bayesian item response theory IRT study aim ethical reasoning content expert elicit prior information incorporate information Markov Chain Monte Carlo MCMC estimation sixstep elicitation approach follow relevant detail stage IRT item parameter difficulty guess result indicate content expert preferred approach noninformative prior parameter type noninformative prior small sample provide dramatically different result compare result content expert – elicit prior WAMBS worry avoid Misuse Bayesian Statistics checklist aid comparison © 2018 National Council,subjective Priors Item Response Models Application Elicitation Design,0.02860521571299792,0.8868606389557837,0.02819971018772323,0.02796947995027575,0.028364955193219517,0.04566726934271514,0.0,0.007307418597711805,0.0024423028774045548,0.0
Man K.; Harring J.R.; Sinharay S.,Use of Data Mining Methods to Detect Test Fraud,2019,56,"Data mining methods have drawn considerable attention across diverse scientific fields. However, few applications could be found in the areas of psychological and educational measurement, and particularly pertinent to this article, in test security research. In this study, various data mining methods for detecting cheating behaviors on large-scale assessments are explored as an alternative to the traditional methods including person-fit statistics and similarity analysis. A common data set from the Handbook of Quantitative Methods for Detecting Cheating on Tests (Cizek & Wollack) was used for comparing the performance of the different methods. The results indicated that the use of data mining methods may combine multiple sources of information about test takers' performance, which may lead to higher detection rate over traditional item response and response time methods. Several recommendations, all based on our findings, are provided to practitioners. © 2019 by the National Council on Measurement in Education",,"Data mining methods have drawn considerable attention across diverse scientific fields. However, few applications could be found in the areas of psychological and educational measurement, and particularly pertinent to this article, in test security research. In this study, various data mining methods for detecting cheating behaviors on large-scale assessments are explored as an alternative to the traditional methods including person-fit statistics and similarity analysis. A common data set from the Handbook of Quantitative Methods for Detecting Cheating on Tests (Cizek & Wollack) was used for comparing the performance of the different methods. The results indicated that the use of data mining methods may combine multiple sources of information about test takers' performance, which may lead to higher detection rate over traditional item response and response time methods. Several recommendations, all based on our findings, are provided to practitioners. © 2019 by the National Council on Measurement in Education","['datum', 'mining', 'method', 'draw', 'considerable', 'attention', 'diverse', 'scientific', 'field', 'application', 'find', 'area', 'psychological', 'educational', 'particularly', 'pertinent', 'article', 'test', 'security', 'research', 'study', 'datum', 'mining', 'method', 'detect', 'cheat', 'behavior', 'largescale', 'assessment', 'explore', 'alternative', 'traditional', 'method', 'include', 'personfit', 'statistic', 'similarity', 'analysis', 'common', 'datum', 'set', 'Handbook', 'Quantitative', 'Methods', 'detect', 'cheating', 'Tests', 'Cizek', 'Wollack', 'compare', 'performance', 'different', 'method', 'result', 'indicate', 'datum', 'mining', 'method', 'combine', 'multiple', 'source', 'information', 'test', 'taker', 'performance', 'lead', 'high', 'detection', 'rate', 'traditional', 'item', 'response', 'response', 'time', 'method', 'recommendation', 'base', 'finding', 'provide', 'practitioner', '©', '2019', 'National', 'Council']",,datum mining method draw considerable attention diverse scientific field application find area psychological educational particularly pertinent article test security research study datum mining method detect cheat behavior largescale assessment explore alternative traditional method include personfit statistic similarity analysis common datum set Handbook Quantitative Methods detect cheating Tests Cizek Wollack compare performance different method result indicate datum mining method combine multiple source information test taker performance lead high detection rate traditional item response response time method recommendation base finding provide practitioner © 2019 National Council,,0.026419206289839785,0.02631415757589401,0.02582877751217503,0.026114448728059757,0.8953234098940315,0.045447388252013284,0.003886643267027654,0.05164202566923219,0.017165450293882323,0.0
Skaggs G.; Hein S.F.; Wilkins J.L.M.,Diagnostic Profiles: A Standard Setting Method for Use With a Cognitive Diagnostic Model,2016,53,"This article introduces the Diagnostic Profiles (DP) standard setting method for setting a performance standard on a test developed from a cognitive diagnostic model (CDM), the outcome of which is a profile of mastered and not-mastered skills or attributes rather than a single test score. In the DP method, the key judgment task for panelists is a decision on whether or not individual cognitive skill profiles meet the performance standard. A randomized experiment was carried out in which secondary mathematics teachers were randomly assigned to either the DP method or the modified Angoff method. The standard setting methods were applied to a test of student readiness to enter high school algebra (Algebra I). While the DP profile judgments were perceived to be more difficult than the Angoff item judgments, there was a high degree of agreement among the panelists for most of the profiles. In order to compare the methods, cut scores were generated from the DP method. The results of the DP group were comparable to the Angoff group, with less cut score variability in the DP group. The DP method shows promise for testing situations in which diagnostic information is needed about examinees and where that information needs to be linked to a performance standard. Copyright © 2016 by the National Council on Measurement in Education",Diagnostic Profiles: A Standard Setting Method for Use With a Cognitive Diagnostic Model,"This article introduces the Diagnostic Profiles (DP) standard setting method for setting a performance standard on a test developed from a cognitive diagnostic model (CDM), the outcome of which is a profile of mastered and not-mastered skills or attributes rather than a single test score. In the DP method, the key judgment task for panelists is a decision on whether or not individual cognitive skill profiles meet the performance standard. A randomized experiment was carried out in which secondary mathematics teachers were randomly assigned to either the DP method or the modified Angoff method. The standard setting methods were applied to a test of student readiness to enter high school algebra (Algebra I). While the DP profile judgments were perceived to be more difficult than the Angoff item judgments, there was a high degree of agreement among the panelists for most of the profiles. In order to compare the methods, cut scores were generated from the DP method. The results of the DP group were comparable to the Angoff group, with less cut score variability in the DP group. The DP method shows promise for testing situations in which diagnostic information is needed about examinees and where that information needs to be linked to a performance standard. Copyright © 2016 by the National Council on Measurement in Education","['article', 'introduce', 'Diagnostic', 'Profiles', 'dp', 'standard', 'set', 'method', 'set', 'performance', 'standard', 'test', 'develop', 'cognitive', 'diagnostic', 'CDM', 'outcome', 'profile', 'mastered', 'notmastered', 'skill', 'attribute', 'single', 'test', 'score', 'dp', 'method', 'key', 'judgment', 'task', 'panelist', 'decision', 'individual', 'cognitive', 'skill', 'profile', 'meet', 'performance', 'standard', 'randomized', 'experiment', 'carry', 'secondary', 'mathematic', 'teacher', 'randomly', 'assign', 'dp', 'method', 'modify', 'Angoff', 'method', 'standard', 'setting', 'method', 'apply', 'test', 'student', 'readiness', 'enter', 'high', 'school', 'algebra', 'Algebra', 'I', 'dp', 'profile', 'judgment', 'perceive', 'difficult', 'Angoff', 'item', 'judgment', 'high', 'degree', 'agreement', 'panelist', 'profile', 'order', 'compare', 'method', 'cut', 'score', 'generate', 'dp', 'method', 'result', 'dp', 'group', 'comparable', 'Angoff', 'group', 'cut', 'score', 'variability', 'dp', 'group', 'dp', 'method', 'promise', 'testing', 'situation', 'diagnostic', 'information', 'need', 'examinee', 'information', 'need', 'link', 'performance', 'standard', 'copyright', '©', '2016', 'National', 'Council']","['Diagnostic', 'Profiles', 'Standard', 'Setting', 'Method', 'Use', 'Cognitive', 'Diagnostic']",article introduce Diagnostic Profiles dp standard set method set performance standard test develop cognitive diagnostic CDM outcome profile mastered notmastered skill attribute single test score dp method key judgment task panelist decision individual cognitive skill profile meet performance standard randomized experiment carry secondary mathematic teacher randomly assign dp method modify Angoff method standard setting method apply test student readiness enter high school algebra Algebra I dp profile judgment perceive difficult Angoff item judgment high degree agreement panelist profile order compare method cut score generate dp method result dp group comparable Angoff group cut score variability dp group dp method promise testing situation diagnostic information need examinee information need link performance standard copyright © 2016 National Council,Diagnostic Profiles Standard Setting Method Use Cognitive Diagnostic,0.030065443006437997,0.8795956281401779,0.02954818877758993,0.030202141591474944,0.03058859848431938,0.014129080673663204,0.02045141663679245,0.05599398991434516,0.01620724003837205,0.0
Feuerstahler L.; Wilson M.,Scale Alignment in Between-Item Multidimensional Rasch Models,2019,56,"Scores estimated from multidimensional item response theory (IRT) models are not necessarily comparable across dimensions. In this article, the concept of aligned dimensions is formalized in the context of Rasch models, and two methods are described—delta dimensional alignment (DDA) and logistic regression alignment (LRA)—to transform estimated item parameters so that dimensions are aligned. Both the DDA and LRA methods are applied to real and simulated data, and it is demonstrated that both methods are broadly effective for achieving aligned scales. The routine use of scale alignment methods is recommended prior to comparing scores across dimensions. © 2019 by the National Council on Measurement in Education",Scale Alignment in Between-Item Multidimensional Rasch Models,"Scores estimated from multidimensional item response theory (IRT) models are not necessarily comparable across dimensions. In this article, the concept of aligned dimensions is formalized in the context of Rasch models, and two methods are described—delta dimensional alignment (DDA) and logistic regression alignment (LRA)—to transform estimated item parameters so that dimensions are aligned. Both the DDA and LRA methods are applied to real and simulated data, and it is demonstrated that both methods are broadly effective for achieving aligned scales. The routine use of scale alignment methods is recommended prior to comparing scores across dimensions. © 2019 by the National Council on Measurement in Education","['score', 'estimate', 'multidimensional', 'item', 'response', 'theory', 'IRT', 'necessarily', 'comparable', 'dimension', 'article', 'concept', 'align', 'dimension', 'formalize', 'context', 'Rasch', 'method', 'describe', '—', 'delta', 'dimensional', 'alignment', 'DDA', 'logistic', 'regression', 'alignment', 'LRA', '—', 'transform', 'estimate', 'item', 'parameter', 'dimension', 'align', 'DDA', 'LRA', 'method', 'apply', 'real', 'simulated', 'datum', 'demonstrate', 'method', 'broadly', 'effective', 'achieve', 'align', 'scale', 'routine', 'scale', 'alignment', 'method', 'recommend', 'prior', 'compare', 'score', 'dimension', '©', '2019', 'National', 'Council']","['Scale', 'Alignment', 'BetweenItem', 'Multidimensional', 'Rasch', 'Models']",score estimate multidimensional item response theory IRT necessarily comparable dimension article concept align dimension formalize context Rasch method describe — delta dimensional alignment DDA logistic regression alignment LRA — transform estimate item parameter dimension align DDA LRA method apply real simulated datum demonstrate method broadly effective achieve align scale routine scale alignment method recommend prior compare score dimension © 2019 National Council,Scale Alignment BetweenItem Multidimensional Rasch Models,0.03220510091512809,0.8709718451676113,0.032152354005846935,0.03241123355762653,0.03225946635378731,0.03572271648931442,0.0016413293315938216,0.016092852087597195,0.011241457907790822,0.0028776278883110335
Leighton J.P.,Students’ Interpretation of Formative Assessment Feedback: Three Claims for Why We Know So Little About Something So Important,2019,56,"If K-12 students are to be fully integrated as active participants in their own learning, understanding how they interpret formative assessment feedback is needed. The objective of this article is to advance three claims about why teachers and assessment scholars/specialists may have little understanding of students’ interpretation of formative assessment feedback. The three claims are as follows. First, there is little systematic research of K-12 students’ interpretations of feedback. Systematic research requires gathering substantive evidence of students’ cognitive and emotional processes using psychological methods and tools. Second, there is an overemphasis on the external assessment process at the expense of uncovering learners’ internal reasoning and emotional processes. This overemphasis may be due to vestiges of behavioral approaches and lack of training in social cognitive methods. Third, there are psychological tools such as the clinical interview, pioneered by Piaget and used by psychologists to “enter the child's mind,” which may be helpful in uncovering students’ interpretation of feedback and associated behavioral responses. If the purpose of formative assessment is to change student learning, and feedback is delivered as a conduit to help with this long-term change, understanding students’ interpretation of feedback plays a central role in the validity of the process. © 2019 by the National Council on Measurement in Education",Students’ Interpretation of Formative Assessment Feedback: Three Claims for Why We Know So Little About Something So Important,"If K-12 students are to be fully integrated as active participants in their own learning, understanding how they interpret formative assessment feedback is needed. The objective of this article is to advance three claims about why teachers and assessment scholars/specialists may have little understanding of students’ interpretation of formative assessment feedback. The three claims are as follows. First, there is little systematic research of K-12 students’ interpretations of feedback. Systematic research requires gathering substantive evidence of students’ cognitive and emotional processes using psychological methods and tools. Second, there is an overemphasis on the external assessment process at the expense of uncovering learners’ internal reasoning and emotional processes. This overemphasis may be due to vestiges of behavioral approaches and lack of training in social cognitive methods. Third, there are psychological tools such as the clinical interview, pioneered by Piaget and used by psychologists to “enter the child's mind,” which may be helpful in uncovering students’ interpretation of feedback and associated behavioral responses. If the purpose of formative assessment is to change student learning, and feedback is delivered as a conduit to help with this long-term change, understanding students’ interpretation of feedback plays a central role in the validity of the process. © 2019 by the National Council on Measurement in Education","['K12', 'student', 'fully', 'integrate', 'active', 'participant', 'learning', 'understand', 'interpret', 'formative', 'assessment', 'feedback', 'need', 'objective', 'article', 'advance', 'claim', 'teacher', 'assessment', 'scholarsspecialist', 'little', 'understanding', 'student', '’', 'interpretation', 'formative', 'assessment', 'feedback', 'claim', 'follow', 'First', 'little', 'systematic', 'research', 'K12', 'student', '’', 'interpretation', 'feedback', 'systematic', 'research', 'require', 'gather', 'substantive', 'evidence', 'student', '’', 'cognitive', 'emotional', 'process', 'psychological', 'method', 'tool', 'Second', 'overemphasis', 'external', 'assessment', 'process', 'expense', 'uncover', 'learner', '’', 'internal', 'reasoning', 'emotional', 'process', 'overemphasis', 'vestige', 'behavioral', 'approach', 'lack', 'training', 'social', 'cognitive', 'method', 'Third', 'psychological', 'tool', 'clinical', 'interview', 'pioneer', 'Piaget', 'psychologist', '""', 'enter', 'child', 'mind', '""', 'helpful', 'uncover', 'student', '’', 'interpretation', 'feedback', 'associate', 'behavioral', 'response', 'purpose', 'formative', 'assessment', 'change', 'student', 'learning', 'feedback', 'deliver', 'conduit', 'help', 'longterm', 'change', 'understand', 'student', '’', 'interpretation', 'feedback', 'play', 'central', 'role', 'validity', 'process', '©', '2019', 'National', 'Council']","['student', '’', 'Interpretation', 'Formative', 'Assessment', 'Feedback', 'Three', 'Claims', 'know', 'little', 'Something', 'important']","K12 student fully integrate active participant learning understand interpret formative assessment feedback need objective article advance claim teacher assessment scholarsspecialist little understanding student ’ interpretation formative assessment feedback claim follow First little systematic research K12 student ’ interpretation feedback systematic research require gather substantive evidence student ’ cognitive emotional process psychological method tool Second overemphasis external assessment process expense uncover learner ’ internal reasoning emotional process overemphasis vestige behavioral approach lack training social cognitive method Third psychological tool clinical interview pioneer Piaget psychologist "" enter child mind "" helpful uncover student ’ interpretation feedback associate behavioral response purpose formative assessment change student learning feedback deliver conduit help longterm change understand student ’ interpretation feedback play central role validity process © 2019 National Council",student ’ Interpretation Formative Assessment Feedback Three Claims know little Something important,0.02517630486234621,0.025366690288656752,0.025222467265472795,0.025548075908620328,0.898686461674904,0.0,0.0,0.13044771354109566,0.0,0.0
Li J.; van der Linden W.J.,A Comparison of Constraint Programming and Mixed-Integer Programming for Automated Test-Form Generation,2018,55,"The final step of the typical process of developing educational and psychological tests is to place the selected test items in a formatted form. The step involves the grouping and ordering of the items to meet a variety of formatting constraints. As this activity tends to be time-intensive, the use of mixed-integer programming (MIP) has been proposed to automate it. The goal of this article is to show how constraint programming (CP) can be used as an alternative to automate test-form generation problems with a large variety of formatting constraints, and how it compares with MIP-based form generation as for its models, solutions, and running times. Two empirical examples are presented: (i) automated generation of a computerized fixed-form; and (ii) automated generation of shadow tests for multistage testing. Both examples show that CP works well with feasible solutions and running times likely to be better than that for MIP-based applications. © 2018 by the National Council on Measurement in Education",A Comparison of Constraint Programming and Mixed-Integer Programming for Automated Test-Form Generation,"The final step of the typical process of developing educational and psychological tests is to place the selected test items in a formatted form. The step involves the grouping and ordering of the items to meet a variety of formatting constraints. As this activity tends to be time-intensive, the use of mixed-integer programming (MIP) has been proposed to automate it. The goal of this article is to show how constraint programming (CP) can be used as an alternative to automate test-form generation problems with a large variety of formatting constraints, and how it compares with MIP-based form generation as for its models, solutions, and running times. Two empirical examples are presented: (i) automated generation of a computerized fixed-form; and (ii) automated generation of shadow tests for multistage testing. Both examples show that CP works well with feasible solutions and running times likely to be better than that for MIP-based applications. © 2018 by the National Council on Measurement in Education","['final', 'step', 'typical', 'process', 'develop', 'educational', 'psychological', 'test', 'place', 'select', 'test', 'item', 'formatted', 'form', 'step', 'involve', 'grouping', 'ordering', 'item', 'meet', 'variety', 'format', 'constraint', 'activity', 'tend', 'timeintensive', 'mixedinteger', 'programming', 'MIP', 'propose', 'automate', 'goal', 'article', 'constraint', 'programming', 'CP', 'alternative', 'automate', 'testform', 'generation', 'problem', 'large', 'variety', 'format', 'constraint', 'compare', 'mipbase', 'form', 'generation', 'solution', 'run', 'time', 'empirical', 'example', 'present', 'I', 'automate', 'generation', 'computerized', 'fixedform', 'ii', 'automate', 'generation', 'shadow', 'test', 'multistage', 'testing', 'example', 'CP', 'work', 'feasible', 'solution', 'running', 'time', 'likely', 'mipbase', 'application', '©', '2018', 'National', 'Council']","['Comparison', 'Constraint', 'Programming', 'MixedInteger', 'Programming', 'Automated', 'TestForm', 'generation']",final step typical process develop educational psychological test place select test item formatted form step involve grouping ordering item meet variety format constraint activity tend timeintensive mixedinteger programming MIP propose automate goal article constraint programming CP alternative automate testform generation problem large variety format constraint compare mipbase form generation solution run time empirical example present I automate generation computerized fixedform ii automate generation shadow test multistage testing example CP work feasible solution running time likely mipbase application © 2018 National Council,Comparison Constraint Programming MixedInteger Programming Automated TestForm generation,0.028482383163510292,0.02861670938769654,0.028345066724256494,0.028397196743556757,0.8861586439809799,0.028116341006496036,0.0014627490968605267,0.026259608241457556,0.0002178471557655201,0.0
Ju U.; Falk C.F.,Modeling Response Styles in Cross-Country Self-Reports: An Application of a Multilevel Multidimensional Nominal Response Model,2019,56,"We examined the feasibility and results of a multilevel multidimensional nominal response model (ML-MNRM) for measuring both substantive constructs and extreme response style (ERS) across countries. The ML-MNRM considers within-country clustering while allowing overall item slopes to vary across items and examination of whether certain items were more prone to ERS. We applied this model to survey items from TALIS 2013. Results indicated that self-efficacy items were more likely to trigger ERS compared to need for professional development, and the between-country relationships among constructs can change due to ERS. Simulations assessed the estimation approach and found adequate recovery of model parameters and factor scores. We stress the importance of additional validity studies to improve the cross-cultural comparability of substantive constructs. © 2019 by the National Council on Measurement in Education",Modeling Response Styles in Cross-Country Self-Reports: An Application of a Multilevel Multidimensional Nominal Response Model,"We examined the feasibility and results of a multilevel multidimensional nominal response model (ML-MNRM) for measuring both substantive constructs and extreme response style (ERS) across countries. The ML-MNRM considers within-country clustering while allowing overall item slopes to vary across items and examination of whether certain items were more prone to ERS. We applied this model to survey items from TALIS 2013. Results indicated that self-efficacy items were more likely to trigger ERS compared to need for professional development, and the between-country relationships among constructs can change due to ERS. Simulations assessed the estimation approach and found adequate recovery of model parameters and factor scores. We stress the importance of additional validity studies to improve the cross-cultural comparability of substantive constructs. © 2019 by the National Council on Measurement in Education","['examine', 'feasibility', 'result', 'multilevel', 'multidimensional', 'nominal', 'response', 'MLMNRM', 'measure', 'substantive', 'construct', 'extreme', 'response', 'style', 'ERS', 'country', 'MLMNRM', 'consider', 'withincountry', 'clustering', 'allow', 'overall', 'item', 'slope', 'vary', 'item', 'examination', 'certain', 'item', 'prone', 'ERS', 'apply', 'survey', 'item', 'talis', '2013', 'result', 'indicate', 'selfefficacy', 'item', 'likely', 'trigger', 'er', 'compare', 'need', 'professional', 'development', 'betweencountry', 'relationship', 'construct', 'change', 'ers', 'simulation', 'assess', 'estimation', 'approach', 'find', 'adequate', 'recovery', 'parameter', 'factor', 'score', 'stress', 'importance', 'additional', 'validity', 'study', 'improve', 'crosscultural', 'comparability', 'substantive', 'construct', '©', '2019', 'National', 'Council']","['Response', 'Styles', 'CrossCountry', 'SelfReports', 'application', 'Multilevel', 'Multidimensional', 'Nominal', 'Response']",examine feasibility result multilevel multidimensional nominal response MLMNRM measure substantive construct extreme response style ERS country MLMNRM consider withincountry clustering allow overall item slope vary item examination certain item prone ERS apply survey item talis 2013 result indicate selfefficacy item likely trigger er compare need professional development betweencountry relationship construct change ers simulation assess estimation approach find adequate recovery parameter factor score stress importance additional validity study improve crosscultural comparability substantive construct © 2019 National Council,Response Styles CrossCountry SelfReports application Multilevel Multidimensional Nominal Response,0.027285545120305628,0.02707620122697327,0.027046406380576822,0.02701850665791347,0.8915733406142309,0.044959672101546465,0.0,0.019156712648201877,0.0,0.0048327318609503655
Köhler C.; Pohl S.; Carstensen C.H.,Dealing With Item Nonresponse in Large-Scale Cognitive Assessments: The Impact of Missing Data Methods on Estimated Explanatory Relationships,2017,54,"Competence data from low-stakes educational large-scale assessment studies allow for evaluating relationships between competencies and other variables. The impact of item-level nonresponse has not been investigated with regard to statistics that determine the size of these relationships (e.g., correlations, regression coefficients). Classical approaches such as ignoring missing values or treating them as incorrect are currently applied in many large-scale studies, while recent model-based approaches that can account for nonignorable nonresponse have been developed. Estimates of item and person parameters have been demonstrated to be biased for classical approaches when missing data are missing not at random (MNAR). In our study, we focus on parameter estimates of the structural model (i.e., the true regression coefficient when regressing competence on an explanatory variable), simulating data according to various missing data mechanisms. We found that model-based approaches and ignoring missing values performed well in retrieving regression coefficients even when we induced missing data that were MNAR. Treating missing values as incorrect responses can lead to substantial bias. We demonstrate the validity of our approach empirically and discuss the relevance of our results. Copyright © 2017 by the National Council on Measurement in Education",Dealing With Item Nonresponse in Large-Scale Cognitive Assessments: The Impact of Missing Data Methods on Estimated Explanatory Relationships,"Competence data from low-stakes educational large-scale assessment studies allow for evaluating relationships between competencies and other variables. The impact of item-level nonresponse has not been investigated with regard to statistics that determine the size of these relationships (e.g., correlations, regression coefficients). Classical approaches such as ignoring missing values or treating them as incorrect are currently applied in many large-scale studies, while recent model-based approaches that can account for nonignorable nonresponse have been developed. Estimates of item and person parameters have been demonstrated to be biased for classical approaches when missing data are missing not at random (MNAR). In our study, we focus on parameter estimates of the structural model (i.e., the true regression coefficient when regressing competence on an explanatory variable), simulating data according to various missing data mechanisms. We found that model-based approaches and ignoring missing values performed well in retrieving regression coefficients even when we induced missing data that were MNAR. Treating missing values as incorrect responses can lead to substantial bias. We demonstrate the validity of our approach empirically and discuss the relevance of our results. Copyright © 2017 by the National Council on Measurement in Education","['competence', 'datum', 'lowstake', 'educational', 'largescale', 'assessment', 'study', 'allow', 'evaluate', 'relationship', 'competency', 'variable', 'impact', 'itemlevel', 'nonresponse', 'investigate', 'regard', 'statistic', 'determine', 'size', 'relationship', 'eg', 'correlation', 'regression', 'coefficient', 'classical', 'approach', 'ignore', 'miss', 'value', 'treat', 'incorrect', 'currently', 'apply', 'largescale', 'study', 'recent', 'modelbase', 'approach', 'account', 'nonignorable', 'nonresponse', 'develop', 'estimate', 'item', 'person', 'parameter', 'demonstrate', 'bias', 'classical', 'approach', 'miss', 'datum', 'miss', 'random', 'MNAR', 'study', 'focus', 'parameter', 'estimate', 'structural', 'ie', 'true', 'regression', 'coefficient', 'regress', 'competence', 'explanatory', 'variable', 'simulating', 'datum', 'accord', 'miss', 'data', 'mechanism', 'find', 'modelbase', 'approach', 'ignore', 'miss', 'value', 'perform', 'retrieve', 'regression', 'coefficient', 'induce', 'missing', 'datum', 'MNAR', 'treat', 'missing', 'value', 'incorrect', 'response', 'lead', 'substantial', 'bias', 'demonstrate', 'validity', 'approach', 'empirically', 'discuss', 'relevance', 'result', 'Copyright', '©', '2017', 'National', 'Council']","['deal', 'Item', 'Nonresponse', 'LargeScale', 'Cognitive', 'assessment', 'Impact', 'Missing', 'Data', 'Methods', 'Estimated', 'Explanatory', 'Relationships']",competence datum lowstake educational largescale assessment study allow evaluate relationship competency variable impact itemlevel nonresponse investigate regard statistic determine size relationship eg correlation regression coefficient classical approach ignore miss value treat incorrect currently apply largescale study recent modelbase approach account nonignorable nonresponse develop estimate item person parameter demonstrate bias classical approach miss datum miss random MNAR study focus parameter estimate structural ie true regression coefficient regress competence explanatory variable simulating datum accord miss data mechanism find modelbase approach ignore miss value perform retrieve regression coefficient induce missing datum MNAR treat missing value incorrect response lead substantial bias demonstrate validity approach empirically discuss relevance result Copyright © 2017 National Council,deal Item Nonresponse LargeScale Cognitive assessment Impact Missing Data Methods Estimated Explanatory Relationships,0.026585370685311974,0.026269584246352153,0.02652730069672575,0.8939527426010503,0.026665001770559752,0.03931890726176551,0.018369591918559136,0.03498904461363254,0.0003270667840674576,0.001127930605810685
Sinharay S.,A New Interpretation of Augmented Subscores and Their Added Value in Terms of Parallel Forms,2018,55,The value-added method of Haberman is arguably one of the most popular methods to evaluate the quality of subscores. The method is based on the classical test theory and deems a subscore to be of added value if the subscore predicts the corresponding true subscore better than does the total score. Sinharay provided an interpretation of the added value of subscores in terms of scores and subscores on parallel forms. This article extends the results of Sinharay and considers the prediction of a subscore on a parallel form from both the subscore and the total raw score on the original form. The resulting predictor essentially becomes the augmented subscore suggested by Haberman. The proportional reduction in mean squared error of the resulting predictor is interpreted as a squared multiple correlation coefficient. The practical usefulness of the derived results is demonstrated using an operational data set. Copyright © 2018 by the National Council on Measurement in Education,A New Interpretation of Augmented Subscores and Their Added Value in Terms of Parallel Forms,The value-added method of Haberman is arguably one of the most popular methods to evaluate the quality of subscores. The method is based on the classical test theory and deems a subscore to be of added value if the subscore predicts the corresponding true subscore better than does the total score. Sinharay provided an interpretation of the added value of subscores in terms of scores and subscores on parallel forms. This article extends the results of Sinharay and considers the prediction of a subscore on a parallel form from both the subscore and the total raw score on the original form. The resulting predictor essentially becomes the augmented subscore suggested by Haberman. The proportional reduction in mean squared error of the resulting predictor is interpreted as a squared multiple correlation coefficient. The practical usefulness of the derived results is demonstrated using an operational data set. Copyright © 2018 by the National Council on Measurement in Education,"['valueadde', 'method', 'Haberman', 'arguably', 'popular', 'method', 'evaluate', 'quality', 'subscore', 'method', 'base', 'classical', 'test', 'theory', 'deem', 'subscore', 'add', 'value', 'subscore', 'predict', 'correspond', 'true', 'subscore', 'total', 'score', 'sinharay', 'provide', 'interpretation', 'add', 'value', 'subscore', 'term', 'score', 'subscore', 'parallel', 'form', 'article', 'extend', 'result', 'sinharay', 'consider', 'prediction', 'subscore', 'parallel', 'form', 'subscore', 'total', 'raw', 'score', 'original', 'form', 'result', 'predictor', 'essentially', 'augmented', 'subscore', 'suggest', 'Haberman', 'proportional', 'reduction', 'mean', 'squared', 'error', 'result', 'predictor', 'interpret', 'squared', 'multiple', 'correlation', 'coefficient', 'practical', 'usefulness', 'derive', 'result', 'demonstrate', 'operational', 'datum', 'set', 'Copyright', '©', '2018', 'National', 'Council']","['New', 'Interpretation', 'Augmented', 'Subscores', 'Added', 'Value', 'term', 'parallel', 'form']",valueadde method Haberman arguably popular method evaluate quality subscore method base classical test theory deem subscore add value subscore predict correspond true subscore total score sinharay provide interpretation add value subscore term score subscore parallel form article extend result sinharay consider prediction subscore parallel form subscore total raw score original form result predictor essentially augmented subscore suggest Haberman proportional reduction mean squared error result predictor interpret squared multiple correlation coefficient practical usefulness derive result demonstrate operational datum set Copyright © 2018 National Council,New Interpretation Augmented Subscores Added Value term parallel form,0.03236111191938504,0.032603771942760956,0.8699795735281378,0.032378612478040754,0.03267693013167534,0.0,0.35345353708952654,0.0,0.0,0.0
Fox J.-P.; Marianti S.,Person-Fit Statistics for Joint Models for Accuracy and Speed,2017,54,"Response accuracy and response time data can be analyzed with a joint model to measure ability and speed of working, while accounting for relationships between item and person characteristics. In this study, person-fit statistics are proposed for joint models to detect aberrant response accuracy and/or response time patterns. The person-fit tests take the correlation between ability and speed into account, as well as the correlation between item characteristics. They are posited as Bayesian significance tests, which have the advantage that the extremeness of a test statistic value is quantified by a posterior probability. The person-fit tests can be computed as by-products of a Markov chain Monte Carlo algorithm. Simulation studies were conducted in order to evaluate their performance. For all person-fit tests, the simulation studies showed good detection rates in identifying aberrant patterns. A real data example is given to illustrate the person-fit statistics for the evaluation of the joint model. Copyright © 2017 by the National Council on Measurement in Education",Person-Fit Statistics for Joint Models for Accuracy and Speed,"Response accuracy and response time data can be analyzed with a joint model to measure ability and speed of working, while accounting for relationships between item and person characteristics. In this study, person-fit statistics are proposed for joint models to detect aberrant response accuracy and/or response time patterns. The person-fit tests take the correlation between ability and speed into account, as well as the correlation between item characteristics. They are posited as Bayesian significance tests, which have the advantage that the extremeness of a test statistic value is quantified by a posterior probability. The person-fit tests can be computed as by-products of a Markov chain Monte Carlo algorithm. Simulation studies were conducted in order to evaluate their performance. For all person-fit tests, the simulation studies showed good detection rates in identifying aberrant patterns. A real data example is given to illustrate the person-fit statistics for the evaluation of the joint model. Copyright © 2017 by the National Council on Measurement in Education","['response', 'accuracy', 'response', 'time', 'datum', 'analyze', 'joint', 'measure', 'ability', 'speed', 'working', 'account', 'relationship', 'item', 'person', 'characteristic', 'study', 'personfit', 'statistic', 'propose', 'joint', 'detect', 'aberrant', 'response', 'accuracy', 'andor', 'response', 'time', 'pattern', 'personfit', 'test', 'correlation', 'ability', 'speed', 'account', 'correlation', 'item', 'characteristic', 'posit', 'bayesian', 'significance', 'test', 'advantage', 'extremeness', 'test', 'statistic', 'value', 'quantify', 'posterior', 'probability', 'personfit', 'test', 'compute', 'byproduct', 'Markov', 'chain', 'Monte', 'Carlo', 'algorithm', 'Simulation', 'study', 'conduct', 'order', 'evaluate', 'performance', 'personfit', 'test', 'simulation', 'study', 'good', 'detection', 'rate', 'identify', 'aberrant', 'pattern', 'real', 'datum', 'example', 'illustrate', 'personfit', 'statistic', 'evaluation', 'joint', 'Copyright', '©', '2017', 'National', 'Council']","['PersonFit', 'Statistics', 'Joint', 'Models', 'Accuracy', 'speed']",response accuracy response time datum analyze joint measure ability speed working account relationship item person characteristic study personfit statistic propose joint detect aberrant response accuracy andor response time pattern personfit test correlation ability speed account correlation item characteristic posit bayesian significance test advantage extremeness test statistic value quantify posterior probability personfit test compute byproduct Markov chain Monte Carlo algorithm Simulation study conduct order evaluate performance personfit test simulation study good detection rate identify aberrant pattern real datum example illustrate personfit statistic evaluation joint Copyright © 2017 National Council,PersonFit Statistics Joint Models Accuracy speed,0.8893296477099477,0.02783316990225793,0.02746072122081167,0.027601211032112143,0.02777525013487058,0.06680779882036064,0.007486953370015835,0.010918026044564145,0.0,0.0
Briggs D.C.; Chattergoon R.; Burkhardt A.,Examining the Dual Purpose Use of Student Learning Objectives for Classroom Assessment and Teacher Evaluation,2019,56,"The process of setting and evaluating student learning objectives (SLOs) has become increasingly popular as an example where classroom assessment is intended to fulfill the dual purpose use of informing instruction and holding teachers accountable. A concern is that the high-stakes purpose may lead to distortions in the inferences about students and teachers that SLOs can support. This concern is explored in the present study by contrasting student SLO scores in a large urban school district to performance on a common objective external criterion. This external criterion is used to evaluate the extent to which student growth scores appear to be inflated. Using 2 years of data, growth comparisons are also made at the teacher level for teachers who submit SLOs and have students that take the state-administered large-scale assessment. Although they do show similar relationships with demographic covariates and have the same degree of stability across years, the two different measures of growth are weakly correlated. © 2019 by the National Council on Measurement in Education",Examining the Dual Purpose Use of Student Learning Objectives for Classroom Assessment and Teacher Evaluation,"The process of setting and evaluating student learning objectives (SLOs) has become increasingly popular as an example where classroom assessment is intended to fulfill the dual purpose use of informing instruction and holding teachers accountable. A concern is that the high-stakes purpose may lead to distortions in the inferences about students and teachers that SLOs can support. This concern is explored in the present study by contrasting student SLO scores in a large urban school district to performance on a common objective external criterion. This external criterion is used to evaluate the extent to which student growth scores appear to be inflated. Using 2 years of data, growth comparisons are also made at the teacher level for teachers who submit SLOs and have students that take the state-administered large-scale assessment. Although they do show similar relationships with demographic covariates and have the same degree of stability across years, the two different measures of growth are weakly correlated. © 2019 by the National Council on Measurement in Education","['process', 'set', 'evaluate', 'student', 'learning', 'objective', 'slo', 'increasingly', 'popular', 'example', 'classroom', 'assessment', 'intend', 'fulfill', 'dual', 'purpose', 'inform', 'instruction', 'hold', 'teacher', 'accountable', 'concern', 'highstake', 'purpose', 'lead', 'distortion', 'inference', 'student', 'teacher', 'slo', 'support', 'concern', 'explore', 'present', 'study', 'contrast', 'student', 'SLO', 'score', 'large', 'urban', 'school', 'district', 'performance', 'common', 'objective', 'external', 'criterion', 'external', 'criterion', 'evaluate', 'extent', 'student', 'growth', 'score', 'appear', 'inflate', '2', 'year', 'data', 'growth', 'comparison', 'teacher', 'level', 'teacher', 'submit', 'slo', 'student', 'stateadministered', 'largescale', 'assessment', 'similar', 'relationship', 'demographic', 'covariate', 'degree', 'stability', 'year', 'different', 'measure', 'growth', 'weakly', 'correlate', '©', '2019', 'National', 'Council']","['examine', 'Dual', 'Purpose', 'Use', 'Student', 'Learning', 'Objectives', 'Classroom', 'Assessment', 'Teacher', 'Evaluation']",process set evaluate student learning objective slo increasingly popular example classroom assessment intend fulfill dual purpose inform instruction hold teacher accountable concern highstake purpose lead distortion inference student teacher slo support concern explore present study contrast student SLO score large urban school district performance common objective external criterion external criterion evaluate extent student growth score appear inflate 2 year data growth comparison teacher level teacher submit slo student stateadministered largescale assessment similar relationship demographic covariate degree stability year different measure growth weakly correlate © 2019 National Council,examine Dual Purpose Use Student Learning Objectives Classroom Assessment Teacher Evaluation,0.026912159170947303,0.02661809695356079,0.02675223865926859,0.8930070883414633,0.02671041687476008,0.0,0.0,0.11539476982048898,0.0,0.0
Steedle J.T.; Radunzel J.; Mattern K.D.,Comparing Academic Readiness Requirements for Different Postsecondary Pathways: What Admissions Tests Tell Us,2019,56,"Ensuring postsecondary readiness is a goal of K-12 education, but it is unclear whether high school students should get different messages about the required levels of academic preparation depending on their postsecondary trajectories. This study estimated readiness benchmark scores on a college admissions test predictive of earning good grades in majors associated with middle-skills occupations at 2-year postsecondary institutions. Results generally indicated similarity between those scores, the corresponding scores for students preparing for high-skills jobs requiring a bachelor's degree, and established readiness benchmarks for the general college-going population. Subsequent analyses revealed small variation between readiness benchmarks for different college majors. Overall, results suggest that high school graduates need a strong academic foundation regardless of the postsecondary path they choose. © 2019 by the National Council on Measurement in Education",Comparing Academic Readiness Requirements for Different Postsecondary Pathways: What Admissions Tests Tell Us,"Ensuring postsecondary readiness is a goal of K-12 education, but it is unclear whether high school students should get different messages about the required levels of academic preparation depending on their postsecondary trajectories. This study estimated readiness benchmark scores on a college admissions test predictive of earning good grades in majors associated with middle-skills occupations at 2-year postsecondary institutions. Results generally indicated similarity between those scores, the corresponding scores for students preparing for high-skills jobs requiring a bachelor's degree, and established readiness benchmarks for the general college-going population. Subsequent analyses revealed small variation between readiness benchmarks for different college majors. Overall, results suggest that high school graduates need a strong academic foundation regardless of the postsecondary path they choose. © 2019 by the National Council on Measurement in Education","['ensure', 'postsecondary', 'readiness', 'goal', 'K12', 'unclear', 'high', 'school', 'student', 'different', 'message', 'require', 'level', 'academic', 'preparation', 'depend', 'postsecondary', 'trajectory', 'study', 'estimate', 'readiness', 'benchmark', 'score', 'college', 'admission', 'test', 'predictive', 'earn', 'good', 'grade', 'major', 'associate', 'middleskill', 'occupation', '2year', 'postsecondary', 'institution', 'result', 'generally', 'indicate', 'similarity', 'score', 'correspond', 'score', 'student', 'prepare', 'highskill', 'job', 'require', 'bachelor', 'degree', 'establish', 'readiness', 'benchmark', 'general', 'collegegoe', 'population', 'Subsequent', 'analysis', 'reveal', 'small', 'variation', 'readiness', 'benchmark', 'different', 'college', 'major', 'overall', 'result', 'suggest', 'high', 'school', 'graduate', 'need', 'strong', 'academic', 'foundation', 'regardless', 'postsecondary', 'path', 'choose', '©', '2019', 'National', 'Council']","['compare', 'Academic', 'Readiness', 'Requirements', 'Different', 'Postsecondary', 'pathway', 'Admissions', 'test', 'tell']",ensure postsecondary readiness goal K12 unclear high school student different message require level academic preparation depend postsecondary trajectory study estimate readiness benchmark score college admission test predictive earn good grade major associate middleskill occupation 2year postsecondary institution result generally indicate similarity score correspond score student prepare highskill job require bachelor degree establish readiness benchmark general collegegoe population Subsequent analysis reveal small variation readiness benchmark different college major overall result suggest high school graduate need strong academic foundation regardless postsecondary path choose © 2019 National Council,compare Academic Readiness Requirements Different Postsecondary pathway Admissions test tell,0.026651667951484093,0.026725559228250297,0.02678939876869854,0.026676814086095044,0.893156559965472,0.0003076547162373505,0.006435244734758659,0.058577633338651604,0.005392105545828423,0.0
Sinharay S.,How to Compare Parametric and Nonparametric Person-Fit Statistics Using Real Data,2017,54,"Person-fit assessment (PFA) is concerned with uncovering atypical test performance as reflected in the pattern of scores on individual items on a test. Existing person-fit statistics (PFSs) include both parametric and nonparametric statistics. Comparison of PFSs has been a popular research topic in PFA, but almost all comparisons have employed simulated data. This article suggests an approach for comparing the performance of parametric and nonparametric PFSs using real data. This article then shows that there is no clear winner between l*Z, a popular parametric PFS, and HT, a popular nonparametric statistic, in a comparison using the suggested approach. This finding is contradictory to the common finding shown by Karabatsos, Dimitrov and Smith, and Tendeiro and Meijer that HT is more powerful than several parametric PFSs including l*Z and lZ. Copyright © 2017 by the National Council on Measurement in Education",How to Compare Parametric and Nonparametric Person-Fit Statistics Using Real Data,"Person-fit assessment (PFA) is concerned with uncovering atypical test performance as reflected in the pattern of scores on individual items on a test. Existing person-fit statistics (PFSs) include both parametric and nonparametric statistics. Comparison of PFSs has been a popular research topic in PFA, but almost all comparisons have employed simulated data. This article suggests an approach for comparing the performance of parametric and nonparametric PFSs using real data. This article then shows that there is no clear winner between l*Z, a popular parametric PFS, and HT, a popular nonparametric statistic, in a comparison using the suggested approach. This finding is contradictory to the common finding shown by Karabatsos, Dimitrov and Smith, and Tendeiro and Meijer that HT is more powerful than several parametric PFSs including l*Z and lZ. Copyright © 2017 by the National Council on Measurement in Education","['personfit', 'assessment', 'PFA', 'concern', 'uncover', 'atypical', 'test', 'performance', 'reflect', 'pattern', 'score', 'individual', 'item', 'test', 'existing', 'personfit', 'statistic', 'pfs', 'include', 'parametric', 'nonparametric', 'statistic', 'Comparison', 'PFSs', 'popular', 'research', 'topic', 'PFA', 'comparison', 'employ', 'simulated', 'datum', 'article', 'suggest', 'approach', 'compare', 'performance', 'parametric', 'nonparametric', 'pfs', 'real', 'datum', 'article', 'clear', 'winner', 'lz', 'popular', 'parametric', 'PFS', 'HT', 'popular', 'nonparametric', 'statistic', 'comparison', 'suggest', 'approach', 'finding', 'contradictory', 'common', 'finding', 'Karabatsos', 'Dimitrov', 'Smith', 'Tendeiro', 'Meijer', 'HT', 'powerful', 'parametric', 'pfs', 'include', 'lz', 'lZ', 'copyright', '©', '2017', 'National', 'Council']","['compare', 'Parametric', 'Nonparametric', 'PersonFit', 'Statistics', 'Real', 'Data']",personfit assessment PFA concern uncover atypical test performance reflect pattern score individual item test existing personfit statistic pfs include parametric nonparametric statistic Comparison PFSs popular research topic PFA comparison employ simulated datum article suggest approach compare performance parametric nonparametric pfs real datum article clear winner lz popular parametric PFS HT popular nonparametric statistic comparison suggest approach finding contradictory common finding Karabatsos Dimitrov Smith Tendeiro Meijer HT powerful parametric pfs include lz lZ copyright © 2017 National Council,compare Parametric Nonparametric PersonFit Statistics Real Data,0.029591591087348174,0.029623089435941684,0.029220503961911126,0.8821501581059983,0.029414657408800723,0.015773769733011447,0.005131621472891787,0.04965449566637551,0.0,0.0
Wind S.A.,"Nonparametric Evidence of Validity, Reliability, and Fairness for Rater-Mediated Assessments: An Illustration Using Mokken Scale Analysis",2019,56,"Numerous researchers have proposed methods for evaluating the quality of rater-mediated assessments using nonparametric methods (e.g., kappa coefficients) and parametric methods (e.g., the many-facet Rasch model). Generally speaking, popular nonparametric methods for evaluating rating quality are not based on a particular measurement theory. On the other hand, popular parametric methods for evaluating rating quality are often based on measurement theories such as invariant measurement. However, these methods are based on assumptions and transformations that may not be appropriate for ordinal ratings. In this study, I show how researchers can use Mokken scale analysis (MSA), which is a nonparametric approach to item response theory, to evaluate rating quality within the framework of invariant measurement without the use of potentially inappropriate parametric techniques. I use an illustrative analysis of data from a rater-mediated writing assessment to demonstrate how one can use numeric and graphical indicators from MSA to gather evidence of validity, reliability, and fairness. The results from the analyses suggest that MSA provides a useful framework within which to evaluate rater-mediated assessments for evidence of validity, reliability, and fairness that can supplement existing popular methods for evaluating ratings. © 2019 by the National Council on Measurement in Education","Nonparametric Evidence of Validity, Reliability, and Fairness for Rater-Mediated Assessments: An Illustration Using Mokken Scale Analysis","Numerous researchers have proposed methods for evaluating the quality of rater-mediated assessments using nonparametric methods (e.g., kappa coefficients) and parametric methods (e.g., the many-facet Rasch model). Generally speaking, popular nonparametric methods for evaluating rating quality are not based on a particular measurement theory. On the other hand, popular parametric methods for evaluating rating quality are often based on measurement theories such as invariant measurement. However, these methods are based on assumptions and transformations that may not be appropriate for ordinal ratings. In this study, I show how researchers can use Mokken scale analysis (MSA), which is a nonparametric approach to item response theory, to evaluate rating quality within the framework of invariant measurement without the use of potentially inappropriate parametric techniques. I use an illustrative analysis of data from a rater-mediated writing assessment to demonstrate how one can use numeric and graphical indicators from MSA to gather evidence of validity, reliability, and fairness. The results from the analyses suggest that MSA provides a useful framework within which to evaluate rater-mediated assessments for evidence of validity, reliability, and fairness that can supplement existing popular methods for evaluating ratings. © 2019 by the National Council on Measurement in Education","['numerous', 'researcher', 'propose', 'method', 'evaluate', 'quality', 'ratermediate', 'assessment', 'nonparametric', 'method', 'eg', 'kappa', 'coefficient', 'parametric', 'method', 'eg', 'manyfacet', 'Rasch', 'generally', 'speak', 'popular', 'nonparametric', 'method', 'evaluate', 'rating', 'quality', 'base', 'particular', 'theory', 'hand', 'popular', 'parametric', 'method', 'evaluate', 'rating', 'quality', 'base', 'theory', 'invariant', 'method', 'base', 'assumption', 'transformation', 'appropriate', 'ordinal', 'rating', 'study', 'I', 'researcher', 'Mokken', 'scale', 'analysis', 'MSA', 'nonparametric', 'approach', 'item', 'response', 'theory', 'evaluate', 'rating', 'quality', 'framework', 'invariant', 'potentially', 'inappropriate', 'parametric', 'technique', 'I', 'illustrative', 'analysis', 'datum', 'ratermediate', 'writing', 'assessment', 'demonstrate', 'numeric', 'graphical', 'indicator', 'MSA', 'gather', 'evidence', 'validity', 'reliability', 'fairness', 'result', 'analysis', 'suggest', 'MSA', 'provide', 'useful', 'framework', 'evaluate', 'ratermediate', 'assessment', 'evidence', 'validity', 'reliability', 'fairness', 'supplement', 'exist', 'popular', 'method', 'evaluate', 'rating', '©', '2019', 'National', 'Council']","['Nonparametric', 'Evidence', 'Validity', 'Reliability', 'Fairness', 'RaterMediated', 'assessment', 'Illustration', 'Mokken', 'Scale', 'Analysis']",numerous researcher propose method evaluate quality ratermediate assessment nonparametric method eg kappa coefficient parametric method eg manyfacet Rasch generally speak popular nonparametric method evaluate rating quality base particular theory hand popular parametric method evaluate rating quality base theory invariant method base assumption transformation appropriate ordinal rating study I researcher Mokken scale analysis MSA nonparametric approach item response theory evaluate rating quality framework invariant potentially inappropriate parametric technique I illustrative analysis datum ratermediate writing assessment demonstrate numeric graphical indicator MSA gather evidence validity reliability fairness result analysis suggest MSA provide useful framework evaluate ratermediate assessment evidence validity reliability fairness supplement exist popular method evaluate rating © 2019 National Council,Nonparametric Evidence Validity Reliability Fairness RaterMediated assessment Illustration Mokken Scale Analysis,0.027002089094727576,0.8915523832899167,0.02712812353352889,0.027252434761804698,0.0270649693200221,0.011418117186372369,0.019135519915251798,0.05527745913250913,0.011302997097504618,0.055544467139414556
Nieto R.; Casabianca J.M.,Accounting for Rater Effects With the Hierarchical Rater Model Framework When Scoring Simple Structured Constructed Response Tests,2019,56,"Many large-scale assessments are designed to yield two or more scores for an individual by administering multiple sections measuring different but related skills. Multidimensional tests, or more specifically, simple structured tests, such as these rely on multiple multiple-choice and/or constructed responses sections of items to generate multiple scores. In the current article, we propose an extension of the hierarchical rater model (HRM) to be applied with simple structured tests with constructed response items. In addition to modeling the appropriate trait structure, the multidimensional HRM (M-HRM) presented here also accounts for rater severity bias and rater variability or inconsistency. We introduce the model formulation, test parameter recovery with a focus on latent traits, and compare the M-HRM to other scoring approaches (unidimensional HRMs and a traditional multidimensional item response theory model) using simulated and empirical data. Results show more precise scores under the M-HRM, with a major improvement in scores when incorporating rater effects versus ignoring them in the traditional multidimensional item response theory model. © 2019 by the National Council on Measurement in Education",Accounting for Rater Effects With the Hierarchical Rater Model Framework When Scoring Simple Structured Constructed Response Tests,"Many large-scale assessments are designed to yield two or more scores for an individual by administering multiple sections measuring different but related skills. Multidimensional tests, or more specifically, simple structured tests, such as these rely on multiple multiple-choice and/or constructed responses sections of items to generate multiple scores. In the current article, we propose an extension of the hierarchical rater model (HRM) to be applied with simple structured tests with constructed response items. In addition to modeling the appropriate trait structure, the multidimensional HRM (M-HRM) presented here also accounts for rater severity bias and rater variability or inconsistency. We introduce the model formulation, test parameter recovery with a focus on latent traits, and compare the M-HRM to other scoring approaches (unidimensional HRMs and a traditional multidimensional item response theory model) using simulated and empirical data. Results show more precise scores under the M-HRM, with a major improvement in scores when incorporating rater effects versus ignoring them in the traditional multidimensional item response theory model. © 2019 by the National Council on Measurement in Education","['largescale', 'assessment', 'design', 'yield', 'score', 'individual', 'administer', 'multiple', 'section', 'measure', 'different', 'related', 'skill', 'multidimensional', 'test', 'specifically', 'simple', 'structured', 'test', 'rely', 'multiple', 'multiplechoice', 'andor', 'construct', 'response', 'section', 'item', 'generate', 'multiple', 'score', 'current', 'article', 'propose', 'extension', 'hierarchical', 'rater', 'HRM', 'apply', 'simple', 'structured', 'test', 'construct', 'response', 'item', 'addition', 'appropriate', 'trait', 'structure', 'multidimensional', 'HRM', 'MHRM', 'present', 'account', 'rater', 'severity', 'bias', 'rater', 'variability', 'inconsistency', 'introduce', 'formulation', 'test', 'parameter', 'recovery', 'focus', 'latent', 'trait', 'compare', 'MHRM', 'scoring', 'approach', 'unidimensional', 'hrm', 'traditional', 'multidimensional', 'item', 'response', 'theory', 'simulated', 'empirical', 'datum', 'result', 'precise', 'score', 'MHRM', 'major', 'improvement', 'score', 'incorporate', 'rater', 'effect', 'versus', 'ignore', 'traditional', 'multidimensional', 'item', 'response', 'theory', '©', '2019', 'National', 'Council']","['account', 'Rater', 'Effects', 'Hierarchical', 'Rater', 'Framework', 'Scoring', 'Simple', 'Structured', 'Constructed', 'Response', 'test']",largescale assessment design yield score individual administer multiple section measure different related skill multidimensional test specifically simple structured test rely multiple multiplechoice andor construct response section item generate multiple score current article propose extension hierarchical rater HRM apply simple structured test construct response item addition appropriate trait structure multidimensional HRM MHRM present account rater severity bias rater variability inconsistency introduce formulation test parameter recovery focus latent trait compare MHRM scoring approach unidimensional hrm traditional multidimensional item response theory simulated empirical datum result precise score MHRM major improvement score incorporate rater effect versus ignore traditional multidimensional item response theory © 2019 National Council,account Rater Effects Hierarchical Rater Framework Scoring Simple Structured Constructed Response test,0.8975999656106556,0.02551616489660584,0.025557094052050658,0.025544044147054985,0.02578273129363293,0.042682597383128594,0.0,0.013770936770129248,0.0018067315353453672,0.13959321200276595
Embretson S.E.; Kingston N.M.,Automatic Item Generation: A More Efficient Process for Developing Mathematics Achievement Items?,2018,55,"The continual supply of new items is crucial to maintaining quality for many tests. Automatic item generation (AIG) has the potential to rapidly increase the number of items that are available. However, the efficiency of AIG will be mitigated if the generated items must be submitted to traditional, time-consuming review processes. In two studies, generated mathematics achievement items were subjected to multiple stages of qualitative review for measuring the intended skills, followed by empirical tryout in operational testing. High rates of success were found. Further, items generated from the same item structure had predictable psychometric properties. Thus, the feasibility of a more limited and expedient review processes was supported. Additionally, positive results were obtained on measuring the same skills from item structures with reduced cognitive complexity. Copyright © 2018 by the National Council on Measurement in Education",Automatic Item Generation: A More Efficient Process for Developing Mathematics Achievement Items?,"The continual supply of new items is crucial to maintaining quality for many tests. Automatic item generation (AIG) has the potential to rapidly increase the number of items that are available. However, the efficiency of AIG will be mitigated if the generated items must be submitted to traditional, time-consuming review processes. In two studies, generated mathematics achievement items were subjected to multiple stages of qualitative review for measuring the intended skills, followed by empirical tryout in operational testing. High rates of success were found. Further, items generated from the same item structure had predictable psychometric properties. Thus, the feasibility of a more limited and expedient review processes was supported. Additionally, positive results were obtained on measuring the same skills from item structures with reduced cognitive complexity. Copyright © 2018 by the National Council on Measurement in Education","['continual', 'supply', 'new', 'item', 'crucial', 'maintain', 'quality', 'test', 'automatic', 'item', 'generation', 'AIG', 'potential', 'rapidly', 'increase', 'number', 'item', 'available', 'efficiency', 'AIG', 'mitigate', 'generate', 'item', 'submit', 'traditional', 'timeconsuming', 'review', 'process', 'study', 'generate', 'mathematics', 'achievement', 'item', 'subject', 'multiple', 'stage', 'qualitative', 'review', 'measure', 'intend', 'skill', 'follow', 'empirical', 'tryout', 'operational', 'testing', 'high', 'rate', 'success', 'find', 'item', 'generate', 'item', 'structure', 'predictable', 'psychometric', 'property', 'feasibility', 'limited', 'expedient', 'review', 'process', 'support', 'additionally', 'positive', 'result', 'obtain', 'measure', 'skill', 'item', 'structure', 'reduce', 'cognitive', 'complexity', 'copyright', '©', '2018', 'National', 'Council']","['Automatic', 'Item', 'Generation', 'A', 'efficient', 'Process', 'develop', 'Mathematics', 'Achievement', 'item']",continual supply new item crucial maintain quality test automatic item generation AIG potential rapidly increase number item available efficiency AIG mitigate generate item submit traditional timeconsuming review process study generate mathematics achievement item subject multiple stage qualitative review measure intend skill follow empirical tryout operational testing high rate success find item generate item structure predictable psychometric property feasibility limited expedient review process support additionally positive result obtain measure skill item structure reduce cognitive complexity copyright © 2018 National Council,Automatic Item Generation A efficient Process develop Mathematics Achievement item,0.8984515807982323,0.02527043592498922,0.02525976937477064,0.02538950613821971,0.025628707763788047,0.05233355040170631,0.0,0.024247246866049264,0.0,0.0
Guo H.; Robin F.; Dorans N.,Detecting Item Drift in Large-Scale Testing,2017,54,"The early detection of item drift is an important issue for frequently administered testing programs because items are reused over time. Unfortunately, operational data tend to be very sparse and do not lend themselves to frequent monitoring analyses, particularly for on-demand testing. Building on existing residual analyses, the authors propose an item index that requires only moderate-to-small sample sizes to form data for time-series analysis. Asymptotic results are presented to facilitate statistical significance tests. The authors show that the proposed index combined with time-series techniques may be useful in detecting and predicting item drift. Most important, this index is related to a well-known differential item functioning analysis so that a meaningful effect size can be proposed for item drift detection. Copyright © 2017 by the National Council on Measurement in Education",,"The early detection of item drift is an important issue for frequently administered testing programs because items are reused over time. Unfortunately, operational data tend to be very sparse and do not lend themselves to frequent monitoring analyses, particularly for on-demand testing. Building on existing residual analyses, the authors propose an item index that requires only moderate-to-small sample sizes to form data for time-series analysis. Asymptotic results are presented to facilitate statistical significance tests. The authors show that the proposed index combined with time-series techniques may be useful in detecting and predicting item drift. Most important, this index is related to a well-known differential item functioning analysis so that a meaningful effect size can be proposed for item drift detection. Copyright © 2017 by the National Council on Measurement in Education","['early', 'detection', 'item', 'drift', 'important', 'issue', 'frequently', 'administer', 'testing', 'program', 'item', 'reuse', 'time', 'unfortunately', 'operational', 'datum', 'tend', 'sparse', 'lend', 'frequent', 'monitoring', 'analysis', 'particularly', 'ondemand', 'testing', 'building', 'exist', 'residual', 'analysis', 'author', 'propose', 'item', 'index', 'require', 'moderatetosmall', 'sample', 'size', 'form', 'datum', 'timeserie', 'analysis', 'asymptotic', 'result', 'present', 'facilitate', 'statistical', 'significance', 'test', 'author', 'propose', 'index', 'combine', 'timeserie', 'technique', 'useful', 'detect', 'predict', 'item', 'drift', 'important', 'index', 'relate', 'wellknown', 'differential', 'item', 'function', 'analysis', 'meaningful', 'effect', 'size', 'propose', 'item', 'drift', 'detection', 'Copyright', '©', '2017', 'National', 'Council']",,early detection item drift important issue frequently administer testing program item reuse time unfortunately operational datum tend sparse lend frequent monitoring analysis particularly ondemand testing building exist residual analysis author propose item index require moderatetosmall sample size form datum timeserie analysis asymptotic result present facilitate statistical significance test author propose index combine timeserie technique useful detect predict item drift important index relate wellknown differential item function analysis meaningful effect size propose item drift detection Copyright © 2017 National Council,,0.02661951115207916,0.02644814140199798,0.026530243489532628,0.026722984318055507,0.8936791196383347,0.06691424999290556,0.0,0.00022773129422610545,0.0,0.0025678288608699555
Guo H.; Deane P.D.; van Rijn P.W.; Zhang M.; Bennett R.E.,Modeling Basic Writing Processes From Keystroke Logs,2018,55,"The goal of this study is to model pauses extracted from writing keystroke logs as a way of characterizing the processes students use in essay composition. Low-level timing data were modeled, the interkey interval and its subtype, the intraword duration, thought to reflect processes associated with keyboarding skills and composition fluency. Heavy-tailed probability distributions (lognormal and stable distributions) were fit to individual students' data. Both density functions fit reasonably well, and estimated parameters were found to be robust across prompts designed to assess student proficiency for the same writing purpose. In addition, estimated parameters for both density functions were statistically significantly associated with human essay scores after accounting for total time spent writing the essay, a result consistent with cognitive theory on the role of low-level processes in writing. Copyright © 2018 by the National Council on Measurement in Education",Modeling Basic Writing Processes From Keystroke Logs,"The goal of this study is to model pauses extracted from writing keystroke logs as a way of characterizing the processes students use in essay composition. Low-level timing data were modeled, the interkey interval and its subtype, the intraword duration, thought to reflect processes associated with keyboarding skills and composition fluency. Heavy-tailed probability distributions (lognormal and stable distributions) were fit to individual students' data. Both density functions fit reasonably well, and estimated parameters were found to be robust across prompts designed to assess student proficiency for the same writing purpose. In addition, estimated parameters for both density functions were statistically significantly associated with human essay scores after accounting for total time spent writing the essay, a result consistent with cognitive theory on the role of low-level processes in writing. Copyright © 2018 by the National Council on Measurement in Education","['goal', 'study', 'pause', 'extract', 'write', 'keystroke', 'log', 'way', 'characterize', 'process', 'student', 'essay', 'composition', 'Lowlevel', 'timing', 'datum', 'interkey', 'interval', 'subtype', 'intraword', 'duration', 'think', 'reflect', 'process', 'associate', 'keyboarde', 'skill', 'composition', 'fluency', 'heavytailed', 'probability', 'distribution', 'lognormal', 'stable', 'distribution', 'fit', 'individual', 'student', 'data', 'density', 'function', 'fit', 'reasonably', 'estimate', 'parameter', 'find', 'robust', 'prompt', 'design', 'assess', 'student', 'proficiency', 'writing', 'purpose', 'addition', 'estimate', 'parameter', 'density', 'function', 'statistically', 'significantly', 'associate', 'human', 'essay', 'score', 'account', 'total', 'time', 'spend', 'write', 'essay', 'result', 'consistent', 'cognitive', 'theory', 'role', 'lowlevel', 'process', 'write', 'Copyright', '©', '2018', 'National', 'Council']","['Basic', 'Writing', 'Processes', 'Keystroke', 'Logs']",goal study pause extract write keystroke log way characterize process student essay composition Lowlevel timing datum interkey interval subtype intraword duration think reflect process associate keyboarde skill composition fluency heavytailed probability distribution lognormal stable distribution fit individual student data density function fit reasonably estimate parameter find robust prompt design assess student proficiency writing purpose addition estimate parameter density function statistically significantly associate human essay score account total time spend write essay result consistent cognitive theory role lowlevel process write Copyright © 2018 National Council,Basic Writing Processes Keystroke Logs,0.02514513396413871,0.8993109531088681,0.025115223634359946,0.025189827607415814,0.025238861685217464,0.011433751906406579,0.0045174028889928145,0.05560106625923266,0.0,0.024254532194094895
Lee S.; Bolt D.M.,An Alternative to the 3PL: Using Asymmetric Item Characteristic Curves to Address Guessing Effects,2018,55,"Both the statistical and interpretational shortcomings of the three-parameter logistic (3PL) model in accommodating guessing effects on multiple-choice items are well documented. We consider the use of a residual heteroscedasticity (RH) model as an alternative, and compare its performance to the 3PL with real test data sets and through simulation analyses. Our results suggest advantages to the RH approach, including closer fit to real data, more interpretable parameter estimates, and greater psychological plausibility. Copyright © 2018 by the National Council on Measurement in Education",An Alternative to the 3PL: Using Asymmetric Item Characteristic Curves to Address Guessing Effects,"Both the statistical and interpretational shortcomings of the three-parameter logistic (3PL) model in accommodating guessing effects on multiple-choice items are well documented. We consider the use of a residual heteroscedasticity (RH) model as an alternative, and compare its performance to the 3PL with real test data sets and through simulation analyses. Our results suggest advantages to the RH approach, including closer fit to real data, more interpretable parameter estimates, and greater psychological plausibility. Copyright © 2018 by the National Council on Measurement in Education","['statistical', 'interpretational', 'shortcoming', 'threeparameter', 'logistic', '3pl', 'accommodate', 'guess', 'effect', 'multiplechoice', 'item', 'document', 'consider', 'residual', 'heteroscedasticity', 'RH', 'alternative', 'compare', 'performance', '3PL', 'real', 'test', 'data', 'set', 'simulation', 'analyse', 'result', 'suggest', 'advantage', 'RH', 'approach', 'include', 'close', 'fit', 'real', 'datum', 'interpretable', 'parameter', 'estimate', 'great', 'psychological', 'plausibility', 'Copyright', '©', '2018', 'National', 'Council']","['alternative', '3PL', 'Asymmetric', 'Item', 'Characteristic', 'curve', 'Address', 'Guessing', 'effect']",statistical interpretational shortcoming threeparameter logistic 3pl accommodate guess effect multiplechoice item document consider residual heteroscedasticity RH alternative compare performance 3PL real test data set simulation analyse result suggest advantage RH approach include close fit real datum interpretable parameter estimate great psychological plausibility Copyright © 2018 National Council,alternative 3PL Asymmetric Item Characteristic curve Address Guessing effect,0.030213366604669317,0.030157022177966435,0.03004104115743412,0.030272132906900805,0.8793164371530293,0.047012526789104545,0.0007454579139446375,0.011272171216727384,0.0,0.006004552619098725
Liu B.; Kennedy P.C.; Seipel B.; Carlson S.E.; Biancarosa G.; Davison M.L.,"Can We Learn From Student Mistakes in a Formative, Reading Comprehension Assessment?",2019,56,"This article describes an ongoing project to develop a formative, inferential reading comprehension assessment of causal story comprehension. It has three features to enhance classroom use: equated scale scores for progress monitoring within and across grades, a scale score to distinguish among low-scoring students based on patterns of mistakes, and a reading efficiency index. Instead of two response types for each multiple-choice item, correct and incorrect, each item has three response types: correct and two incorrect response types. Prior results on reliability, convergent and discriminant validity, and predictive utility of mistake subscores are briefly described. The three-response-type structure of items required rethinking the item response theory (IRT) modeling. IRT-modeling results are presented, and implications for formative assessments and instructional use are discussed. © 2019 by the National Council on Measurement in Education","Can We Learn From Student Mistakes in a Formative, Reading Comprehension Assessment?","This article describes an ongoing project to develop a formative, inferential reading comprehension assessment of causal story comprehension. It has three features to enhance classroom use: equated scale scores for progress monitoring within and across grades, a scale score to distinguish among low-scoring students based on patterns of mistakes, and a reading efficiency index. Instead of two response types for each multiple-choice item, correct and incorrect, each item has three response types: correct and two incorrect response types. Prior results on reliability, convergent and discriminant validity, and predictive utility of mistake subscores are briefly described. The three-response-type structure of items required rethinking the item response theory (IRT) modeling. IRT-modeling results are presented, and implications for formative assessments and instructional use are discussed. © 2019 by the National Council on Measurement in Education","['article', 'describe', 'ongoing', 'project', 'develop', 'formative', 'inferential', 'reading', 'comprehension', 'assessment', 'causal', 'story', 'comprehension', 'feature', 'enhance', 'classroom', 'equate', 'scale', 'score', 'progress', 'monitor', 'grade', 'scale', 'score', 'distinguish', 'lowscore', 'student', 'base', 'pattern', 'mistake', 'reading', 'efficiency', 'index', 'instead', 'response', 'type', 'multiplechoice', 'item', 'correct', 'incorrect', 'item', 'response', 'type', 'correct', 'incorrect', 'response', 'type', 'prior', 'result', 'reliability', 'convergent', 'discriminant', 'validity', 'predictive', 'utility', 'mistake', 'subscore', 'briefly', 'describe', 'threeresponsetype', 'structure', 'item', 'require', 'rethink', 'item', 'response', 'theory', 'IRT', 'irtmodele', 'result', 'present', 'implication', 'formative', 'assessment', 'instructional', 'discuss', '©', '2019', 'National', 'Council']","['learn', 'student', 'mistake', 'formative', 'Reading', 'Comprehension', 'Assessment']",article describe ongoing project develop formative inferential reading comprehension assessment causal story comprehension feature enhance classroom equate scale score progress monitor grade scale score distinguish lowscore student base pattern mistake reading efficiency index instead response type multiplechoice item correct incorrect item response type correct incorrect response type prior result reliability convergent discriminant validity predictive utility mistake subscore briefly describe threeresponsetype structure item require rethink item response theory IRT irtmodele result present implication formative assessment instructional discuss © 2019 National Council,learn student mistake formative Reading Comprehension Assessment,0.024920748433653068,0.02497890912578293,0.8998616407419199,0.025054333762799225,0.025184367935844797,0.030056464041555122,0.038829374952075085,0.0543856646118549,0.0069382058780436285,0.0051531694402398315
Wang W.; Song L.; Chen P.; Ding S.,An Item-Level Expected Classification Accuracy and Its Applications in Cognitive Diagnostic Assessment,2019,56,"Most of the existing classification accuracy indices of attribute patterns lose effectiveness when the response data is absent in diagnostic testing. To handle this issue, this article proposes new indices to predict the correct classification rate of a diagnostic test before administering the test under the deterministic noise input “and” gate (DINA) model. The new indices include an item-level expected classification accuracy (ECA) for attributes and a test-level ECA for attributes and attribute patterns, and both of them are calculated based solely on the known item parameters and Q-matrix. Theoretical analysis showed that the item-level ECA could be regarded as a measure of correct classification rates of attributes contributed by an item. This article also illustrates how to apply the item-level ECA for attributes to estimate the correct classification rate of attributes patterns at the test level. Simulation results showed that two test-level ECA indices, ECA_I_W (an index based on the independence assumption and the weighted sum of the item-level ECAs) and ECA_C_M (an index based on Gaussian Copula function that incorporates the dependence structure of the events of attribute classification and the simple average of the item-level ECAs), could make an accurate prediction for correct classification rates of attribute patterns. © 2019 by the National Council on Measurement in Education",An Item-Level Expected Classification Accuracy and Its Applications in Cognitive Diagnostic Assessment,"Most of the existing classification accuracy indices of attribute patterns lose effectiveness when the response data is absent in diagnostic testing. To handle this issue, this article proposes new indices to predict the correct classification rate of a diagnostic test before administering the test under the deterministic noise input “and” gate (DINA) model. The new indices include an item-level expected classification accuracy (ECA) for attributes and a test-level ECA for attributes and attribute patterns, and both of them are calculated based solely on the known item parameters and Q-matrix. Theoretical analysis showed that the item-level ECA could be regarded as a measure of correct classification rates of attributes contributed by an item. This article also illustrates how to apply the item-level ECA for attributes to estimate the correct classification rate of attributes patterns at the test level. Simulation results showed that two test-level ECA indices, ECA_I_W (an index based on the independence assumption and the weighted sum of the item-level ECAs) and ECA_C_M (an index based on Gaussian Copula function that incorporates the dependence structure of the events of attribute classification and the simple average of the item-level ECAs), could make an accurate prediction for correct classification rates of attribute patterns. © 2019 by the National Council on Measurement in Education","['Most', 'exist', 'classification', 'accuracy', 'index', 'attribute', 'pattern', 'lose', 'effectiveness', 'response', 'datum', 'absent', 'diagnostic', 'testing', 'handle', 'issue', 'article', 'propose', 'new', 'index', 'predict', 'correct', 'classification', 'rate', 'diagnostic', 'test', 'administer', 'test', 'deterministic', 'noise', 'input', '""', '""', 'gate', 'DINA', 'new', 'index', 'include', 'itemlevel', 'expect', 'classification', 'accuracy', 'ECA', 'attribute', 'testlevel', 'eca', 'attribute', 'attribute', 'pattern', 'calculate', 'base', 'solely', 'know', 'item', 'parameter', 'qmatrix', 'theoretical', 'analysis', 'itemlevel', 'ECA', 'regard', 'measure', 'correct', 'classification', 'rate', 'attribute', 'contribute', 'item', 'article', 'illustrate', 'apply', 'itemlevel', 'ECA', 'attribute', 'estimate', 'correct', 'classification', 'rate', 'attribute', 'pattern', 'test', 'level', 'Simulation', 'result', 'testlevel', 'ECA', 'indice', 'ECAIW', 'index', 'base', 'independence', 'assumption', 'weighted', 'sum', 'itemlevel', 'eca', 'ECACM', 'index', 'base', 'Gaussian', 'Copula', 'function', 'incorporate', 'dependence', 'structure', 'event', 'attribute', 'classification', 'simple', 'average', 'itemlevel', 'eca', 'accurate', 'prediction', 'correct', 'classification', 'rate', 'attribute', 'pattern', '©', '2019', 'National', 'Council']","['ItemLevel', 'Expected', 'Classification', 'Accuracy', 'Applications', 'Cognitive', 'Diagnostic', 'Assessment']","Most exist classification accuracy index attribute pattern lose effectiveness response datum absent diagnostic testing handle issue article propose new index predict correct classification rate diagnostic test administer test deterministic noise input "" "" gate DINA new index include itemlevel expect classification accuracy ECA attribute testlevel eca attribute attribute pattern calculate base solely know item parameter qmatrix theoretical analysis itemlevel ECA regard measure correct classification rate attribute contribute item article illustrate apply itemlevel ECA attribute estimate correct classification rate attribute pattern test level Simulation result testlevel ECA indice ECAIW index base independence assumption weighted sum itemlevel eca ECACM index base Gaussian Copula function incorporate dependence structure event attribute classification simple average itemlevel eca accurate prediction correct classification rate attribute pattern © 2019 National Council",ItemLevel Expected Classification Accuracy Applications Cognitive Diagnostic Assessment,0.030493723995232248,0.03042728471264375,0.030197187819670526,0.4984237061761285,0.41045809729632493,0.04536229760631063,0.039527430620297274,0.0014343326370245824,0.0,0.0
Shear B.R.,Using Hierarchical Logistic Regression to Study DIF and DIF Variance in Multilevel Data,2018,55,"When contextual features of test-taking environments differentially affect item responding for different test takers and these features vary across test administrations, they may cause differential item functioning (DIF) that varies across test administrations. Because many common DIF detection methods ignore potential DIF variance, this article proposes the use of random coefficient hierarchical logistic regression (RC-HLR) models to test for both uniform DIF and DIF variance simultaneously. A simulation study and real data analysis are used to demonstrate and evaluate the proposed RC-HLR model. Results show the RC-HLR model can detect uniform DIF and DIF variance more accurately than standard logistic regression DIF models in terms of bias and Type I error rates. © 2018 by the National Council on Measurement in Education",Using Hierarchical Logistic Regression to Study DIF and DIF Variance in Multilevel Data,"When contextual features of test-taking environments differentially affect item responding for different test takers and these features vary across test administrations, they may cause differential item functioning (DIF) that varies across test administrations. Because many common DIF detection methods ignore potential DIF variance, this article proposes the use of random coefficient hierarchical logistic regression (RC-HLR) models to test for both uniform DIF and DIF variance simultaneously. A simulation study and real data analysis are used to demonstrate and evaluate the proposed RC-HLR model. Results show the RC-HLR model can detect uniform DIF and DIF variance more accurately than standard logistic regression DIF models in terms of bias and Type I error rates. © 2018 by the National Council on Measurement in Education","['contextual', 'feature', 'testtake', 'environment', 'differentially', 'affect', 'item', 'respond', 'different', 'test', 'taker', 'feature', 'vary', 'test', 'administration', 'cause', 'differential', 'item', 'function', 'DIF', 'vary', 'test', 'administration', 'common', 'dif', 'detection', 'method', 'ignore', 'potential', 'DIF', 'variance', 'article', 'propose', 'random', 'coefficient', 'hierarchical', 'logistic', 'regression', 'RCHLR', 'test', 'uniform', 'DIF', 'DIF', 'variance', 'simultaneously', 'simulation', 'study', 'real', 'datum', 'analysis', 'demonstrate', 'evaluate', 'propose', 'rchlr', 'result', 'rchlr', 'detect', 'uniform', 'DIF', 'dif', 'variance', 'accurately', 'standard', 'logistic', 'regression', 'dif', 'term', 'bias', 'Type', 'I', 'error', 'rate', '©', '2018', 'National', 'Council']","['hierarchical', 'Logistic', 'regression', 'study', 'DIF', 'DIF', 'Variance', 'Multilevel', 'Data']",contextual feature testtake environment differentially affect item respond different test taker feature vary test administration cause differential item function DIF vary test administration common dif detection method ignore potential DIF variance article propose random coefficient hierarchical logistic regression RCHLR test uniform DIF DIF variance simultaneously simulation study real datum analysis demonstrate evaluate propose rchlr result rchlr detect uniform DIF dif variance accurately standard logistic regression dif term bias Type I error rate © 2018 National Council,hierarchical Logistic regression study DIF DIF Variance Multilevel Data,0.8710399175302023,0.03216618003141392,0.03243421651103935,0.03199880575659033,0.03236088017075412,0.0832820680687789,0.0,0.0,0.0,0.0
Zhang Z.; Zhao M.,"Standard Errors of IRT Parameter Scale Transformation Coefficients: Comparison of Bootstrap Method, Delta Method, and Multiple Imputation Method",2019,56,"The present study evaluated the multiple imputation method, a procedure that is similar to the one suggested by Li and Lissitz (2004), and compared the performance of this method with that of the bootstrap method and the delta method in obtaining the standard errors for the estimates of the parameter scale transformation coefficients in item response theory (IRT) equating in the context of the common-item nonequivalent groups design. Two different estimation procedures for the variance-covariance matrix of the IRT item parameter estimates, which were used in both the delta method and the multiple imputation method, were considered: empirical cross-product (XPD) and supplemented expectation maximization (SEM). The results of the analyses with simulated and real data indicate that the multiple imputation method generally produced very similar results to the bootstrap method and the delta method in most of the conditions. The differences between the estimated standard errors obtained by the methods using the XPD matrices and the SEM matrices were very small when the sample size was reasonably large. When the sample size was small, the methods using the XPD matrices appeared to yield slight upward bias for the standard errors of the IRT parameter scale transformation coefficients. © 2019 by the National Council on Measurement in Education","Standard Errors of IRT Parameter Scale Transformation Coefficients: Comparison of Bootstrap Method, Delta Method, and Multiple Imputation Method","The present study evaluated the multiple imputation method, a procedure that is similar to the one suggested by Li and Lissitz (2004), and compared the performance of this method with that of the bootstrap method and the delta method in obtaining the standard errors for the estimates of the parameter scale transformation coefficients in item response theory (IRT) equating in the context of the common-item nonequivalent groups design. Two different estimation procedures for the variance-covariance matrix of the IRT item parameter estimates, which were used in both the delta method and the multiple imputation method, were considered: empirical cross-product (XPD) and supplemented expectation maximization (SEM). The results of the analyses with simulated and real data indicate that the multiple imputation method generally produced very similar results to the bootstrap method and the delta method in most of the conditions. The differences between the estimated standard errors obtained by the methods using the XPD matrices and the SEM matrices were very small when the sample size was reasonably large. When the sample size was small, the methods using the XPD matrices appeared to yield slight upward bias for the standard errors of the IRT parameter scale transformation coefficients. © 2019 by the National Council on Measurement in Education","['present', 'study', 'evaluate', 'multiple', 'imputation', 'method', 'procedure', 'similar', 'suggest', 'Li', 'Lissitz', '2004', 'compare', 'performance', 'method', 'bootstrap', 'method', 'delta', 'method', 'obtain', 'standard', 'error', 'estimate', 'parameter', 'scale', 'transformation', 'coefficient', 'item', 'response', 'theory', 'IRT', 'equate', 'context', 'commonitem', 'nonequivalent', 'group', 'design', 'different', 'estimation', 'procedure', 'variancecovariance', 'matrix', 'IRT', 'item', 'parameter', 'estimate', 'delta', 'method', 'multiple', 'imputation', 'method', 'consider', 'empirical', 'crossproduct', 'XPD', 'supplement', 'expectation', 'maximization', 'SEM', 'result', 'analysis', 'simulated', 'real', 'datum', 'indicate', 'multiple', 'imputation', 'method', 'generally', 'produce', 'similar', 'result', 'bootstrap', 'method', 'delta', 'method', 'condition', 'difference', 'estimate', 'standard', 'error', 'obtain', 'method', 'XPD', 'matrix', 'SEM', 'matrix', 'small', 'sample', 'size', 'reasonably', 'large', 'sample', 'size', 'small', 'method', 'XPD', 'matrix', 'appear', 'yield', 'slight', 'upward', 'bias', 'standard', 'error', 'IRT', 'parameter', 'scale', 'transformation', 'coefficient', '©', '2019', 'National', 'Council']","['Standard', 'error', 'IRT', 'Parameter', 'Scale', 'Transformation', 'Coefficients', 'Comparison', 'Bootstrap', 'Method', 'Delta', 'Method', 'Multiple', 'Imputation', 'Method']",present study evaluate multiple imputation method procedure similar suggest Li Lissitz 2004 compare performance method bootstrap method delta method obtain standard error estimate parameter scale transformation coefficient item response theory IRT equate context commonitem nonequivalent group design different estimation procedure variancecovariance matrix IRT item parameter estimate delta method multiple imputation method consider empirical crossproduct XPD supplement expectation maximization SEM result analysis simulated real datum indicate multiple imputation method generally produce similar result bootstrap method delta method condition difference estimate standard error obtain method XPD matrix SEM matrix small sample size reasonably large sample size small method XPD matrix appear yield slight upward bias standard error IRT parameter scale transformation coefficient © 2019 National Council,Standard error IRT Parameter Scale Transformation Coefficients Comparison Bootstrap Method Delta Method Multiple Imputation Method,0.0270385560563792,0.026661134280294273,0.02682366990482573,0.026712596762074985,0.8927640429964259,0.05265323784770625,0.0011888469332864428,0.0,0.06177461085274782,0.0005146619507054189
Hopster-den Otter D.; Wools S.; Eggen T.J.H.M.; Veldkamp B.P.,A General Framework for the Validation of Embedded Formative Assessment,2019,56,"In educational practice, test results are used for several purposes. However, validity research is especially focused on the validity of summative assessment. This article aimed to provide a general framework for validating formative assessment. The authors applied the argument-based approach to validation to the context of formative assessment. This resulted in a proposed interpretation and use argument consisting of a score interpretation and a score use. The former involves inferences linking specific task performance to an interpretation of a student's general performance. The latter involves inferences regarding decisions about actions and educational consequences. The validity argument should focus on critical claims regarding score interpretation and score use, since both are critical to the effectiveness of formative assessment. The proposed framework is illustrated by an operational example including a presentation of evidence that can be collected on the basis of the framework. © 2019 by the National Council on Measurement in Education",A General Framework for the Validation of Embedded Formative Assessment,"In educational practice, test results are used for several purposes. However, validity research is especially focused on the validity of summative assessment. This article aimed to provide a general framework for validating formative assessment. The authors applied the argument-based approach to validation to the context of formative assessment. This resulted in a proposed interpretation and use argument consisting of a score interpretation and a score use. The former involves inferences linking specific task performance to an interpretation of a student's general performance. The latter involves inferences regarding decisions about actions and educational consequences. The validity argument should focus on critical claims regarding score interpretation and score use, since both are critical to the effectiveness of formative assessment. The proposed framework is illustrated by an operational example including a presentation of evidence that can be collected on the basis of the framework. © 2019 by the National Council on Measurement in Education","['educational', 'practice', 'test', 'result', 'purpose', 'validity', 'research', 'especially', 'focus', 'validity', 'summative', 'assessment', 'article', 'aim', 'provide', 'general', 'framework', 'validate', 'formative', 'assessment', 'author', 'apply', 'argumentbased', 'approach', 'validation', 'context', 'formative', 'assessment', 'result', 'propose', 'interpretation', 'argument', 'consist', 'score', 'interpretation', 'score', 'involve', 'inference', 'link', 'specific', 'task', 'performance', 'interpretation', 'student', 'general', 'performance', 'involve', 'inference', 'regard', 'decision', 'action', 'educational', 'consequence', 'validity', 'argument', 'focus', 'critical', 'claim', 'regard', 'score', 'interpretation', 'score', 'critical', 'effectiveness', 'formative', 'assessment', 'propose', 'framework', 'illustrate', 'operational', 'example', 'include', 'presentation', 'evidence', 'collect', 'basis', 'framework', '©', '2019', 'National', 'Council']","['General', 'Framework', 'Validation', 'Embedded', 'Formative', 'Assessment']",educational practice test result purpose validity research especially focus validity summative assessment article aim provide general framework validate formative assessment author apply argumentbased approach validation context formative assessment result propose interpretation argument consist score interpretation score involve inference link specific task performance interpretation student general performance involve inference regard decision action educational consequence validity argument focus critical claim regard score interpretation score critical effectiveness formative assessment propose framework illustrate operational example include presentation evidence collect basis framework © 2019 National Council,General Framework Validation Embedded Formative Assessment,0.02751204263394627,0.027714500997157512,0.027561975216664102,0.888895451641891,0.02831602951034124,0.0,0.0,0.17739527352284798,0.0,0.0
Ip E.H.; Strachan T.; Fu Y.; Lay A.; Willse J.T.; Chen S.-H.; Rutkowski L.; Ackerman T.,Bias and Bias Correction Method for Nonproportional Abilities Requirement (NPAR) Tests,2019,56,"Test items must often be broad in scope to be ecologically valid. It is therefore almost inevitable that secondary dimensions are introduced into a test during test development. A cognitive test may require one or more abilities besides the primary ability to correctly respond to an item, in which case a unidimensional test score overestimates the primary ability and creates interpretability problems. In this article, we demonstrate the nonproportional abilities requirement, a phenomenon with which secondary abilities are more required for difficult items. A novel and practical method for correcting bias in the primary ability is proposed and illustrated using a real data set from an international assessment. Simulation data are also used to evaluate the performance of the method. © 2019 by the National Council on Measurement in Education",Bias and Bias Correction Method for Nonproportional Abilities Requirement (NPAR) Tests,"Test items must often be broad in scope to be ecologically valid. It is therefore almost inevitable that secondary dimensions are introduced into a test during test development. A cognitive test may require one or more abilities besides the primary ability to correctly respond to an item, in which case a unidimensional test score overestimates the primary ability and creates interpretability problems. In this article, we demonstrate the nonproportional abilities requirement, a phenomenon with which secondary abilities are more required for difficult items. A novel and practical method for correcting bias in the primary ability is proposed and illustrated using a real data set from an international assessment. Simulation data are also used to evaluate the performance of the method. © 2019 by the National Council on Measurement in Education","['test', 'item', 'broad', 'scope', 'ecologically', 'valid', 'inevitable', 'secondary', 'dimension', 'introduce', 'test', 'test', 'development', 'cognitive', 'test', 'require', 'ability', 'primary', 'ability', 'correctly', 'respond', 'item', 'case', 'unidimensional', 'test', 'score', 'overestimate', 'primary', 'ability', 'create', 'interpretability', 'problem', 'article', 'demonstrate', 'nonproportional', 'ability', 'requirement', 'phenomenon', 'secondary', 'ability', 'require', 'difficult', 'item', 'novel', 'practical', 'method', 'correct', 'bias', 'primary', 'ability', 'propose', 'illustrate', 'real', 'datum', 'set', 'international', 'assessment', 'Simulation', 'datum', 'evaluate', 'performance', 'method', '©', '2019', 'National', 'Council']","['Bias', 'Bias', 'Correction', 'Method', 'Nonproportional', 'Abilities', 'Requirement', 'NPAR', 'test']",test item broad scope ecologically valid inevitable secondary dimension introduce test test development cognitive test require ability primary ability correctly respond item case unidimensional test score overestimate primary ability create interpretability problem article demonstrate nonproportional ability requirement phenomenon secondary ability require difficult item novel practical method correct bias primary ability propose illustrate real datum set international assessment Simulation datum evaluate performance method © 2019 National Council,Bias Bias Correction Method Nonproportional Abilities Requirement NPAR test,0.029950294763755617,0.8803819015898415,0.02980352716236848,0.02993756812947411,0.02992670835456023,0.052672847672804175,0.0046830368291467485,0.023573243799109432,0.0052382124958991715,0.0
Cui Z.; Liu C.; He Y.; Chen H.,Evaluation of a New Method for Providing Full Review Opportunities in Computerized Adaptive Testing—Computerized Adaptive Testing With Salt,2018,55,"Allowing item review in computerized adaptive testing (CAT) is getting more attention in the educational measurement field as more and more testing programs adopt CAT. The research literature has shown that allowing item review in an educational test could result in more accurate estimates of examinees’ abilities. The practice of item review in CAT, however, is hindered by the potential danger of test-manipulation strategies. To provide review opportunities to examinees while minimizing the effect of test-manipulation strategies, researchers have proposed different algorithms to implement CAT with restricted revision options. In this article, we propose and evaluate a new method that implements CAT without any restriction on item review. In particular, we evaluate the new method in terms of the accuracy on ability estimates and the robustness against test-manipulation strategies. This study shows that the newly proposed method is promising in a win-win situation: examinees have full freedom to review and change answers, and the impacts of test-manipulation strategies are undermined. © 2018 by the National Council on Measurement in Education",Evaluation of a New Method for Providing Full Review Opportunities in Computerized Adaptive Testing—Computerized Adaptive Testing With Salt,"Allowing item review in computerized adaptive testing (CAT) is getting more attention in the educational measurement field as more and more testing programs adopt CAT. The research literature has shown that allowing item review in an educational test could result in more accurate estimates of examinees’ abilities. The practice of item review in CAT, however, is hindered by the potential danger of test-manipulation strategies. To provide review opportunities to examinees while minimizing the effect of test-manipulation strategies, researchers have proposed different algorithms to implement CAT with restricted revision options. In this article, we propose and evaluate a new method that implements CAT without any restriction on item review. In particular, we evaluate the new method in terms of the accuracy on ability estimates and the robustness against test-manipulation strategies. This study shows that the newly proposed method is promising in a win-win situation: examinees have full freedom to review and change answers, and the impacts of test-manipulation strategies are undermined. © 2018 by the National Council on Measurement in Education","['allow', 'item', 'review', 'computerized', 'adaptive', 'testing', 'CAT', 'attention', 'educational', 'field', 'testing', 'program', 'adopt', 'CAT', 'research', 'literature', 'allow', 'item', 'review', 'educational', 'test', 'result', 'accurate', 'estimate', 'examinee', ""'"", 'ability', 'practice', 'item', 'review', 'CAT', 'hinder', 'potential', 'danger', 'testmanipulation', 'strategy', 'provide', 'review', 'opportunity', 'examinee', 'minimize', 'effect', 'testmanipulation', 'strategy', 'researcher', 'propose', 'different', 'algorithm', 'implement', 'CAT', 'restricted', 'revision', 'option', 'article', 'propose', 'evaluate', 'new', 'method', 'implement', 'CAT', 'restriction', 'item', 'review', 'particular', 'evaluate', 'new', 'method', 'term', 'accuracy', 'ability', 'estimate', 'robustness', 'testmanipulation', 'strategy', 'study', 'newly', 'propose', 'method', 'promise', 'winwin', 'situation', 'examinee', 'freedom', 'review', 'change', 'answer', 'impact', 'testmanipulation', 'strategy', 'undermine', '©', '2018', 'National', 'Council']","['evaluation', 'New', 'Method', 'provide', 'Full', 'Review', 'Opportunities', 'Computerized', 'Adaptive', 'Testing', '—', 'Computerized', 'Adaptive', 'Testing', 'salt']",allow item review computerized adaptive testing CAT attention educational field testing program adopt CAT research literature allow item review educational test result accurate estimate examinee ' ability practice item review CAT hinder potential danger testmanipulation strategy provide review opportunity examinee minimize effect testmanipulation strategy researcher propose different algorithm implement CAT restricted revision option article propose evaluate new method implement CAT restriction item review particular evaluate new method term accuracy ability estimate robustness testmanipulation strategy study newly propose method promise winwin situation examinee freedom review change answer impact testmanipulation strategy undermine © 2018 National Council,evaluation New Method provide Full Review Opportunities Computerized Adaptive Testing — Computerized Adaptive Testing salt,0.8819169770138066,0.02944103755432254,0.029365181893854158,0.029304746301616717,0.029972057236399917,0.05536562774635602,0.0,0.010863923618290952,0.0,0.0
Wyse A.E.; Babcock B.,A Method for Detecting Regression of Hard and Easy Item Angoff Ratings,2019,56,"One common phenomenon in Angoff standard setting is that panelists regress their ratings in toward the middle of the probability scale. This study describes two indices based on taking ratios of standard deviations that can be utilized with a scatterplot of item ratings versus expected probabilities of success to identify whether ratings are regressed in toward the middle of the probability scale. Results from a simulation study show that the standard deviation ratio indices can successfully detect ratings for hard and easy items that are regressed in toward the middle of the probability scale in Angoff standard-setting data, where previously proposed indices often do not work as well to detect these effects. Results from a real data set show that, while virtually all raters improve from Round 1 to Round 2 as measured by previously developed indices, the standard deviation ratios in conjunction with a scatterplot of item ratings versus expected probabilities of success can identify individuals who may still be regressing their ratings in toward the middle of the probability scale even after receiving feedback. The authors suggest using the scatterplot along with the standard deviation ratio indices and other statistics for measuring the quality of Angoff standard-setting data. © 2019 by the National Council on Measurement in Education",A Method for Detecting Regression of Hard and Easy Item Angoff Ratings,"One common phenomenon in Angoff standard setting is that panelists regress their ratings in toward the middle of the probability scale. This study describes two indices based on taking ratios of standard deviations that can be utilized with a scatterplot of item ratings versus expected probabilities of success to identify whether ratings are regressed in toward the middle of the probability scale. Results from a simulation study show that the standard deviation ratio indices can successfully detect ratings for hard and easy items that are regressed in toward the middle of the probability scale in Angoff standard-setting data, where previously proposed indices often do not work as well to detect these effects. Results from a real data set show that, while virtually all raters improve from Round 1 to Round 2 as measured by previously developed indices, the standard deviation ratios in conjunction with a scatterplot of item ratings versus expected probabilities of success can identify individuals who may still be regressing their ratings in toward the middle of the probability scale even after receiving feedback. The authors suggest using the scatterplot along with the standard deviation ratio indices and other statistics for measuring the quality of Angoff standard-setting data. © 2019 by the National Council on Measurement in Education","['common', 'phenomenon', 'Angoff', 'standard', 'setting', 'panelist', 'regress', 'rating', 'middle', 'probability', 'scale', 'study', 'describe', 'index', 'base', 'ratio', 'standard', 'deviation', 'utilize', 'scatterplot', 'item', 'rating', 'versus', 'expect', 'probability', 'success', 'identify', 'rating', 'regress', 'middle', 'probability', 'scale', 'result', 'simulation', 'study', 'standard', 'deviation', 'ratio', 'index', 'successfully', 'detect', 'rating', 'hard', 'easy', 'item', 'regress', 'middle', 'probability', 'scale', 'Angoff', 'standardsette', 'datum', 'previously', 'propose', 'index', 'work', 'detect', 'effect', 'result', 'real', 'datum', 'set', 'virtually', 'rater', 'improve', 'Round', '1', 'Round', '2', 'measure', 'previously', 'develop', 'index', 'standard', 'deviation', 'ratio', 'conjunction', 'scatterplot', 'item', 'rating', 'versus', 'expect', 'probability', 'success', 'identify', 'individual', 'regress', 'rating', 'middle', 'probability', 'scale', 'receive', 'feedback', 'author', 'suggest', 'scatterplot', 'standard', 'deviation', 'ratio', 'index', 'statistic', 'measure', 'quality', 'Angoff', 'standardsette', 'datum', '©', '2019', 'National', 'Council']","['Method', 'detect', 'Regression', 'Hard', 'Easy', 'Item', 'Angoff', 'Ratings']",common phenomenon Angoff standard setting panelist regress rating middle probability scale study describe index base ratio standard deviation utilize scatterplot item rating versus expect probability success identify rating regress middle probability scale result simulation study standard deviation ratio index successfully detect rating hard easy item regress middle probability scale Angoff standardsette datum previously propose index work detect effect result real datum set virtually rater improve Round 1 Round 2 measure previously develop index standard deviation ratio conjunction scatterplot item rating versus expect probability success identify individual regress rating middle probability scale receive feedback author suggest scatterplot standard deviation ratio index statistic measure quality Angoff standardsette datum © 2019 National Council,Method detect Regression Hard Easy Item Angoff Ratings,0.029504778480609284,0.029537779923189234,0.029204029441178014,0.8821245518753866,0.02962886027963679,0.026380876783917893,0.002758235475534529,0.009460116262200573,0.002411813369620356,0.06356404460943173
Fay D.M.; Levy R.; Mehta V.,Investigating Psychometric Isomorphism for Traditional and Performance-Based Assessment,2018,55,"A common practice in educational assessment is to construct multiple forms of an assessment that consists of tasks with similar psychometric properties. This study utilizes a Bayesian multilevel item response model and descriptive graphical representations to evaluate the psychometric similarity of variations of the same task. These approaches for describing the psychometric similarity of task variants were applied to two different types of assessments (one traditional assessment and one performance-based assessment) with markedly different response formats. Due to the general nature of the multilevel item response model and graphical approaches that were utilized, the methods used for this work can readily be applied to many assessment contexts for the purposes of evaluating the psychometric similarity of tasks. Copyright © 2018 by the National Council on Measurement in Education",Investigating Psychometric Isomorphism for Traditional and Performance-Based Assessment,"A common practice in educational assessment is to construct multiple forms of an assessment that consists of tasks with similar psychometric properties. This study utilizes a Bayesian multilevel item response model and descriptive graphical representations to evaluate the psychometric similarity of variations of the same task. These approaches for describing the psychometric similarity of task variants were applied to two different types of assessments (one traditional assessment and one performance-based assessment) with markedly different response formats. Due to the general nature of the multilevel item response model and graphical approaches that were utilized, the methods used for this work can readily be applied to many assessment contexts for the purposes of evaluating the psychometric similarity of tasks. Copyright © 2018 by the National Council on Measurement in Education","['common', 'practice', 'educational', 'assessment', 'construct', 'multiple', 'form', 'assessment', 'consist', 'task', 'similar', 'psychometric', 'property', 'study', 'utilize', 'bayesian', 'multilevel', 'item', 'response', 'descriptive', 'graphical', 'representation', 'evaluate', 'psychometric', 'similarity', 'variation', 'task', 'approach', 'describe', 'psychometric', 'similarity', 'task', 'variant', 'apply', 'different', 'type', 'assessment', 'traditional', 'assessment', 'performancebase', 'assessment', 'markedly', 'different', 'response', 'format', 'general', 'nature', 'multilevel', 'item', 'response', 'graphical', 'approach', 'utilize', 'method', 'work', 'readily', 'apply', 'assessment', 'context', 'purpose', 'evaluate', 'psychometric', 'similarity', 'task', 'Copyright', '©', '2018', 'National', 'Council']","['investigate', 'Psychometric', 'Isomorphism', 'Traditional', 'PerformanceBased', 'Assessment']",common practice educational assessment construct multiple form assessment consist task similar psychometric property study utilize bayesian multilevel item response descriptive graphical representation evaluate psychometric similarity variation task approach describe psychometric similarity task variant apply different type assessment traditional assessment performancebase assessment markedly different response format general nature multilevel item response graphical approach utilize method work readily apply assessment context purpose evaluate psychometric similarity task Copyright © 2018 National Council,investigate Psychometric Isomorphism Traditional PerformanceBased Assessment,0.031235293683197305,0.8753832276694671,0.030946748318502977,0.031202140834564063,0.031232589494268524,0.017807365614590565,0.0,0.09377894934721075,0.0,0.0
Lin C.-K.; Zhang J.,Detecting Nonadditivity in Single-Facet Generalizability Theory Applications: Tukey's Test,2018,55,"Under the generalizability-theory (G-theory) framework, the estimation precision of variance components (VCs) is of significant importance in that they serve as the foundation of estimating reliability. Zhang and Lin advanced the discussion of nonadditivity in data from a theoretical perspective and showed the adverse effects of nonadditivity on the estimation precision of VCs in 2016. Contributing to this line of research, the current article directs the discussion of nonadditivity from a theoretical perspective to a practical application and highlights the importance of detecting nonadditivity in G-theory applications. To this end, Tukey's test for nonadditivity is the only method to date that is appropriate for the typical single-facet G-theory design, in which a single observation is made per element within a facet. The current article evaluates the Type I and Type II error rates of Tukey's test. Results show that Tukey's test is satisfactory in controlling for falsely detecting nonadditivity when the data are actually additive and that it is generally powerful in detecting nonadditivity when it exists. Finally, the article demonstrates an application of Tukey's test in detecting nonadditivity in a judgmental study of educational standards and shows how Tukey's test results can be used to correct imprecision in the estimated VC in the presence of nonadditivity. Copyright © 2018 by the National Council on Measurement in Education",Detecting Nonadditivity in Single-Facet Generalizability Theory Applications: Tukey's Test,"Under the generalizability-theory (G-theory) framework, the estimation precision of variance components (VCs) is of significant importance in that they serve as the foundation of estimating reliability. Zhang and Lin advanced the discussion of nonadditivity in data from a theoretical perspective and showed the adverse effects of nonadditivity on the estimation precision of VCs in 2016. Contributing to this line of research, the current article directs the discussion of nonadditivity from a theoretical perspective to a practical application and highlights the importance of detecting nonadditivity in G-theory applications. To this end, Tukey's test for nonadditivity is the only method to date that is appropriate for the typical single-facet G-theory design, in which a single observation is made per element within a facet. The current article evaluates the Type I and Type II error rates of Tukey's test. Results show that Tukey's test is satisfactory in controlling for falsely detecting nonadditivity when the data are actually additive and that it is generally powerful in detecting nonadditivity when it exists. Finally, the article demonstrates an application of Tukey's test in detecting nonadditivity in a judgmental study of educational standards and shows how Tukey's test results can be used to correct imprecision in the estimated VC in the presence of nonadditivity. Copyright © 2018 by the National Council on Measurement in Education","['generalizabilitytheory', 'Gtheory', 'framework', 'estimation', 'precision', 'variance', 'component', 'vcs', 'significant', 'importance', 'serve', 'foundation', 'estimate', 'reliability', 'Zhang', 'Lin', 'advance', 'discussion', 'nonadditivity', 'datum', 'theoretical', 'perspective', 'adverse', 'effect', 'nonadditivity', 'estimation', 'precision', 'vcs', '2016', 'contribute', 'line', 'research', 'current', 'article', 'direct', 'discussion', 'nonadditivity', 'theoretical', 'perspective', 'practical', 'application', 'highlight', 'importance', 'detect', 'nonadditivity', 'Gtheory', 'application', 'end', 'Tukeys', 'test', 'nonadditivity', 'method', 'date', 'appropriate', 'typical', 'singlefacet', 'Gtheory', 'design', 'single', 'observation', 'element', 'facet', 'current', 'article', 'evaluate', 'Type', 'I', 'Type', 'II', 'error', 'rate', 'Tukeys', 'test', 'result', 'Tukeys', 'test', 'satisfactory', 'control', 'falsely', 'detect', 'nonadditivity', 'datum', 'actually', 'additive', 'generally', 'powerful', 'detect', 'nonadditivity', 'exist', 'finally', 'article', 'demonstrate', 'application', 'Tukeys', 'test', 'detect', 'nonadditivity', 'judgmental', 'study', 'educational', 'standard', 'Tukeys', 'test', 'result', 'correct', 'imprecision', 'estimate', 'VC', 'presence', 'nonadditivity', 'Copyright', '©', '2018', 'National', 'Council']","['detect', 'Nonadditivity', 'SingleFacet', 'Generalizability', 'Theory', 'Applications', 'Tukeys', 'Test']",generalizabilitytheory Gtheory framework estimation precision variance component vcs significant importance serve foundation estimate reliability Zhang Lin advance discussion nonadditivity datum theoretical perspective adverse effect nonadditivity estimation precision vcs 2016 contribute line research current article direct discussion nonadditivity theoretical perspective practical application highlight importance detect nonadditivity Gtheory application end Tukeys test nonadditivity method date appropriate typical singlefacet Gtheory design single observation element facet current article evaluate Type I Type II error rate Tukeys test result Tukeys test satisfactory control falsely detect nonadditivity datum actually additive generally powerful detect nonadditivity exist finally article demonstrate application Tukeys test detect nonadditivity judgmental study educational standard Tukeys test result correct imprecision estimate VC presence nonadditivity Copyright © 2018 National Council,detect Nonadditivity SingleFacet Generalizability Theory Applications Tukeys Test,0.029978050678092564,0.0297566736325376,0.02996778339576434,0.8805087448496212,0.029788747443984336,0.027177862919633984,0.003464114742426695,0.014672002185667197,0.004442603271760473,0.0032028151078870412
Barrett M.D.; van der Linden W.J.,Optimal Linking Design for Response Model Parameters,2017,54,"Linking functions adjust for differences between identifiability restrictions used in different instances of the estimation of item response model parameters. These adjustments are necessary when results from those instances are to be compared. As linking functions are derived from estimated item response model parameters, parameter estimation error automatically propagates into linking error. This article explores an optimal linking design approach in which mixed-integer programming is used to select linking items to minimize linking error. Results indicate that the method holds promise for selection of linking items. Copyright © 2017 by the National Council on Measurement in Education",Optimal Linking Design for Response Model Parameters,"Linking functions adjust for differences between identifiability restrictions used in different instances of the estimation of item response model parameters. These adjustments are necessary when results from those instances are to be compared. As linking functions are derived from estimated item response model parameters, parameter estimation error automatically propagates into linking error. This article explores an optimal linking design approach in which mixed-integer programming is used to select linking items to minimize linking error. Results indicate that the method holds promise for selection of linking items. Copyright © 2017 by the National Council on Measurement in Education","['link', 'function', 'adjust', 'difference', 'identifiability', 'restriction', 'different', 'instance', 'estimation', 'item', 'response', 'parameter', 'adjustment', 'necessary', 'result', 'instance', 'compare', 'link', 'function', 'derive', 'estimate', 'item', 'response', 'parameters', 'parameter', 'estimation', 'error', 'automatically', 'propagate', 'link', 'error', 'article', 'explore', 'optimal', 'linking', 'design', 'approach', 'mixedinteger', 'programming', 'select', 'link', 'item', 'minimize', 'link', 'error', 'result', 'indicate', 'method', 'hold', 'promise', 'selection', 'link', 'item', 'copyright', '©', '2017', 'National', 'Council']","['optimal', 'Linking', 'Design', 'Response', 'Parameters']",link function adjust difference identifiability restriction different instance estimation item response parameter adjustment necessary result instance compare link function derive estimate item response parameters parameter estimation error automatically propagate link error article explore optimal linking design approach mixedinteger programming select link item minimize link error result indicate method hold promise selection link item copyright © 2017 National Council,optimal Linking Design Response Parameters,0.03294193027472104,0.8685225822885401,0.03279995253912731,0.03275867558692488,0.03297685931068676,0.06712069322421112,0.0,0.0,0.008583903114167016,0.0
Harik P.; Clauser B.E.; Grabovsky I.; Baldwin P.; Margolis M.J.; Bucak D.; Jodoin M.; Walsh W.; Haist S.,A Comparison of Experimental and Observational Approaches to Assessing the Effects of Time Constraints in a Medical Licensing Examination,2018,55,"Test administrators are appropriately concerned about the potential for time constraints to impact the validity of score interpretations; psychometric efforts to evaluate the impact of speededness date back more than half a century. The widespread move to computerized test delivery has led to the development of new approaches to evaluating how examinees use testing time and to new metrics designed to provide evidence about the extent to which time limits impact performance. Much of the existing research is based on these types of observational metrics; relatively few studies use randomized experiments to evaluate the impact time limits on scores. Of those studies that do report on randomized experiments, none directly compare the experimental results to evidence from observational metrics to evaluate the extent to which these metrics are able to sensitively identify conditions in which time constraints actually impact scores. The present study provides such evidence based on data from a medical licensing examination. The results indicate that these observational metrics are useful but provide an imprecise evaluation of the impact of time constraints on test performance. Copyright © 2018 by the National Council on Measurement in Education",A Comparison of Experimental and Observational Approaches to Assessing the Effects of Time Constraints in a Medical Licensing Examination,"Test administrators are appropriately concerned about the potential for time constraints to impact the validity of score interpretations; psychometric efforts to evaluate the impact of speededness date back more than half a century. The widespread move to computerized test delivery has led to the development of new approaches to evaluating how examinees use testing time and to new metrics designed to provide evidence about the extent to which time limits impact performance. Much of the existing research is based on these types of observational metrics; relatively few studies use randomized experiments to evaluate the impact time limits on scores. Of those studies that do report on randomized experiments, none directly compare the experimental results to evidence from observational metrics to evaluate the extent to which these metrics are able to sensitively identify conditions in which time constraints actually impact scores. The present study provides such evidence based on data from a medical licensing examination. The results indicate that these observational metrics are useful but provide an imprecise evaluation of the impact of time constraints on test performance. Copyright © 2018 by the National Council on Measurement in Education","['test', 'administrator', 'appropriately', 'concerned', 'potential', 'time', 'constraint', 'impact', 'validity', 'score', 'interpretation', 'psychometric', 'effort', 'evaluate', 'impact', 'speededness', 'date', 'half', 'century', 'widespread', 'computerized', 'test', 'delivery', 'lead', 'development', 'new', 'approach', 'evaluate', 'examinee', 'testing', 'time', 'new', 'metric', 'design', 'provide', 'evidence', 'extent', 'time', 'limit', 'impact', 'performance', 'exist', 'research', 'base', 'type', 'observational', 'metric', 'relatively', 'study', 'randomize', 'experiment', 'evaluate', 'impact', 'time', 'limit', 'score', 'study', 'report', 'randomize', 'experiment', 'directly', 'compare', 'experimental', 'result', 'evidence', 'observational', 'metric', 'evaluate', 'extent', 'metric', 'able', 'sensitively', 'identify', 'condition', 'time', 'constraint', 'actually', 'impact', 'score', 'present', 'study', 'provide', 'evidence', 'base', 'datum', 'medical', 'licensing', 'examination', 'result', 'indicate', 'observational', 'metric', 'useful', 'provide', 'imprecise', 'evaluation', 'impact', 'time', 'constraint', 'test', 'performance', 'Copyright', '©', '2018', 'National', 'Council']","['Comparison', 'Experimental', 'Observational', 'Approaches', 'assess', 'effect', 'Time', 'Constraints', 'Medical', 'Licensing', 'Examination']",test administrator appropriately concerned potential time constraint impact validity score interpretation psychometric effort evaluate impact speededness date half century widespread computerized test delivery lead development new approach evaluate examinee testing time new metric design provide evidence extent time limit impact performance exist research base type observational metric relatively study randomize experiment evaluate impact time limit score study report randomize experiment directly compare experimental result evidence observational metric evaluate extent metric able sensitively identify condition time constraint actually impact score present study provide evidence base datum medical licensing examination result indicate observational metric useful provide imprecise evaluation impact time constraint test performance Copyright © 2018 National Council,Comparison Experimental Observational Approaches assess effect Time Constraints Medical Licensing Examination,0.8909895878136544,0.02728817838350607,0.02729840820218965,0.02717727523882859,0.027246550361821234,0.02401167030035662,0.01043086245114452,0.056947969997715266,0.0025851214781779344,0.002805167556578837
Drabinová A.; Martinková P.,Detection of Differential Item Functioning with Nonlinear Regression: A Non-IRT Approach Accounting for Guessing,2017,54,"In this article we present a general approach not relying on item response theory models (non-IRT) to detect differential item functioning (DIF) in dichotomous items with presence of guessing. The proposed nonlinear regression (NLR) procedure for DIF detection is an extension of method based on logistic regression. As a non-IRT approach, NLR can be seen as a proxy of detection based on the three-parameter IRT model which is a standard tool in the study field. Hence, NLR fills a logical gap in DIF detection methodology and as such is important for educational purposes. Moreover, the advantages of the NLR procedure as well as comparison to other commonly used methods are demonstrated in a simulation study. A real data analysis is offered to demonstrate practical use of the method. Copyright © 2017 by the National Council on Measurement in Education",Detection of Differential Item Functioning with Nonlinear Regression: A Non-IRT Approach Accounting for Guessing,"In this article we present a general approach not relying on item response theory models (non-IRT) to detect differential item functioning (DIF) in dichotomous items with presence of guessing. The proposed nonlinear regression (NLR) procedure for DIF detection is an extension of method based on logistic regression. As a non-IRT approach, NLR can be seen as a proxy of detection based on the three-parameter IRT model which is a standard tool in the study field. Hence, NLR fills a logical gap in DIF detection methodology and as such is important for educational purposes. Moreover, the advantages of the NLR procedure as well as comparison to other commonly used methods are demonstrated in a simulation study. A real data analysis is offered to demonstrate practical use of the method. Copyright © 2017 by the National Council on Measurement in Education","['article', 'present', 'general', 'approach', 'rely', 'item', 'response', 'theory', 'nonIRT', 'detect', 'differential', 'item', 'function', 'DIF', 'dichotomous', 'item', 'presence', 'guess', 'propose', 'nonlinear', 'regression', 'NLR', 'procedure', 'dif', 'detection', 'extension', 'method', 'base', 'logistic', 'regression', 'nonIRT', 'approach', 'NLR', 'proxy', 'detection', 'base', 'threeparameter', 'IRT', 'standard', 'tool', 'study', 'field', 'NLR', 'fill', 'logical', 'gap', 'DIF', 'detection', 'methodology', 'important', 'educational', 'purpose', 'advantage', 'NLR', 'procedure', 'comparison', 'commonly', 'method', 'demonstrate', 'simulation', 'study', 'real', 'datum', 'analysis', 'offer', 'demonstrate', 'practical', 'method', 'Copyright', '©', '2017', 'National', 'Council']","['detection', 'Differential', 'Item', 'Functioning', 'Nonlinear', 'Regression', 'NonIRT', 'Approach', 'Accounting', 'Guessing']",article present general approach rely item response theory nonIRT detect differential item function DIF dichotomous item presence guess propose nonlinear regression NLR procedure dif detection extension method base logistic regression nonIRT approach NLR proxy detection base threeparameter IRT standard tool study field NLR fill logical gap DIF detection methodology important educational purpose advantage NLR procedure comparison commonly method demonstrate simulation study real datum analysis offer demonstrate practical method Copyright © 2017 National Council,detection Differential Item Functioning Nonlinear Regression NonIRT Approach Accounting Guessing,0.029874157506140652,0.029591416008079418,0.8803820491776928,0.02978391134084848,0.030368465967238606,0.07689758238210197,0.0,0.0017626235840627078,0.0,0.0
Bergner Y.; Choi I.; Castellano K.E.,Item Response Models for Multiple Attempts With Incomplete Data,2019,56,"Allowance for multiple chances to answer constructed response questions is a prevalent feature in computer-based homework and exams. We consider the use of item response theory in the estimation of item characteristics and student ability when multiple attempts are allowed but no explicit penalty is deducted for extra tries. This is common practice in online formative assessments, where the number of attempts is often unlimited. In these environments, some students may not always answer-until-correct, but may rather terminate a response process after one or more incorrect tries. We contrast the cases of graded and sequential item response models, both unidimensional models which do not explicitly account for factors other than ability. These approaches differ not only in terms of log-odds assumptions but, importantly, in terms of handling incomplete data. We explore the consequences of model misspecification through a simulation study and with four online homework data sets. Our results suggest that model selection is insensitive for complete data, but quite sensitive to whether missing responses are regarded as informative (of inability) or not (e.g., missing at random). Under realistic conditions, a sequential model with similar parametric degrees of freedom to a graded model can account for more response patterns and outperforms the latter in terms of model fit. © 2019 by the National Council on Measurement in Education",Item Response Models for Multiple Attempts With Incomplete Data,"Allowance for multiple chances to answer constructed response questions is a prevalent feature in computer-based homework and exams. We consider the use of item response theory in the estimation of item characteristics and student ability when multiple attempts are allowed but no explicit penalty is deducted for extra tries. This is common practice in online formative assessments, where the number of attempts is often unlimited. In these environments, some students may not always answer-until-correct, but may rather terminate a response process after one or more incorrect tries. We contrast the cases of graded and sequential item response models, both unidimensional models which do not explicitly account for factors other than ability. These approaches differ not only in terms of log-odds assumptions but, importantly, in terms of handling incomplete data. We explore the consequences of model misspecification through a simulation study and with four online homework data sets. Our results suggest that model selection is insensitive for complete data, but quite sensitive to whether missing responses are regarded as informative (of inability) or not (e.g., missing at random). Under realistic conditions, a sequential model with similar parametric degrees of freedom to a graded model can account for more response patterns and outperforms the latter in terms of model fit. © 2019 by the National Council on Measurement in Education","['allowance', 'multiple', 'chance', 'answer', 'construct', 'response', 'question', 'prevalent', 'feature', 'computerbase', 'homework', 'exam', 'consider', 'item', 'response', 'theory', 'estimation', 'item', 'characteristic', 'student', 'ability', 'multiple', 'attempt', 'allow', 'explicit', 'penalty', 'deduct', 'extra', 'try', 'common', 'practice', 'online', 'formative', 'assessment', 'number', 'attempt', 'unlimited', 'environment', 'student', 'answeruntilcorrect', 'terminate', 'response', 'process', 'incorrect', 'try', 'contrast', 'case', 'grade', 'sequential', 'item', 'response', 'unidimensional', 'explicitly', 'account', 'factor', 'ability', 'approach', 'differ', 'term', 'logodds', 'assumption', 'importantly', 'term', 'handle', 'incomplete', 'datum', 'explore', 'consequence', 'misspecification', 'simulation', 'study', 'online', 'homework', 'datum', 'set', 'result', 'suggest', 'selection', 'insensitive', 'complete', 'datum', 'sensitive', 'miss', 'response', 'regard', 'informative', 'inability', 'eg', 'miss', 'random', 'realistic', 'condition', 'sequential', 'similar', 'parametric', 'degree', 'freedom', 'grade', 'account', 'response', 'pattern', 'outperform', 'term', 'fit', '©', '2019', 'National', 'Council']","['Item', 'Response', 'Models', 'Multiple', 'Attempts', 'Incomplete', 'Data']",allowance multiple chance answer construct response question prevalent feature computerbase homework exam consider item response theory estimation item characteristic student ability multiple attempt allow explicit penalty deduct extra try common practice online formative assessment number attempt unlimited environment student answeruntilcorrect terminate response process incorrect try contrast case grade sequential item response unidimensional explicitly account factor ability approach differ term logodds assumption importantly term handle incomplete datum explore consequence misspecification simulation study online homework datum set result suggest selection insensitive complete datum sensitive miss response regard informative inability eg miss random realistic condition sequential similar parametric degree freedom grade account response pattern outperform term fit © 2019 National Council,Item Response Models Multiple Attempts Incomplete Data,0.9127746403698341,0.021684771910861135,0.02180369153658624,0.021854672774083976,0.02188222340863439,0.040875082994287985,0.0,0.051218836255280654,0.0,0.001966414474764388
Bolt D.M.; Kim J.-S.,Parameter Invariance and Skill Attribute Continuity in the DINA Model,2018,55,"Cognitive diagnosis models (CDMs) typically assume skill attributes with discrete (often binary) levels of skill mastery, making the existence of skill continuity an anticipated form of model misspecification. In this article, misspecification due to skill continuity is argued to be of particular concern for several CDM applications due to the lack of invariance it yields in CDM skill attribute metrics, or what in this article are viewed as the “thresholds” applied to continuous attributes in distinguishing masters from nonmasters. Using the deterministic input noisy and (DINA) model as an illustration, the effects observed in real data are found to be systematic, with higher thresholds for mastery tending to emerge in higher ability populations. The results are shown to have significant implications for applications of CDMs that rely heavily upon the parameter invariance properties of the models, including, for example, applications toward the measurement of growth and differential item functioning analyses. Copyright © 2018 by the National Council on Measurement in Education",Parameter Invariance and Skill Attribute Continuity in the DINA Model,"Cognitive diagnosis models (CDMs) typically assume skill attributes with discrete (often binary) levels of skill mastery, making the existence of skill continuity an anticipated form of model misspecification. In this article, misspecification due to skill continuity is argued to be of particular concern for several CDM applications due to the lack of invariance it yields in CDM skill attribute metrics, or what in this article are viewed as the “thresholds” applied to continuous attributes in distinguishing masters from nonmasters. Using the deterministic input noisy and (DINA) model as an illustration, the effects observed in real data are found to be systematic, with higher thresholds for mastery tending to emerge in higher ability populations. The results are shown to have significant implications for applications of CDMs that rely heavily upon the parameter invariance properties of the models, including, for example, applications toward the measurement of growth and differential item functioning analyses. Copyright © 2018 by the National Council on Measurement in Education","['cognitive', 'diagnosis', 'CDMs', 'typically', 'assume', 'skill', 'attribute', 'discrete', 'binary', 'level', 'skill', 'mastery', 'existence', 'skill', 'continuity', 'anticipate', 'form', 'misspecification', 'article', 'misspecification', 'skill', 'continuity', 'argue', 'particular', 'concern', 'CDM', 'application', 'lack', 'invariance', 'yield', 'CDM', 'skill', 'attribute', 'metric', 'article', 'view', '""', 'threshold', '""', 'apply', 'continuous', 'attribute', 'distinguish', 'master', 'nonmaster', 'deterministic', 'input', 'noisy', 'DINA', 'illustration', 'effect', 'observe', 'real', 'datum', 'find', 'systematic', 'high', 'threshold', 'mastery', 'tend', 'emerge', 'high', 'ability', 'population', 'result', 'significant', 'implication', 'application', 'cdm', 'rely', 'heavily', 'parameter', 'invariance', 'property', 'include', 'example', 'application', 'growth', 'differential', 'item', 'functioning', 'analyse', 'copyright', '©', '2018', 'National', 'Council']","['Parameter', 'Invariance', 'Skill', 'Attribute', 'Continuity', 'DINA']","cognitive diagnosis CDMs typically assume skill attribute discrete binary level skill mastery existence skill continuity anticipate form misspecification article misspecification skill continuity argue particular concern CDM application lack invariance yield CDM skill attribute metric article view "" threshold "" apply continuous attribute distinguish master nonmaster deterministic input noisy DINA illustration effect observe real datum find systematic high threshold mastery tend emerge high ability population result significant implication application cdm rely heavily parameter invariance property include example application growth differential item functioning analyse copyright © 2018 National Council",Parameter Invariance Skill Attribute Continuity DINA,0.025400912919878737,0.02540949935630003,0.02535101779524721,0.8980707953190576,0.02576777460951635,0.03269466481187822,0.0,0.03886878258902721,0.0,0.0014388261588793867
Huang H.-Y.,Multilevel Cognitive Diagnosis Models for Assessing Changes in Latent Attributes,2017,54,"Cognitive diagnosis models (CDMs) have been developed to evaluate the mastery status of individuals with respect to a set of defined attributes or skills that are measured through testing. When individuals are repeatedly administered a cognitive diagnosis test, a new class of multilevel CDMs is required to assess the changes in their attributes and simultaneously estimate the model parameters from the different measurements. In this study, the most general CDM of the generalized deterministic input, noisy “and” gate (G-DINA) model was extended to a multilevel higher order CDM by embedding a multilevel structure into higher order latent traits. A series of simulations based on diverse factors was conducted to assess the quality of the parameter estimation. The results demonstrate that the model parameters can be recovered fairly well and attribute mastery can be precisely estimated if the sample size is large and the test is sufficiently long. The range of the location parameters had opposing effects on the recovery of the item and person parameters. Ignoring the multilevel structure in the data by fitting a single-level G-DINA model decreased the attribute classification accuracy and the precision of latent trait estimation. The number of measurement occasions had a substantial impact on latent trait estimation. Satisfactory model and person parameter recoveries could be achieved even when assumptions of the measurement invariance of the model parameters over time were violated. A longitudinal basic ability assessment is outlined to demonstrate the application of the new models. Copyright © 2017 by the National Council on Measurement in Education",Multilevel Cognitive Diagnosis Models for Assessing Changes in Latent Attributes,"Cognitive diagnosis models (CDMs) have been developed to evaluate the mastery status of individuals with respect to a set of defined attributes or skills that are measured through testing. When individuals are repeatedly administered a cognitive diagnosis test, a new class of multilevel CDMs is required to assess the changes in their attributes and simultaneously estimate the model parameters from the different measurements. In this study, the most general CDM of the generalized deterministic input, noisy “and” gate (G-DINA) model was extended to a multilevel higher order CDM by embedding a multilevel structure into higher order latent traits. A series of simulations based on diverse factors was conducted to assess the quality of the parameter estimation. The results demonstrate that the model parameters can be recovered fairly well and attribute mastery can be precisely estimated if the sample size is large and the test is sufficiently long. The range of the location parameters had opposing effects on the recovery of the item and person parameters. Ignoring the multilevel structure in the data by fitting a single-level G-DINA model decreased the attribute classification accuracy and the precision of latent trait estimation. The number of measurement occasions had a substantial impact on latent trait estimation. Satisfactory model and person parameter recoveries could be achieved even when assumptions of the measurement invariance of the model parameters over time were violated. A longitudinal basic ability assessment is outlined to demonstrate the application of the new models. Copyright © 2017 by the National Council on Measurement in Education","['cognitive', 'diagnosis', 'CDMs', 'develop', 'evaluate', 'mastery', 'status', 'individual', 'respect', 'set', 'define', 'attribute', 'skill', 'measure', 'testing', 'individual', 'repeatedly', 'administer', 'cognitive', 'diagnosis', 'test', 'new', 'class', 'multilevel', 'CDMs', 'require', 'assess', 'change', 'attribute', 'simultaneously', 'estimate', 'parameter', 'different', 'study', 'general', 'CDM', 'generalized', 'deterministic', 'input', 'noisy', '""', '""', 'gate', 'GDINA', 'extend', 'multilevel', 'high', 'order', 'CDM', 'embed', 'multilevel', 'structure', 'high', 'order', 'latent', 'trait', 'series', 'simulation', 'base', 'diverse', 'factor', 'conduct', 'assess', 'quality', 'parameter', 'estimation', 'result', 'demonstrate', 'parameter', 'recover', 'fairly', 'attribute', 'mastery', 'precisely', 'estimate', 'sample', 'size', 'large', 'test', 'sufficiently', 'long', 'range', 'location', 'parameter', 'oppose', 'effect', 'recovery', 'item', 'person', 'parameter', 'ignore', 'multilevel', 'structure', 'datum', 'fit', 'singlelevel', 'GDINA', 'decrease', 'attribute', 'classification', 'accuracy', 'precision', 'latent', 'trait', 'estimation', 'number', 'occasion', 'substantial', 'impact', 'latent', 'trait', 'estimation', 'Satisfactory', 'person', 'parameter', 'recovery', 'achieve', 'assumption', 'invariance', 'parameter', 'time', 'violate', 'longitudinal', 'basic', 'ability', 'assessment', 'outline', 'demonstrate', 'application', 'new', 'Copyright', '©', '2017', 'National', 'Council']","['Multilevel', 'Cognitive', 'Diagnosis', 'Models', 'assess', 'Changes', 'Latent', 'Attributes']","cognitive diagnosis CDMs develop evaluate mastery status individual respect set define attribute skill measure testing individual repeatedly administer cognitive diagnosis test new class multilevel CDMs require assess change attribute simultaneously estimate parameter different study general CDM generalized deterministic input noisy "" "" gate GDINA extend multilevel high order CDM embed multilevel structure high order latent trait series simulation base diverse factor conduct assess quality parameter estimation result demonstrate parameter recover fairly attribute mastery precisely estimate sample size large test sufficiently long range location parameter oppose effect recovery item person parameter ignore multilevel structure datum fit singlelevel GDINA decrease attribute classification accuracy precision latent trait estimation number occasion substantial impact latent trait estimation Satisfactory person parameter recovery achieve assumption invariance parameter time violate longitudinal basic ability assessment outline demonstrate application new Copyright © 2017 National Council",Multilevel Cognitive Diagnosis Models assess Changes Latent Attributes,0.02183858019744051,0.02158861000121933,0.02145112873038339,0.9122336778442569,0.02288800322669989,0.08378449323104487,0.0,0.006726505012698253,0.0,0.01781371855343864
Liu C.-W.; Wang W.-C.,Parameter Estimation in Rasch Models for Examinee-Selected Items,2017,54,"The examinee-selected-item (ESI) design, in which examinees are required to respond to a fixed number of items in a given set of items (e.g., choose one item to respond from a pair of items), always yields incomplete data (i.e., only the selected items are answered and the others have missing data) that are likely nonignorable. Therefore, using standard item response theory models, which assume ignorable missing data, can yield biased parameter estimates so that examinees taking different sets of items to answer cannot be compared. To solve this fundamental problem, in this study the researchers utilized the specific objectivity of Rasch models by adopting the conditional maximum likelihood estimation (CMLE) and pairwise estimation (PE) methods to analyze ESI data, and conducted a series of simulations to demonstrate the advantages of the CMLE and PE methods over traditional estimation methods in recovering item parameters in ESI data. An empirical data set obtained from an experiment on the ESI design was analyzed to illustrate the implications and applications of the proposed approach to ESI data. Copyright © 2017 by the National Council on Measurement in Education",Parameter Estimation in Rasch Models for Examinee-Selected Items,"The examinee-selected-item (ESI) design, in which examinees are required to respond to a fixed number of items in a given set of items (e.g., choose one item to respond from a pair of items), always yields incomplete data (i.e., only the selected items are answered and the others have missing data) that are likely nonignorable. Therefore, using standard item response theory models, which assume ignorable missing data, can yield biased parameter estimates so that examinees taking different sets of items to answer cannot be compared. To solve this fundamental problem, in this study the researchers utilized the specific objectivity of Rasch models by adopting the conditional maximum likelihood estimation (CMLE) and pairwise estimation (PE) methods to analyze ESI data, and conducted a series of simulations to demonstrate the advantages of the CMLE and PE methods over traditional estimation methods in recovering item parameters in ESI data. An empirical data set obtained from an experiment on the ESI design was analyzed to illustrate the implications and applications of the proposed approach to ESI data. Copyright © 2017 by the National Council on Measurement in Education","['examineeselecteditem', 'ESI', 'design', 'examinee', 'require', 'respond', 'fix', 'number', 'item', 'set', 'item', 'eg', 'choose', 'item', 'respond', 'pair', 'item', 'yield', 'incomplete', 'datum', 'ie', 'select', 'item', 'answer', 'miss', 'datum', 'likely', 'nonignorable', 'standard', 'item', 'response', 'theory', 'assume', 'ignorable', 'miss', 'datum', 'yield', 'biased', 'parameter', 'estimate', 'examine', 'different', 'set', 'item', 'answer', 'compare', 'solve', 'fundamental', 'problem', 'study', 'researcher', 'utilize', 'specific', 'objectivity', 'Rasch', 'adopt', 'conditional', 'maximum', 'likelihood', 'estimation', 'CMLE', 'pairwise', 'estimation', 'PE', 'method', 'analyze', 'ESI', 'datum', 'conduct', 'series', 'simulation', 'demonstrate', 'advantage', 'CMLE', 'PE', 'method', 'traditional', 'estimation', 'method', 'recover', 'item', 'parameter', 'ESI', 'datum', 'empirical', 'data', 'set', 'obtain', 'experiment', 'ESI', 'design', 'analyze', 'illustrate', 'implication', 'application', 'propose', 'approach', 'ESI', 'datum', 'Copyright', '©', '2017', 'National', 'Council']","['Parameter', 'Estimation', 'Rasch', 'Models', 'ExamineeSelected', 'Items']",examineeselecteditem ESI design examinee require respond fix number item set item eg choose item respond pair item yield incomplete datum ie select item answer miss datum likely nonignorable standard item response theory assume ignorable miss datum yield biased parameter estimate examine different set item answer compare solve fundamental problem study researcher utilize specific objectivity Rasch adopt conditional maximum likelihood estimation CMLE pairwise estimation PE method analyze ESI datum conduct series simulation demonstrate advantage CMLE PE method traditional estimation method recover item parameter ESI datum empirical data set obtain experiment ESI design analyze illustrate implication application propose approach ESI datum Copyright © 2017 National Council,Parameter Estimation Rasch Models ExamineeSelected Items,0.026936036970009118,0.02688688601430157,0.02680749157236793,0.8925094211249357,0.026860164318385674,0.07682632758092918,0.0,0.0032229555694141053,0.0028889278551725737,0.0021582857913190626
Sinharay S.,Assessment of Person Fit Using Resampling-Based Approaches,2016,53,"De la Torre and Deng suggested a resampling-based approach for person-fit assessment (PFA). The approach involves the use of the statistic, a corrected expected a posteriori estimate of the examinee ability, and the Monte Carlo (MC) resampling method. The Type I error rate of the approach was closer to the nominal level than that of the traditional approach of using along with the assumption of a standard normal null distribution. This article suggests a generalized resampling-based approach for PFA that allows one to employ or another person-fit statistic (PFS) based on item response theory, the corrected expected a posteriori estimate or another ability estimate, and the MC method or another resampling method. The suggested approach includes the approach of de la Torre and Deng as a special case. Several approaches belonging to the generalized approach perform very similarly to the approach of de la Torre and Deng's in two simulation studies and in applications to three real data sets, irrespective of the PFS used. The generalized approach promises to be useful to those interested in resampling-based PFA. © 2016 by the National Council on Measurement in Education.",Assessment of Person Fit Using Resampling-Based Approaches,"De la Torre and Deng suggested a resampling-based approach for person-fit assessment (PFA). The approach involves the use of the statistic, a corrected expected a posteriori estimate of the examinee ability, and the Monte Carlo (MC) resampling method. The Type I error rate of the approach was closer to the nominal level than that of the traditional approach of using along with the assumption of a standard normal null distribution. This article suggests a generalized resampling-based approach for PFA that allows one to employ or another person-fit statistic (PFS) based on item response theory, the corrected expected a posteriori estimate or another ability estimate, and the MC method or another resampling method. The suggested approach includes the approach of de la Torre and Deng as a special case. Several approaches belonging to the generalized approach perform very similarly to the approach of de la Torre and Deng's in two simulation studies and in applications to three real data sets, irrespective of the PFS used. The generalized approach promises to be useful to those interested in resampling-based PFA. © 2016 by the National Council on Measurement in Education.","['De', 'la', 'Torre', 'Deng', 'suggest', 'resamplingbase', 'approach', 'personfit', 'assessment', 'PFA', 'approach', 'involve', 'statistic', 'correct', 'expect', 'posteriori', 'estimate', 'examinee', 'ability', 'Monte', 'Carlo', 'MC', 'resampling', 'method', 'Type', 'I', 'error', 'rate', 'approach', 'close', 'nominal', 'level', 'traditional', 'approach', 'assumption', 'standard', 'normal', 'null', 'distribution', 'article', 'suggest', 'generalized', 'resamplingbased', 'approach', 'PFA', 'allow', 'employ', 'personfit', 'statistic', 'PFS', 'base', 'item', 'response', 'theory', 'correct', 'expect', 'posteriori', 'estimate', 'ability', 'estimate', 'MC', 'method', 'resampling', 'method', 'suggest', 'approach', 'include', 'approach', 'de', 'la', 'Torre', 'Deng', 'special', 'case', 'approach', 'belong', 'generalized', 'approach', 'perform', 'similarly', 'approach', 'de', 'la', 'Torre', 'Dengs', 'simulation', 'study', 'application', 'real', 'datum', 'set', 'irrespective', 'PFS', 'generalized', 'approach', 'promise', 'useful', 'interested', 'resamplingbase', 'PFA', '©', '2016', 'National', 'Council']","['assessment', 'Person', 'Fit', 'ResamplingBased', 'Approaches']",De la Torre Deng suggest resamplingbase approach personfit assessment PFA approach involve statistic correct expect posteriori estimate examinee ability Monte Carlo MC resampling method Type I error rate approach close nominal level traditional approach assumption standard normal null distribution article suggest generalized resamplingbased approach PFA allow employ personfit statistic PFS base item response theory correct expect posteriori estimate ability estimate MC method resampling method suggest approach include approach de la Torre Deng special case approach belong generalized approach perform similarly approach de la Torre Dengs simulation study application real datum set irrespective PFS generalized approach promise useful interested resamplingbase PFA © 2016 National Council,assessment Person Fit ResamplingBased Approaches,0.8883801211050241,0.027981040170883698,0.027696049071132673,0.02809110826125158,0.02785168139170796,0.0364528251623689,0.0,0.03345305115379899,0.0041532759180987035,0.0
Shang Y.,Measurement Error Adjustment Using the SIMEX Method: An Application to Student Growth Percentiles,2012,49,"Growth models are used extensively in the context of educational accountability to evaluate student-, class-, and school-level growth. However, when error-prone test scores are used as independent variables or right-hand-side controls, the estimation of such growth models can be substantially biased. This article introduces a simulation-extrapolation (SIMEX) method that corrects measurement error induced bias. The SIMEX method is applied to quantile regression, which is the basis of Student Growth Percentile, a descriptive growth model adopted in a number of states to diagnose and project student growth. A simulation study is conducted to demonstrate the performance of the SIMEX method in reducing bias and mean squared error in quantile regression with a mismeasured predictor. One of the simulation cases is based on longitudinal state assessment data. The analysis shows that measurement error differentially biases growth percentile results for students at different achievement levels and that the SIMEX method corrects such biases and closely reproduces conditional distributions of current test scores given past true scores. The potential applications and limitations of the method are discussed at the end of this paper with suggestions for further studies. © 2012 by the National Council on Measurement in Education.",Measurement Error Adjustment Using the SIMEX Method: An Application to Student Growth Percentiles,"Growth models are used extensively in the context of educational accountability to evaluate student-, class-, and school-level growth. However, when error-prone test scores are used as independent variables or right-hand-side controls, the estimation of such growth models can be substantially biased. This article introduces a simulation-extrapolation (SIMEX) method that corrects measurement error induced bias. The SIMEX method is applied to quantile regression, which is the basis of Student Growth Percentile, a descriptive growth model adopted in a number of states to diagnose and project student growth. A simulation study is conducted to demonstrate the performance of the SIMEX method in reducing bias and mean squared error in quantile regression with a mismeasured predictor. One of the simulation cases is based on longitudinal state assessment data. The analysis shows that measurement error differentially biases growth percentile results for students at different achievement levels and that the SIMEX method corrects such biases and closely reproduces conditional distributions of current test scores given past true scores. The potential applications and limitations of the method are discussed at the end of this paper with suggestions for further studies. © 2012 by the National Council on Measurement in Education.","['growth', 'extensively', 'context', 'educational', 'accountability', 'evaluate', 'student', 'class', 'schoollevel', 'growth', 'errorprone', 'test', 'score', 'independent', 'variable', 'righthandside', 'control', 'estimation', 'growth', 'substantially', 'bias', 'article', 'introduce', 'simulationextrapolation', 'simex', 'method', 'correct', 'error', 'induce', 'bias', 'simex', 'method', 'apply', 'quantile', 'regression', 'basis', 'Student', 'Growth', 'Percentile', 'descriptive', 'growth', 'adopt', 'number', 'state', 'diagnose', 'project', 'student', 'growth', 'simulation', 'study', 'conduct', 'demonstrate', 'performance', 'simex', 'method', 'reduce', 'bias', 'mean', 'squared', 'error', 'quantile', 'regression', 'mismeasured', 'predictor', 'simulation', 'case', 'base', 'longitudinal', 'state', 'assessment', 'datum', 'analysis', 'error', 'differentially', 'bias', 'growth', 'percentile', 'result', 'student', 'different', 'achievement', 'level', 'simex', 'method', 'correct', 'bias', 'closely', 'reproduce', 'conditional', 'distribution', 'current', 'test', 'score', 'past', 'true', 'score', 'potential', 'application', 'limitation', 'method', 'discuss', 'end', 'paper', 'suggestion', 'study', '©', '2012', 'National', 'Council']","['Error', 'Adjustment', 'simex', 'Method', 'application', 'Student', 'Growth', 'Percentiles']",growth extensively context educational accountability evaluate student class schoollevel growth errorprone test score independent variable righthandside control estimation growth substantially bias article introduce simulationextrapolation simex method correct error induce bias simex method apply quantile regression basis Student Growth Percentile descriptive growth adopt number state diagnose project student growth simulation study conduct demonstrate performance simex method reduce bias mean squared error quantile regression mismeasured predictor simulation case base longitudinal state assessment datum analysis error differentially bias growth percentile result student different achievement level simex method correct bias closely reproduce conditional distribution current test score past true score potential application limitation method discuss end paper suggestion study © 2012 National Council,Error Adjustment simex Method application Student Growth Percentiles,0.02717367429447176,0.8911909705315123,0.027263146963130567,0.027345205385301167,0.02702700282558431,0.016781829379974064,0.006181034710257549,0.06707414473414083,0.022272128220820035,0.0
Moses T.,Adjoined piecewise linear approximations (APLAs) for equating: Accuracy evaluations of a postsmoothing equating method,2013,50,"The purpose of this study was to evaluate the use of adjoined and piecewise linear approximations (APLAs) of raw equipercentile equating functions as a postsmoothing equating method. APLAs are less familiar than other postsmoothing equating methods (i.e., cubic splines), but their use has been described in historical equating practices of large-scale testing programs. This study used simulations to evaluate APLA equating results and compare these results with those from cubic spline postsmoothing and from several presmoothing equating methods. The overall results suggested that APLAs based on four line segments have accuracy advantages similar to or better than cubic splines and can sometimes produce more accurate smoothed equating functions than those produced using presmoothing methods. © 2013 by the National Council on Measurement in Education.",Adjoined piecewise linear approximations (APLAs) for equating: Accuracy evaluations of a postsmoothing equating method,"The purpose of this study was to evaluate the use of adjoined and piecewise linear approximations (APLAs) of raw equipercentile equating functions as a postsmoothing equating method. APLAs are less familiar than other postsmoothing equating methods (i.e., cubic splines), but their use has been described in historical equating practices of large-scale testing programs. This study used simulations to evaluate APLA equating results and compare these results with those from cubic spline postsmoothing and from several presmoothing equating methods. The overall results suggested that APLAs based on four line segments have accuracy advantages similar to or better than cubic splines and can sometimes produce more accurate smoothed equating functions than those produced using presmoothing methods. © 2013 by the National Council on Measurement in Education.","['purpose', 'study', 'evaluate', 'adjoined', 'piecewise', 'linear', 'approximation', 'apla', 'raw', 'equipercentile', 'equate', 'function', 'postsmoothing', 'equating', 'method', 'apla', 'familiar', 'postsmoothe', 'equate', 'method', 'ie', 'cubic', 'spline', 'describe', 'historical', 'equate', 'practice', 'largescale', 'testing', 'program', 'study', 'simulation', 'evaluate', 'apla', 'equate', 'result', 'compare', 'result', 'cubic', 'spline', 'postsmoothing', 'presmoothe', 'equating', 'method', 'overall', 'result', 'suggest', 'apla', 'base', 'line', 'segment', 'accuracy', 'advantage', 'similar', 'cubic', 'spline', 'produce', 'accurate', 'smoothed', 'equating', 'function', 'produce', 'presmoothe', 'method', '©', '2013', 'National', 'Council']","['adjoined', 'piecewise', 'linear', 'approximation', 'apla', 'equate', 'Accuracy', 'evaluation', 'postsmoothe', 'equating', 'method']",purpose study evaluate adjoined piecewise linear approximation apla raw equipercentile equate function postsmoothing equating method apla familiar postsmoothe equate method ie cubic spline describe historical equate practice largescale testing program study simulation evaluate apla equate result compare result cubic spline postsmoothing presmoothe equating method overall result suggest apla base line segment accuracy advantage similar cubic spline produce accurate smoothed equating function produce presmoothe method © 2013 National Council,adjoined piecewise linear approximation apla equate Accuracy evaluation postsmoothe equating method,0.8643296881126633,0.03387676006583956,0.03402490748202358,0.0338820757684438,0.033886568571029785,0.0,0.0,0.0,0.13055317592179466,0.0
Sinharay S.; Wan P.; Choi S.W.; Kim D.-I.,Assessing individual-level impact of interruptions during online testing,2015,52,"With an increase in the number of online tests, the number of interruptions during testing due to unexpected technical issues seems to be on the rise. For example, interruptions occurred during several recent state tests. When interruptions occur, it is important to determine the extent of their impact on the examinees' scores. Researchers such as Hill and Sinharay et al. examined the impact of interruptions at an aggregate level. However, there is a lack of research on the assessment of impact of interruptions at an individual level. We attempt to fill that void. We suggest four methodological approaches, primarily based on statistical hypothesis testing, linear regression, and item response theory, which can provide evidence on the individual-level impact of interruptions. We perform a realistic simulation study to compare the Type I error rate and power of the suggested approaches. We then apply the approaches to data from the 2013 Indiana Statewide Testing for Educational Progress-Plus (ISTEP+) test that experienced interruptions. © 2015 by the National Council on Measurement in Education.",Assessing individual-level impact of interruptions during online testing,"With an increase in the number of online tests, the number of interruptions during testing due to unexpected technical issues seems to be on the rise. For example, interruptions occurred during several recent state tests. When interruptions occur, it is important to determine the extent of their impact on the examinees' scores. Researchers such as Hill and Sinharay et al. examined the impact of interruptions at an aggregate level. However, there is a lack of research on the assessment of impact of interruptions at an individual level. We attempt to fill that void. We suggest four methodological approaches, primarily based on statistical hypothesis testing, linear regression, and item response theory, which can provide evidence on the individual-level impact of interruptions. We perform a realistic simulation study to compare the Type I error rate and power of the suggested approaches. We then apply the approaches to data from the 2013 Indiana Statewide Testing for Educational Progress-Plus (ISTEP+) test that experienced interruptions. © 2015 by the National Council on Measurement in Education.","['increase', 'number', 'online', 'test', 'number', 'interruption', 'testing', 'unexpected', 'technical', 'issue', 'rise', 'example', 'interruption', 'occur', 'recent', 'state', 'test', 'interruption', 'occur', 'important', 'determine', 'extent', 'impact', 'examinee', 'score', 'Researchers', 'Hill', 'Sinharay', 'et', 'al', 'examine', 'impact', 'interruption', 'aggregate', 'level', 'lack', 'research', 'assessment', 'impact', 'interruption', 'individual', 'level', 'attempt', 'fill', 'void', 'suggest', 'methodological', 'approach', 'primarily', 'base', 'statistical', 'hypothesis', 'testing', 'linear', 'regression', 'item', 'response', 'theory', 'provide', 'evidence', 'individuallevel', 'impact', 'interruption', 'perform', 'realistic', 'simulation', 'study', 'compare', 'type', 'I', 'error', 'rate', 'power', 'suggest', 'approach', 'apply', 'approach', 'datum', '2013', 'Indiana', 'Statewide', 'Testing', 'Educational', 'ProgressPlus', 'istep', 'test', 'experience', 'interruption', '©', '2015', 'National', 'Council']","['assess', 'individuallevel', 'impact', 'interruption', 'online', 'testing']",increase number online test number interruption testing unexpected technical issue rise example interruption occur recent state test interruption occur important determine extent impact examinee score Researchers Hill Sinharay et al examine impact interruption aggregate level lack research assessment impact interruption individual level attempt fill void suggest methodological approach primarily base statistical hypothesis testing linear regression item response theory provide evidence individuallevel impact interruption perform realistic simulation study compare type I error rate power suggest approach apply approach datum 2013 Indiana Statewide Testing Educational ProgressPlus istep test experience interruption © 2015 National Council,assess individuallevel impact interruption online testing,0.8850340561593832,0.028675738271792962,0.028705583000449297,0.02866478875291513,0.028919833815459327,0.027944950396354893,0.010811793689581992,0.053065710054140125,0.0,0.0
Yao L.,Multidimensional CAT item selection methods for domain scores and composite scores with item exposure control and content constraints,2014,51,"The intent of this research was to find an item selection procedure in the multidimensional computer adaptive testing (CAT) framework that yielded higher precision for both the domain and composite abilities, had a higher usage of the item pool, and controlled the exposure rate. Five multidimensional CAT item selection procedures (minimum angle volume minimum error variance of the linear combination; minimum error variance of the composite score with optimized weight; and Kullback-Leibler information) were studied and compared with two methods for item exposure control (the Sympson-Hetter procedure and the fixed-rate procedure, the latter simply refers to putting a limit on the item exposure rate) using simulated data. The maximum priority index method was used for the content constraints. Results showed that the Sympson-Hetter procedure yielded better precision than the fixed-rate procedure but had much lower item pool usage and took more time. The five item selection procedures performed similarly under Sympson-Hetter. For the fixed-rate procedure, there was a trade-off between the precision of the ability estimates and the item pool usage: the five procedures had different patterns. It was found that (1) Kullback-Leibler had better precision but lower item pool usage (2) minimum angle and volume had balanced precision and item pool usage and (3) the two methods minimizing the error variance had the best item pool usage and comparable overall score recovery but less precision for certain domains. The priority index for content constraints and item exposure was implemented successfully. © 2014 by the National Council on Measurement in Education.",Multidimensional CAT item selection methods for domain scores and composite scores with item exposure control and content constraints,"The intent of this research was to find an item selection procedure in the multidimensional computer adaptive testing (CAT) framework that yielded higher precision for both the domain and composite abilities, had a higher usage of the item pool, and controlled the exposure rate. Five multidimensional CAT item selection procedures (minimum angle volume minimum error variance of the linear combination; minimum error variance of the composite score with optimized weight; and Kullback-Leibler information) were studied and compared with two methods for item exposure control (the Sympson-Hetter procedure and the fixed-rate procedure, the latter simply refers to putting a limit on the item exposure rate) using simulated data. The maximum priority index method was used for the content constraints. Results showed that the Sympson-Hetter procedure yielded better precision than the fixed-rate procedure but had much lower item pool usage and took more time. The five item selection procedures performed similarly under Sympson-Hetter. For the fixed-rate procedure, there was a trade-off between the precision of the ability estimates and the item pool usage: the five procedures had different patterns. It was found that (1) Kullback-Leibler had better precision but lower item pool usage (2) minimum angle and volume had balanced precision and item pool usage and (3) the two methods minimizing the error variance had the best item pool usage and comparable overall score recovery but less precision for certain domains. The priority index for content constraints and item exposure was implemented successfully. © 2014 by the National Council on Measurement in Education.","['intent', 'research', 'find', 'item', 'selection', 'procedure', 'multidimensional', 'computer', 'adaptive', 'testing', 'CAT', 'framework', 'yield', 'high', 'precision', 'domain', 'composite', 'ability', 'high', 'usage', 'item', 'pool', 'control', 'exposure', 'rate', 'multidimensional', 'CAT', 'item', 'selection', 'procedure', 'minimum', 'angle', 'volume', 'minimum', 'error', 'variance', 'linear', 'combination', 'minimum', 'error', 'variance', 'composite', 'score', 'optimize', 'weight', 'KullbackLeibler', 'information', 'study', 'compare', 'method', 'item', 'exposure', 'control', 'SympsonHetter', 'procedure', 'fixedrate', 'procedure', 'simply', 'refer', 'limit', 'item', 'exposure', 'rate', 'simulate', 'datum', 'maximum', 'priority', 'index', 'method', 'content', 'constraint', 'result', 'SympsonHetter', 'procedure', 'yield', 'precision', 'fixedrate', 'procedure', 'low', 'item', 'pool', 'usage', 'time', 'item', 'selection', 'procedure', 'perform', 'similarly', 'SympsonHetter', 'fixedrate', 'procedure', 'tradeoff', 'precision', 'ability', 'estimate', 'item', 'pool', 'usage', 'procedure', 'different', 'pattern', 'find', '1', 'KullbackLeibler', 'precision', 'low', 'item', 'pool', 'usage', '2', 'minimum', 'angle', 'volume', 'balanced', 'precision', 'item', 'pool', 'usage', '3', 'method', 'minimize', 'error', 'variance', 'good', 'item', 'pool', 'usage', 'comparable', 'overall', 'score', 'recovery', 'precision', 'certain', 'domain', 'priority', 'index', 'content', 'constraint', 'item', 'exposure', 'implement', 'successfully', '©', '2014', 'National', 'Council']","['Multidimensional', 'CAT', 'item', 'selection', 'method', 'domain', 'score', 'composite', 'score', 'item', 'exposure', 'control', 'content', 'constraint']",intent research find item selection procedure multidimensional computer adaptive testing CAT framework yield high precision domain composite ability high usage item pool control exposure rate multidimensional CAT item selection procedure minimum angle volume minimum error variance linear combination minimum error variance composite score optimize weight KullbackLeibler information study compare method item exposure control SympsonHetter procedure fixedrate procedure simply refer limit item exposure rate simulate datum maximum priority index method content constraint result SympsonHetter procedure yield precision fixedrate procedure low item pool usage time item selection procedure perform similarly SympsonHetter fixedrate procedure tradeoff precision ability estimate item pool usage procedure different pattern find 1 KullbackLeibler precision low item pool usage 2 minimum angle volume balanced precision item pool usage 3 method minimize error variance good item pool usage comparable overall score recovery precision certain domain priority index content constraint item exposure implement successfully © 2014 National Council,Multidimensional CAT item selection method domain score composite score item exposure control content constraint,0.027933328569801033,0.8898977315840777,0.02733846872572838,0.027440549452860934,0.027389921667532065,0.07335671130008299,0.0,0.0,0.0,0.0
Shermis M.D.; Lottridge S.; Mayfield E.,The Impact of Anonymization for Automated Essay Scoring,2015,52,"This study investigated the impact of anonymizing text on predicted scores made by two kinds of automated scoring engines: one that incorporates elements of natural language processing (NLP) and one that does not. Eight data sets (N = 22,029) were used to form both training and test sets in which the scoring engines had access to both text and human rater scores for training, but only the text for the test set. Machine ratings were applied under three conditions: (a) both the training and test were conducted with the original data, (b) the training was modeled on the anonymized data, but the predictions were made on the original data, and (c) both the training and test were conducted on the anonymized text. The first condition served as the baseline for subsequent comparisons on the mean, standard deviation, and quadratic weighted kappa. With one exception, results on scoring scales in the range of 1-6 were not significantly different. The results on scales that were much wider did show significant differences. The conclusion was that anonymizing text for operational use may have a differential impact on machine score predictions for both NLP and non-NLP applications. © 2015 by the National Council on Measurement in Education.",The Impact of Anonymization for Automated Essay Scoring,"This study investigated the impact of anonymizing text on predicted scores made by two kinds of automated scoring engines: one that incorporates elements of natural language processing (NLP) and one that does not. Eight data sets (N = 22,029) were used to form both training and test sets in which the scoring engines had access to both text and human rater scores for training, but only the text for the test set. Machine ratings were applied under three conditions: (a) both the training and test were conducted with the original data, (b) the training was modeled on the anonymized data, but the predictions were made on the original data, and (c) both the training and test were conducted on the anonymized text. The first condition served as the baseline for subsequent comparisons on the mean, standard deviation, and quadratic weighted kappa. With one exception, results on scoring scales in the range of 1-6 were not significantly different. The results on scales that were much wider did show significant differences. The conclusion was that anonymizing text for operational use may have a differential impact on machine score predictions for both NLP and non-NLP applications. © 2015 by the National Council on Measurement in Education.","['study', 'investigate', 'impact', 'anonymize', 'text', 'predict', 'score', 'kind', 'automate', 'scoring', 'engine', 'incorporate', 'element', 'natural', 'language', 'process', 'NLP', 'datum', 'set', 'N', '22029', 'form', 'training', 'test', 'set', 'scoring', 'engine', 'access', 'text', 'human', 'rater', 'score', 'training', 'text', 'test', 'set', 'Machine', 'rating', 'apply', 'condition', 'training', 'test', 'conduct', 'original', 'datum', 'b', 'training', 'anonymize', 'datum', 'prediction', 'original', 'datum', 'c', 'training', 'test', 'conduct', 'anonymize', 'text', 'condition', 'serve', 'baseline', 'subsequent', 'comparison', 'mean', 'standard', 'deviation', 'quadratic', 'weight', 'kappa', 'exception', 'result', 'scoring', 'scale', 'range', '16', 'significantly', 'different', 'result', 'scale', 'wide', 'significant', 'difference', 'conclusion', 'anonymize', 'text', 'operational', 'differential', 'impact', 'machine', 'score', 'prediction', 'NLP', 'nonNLP', 'application', '©', '2015', 'National', 'Council']","['Impact', 'Anonymization', 'Automated', 'Essay', 'Scoring']",study investigate impact anonymize text predict score kind automate scoring engine incorporate element natural language process NLP datum set N 22029 form training test set scoring engine access text human rater score training text test set Machine rating apply condition training test conduct original datum b training anonymize datum prediction original datum c training test conduct anonymize text condition serve baseline subsequent comparison mean standard deviation quadratic weight kappa exception result scoring scale range 16 significantly different result scale wide significant difference conclusion anonymize text operational differential impact machine score prediction NLP nonNLP application © 2015 National Council,Impact Anonymization Automated Essay Scoring,0.028477926130553968,0.02830712018581754,0.028180271544259353,0.028374411586574663,0.8866602705527945,0.009152861458902676,0.020055050715964466,0.01703816111789684,0.010950352796412547,0.059589561641885044
González B. J.; von Davier M.,Statistical models and inference for the true equating transformation in the context of local equating,2013,50,"Based on Lord's criterion of equity of equating, van der Linden (this issue) revisits the so-called local equating method and offers alternative as well as new thoughts on several topics including the types of transformations, symmetry, reliability, and population invariance appropriate for equating. A remarkable aspect is to define equating as a standard statistical inference problem in which the true equating transformation is the parameter of interest that has to be estimated and assessed as any standard evaluation of an estimator of an unknown parameter in statistics. We believe that putting equating methods in a general statistical model framework would be an interesting and useful next step in the area. van der Linden's conceptual article on equating is certainly an important contribution to this task. © 2013 by the National Council on Measurement in Education.",Statistical models and inference for the true equating transformation in the context of local equating,"Based on Lord's criterion of equity of equating, van der Linden (this issue) revisits the so-called local equating method and offers alternative as well as new thoughts on several topics including the types of transformations, symmetry, reliability, and population invariance appropriate for equating. A remarkable aspect is to define equating as a standard statistical inference problem in which the true equating transformation is the parameter of interest that has to be estimated and assessed as any standard evaluation of an estimator of an unknown parameter in statistics. We believe that putting equating methods in a general statistical model framework would be an interesting and useful next step in the area. van der Linden's conceptual article on equating is certainly an important contribution to this task. © 2013 by the National Council on Measurement in Education.","['base', 'Lords', 'criterion', 'equity', 'equate', 'van', 'der', 'Linden', 'issue', 'revisit', 'socalled', 'local', 'equating', 'method', 'offer', 'alternative', 'new', 'thought', 'topic', 'include', 'type', 'transformation', 'symmetry', 'reliability', 'population', 'invariance', 'appropriate', 'equate', 'remarkable', 'aspect', 'define', 'equating', 'standard', 'statistical', 'inference', 'problem', 'true', 'equating', 'transformation', 'parameter', 'interest', 'estimate', 'assess', 'standard', 'evaluation', 'estimator', 'unknown', 'parameter', 'statistic', 'believe', 'equating', 'method', 'general', 'statistical', 'framework', 'interesting', 'useful', 'step', 'area', 'van', 'der', 'Lindens', 'conceptual', 'article', 'equating', 'certainly', 'important', 'contribution', 'task', '©', '2013', 'National', 'Council']","['statistical', 'inference', 'true', 'equating', 'transformation', 'context', 'local', 'equating']",base Lords criterion equity equate van der Linden issue revisit socalled local equating method offer alternative new thought topic include type transformation symmetry reliability population invariance appropriate equate remarkable aspect define equating standard statistical inference problem true equating transformation parameter interest estimate assess standard evaluation estimator unknown parameter statistic believe equating method general statistical framework interesting useful step area van der Lindens conceptual article equating certainly important contribution task © 2013 National Council,statistical inference true equating transformation context local equating,0.0262703974191328,0.026347454904456485,0.02649513953812087,0.8944876232866532,0.026399384851636676,0.0,0.0,0.004097592422539139,0.16142148640441426,0.002043288797614217
Wang W.-C.; Su C.-M.; Qiu X.-L.,Item response models for local dependence among multiple ratings,2014,51,"Ratings given to the same item response may have a stronger correlation than those given to different item responses, especially when raters interact with one another before giving ratings. The rater bundle model was developed to account for such local dependence by forming multiple ratings given to an item response as a bundle and assigning fixed-effect parameters to describe response patterns in the bundle. Unfortunately, this model becomes difficult to manage when a polytomous item is graded by more than two raters. In this study, by adding random-effect parameters to the facets model, we propose a class of generalized rater models to account for the local dependence among multiple ratings and intrarater variation in severity. A series of simulations was conducted with the freeware WinBUGS to evaluate parameter recovery of the new models and consequences of ignoring the local dependence or intrarater variation in severity. The results revealed a good parameter recovery when the data-generating models were fit, and a poor estimation of parameters and test reliability when the local dependence or intrarater variation in severity was ignored. An empirical example is provided. © 2014 by the National Council on Measurement in Education.",Item response models for local dependence among multiple ratings,"Ratings given to the same item response may have a stronger correlation than those given to different item responses, especially when raters interact with one another before giving ratings. The rater bundle model was developed to account for such local dependence by forming multiple ratings given to an item response as a bundle and assigning fixed-effect parameters to describe response patterns in the bundle. Unfortunately, this model becomes difficult to manage when a polytomous item is graded by more than two raters. In this study, by adding random-effect parameters to the facets model, we propose a class of generalized rater models to account for the local dependence among multiple ratings and intrarater variation in severity. A series of simulations was conducted with the freeware WinBUGS to evaluate parameter recovery of the new models and consequences of ignoring the local dependence or intrarater variation in severity. The results revealed a good parameter recovery when the data-generating models were fit, and a poor estimation of parameters and test reliability when the local dependence or intrarater variation in severity was ignored. An empirical example is provided. © 2014 by the National Council on Measurement in Education.","['rating', 'item', 'response', 'strong', 'correlation', 'different', 'item', 'response', 'especially', 'rater', 'interact', 'rating', 'rater', 'bundle', 'develop', 'account', 'local', 'dependence', 'form', 'multiple', 'rating', 'item', 'response', 'bundle', 'assign', 'fixedeffect', 'parameter', 'describe', 'response', 'pattern', 'bundle', 'unfortunately', 'difficult', 'manage', 'polytomous', 'item', 'grade', 'rater', 'study', 'add', 'randomeffect', 'parameter', 'facet', 'propose', 'class', 'generalized', 'rater', 'account', 'local', 'dependence', 'multiple', 'rating', 'intrarater', 'variation', 'severity', 'series', 'simulation', 'conduct', 'freeware', 'WinBUGS', 'evaluate', 'parameter', 'recovery', 'new', 'consequence', 'ignore', 'local', 'dependence', 'intrarater', 'variation', 'severity', 'result', 'reveal', 'good', 'parameter', 'recovery', 'datagenerate', 'fit', 'poor', 'estimation', 'parameter', 'test', 'reliability', 'local', 'dependence', 'intrarater', 'variation', 'severity', 'ignore', 'empirical', 'example', 'provide', '©', '2014', 'National', 'Council']","['item', 'response', 'local', 'dependence', 'multiple', 'rating']",rating item response strong correlation different item response especially rater interact rating rater bundle develop account local dependence form multiple rating item response bundle assign fixedeffect parameter describe response pattern bundle unfortunately difficult manage polytomous item grade rater study add randomeffect parameter facet propose class generalized rater account local dependence multiple rating intrarater variation severity series simulation conduct freeware WinBUGS evaluate parameter recovery new consequence ignore local dependence intrarater variation severity result reveal good parameter recovery datagenerate fit poor estimation parameter test reliability local dependence intrarater variation severity ignore empirical example provide © 2014 National Council,item response local dependence multiple rating,0.028900158178718122,0.028424751404336077,0.02874151498041477,0.0281923189322437,0.8857412565042874,0.03492571792097883,0.0,0.0,0.0,0.16077010369026448
Guo H.; Puhan G.,Section preequating under the equivalent groups design without IRT,2014,51,"In this article, we introduce a section preequating (SPE) method (linear and nonlinear) under the randomly equivalent groups design. In this equating design, sections of Test X (a future new form) and another existing Test Y (an old form already on scale) are administered. The sections of Test X are equated to Test Y, after adjusting for the imperfect correlation between sections of Test X, to obtain the equated score on the complete form of X. Simulations and a real-data application show that the proposed SPE method is fairly simple and accurate. © 2014 by the National Council on Measurement in Education.",Section preequating under the equivalent groups design without IRT,"In this article, we introduce a section preequating (SPE) method (linear and nonlinear) under the randomly equivalent groups design. In this equating design, sections of Test X (a future new form) and another existing Test Y (an old form already on scale) are administered. The sections of Test X are equated to Test Y, after adjusting for the imperfect correlation between sections of Test X, to obtain the equated score on the complete form of X. Simulations and a real-data application show that the proposed SPE method is fairly simple and accurate. © 2014 by the National Council on Measurement in Education.","['article', 'introduce', 'section', 'preequate', 'SPE', 'method', 'linear', 'nonlinear', 'randomly', 'equivalent', 'group', 'design', 'equate', 'design', 'section', 'Test', 'X', 'future', 'new', 'form', 'exist', 'Test', 'Y', 'old', 'form', 'scale', 'administer', 'section', 'Test', 'X', 'equate', 'test', 'Y', 'adjust', 'imperfect', 'correlation', 'section', 'Test', 'X', 'obtain', 'equate', 'score', 'complete', 'form', 'X', 'Simulations', 'realdata', 'application', 'propose', 'spe', 'method', 'fairly', 'simple', 'accurate', '©', '2014', 'National', 'Council']","['section', 'preequate', 'equivalent', 'group', 'design', 'IRT']",article introduce section preequate SPE method linear nonlinear randomly equivalent group design equate design section Test X future new form exist Test Y old form scale administer section Test X equate test Y adjust imperfect correlation section Test X obtain equate score complete form X Simulations realdata application propose spe method fairly simple accurate © 2014 National Council,section preequate equivalent group design IRT,0.034973402934270356,0.03452964758798281,0.03488268572971126,0.0348124836368438,0.8608017801111917,0.012852919368273694,0.0109919456310759,0.0059043516565970445,0.0854687377198322,0.0
Terzi R.; Suh Y.,An Odds Ratio Approach for Detecting DDF Under the Nested Logit Modeling Framework,2015,52,"An odds ratio approach (ORA) under the framework of a nested logit model was proposed for evaluating differential distractor functioning (DDF) in multiple-choice items and was compared with an existing ORA developed under the nominal response model. The performances of the two ORAs for detecting DDF were investigated through an extensive simulation study. The impact of model misfit on the performance of each ORA was also examined. To facilitate practical interpretation of each method, effect size measures were obtained and compared. Finally, data from a college-level mathematics placement test were analyzed using the two approaches. © 2015 by the National Council on Measurement in Education.",An Odds Ratio Approach for Detecting DDF Under the Nested Logit Modeling Framework,"An odds ratio approach (ORA) under the framework of a nested logit model was proposed for evaluating differential distractor functioning (DDF) in multiple-choice items and was compared with an existing ORA developed under the nominal response model. The performances of the two ORAs for detecting DDF were investigated through an extensive simulation study. The impact of model misfit on the performance of each ORA was also examined. To facilitate practical interpretation of each method, effect size measures were obtained and compared. Finally, data from a college-level mathematics placement test were analyzed using the two approaches. © 2015 by the National Council on Measurement in Education.","['odd', 'ratio', 'approach', 'ora', 'framework', 'nest', 'logit', 'propose', 'evaluate', 'differential', 'distractor', 'function', 'DDF', 'multiplechoice', 'item', 'compare', 'exist', 'ora', 'develop', 'nominal', 'response', 'performance', 'ORAs', 'detect', 'DDF', 'investigate', 'extensive', 'simulation', 'study', 'impact', 'misfit', 'performance', 'ora', 'examine', 'facilitate', 'practical', 'interpretation', 'method', 'effect', 'size', 'measure', 'obtain', 'compare', 'finally', 'datum', 'collegelevel', 'mathematics', 'placement', 'test', 'analyze', 'approach', '©', '2015', 'National', 'Council']","['Odds', 'ratio', 'Approach', 'detect', 'DDF', 'Nested', 'Logit', 'modeling', 'Framework']",odd ratio approach ora framework nest logit propose evaluate differential distractor function DDF multiplechoice item compare exist ora develop nominal response performance ORAs detect DDF investigate extensive simulation study impact misfit performance ora examine facilitate practical interpretation method effect size measure obtain compare finally datum collegelevel mathematics placement test analyze approach © 2015 National Council,Odds ratio Approach detect DDF Nested Logit modeling Framework,0.03273630859122152,0.03214329787143616,0.032273388537859896,0.8700685196224466,0.03277848537703595,0.04229518454708464,0.0,0.020376306788183755,0.0,0.00039952645206464227
Holland P.W.,Comments on van der Linden's critique and proposal for equating,2013,50,"While agreeing with van der Linden (this issue) that test equating needs better theoretical underpinnings, my comments criticize several aspects of his article. His examples are, for the most part, worthless; he does not use well-established terminology correctly; his view of 100 years of attempts to give a theoretical basis for equating is unreasonably dismissive; he exhibits no understanding of the role of the synthetic population for anchor test equating for the nonequivalent groups with anchor test design; he is obtuse regarding the condition of symmetry, requiring it of the estimand but not of the estimator; and his proposal for a foundational basis for all test equating, the ""true equating transformation,"" allows a different equating function for every examinee, which is way past what equating actually does or hopes to achieve. Most importantly, he appears to think that criticism of others is more important than improved insight that moves a field forward based on the work of many other theorists whose contributions have improved the practice of equating. © 2013 by the National Council on Measurement in Education.",Comments on van der Linden's critique and proposal for equating,"While agreeing with van der Linden (this issue) that test equating needs better theoretical underpinnings, my comments criticize several aspects of his article. His examples are, for the most part, worthless; he does not use well-established terminology correctly; his view of 100 years of attempts to give a theoretical basis for equating is unreasonably dismissive; he exhibits no understanding of the role of the synthetic population for anchor test equating for the nonequivalent groups with anchor test design; he is obtuse regarding the condition of symmetry, requiring it of the estimand but not of the estimator; and his proposal for a foundational basis for all test equating, the ""true equating transformation,"" allows a different equating function for every examinee, which is way past what equating actually does or hopes to achieve. Most importantly, he appears to think that criticism of others is more important than improved insight that moves a field forward based on the work of many other theorists whose contributions have improved the practice of equating. © 2013 by the National Council on Measurement in Education.","['agree', 'van', 'der', 'Linden', 'issue', 'test', 'equating', 'need', 'theoretical', 'underpinning', 'comment', 'criticize', 'aspect', 'article', 'example', 'worthless', 'wellestablished', 'terminology', 'correctly', 'view', '100', 'year', 'attempt', 'theoretical', 'basis', 'equate', 'unreasonably', 'dismissive', 'exhibit', 'understanding', 'role', 'synthetic', 'population', 'anchor', 'test', 'equate', 'nonequivalent', 'group', 'anchor', 'test', 'design', 'obtuse', 'regard', 'condition', 'symmetry', 'require', 'estimand', 'estimator', 'proposal', 'foundational', 'basis', 'test', 'equate', 'true', 'equating', 'transformation', 'allow', 'different', 'equating', 'function', 'examinee', 'way', 'past', 'equate', 'actually', 'hope', 'achieve', 'importantly', 'appear', 'think', 'criticism', 'important', 'improved', 'insight', 'field', 'forward', 'base', 'work', 'theorist', 'contribution', 'improve', 'practice', 'equate', '©', '2013', 'National', 'Council']","['comment', 'van', 'der', 'Lindens', 'critique', 'proposal', 'equate']",agree van der Linden issue test equating need theoretical underpinning comment criticize aspect article example worthless wellestablished terminology correctly view 100 year attempt theoretical basis equate unreasonably dismissive exhibit understanding role synthetic population anchor test equate nonequivalent group anchor test design obtuse regard condition symmetry require estimand estimator proposal foundational basis test equate true equating transformation allow different equating function examinee way past equate actually hope achieve importantly appear think criticism important improved insight field forward base work theorist contribution improve practice equate © 2013 National Council,comment van der Lindens critique proposal equate,0.02324707967838335,0.022850183091393726,0.9077460528038864,0.02314973261454394,0.0230069518117927,0.0,0.0,0.011818275393573241,0.13887698610412222,0.0
Milla J.; Martín E.S.; Van Bellegem S.,Higher Education Value Added Using Multiple Outcomes,2016,53,"In this article we develop a methodology for the joint value added analysis of multiple outcomes that takes into account the inherent correlation between them. This is especially crucial in the analysis of higher education institutions. We use a unique Colombian database on universities, which contains scores in five domains tested in a standardized exit examination that is compulsory in order to graduate. We develop a new estimation procedure that accommodates any number of outcomes. Another novelty of our method is related to the structure of the random effect covariance matrix. Effects of the same school can be correlated and this correlation is allowed to vary among schools. Copyright © 2016 by the National Council on Measurement in Education",Higher Education Value Added Using Multiple Outcomes,"In this article we develop a methodology for the joint value added analysis of multiple outcomes that takes into account the inherent correlation between them. This is especially crucial in the analysis of higher education institutions. We use a unique Colombian database on universities, which contains scores in five domains tested in a standardized exit examination that is compulsory in order to graduate. We develop a new estimation procedure that accommodates any number of outcomes. Another novelty of our method is related to the structure of the random effect covariance matrix. Effects of the same school can be correlated and this correlation is allowed to vary among schools. Copyright © 2016 by the National Council on Measurement in Education","['article', 'develop', 'methodology', 'joint', 'value', 'add', 'analysis', 'multiple', 'outcome', 'account', 'inherent', 'correlation', 'especially', 'crucial', 'analysis', 'high', 'institution', 'unique', 'colombian', 'database', 'university', 'contain', 'score', 'domain', 'test', 'standardized', 'exit', 'examination', 'compulsory', 'order', 'graduate', 'develop', 'new', 'estimation', 'procedure', 'accommodate', 'number', 'outcome', 'novelty', 'method', 'relate', 'structure', 'random', 'effect', 'covariance', 'matrix', 'effect', 'school', 'correlate', 'correlation', 'allow', 'vary', 'school', 'Copyright', '©', '2016', 'National', 'Council']","['high', 'Value', 'add', 'Multiple', 'Outcomes']",article develop methodology joint value add analysis multiple outcome account inherent correlation especially crucial analysis high institution unique colombian database university contain score domain test standardized exit examination compulsory order graduate develop new estimation procedure accommodate number outcome novelty method relate structure random effect covariance matrix effect school correlate correlation allow vary school Copyright © 2016 National Council,high Value add Multiple Outcomes,0.02637595122479415,0.026363802875556334,0.026333552680755466,0.026246466255723824,0.8946802269631703,0.02433824206729111,0.043445004762541754,0.029204547810737665,0.0,0.013399121134350843
Briggs D.C.,Measuring growth with vertical scales,2013,50,"A vertical score scale is needed to measure growth across multiple tests in terms of absolute changes in magnitude. Since the warrant for subsequent growth interpretations depends upon the assumption that the scale has interval properties, the validation of a vertical scale would seem to require methods for distinguishing interval scales from ordinal scales. In taking up this issue, two different perspectives on educational measurement are contrasted: a metaphorical perspective and a classical perspective. Although the metaphorical perspective is more predominant, at present it provides no objective methods whereby the properties of a vertical scale can be validated. In contrast, when taking a classical perspective, the axioms of additive conjoint measurement can be used to test the hypothesis that the latent variable underlying a vertical scale is quantitative (supporting ratio or interval properties) rather than merely qualitative (supporting ordinal or nominal properties). The application of such an approach is illustrated with both a hypothetical example and by drawing upon recent research that has been conducted on the Lexile scale for reading comprehension. © 2013 by the National Council on Measurement in Education.",,"A vertical score scale is needed to measure growth across multiple tests in terms of absolute changes in magnitude. Since the warrant for subsequent growth interpretations depends upon the assumption that the scale has interval properties, the validation of a vertical scale would seem to require methods for distinguishing interval scales from ordinal scales. In taking up this issue, two different perspectives on educational measurement are contrasted: a metaphorical perspective and a classical perspective. Although the metaphorical perspective is more predominant, at present it provides no objective methods whereby the properties of a vertical scale can be validated. In contrast, when taking a classical perspective, the axioms of additive conjoint measurement can be used to test the hypothesis that the latent variable underlying a vertical scale is quantitative (supporting ratio or interval properties) rather than merely qualitative (supporting ordinal or nominal properties). The application of such an approach is illustrated with both a hypothetical example and by drawing upon recent research that has been conducted on the Lexile scale for reading comprehension. © 2013 by the National Council on Measurement in Education.","['vertical', 'score', 'scale', 'need', 'measure', 'growth', 'multiple', 'test', 'term', 'absolute', 'change', 'magnitude', 'warrant', 'subsequent', 'growth', 'interpretation', 'depend', 'assumption', 'scale', 'interval', 'property', 'validation', 'vertical', 'scale', 'require', 'method', 'distinguish', 'interval', 'scale', 'ordinal', 'scale', 'issue', 'different', 'perspective', 'educational', 'contrast', 'metaphorical', 'perspective', 'classical', 'perspective', 'metaphorical', 'perspective', 'predominant', 'present', 'provide', 'objective', 'method', 'property', 'vertical', 'scale', 'validate', 'contrast', 'classical', 'perspective', 'axiom', 'additive', 'conjoint', 'test', 'hypothesis', 'latent', 'variable', 'underlie', 'vertical', 'scale', 'quantitative', 'support', 'ratio', 'interval', 'property', 'merely', 'qualitative', 'support', 'ordinal', 'nominal', 'property', 'application', 'approach', 'illustrate', 'hypothetical', 'example', 'draw', 'recent', 'research', 'conduct', 'Lexile', 'scale', 'read', 'comprehension', '©', '2013', 'National', 'Council']",,vertical score scale need measure growth multiple test term absolute change magnitude warrant subsequent growth interpretation depend assumption scale interval property validation vertical scale require method distinguish interval scale ordinal scale issue different perspective educational contrast metaphorical perspective classical perspective metaphorical perspective predominant present provide objective method property vertical scale validate contrast classical perspective axiom additive conjoint test hypothesis latent variable underlie vertical scale quantitative support ratio interval property merely qualitative support ordinal nominal property application approach illustrate hypothetical example draw recent research conduct Lexile scale read comprehension © 2013 National Council,,0.029084572185557326,0.029150290525145544,0.882357739809296,0.030162871749554697,0.029244525730446478,0.010888625310872374,0.005781255980238233,0.052577831656947356,0.0043001862710653,0.0023789114835857305
Fitzpatrick J.; Skorupski W.P.,Equating With Miditests Using IRT,2016,53,"The equating performance of two internal anchor test structures—miditests and minitests—is studied for four IRT equating methods using simulated data. Originally proposed by Sinharay and Holland, miditests are anchors that have the same mean difficulty as the overall test but less variance in item difficulties. Four popular IRT equating methods were tested, and both the means and SDs of the true ability of the group to be equated were varied. We evaluate equating accuracy marginally and conditional on true ability. Our results suggest miditests perform about as well as traditional minitests for most conditions. Findings are discussed in terms of comparability to the typical minitest design and the trade-off between accuracy and flexibility in test construction. Copyright © 2016 by the National Council on Measurement in Education",,"The equating performance of two internal anchor test structures—miditests and minitests—is studied for four IRT equating methods using simulated data. Originally proposed by Sinharay and Holland, miditests are anchors that have the same mean difficulty as the overall test but less variance in item difficulties. Four popular IRT equating methods were tested, and both the means and SDs of the true ability of the group to be equated were varied. We evaluate equating accuracy marginally and conditional on true ability. Our results suggest miditests perform about as well as traditional minitests for most conditions. Findings are discussed in terms of comparability to the typical minitest design and the trade-off between accuracy and flexibility in test construction. Copyright © 2016 by the National Council on Measurement in Education","['equate', 'performance', 'internal', 'anchor', 'test', 'structure', '—', 'miditest', 'minitest', '—', 'study', 'IRT', 'equating', 'method', 'simulated', 'datum', 'originally', 'propose', 'Sinharay', 'Holland', 'miditest', 'anchor', 'mean', 'difficulty', 'overall', 'test', 'variance', 'item', 'difficulty', 'popular', 'IRT', 'equating', 'method', 'test', 'mean', 'sd', 'true', 'ability', 'group', 'equate', 'varied', 'evaluate', 'equate', 'accuracy', 'marginally', 'conditional', 'true', 'ability', 'result', 'suggest', 'miditest', 'perform', 'traditional', 'minitest', 'condition', 'Findings', 'discuss', 'term', 'comparability', 'typical', 'minit', 'design', 'tradeoff', 'accuracy', 'flexibility', 'test', 'construction', 'Copyright', '©', '2016', 'National', 'Council']",,equate performance internal anchor test structure — miditest minitest — study IRT equating method simulated datum originally propose Sinharay Holland miditest anchor mean difficulty overall test variance item difficulty popular IRT equating method test mean sd true ability group equate varied evaluate equate accuracy marginally conditional true ability result suggest miditest perform traditional minitest condition Findings discuss term comparability typical minit design tradeoff accuracy flexibility test construction Copyright © 2016 National Council,,0.029169793393611566,0.028598254881226488,0.028489365751272297,0.8851587127279296,0.02858387324596007,0.0187223330376326,0.00532068226548735,0.0,0.12635198405543496,0.0
Liu O.L.; Liu H.; Roohr K.C.; McCaffrey D.F.,Investigating College Learning Gain: Exploring a Propensity Score Weighting Approach,2016,53,"Learning outcomes assessment has been widely used by higher education institutions both nationally and internationally. One of its popular uses is to document learning gains of students. Prior studies have recognized the potential imbalance between freshmen and seniors in terms of their background characteristics and their prior academic performance and have used linear regression adjustments for these differences, which some researchers have argued are not fully adequate. We explored an alternative adjustment via propensity score weighting to balance the samples on background variables including SAT score, gender, and ethnicity. Results involving a cross-sectional sample of freshmen and seniors from seven groups of majors within a large research university showed that students in most of the majors demonstrated significant learning gain. Additionally, there was a slight difference in learning gain rankings across major groupings when compared to multiple regression results. Copyright © 2016 by the National Council on Measurement in Education",Investigating College Learning Gain: Exploring a Propensity Score Weighting Approach,"Learning outcomes assessment has been widely used by higher education institutions both nationally and internationally. One of its popular uses is to document learning gains of students. Prior studies have recognized the potential imbalance between freshmen and seniors in terms of their background characteristics and their prior academic performance and have used linear regression adjustments for these differences, which some researchers have argued are not fully adequate. We explored an alternative adjustment via propensity score weighting to balance the samples on background variables including SAT score, gender, and ethnicity. Results involving a cross-sectional sample of freshmen and seniors from seven groups of majors within a large research university showed that students in most of the majors demonstrated significant learning gain. Additionally, there was a slight difference in learning gain rankings across major groupings when compared to multiple regression results. Copyright © 2016 by the National Council on Measurement in Education","['learn', 'outcome', 'assessment', 'widely', 'high', 'institution', 'nationally', 'internationally', 'popular', 'document', 'learn', 'gain', 'student', 'Prior', 'study', 'recognize', 'potential', 'imbalance', 'freshman', 'senior', 'term', 'background', 'characteristic', 'prior', 'academic', 'performance', 'linear', 'regression', 'adjustment', 'difference', 'researcher', 'argue', 'fully', 'adequate', 'explore', 'alternative', 'adjustment', 'propensity', 'score', 'weighting', 'balance', 'sample', 'background', 'variable', 'include', 'SAT', 'score', 'gender', 'ethnicity', 'result', 'involve', 'crosssectional', 'sample', 'freshman', 'senior', 'seven', 'group', 'major', 'large', 'research', 'university', 'student', 'major', 'demonstrate', 'significant', 'learning', 'gain', 'Additionally', 'slight', 'difference', 'learn', 'gain', 'ranking', 'major', 'grouping', 'compare', 'multiple', 'regression', 'result', 'copyright', '©', '2016', 'National', 'Council']","['investigate', 'College', 'Learning', 'Gain', 'explore', 'Propensity', 'Score', 'Weighting', 'approach']",learn outcome assessment widely high institution nationally internationally popular document learn gain student Prior study recognize potential imbalance freshman senior term background characteristic prior academic performance linear regression adjustment difference researcher argue fully adequate explore alternative adjustment propensity score weighting balance sample background variable include SAT score gender ethnicity result involve crosssectional sample freshman senior seven group major large research university student major demonstrate significant learning gain Additionally slight difference learn gain ranking major grouping compare multiple regression result copyright © 2016 National Council,investigate College Learning Gain explore Propensity Score Weighting approach,0.02509546248730698,0.8993593200975184,0.024904171051967423,0.025061884761706545,0.025579161601500675,0.0003372272651165408,0.0,0.07681872037268034,0.011025553146155463,0.0
Chen J.; de la Torre J.; Zhang Z.,Relative and absolute fit evaluation in cognitive diagnosis modeling,2013,50,"As with any psychometric models, the validity of inferences from cognitive diagnosis models (CDMs) determines the extent to which these models can be useful. For inferences from CDMs to be valid, it is crucial that the fit of the model to the data is ascertained. Based on a simulation study, this study investigated the sensitivity of various fit statistics for absolute or relative fit under different CDM settings. The investigation covered various types of model-data misfit that can occur with the misspecifications of the Q-matrix, the CDM, or both. Six fit statistics were considered: -2 log likelihood (-2LL), Akaike's information criterion (AIC), Bayesian information criterion (BIC), and residuals based on the proportion correct of individual items (p), the correlations (r), and the log-odds ratio of item pairs (l). An empirical example involving real data was used to illustrate how the different fit statistics can be employed in conjunction with each other to identify different types of misspecifications. With these statistics and the saturated model serving as the basis, relative and absolute fit evaluation can be integrated to detect misspecification efficiently. © 2013 by the National Council on Measurement in Education.",Relative and absolute fit evaluation in cognitive diagnosis modeling,"As with any psychometric models, the validity of inferences from cognitive diagnosis models (CDMs) determines the extent to which these models can be useful. For inferences from CDMs to be valid, it is crucial that the fit of the model to the data is ascertained. Based on a simulation study, this study investigated the sensitivity of various fit statistics for absolute or relative fit under different CDM settings. The investigation covered various types of model-data misfit that can occur with the misspecifications of the Q-matrix, the CDM, or both. Six fit statistics were considered: -2 log likelihood (-2LL), Akaike's information criterion (AIC), Bayesian information criterion (BIC), and residuals based on the proportion correct of individual items (p), the correlations (r), and the log-odds ratio of item pairs (l). An empirical example involving real data was used to illustrate how the different fit statistics can be employed in conjunction with each other to identify different types of misspecifications. With these statistics and the saturated model serving as the basis, relative and absolute fit evaluation can be integrated to detect misspecification efficiently. © 2013 by the National Council on Measurement in Education.","['psychometric', 'validity', 'inference', 'cognitive', 'diagnosis', 'CDMs', 'determine', 'extent', 'useful', 'inference', 'cdm', 'valid', 'crucial', 'fit', 'datum', 'ascertain', 'base', 'simulation', 'study', 'study', 'investigate', 'sensitivity', 'fit', 'statistic', 'absolute', 'relative', 'fit', 'different', 'cdm', 'setting', 'investigation', 'cover', 'type', 'modeldata', 'misfit', 'occur', 'misspecification', 'Qmatrix', 'CDM', 'fit', 'statistic', 'consider', '2', 'log', 'likelihood', '2ll', 'Akaikes', 'information', 'criterion', 'AIC', 'bayesian', 'information', 'criterion', 'BIC', 'residual', 'base', 'proportion', 'correct', 'individual', 'item', 'p', 'correlation', 'r', 'logodds', 'ratio', 'item', 'pair', 'l', 'empirical', 'example', 'involve', 'real', 'datum', 'illustrate', 'different', 'fit', 'statistic', 'employ', 'conjunction', 'identify', 'different', 'type', 'misspecification', 'statistic', 'saturated', 'serve', 'basis', 'relative', 'absolute', 'fit', 'evaluation', 'integrate', 'detect', 'misspecification', 'efficiently', '©', '2013', 'National', 'Council']","['relative', 'absolute', 'fit', 'evaluation', 'cognitive', 'diagnosis', 'modeling']",psychometric validity inference cognitive diagnosis CDMs determine extent useful inference cdm valid crucial fit datum ascertain base simulation study study investigate sensitivity fit statistic absolute relative fit different cdm setting investigation cover type modeldata misfit occur misspecification Qmatrix CDM fit statistic consider 2 log likelihood 2ll Akaikes information criterion AIC bayesian information criterion BIC residual base proportion correct individual item p correlation r logodds ratio item pair l empirical example involve real datum illustrate different fit statistic employ conjunction identify different type misspecification statistic saturated serve basis relative absolute fit evaluation integrate detect misspecification efficiently © 2013 National Council,relative absolute fit evaluation cognitive diagnosis modeling,0.024004056334675006,0.023940293137274204,0.023777593074219856,0.9043920243264939,0.02388603312733709,0.042551398981424494,0.0,0.027528166607190024,0.0,0.014284427323836136
Powers S.; Kolen M.J.,Evaluating equating accuracy and assumptions for groups that differ in performance,2014,51,"Accurate equating results are essential when comparing examinee scores across exam forms. Previous research indicates that equating results may not be accurate when group differences are large. This study compared the equating results of frequency estimation, chained equipercentile, item response theory (IRT) true-score, and IRT observed-score equating methods. Using mixed-format test data, equating results were evaluated for group differences ranging from 0 to .75 standard deviations. As group differences increased, equating results became increasingly biased and dissimilar across equating methods. Results suggest that the size of group differences, the likelihood that equating assumptions are violated, and the equating error associated with an equating method should be taken into consideration when choosing an equating method. © 2014 by the National Council on Measurement in Education.",Evaluating equating accuracy and assumptions for groups that differ in performance,"Accurate equating results are essential when comparing examinee scores across exam forms. Previous research indicates that equating results may not be accurate when group differences are large. This study compared the equating results of frequency estimation, chained equipercentile, item response theory (IRT) true-score, and IRT observed-score equating methods. Using mixed-format test data, equating results were evaluated for group differences ranging from 0 to .75 standard deviations. As group differences increased, equating results became increasingly biased and dissimilar across equating methods. Results suggest that the size of group differences, the likelihood that equating assumptions are violated, and the equating error associated with an equating method should be taken into consideration when choosing an equating method. © 2014 by the National Council on Measurement in Education.","['accurate', 'equating', 'result', 'essential', 'compare', 'examinee', 'score', 'exam', 'form', 'previous', 'research', 'indicate', 'equate', 'result', 'accurate', 'group', 'difference', 'large', 'study', 'compare', 'equate', 'result', 'frequency', 'estimation', 'chain', 'equipercentile', 'item', 'response', 'theory', 'IRT', 'truescore', 'IRT', 'observedscore', 'equate', 'method', 'mixedformat', 'test', 'datum', 'equating', 'result', 'evaluate', 'group', 'difference', 'range', '0', '75', 'standard', 'deviation', 'group', 'difference', 'increase', 'equate', 'result', 'increasingly', 'biased', 'dissimilar', 'equate', 'method', 'result', 'suggest', 'size', 'group', 'difference', 'likelihood', 'equate', 'assumption', 'violate', 'equate', 'error', 'associate', 'equate', 'method', 'consideration', 'choose', 'equate', 'method', '©', '2014', 'National', 'Council']","['evaluate', 'equate', 'accuracy', 'assumption', 'group', 'differ', 'performance']",accurate equating result essential compare examinee score exam form previous research indicate equate result accurate group difference large study compare equate result frequency estimation chain equipercentile item response theory IRT truescore IRT observedscore equate method mixedformat test datum equating result evaluate group difference range 0 75 standard deviation group difference increase equate result increasingly biased dissimilar equate method result suggest size group difference likelihood equate assumption violate equate error associate equate method consideration choose equate method © 2014 National Council,evaluate equate accuracy assumption group differ performance,0.0315198619390969,0.030013314811248554,0.030758362607698278,0.877566666455306,0.030141794186650275,0.0036613447487510277,0.0,0.0,0.21260094872680485,0.0
Veldkamp B.P.,On the Issue of Item Selection in Computerized Adaptive Testing With Response Times,2016,53,"Many standardized tests are now administered via computer rather than paper-and-pencil format. The computer-based delivery mode brings with it certain advantages. One advantage is the ability to adapt the difficulty level of the test to the ability level of the test taker in what has been termed computerized adaptive testing (CAT). A second advantage is the ability to record not only the test taker's response to each item (i.e., question), but also the amount of time the test taker spends considering and answering each item. Combining these two advantages, various methods were explored for utilizing response time data in selecting appropriate items for an individual test taker. Four strategies for incorporating response time data were evaluated, and the precision of the final test-taker score was assessed by comparing it to a benchmark value that did not take response time information into account. While differences in measurement precision and testing times were expected, results showed that the strategies did not differ much with respect to measurement precision but that there were differences with regard to the total testing time. Copyright © 2016 by the National Council on Measurement in Education",On the Issue of Item Selection in Computerized Adaptive Testing With Response Times,"Many standardized tests are now administered via computer rather than paper-and-pencil format. The computer-based delivery mode brings with it certain advantages. One advantage is the ability to adapt the difficulty level of the test to the ability level of the test taker in what has been termed computerized adaptive testing (CAT). A second advantage is the ability to record not only the test taker's response to each item (i.e., question), but also the amount of time the test taker spends considering and answering each item. Combining these two advantages, various methods were explored for utilizing response time data in selecting appropriate items for an individual test taker. Four strategies for incorporating response time data were evaluated, and the precision of the final test-taker score was assessed by comparing it to a benchmark value that did not take response time information into account. While differences in measurement precision and testing times were expected, results showed that the strategies did not differ much with respect to measurement precision but that there were differences with regard to the total testing time. Copyright © 2016 by the National Council on Measurement in Education","['standardized', 'test', 'administer', 'computer', 'paperandpencil', 'format', 'computerbase', 'delivery', 'mode', 'bring', 'certain', 'advantage', 'advantage', 'ability', 'adapt', 'difficulty', 'level', 'test', 'ability', 'level', 'test', 'taker', 'term', 'computerized', 'adaptive', 'testing', 'CAT', 'second', 'advantage', 'ability', 'record', 'test', 'taker', 'response', 'item', 'ie', 'question', 'time', 'test', 'taker', 'spend', 'consider', 'answer', 'item', 'combine', 'advantage', 'method', 'explore', 'utilize', 'response', 'time', 'datum', 'select', 'appropriate', 'item', 'individual', 'test', 'taker', 'strategy', 'incorporate', 'response', 'time', 'datum', 'evaluate', 'precision', 'final', 'testtaker', 'score', 'assess', 'compare', 'benchmark', 'value', 'response', 'time', 'information', 'account', 'difference', 'precision', 'testing', 'time', 'expect', 'result', 'strategy', 'differ', 'respect', 'precision', 'difference', 'regard', 'total', 'testing', 'time', 'Copyright', '©', '2016', 'National', 'Council']","['issue', 'Item', 'Selection', 'Computerized', 'Adaptive', 'Testing', 'Response', 'Times']",standardized test administer computer paperandpencil format computerbase delivery mode bring certain advantage advantage ability adapt difficulty level test ability level test taker term computerized adaptive testing CAT second advantage ability record test taker response item ie question time test taker spend consider answer item combine advantage method explore utilize response time datum select appropriate item individual test taker strategy incorporate response time datum evaluate precision final testtaker score assess compare benchmark value response time information account difference precision testing time expect result strategy differ respect precision difference regard total testing time Copyright © 2016 National Council,issue Item Selection Computerized Adaptive Testing Response Times,0.02658505767987746,0.8940035875008536,0.026369088665617393,0.026399881331054847,0.026642384822596784,0.07209493893242581,0.01628647929584726,0.008967948932896092,0.0,0.0
Raymond M.R.; Swygert K.A.; Kahraman N.,Psychometric equivalence of ratings for repeat examinees on a performance assessment for physician licensure,2012,49,"Although a few studies report sizable score gains for examinees who repeat performance-based assessments, research has not yet addressed the reliability and validity of inferences based on ratings of repeat examinees on such tests. This study analyzed scores for 8,457 single-take examinees and 4,030 repeat examinees who completed a 6-hour clinical skills assessment required for physician licensure. Each examinee was rated in four skill domains: data gathering, communication-interpersonal skills, spoken English proficiency, and documentation proficiency. Conditional standard errors of measurement computed for single-take and multiple-take examinees indicated that ratings were of comparable precision for the two groups within each of the four skill domains; however, conditional errors were larger for low-scoring examinees regardless of retest status. In addition, on their first attempt multiple-take examinees exhibited less score consistency across the skill domains but on their second attempt their scores became more consistent. Further, the median correlation between scores on the four clinical skill domains and three external measures was .15 for multiple-take examinees on their first attempt but increased to .27 for their second attempt, a value, which was comparable to the median correlation of .26 for single-take examinees. The findings support the validity of inferences based on scores from the second attempt. © 2012 by the National Council on Measurement in Education.",Psychometric equivalence of ratings for repeat examinees on a performance assessment for physician licensure,"Although a few studies report sizable score gains for examinees who repeat performance-based assessments, research has not yet addressed the reliability and validity of inferences based on ratings of repeat examinees on such tests. This study analyzed scores for 8,457 single-take examinees and 4,030 repeat examinees who completed a 6-hour clinical skills assessment required for physician licensure. Each examinee was rated in four skill domains: data gathering, communication-interpersonal skills, spoken English proficiency, and documentation proficiency. Conditional standard errors of measurement computed for single-take and multiple-take examinees indicated that ratings were of comparable precision for the two groups within each of the four skill domains; however, conditional errors were larger for low-scoring examinees regardless of retest status. In addition, on their first attempt multiple-take examinees exhibited less score consistency across the skill domains but on their second attempt their scores became more consistent. Further, the median correlation between scores on the four clinical skill domains and three external measures was .15 for multiple-take examinees on their first attempt but increased to .27 for their second attempt, a value, which was comparable to the median correlation of .26 for single-take examinees. The findings support the validity of inferences based on scores from the second attempt. © 2012 by the National Council on Measurement in Education.","['study', 'report', 'sizable', 'score', 'gain', 'examinee', 'repeat', 'performancebase', 'assessment', 'research', 'address', 'reliability', 'validity', 'inference', 'base', 'rating', 'repeat', 'examine', 'test', 'study', 'analyze', 'score', '8457', 'singletake', 'examinee', '4030', 'repeat', 'examinee', 'complete', '6hour', 'clinical', 'skill', 'assessment', 'require', 'physician', 'licensure', 'examinee', 'rate', 'skill', 'domain', 'datum', 'gather', 'communicationinterpersonal', 'skill', 'speak', 'english', 'proficiency', 'documentation', 'proficiency', 'Conditional', 'standard', 'error', 'compute', 'singletake', 'multipletake', 'examinee', 'indicate', 'rating', 'comparable', 'precision', 'group', 'skill', 'domain', 'conditional', 'error', 'large', 'lowscore', 'examinee', 'regardless', 'ret', 'status', 'addition', 'attempt', 'multipletake', 'examinee', 'exhibit', 'score', 'consistency', 'skill', 'domain', 'second', 'attempt', 'score', 'consistent', 'far', 'median', 'correlation', 'score', 'clinical', 'skill', 'domain', 'external', 'measure', '15', 'multipletake', 'examine', 'attempt', 'increase', '27', 'second', 'attempt', 'value', 'comparable', 'median', 'correlation', '26', 'singletake', 'examine', 'finding', 'support', 'validity', 'inference', 'base', 'score', 'second', 'attempt', '©', '2012', 'National', 'Council']","['psychometric', 'equivalence', 'rating', 'repeat', 'examinee', 'performance', 'assessment', 'physician', 'licensure']",study report sizable score gain examinee repeat performancebase assessment research address reliability validity inference base rating repeat examine test study analyze score 8457 singletake examinee 4030 repeat examinee complete 6hour clinical skill assessment require physician licensure examinee rate skill domain datum gather communicationinterpersonal skill speak english proficiency documentation proficiency Conditional standard error compute singletake multipletake examinee indicate rating comparable precision group skill domain conditional error large lowscore examinee regardless ret status addition attempt multipletake examinee exhibit score consistency skill domain second attempt score consistent far median correlation score clinical skill domain external measure 15 multipletake examine attempt increase 27 second attempt value comparable median correlation 26 singletake examine finding support validity inference base score second attempt © 2012 National Council,psychometric equivalence rating repeat examinee performance assessment physician licensure,0.02632319573933079,0.026284226853673338,0.8949792200480047,0.026341221787508924,0.026072135571482174,0.0001913538928016075,0.025656326647787355,0.07055510877527793,0.007630765222728644,0.014662389794903849
Hsu C.-L.; Wang W.-C.,Variable-Length Computerized Adaptive Testing Using the Higher Order DINA Model,2015,52,"Cognitive diagnosis models provide profile information about a set of latent binary attributes, whereas item response models yield a summary report on a latent continuous trait. To utilize the advantages of both models, higher order cognitive diagnosis models were developed in which information about both latent binary attributes and latent continuous traits is available. To facilitate the utility of cognitive diagnosis models, corresponding computerized adaptive testing (CAT) algorithms were developed. Most of them adopt the fixed-length rule to terminate CAT and are limited to ordinary cognitive diagnosis models. In this study, the higher order deterministic-input, noisy-and-gate (DINA) model was used as an example, and three criteria based on the minimum-precision termination rule were implemented: one for the latent class, one for the latent trait, and the other for both. The simulation results demonstrated that all of the termination criteria were successful when items were selected according to the Kullback-Leibler information and the posterior-weighted Kullback-Leibler information, and the minimum-precision rule outperformed the fixed-length rule with a similar test length in recovering the latent attributes and the latent trait. © 2015 by the National Council on Measurement in Education.",Variable-Length Computerized Adaptive Testing Using the Higher Order DINA Model,"Cognitive diagnosis models provide profile information about a set of latent binary attributes, whereas item response models yield a summary report on a latent continuous trait. To utilize the advantages of both models, higher order cognitive diagnosis models were developed in which information about both latent binary attributes and latent continuous traits is available. To facilitate the utility of cognitive diagnosis models, corresponding computerized adaptive testing (CAT) algorithms were developed. Most of them adopt the fixed-length rule to terminate CAT and are limited to ordinary cognitive diagnosis models. In this study, the higher order deterministic-input, noisy-and-gate (DINA) model was used as an example, and three criteria based on the minimum-precision termination rule were implemented: one for the latent class, one for the latent trait, and the other for both. The simulation results demonstrated that all of the termination criteria were successful when items were selected according to the Kullback-Leibler information and the posterior-weighted Kullback-Leibler information, and the minimum-precision rule outperformed the fixed-length rule with a similar test length in recovering the latent attributes and the latent trait. © 2015 by the National Council on Measurement in Education.","['cognitive', 'diagnosis', 'provide', 'profile', 'information', 'set', 'latent', 'binary', 'attribute', 'item', 'response', 'yield', 'summary', 'report', 'latent', 'continuous', 'trait', 'utilize', 'advantage', 'high', 'order', 'cognitive', 'diagnosis', 'develop', 'information', 'latent', 'binary', 'attribute', 'latent', 'continuous', 'trait', 'available', 'facilitate', 'utility', 'cognitive', 'diagnosis', 'correspond', 'computerized', 'adaptive', 'testing', 'CAT', 'algorithm', 'develop', 'Most', 'adopt', 'fixedlength', 'rule', 'terminate', 'CAT', 'limit', 'ordinary', 'cognitive', 'diagnosis', 'study', 'high', 'order', 'deterministicinput', 'noisyandgate', 'DINA', 'example', 'criterion', 'base', 'minimumprecision', 'termination', 'rule', 'implement', 'latent', 'class', 'latent', 'trait', 'simulation', 'result', 'demonstrate', 'termination', 'criterion', 'successful', 'item', 'select', 'accord', 'KullbackLeibler', 'information', 'posteriorweighted', 'KullbackLeibler', 'information', 'minimumprecision', 'rule', 'outperform', 'fixedlength', 'rule', 'similar', 'test', 'length', 'recover', 'latent', 'attribute', 'latent', 'trait', '©', '2015', 'National', 'Council']","['VariableLength', 'Computerized', 'Adaptive', 'Testing', 'high', 'Order', 'DINA']",cognitive diagnosis provide profile information set latent binary attribute item response yield summary report latent continuous trait utilize advantage high order cognitive diagnosis develop information latent binary attribute latent continuous trait available facilitate utility cognitive diagnosis correspond computerized adaptive testing CAT algorithm develop Most adopt fixedlength rule terminate CAT limit ordinary cognitive diagnosis study high order deterministicinput noisyandgate DINA example criterion base minimumprecision termination rule implement latent class latent trait simulation result demonstrate termination criterion successful item select accord KullbackLeibler information posteriorweighted KullbackLeibler information minimumprecision rule outperform fixedlength rule similar test length recover latent attribute latent trait © 2015 National Council,VariableLength Computerized Adaptive Testing high Order DINA,0.02825705476303817,0.02824134705562149,0.027943600476190267,0.028318070350691768,0.8872399273544583,0.0505278861807626,0.0020329734207624925,0.0,0.0,0.0075615188158883765
Huang H.-Y.; Wang W.-C.,The random-effect DINA model,2014,51,"The DINA (deterministic input, noisy, and gate) model has been widely used in cognitive diagnosis tests and in the process of test development. The outcomes known as slip and guess are included in the DINA model function representing the responses to the items. This study aimed to extend the DINA model by using the random-effect approach to allow examinees to have different probabilities of slipping and guessing. Two extensions of the DINA model were developed and tested to represent the random components of slipping and guessing. The first model assumed that a random variable can be incorporated in the slipping parameters to allow examinees to have different levels of caution. The second model assumed that the examinees' ability may increase the probability of a correct response if they have not mastered all of the required attributes of an item. The results of a series of simulations based on Markov chain Monte Carlo methods showed that the model parameters and attribute-mastery profiles can be recovered relatively accurately from the generating models and that neglect of the random effects produces biases in parameter estimation. Finally, a fraction subtraction test was used as an empirical example to demonstrate the application of the new models. © 2014 by the National Council on Measurement in Education.",,"The DINA (deterministic input, noisy, and gate) model has been widely used in cognitive diagnosis tests and in the process of test development. The outcomes known as slip and guess are included in the DINA model function representing the responses to the items. This study aimed to extend the DINA model by using the random-effect approach to allow examinees to have different probabilities of slipping and guessing. Two extensions of the DINA model were developed and tested to represent the random components of slipping and guessing. The first model assumed that a random variable can be incorporated in the slipping parameters to allow examinees to have different levels of caution. The second model assumed that the examinees' ability may increase the probability of a correct response if they have not mastered all of the required attributes of an item. The results of a series of simulations based on Markov chain Monte Carlo methods showed that the model parameters and attribute-mastery profiles can be recovered relatively accurately from the generating models and that neglect of the random effects produces biases in parameter estimation. Finally, a fraction subtraction test was used as an empirical example to demonstrate the application of the new models. © 2014 by the National Council on Measurement in Education.","['DINA', 'deterministic', 'input', 'noisy', 'gate', 'widely', 'cognitive', 'diagnosis', 'test', 'process', 'test', 'development', 'outcome', 'know', 'slip', 'guess', 'include', 'DINA', 'function', 'represent', 'response', 'item', 'study', 'aim', 'extend', 'DINA', 'randomeffect', 'approach', 'allow', 'examinee', 'different', 'probability', 'slip', 'guess', 'extension', 'DINA', 'develop', 'test', 'represent', 'random', 'component', 'slip', 'guess', 'assume', 'random', 'variable', 'incorporate', 'slip', 'parameter', 'allow', 'examinee', 'different', 'level', 'caution', 'second', 'assume', 'examinees', 'ability', 'increase', 'probability', 'correct', 'response', 'master', 'require', 'attribute', 'item', 'result', 'series', 'simulation', 'base', 'Markov', 'chain', 'Monte', 'Carlo', 'method', 'parameter', 'attributemastery', 'profile', 'recover', 'relatively', 'accurately', 'generating', 'neglect', 'random', 'effect', 'produce', 'bias', 'parameter', 'estimation', 'finally', 'fraction', 'subtraction', 'test', 'empirical', 'example', 'demonstrate', 'application', 'new', '©', '2014', 'National', 'Council']",,DINA deterministic input noisy gate widely cognitive diagnosis test process test development outcome know slip guess include DINA function represent response item study aim extend DINA randomeffect approach allow examinee different probability slip guess extension DINA develop test represent random component slip guess assume random variable incorporate slip parameter allow examinee different level caution second assume examinees ability increase probability correct response master require attribute item result series simulation base Markov chain Monte Carlo method parameter attributemastery profile recover relatively accurately generating neglect random effect produce bias parameter estimation finally fraction subtraction test empirical example demonstrate application new © 2014 National Council,,0.024720326860734797,0.02450742073261147,0.024431048664019787,0.024630383240427264,0.9017108205022066,0.06842974898124057,0.0,0.006745805355173729,0.00041358944308678787,0.0008829378852014451
Oh H.; Moses T.,Comparison of the One- and Bi-Direction Chained Equipercentile Equating,2012,49,"This study investigated differences between two approaches to chained equipercentile (CE) equating (one- and bi-direction CE equating) in nearly equal groups and relatively unequal groups. In one-direction CE equating, the new form is linked to the anchor in one sample of examinees and the anchor is linked to the reference form in the other sample. In bi-direction CE equating, the anchor is linked to the new form in one sample of examinees and to the reference form in the other sample. The two approaches were evaluated in comparison to a criterion equating function (i.e., equivalent groups equating) using indexes such as root expected squared difference, bias, standard error of equating, root mean squared error, and number of gaps and bumps. The overall results across the equating situations suggested that the two CE equating approaches produced very similar results, whereas the bi-direction results were slightly less erratic, smoother (i.e., fewer gaps and bumps), usually closer to the criterion function, and also less variable. © 2012 by the National Council on Measurement in Education.",Comparison of the One- and Bi-Direction Chained Equipercentile Equating,"This study investigated differences between two approaches to chained equipercentile (CE) equating (one- and bi-direction CE equating) in nearly equal groups and relatively unequal groups. In one-direction CE equating, the new form is linked to the anchor in one sample of examinees and the anchor is linked to the reference form in the other sample. In bi-direction CE equating, the anchor is linked to the new form in one sample of examinees and to the reference form in the other sample. The two approaches were evaluated in comparison to a criterion equating function (i.e., equivalent groups equating) using indexes such as root expected squared difference, bias, standard error of equating, root mean squared error, and number of gaps and bumps. The overall results across the equating situations suggested that the two CE equating approaches produced very similar results, whereas the bi-direction results were slightly less erratic, smoother (i.e., fewer gaps and bumps), usually closer to the criterion function, and also less variable. © 2012 by the National Council on Measurement in Education.","['study', 'investigate', 'difference', 'approach', 'chain', 'equipercentile', 'CE', 'equate', 'bidirection', 'CE', 'equate', 'nearly', 'equal', 'group', 'relatively', 'unequal', 'group', 'onedirection', 'CE', 'equate', 'new', 'form', 'link', 'anchor', 'sample', 'examinee', 'anchor', 'link', 'reference', 'form', 'sample', 'bidirection', 'CE', 'equate', 'anchor', 'link', 'new', 'form', 'sample', 'examinee', 'reference', 'form', 'sample', 'approach', 'evaluate', 'comparison', 'criterion', 'equating', 'function', 'ie', 'equivalent', 'group', 'equate', 'index', 'root', 'expect', 'square', 'difference', 'bias', 'standard', 'error', 'equate', 'root', 'mean', 'square', 'error', 'number', 'gap', 'bump', 'overall', 'result', 'equate', 'situation', 'suggest', 'CE', 'equate', 'approach', 'produce', 'similar', 'result', 'bidirection', 'result', 'slightly', 'erratic', 'smoother', 'ie', 'gap', 'bump', 'usually', 'close', 'criterion', 'function', 'variable', '©', '2012', 'National', 'Council']","['Comparison', 'One', 'BiDirection', 'chain', 'Equipercentile', 'Equating']",study investigate difference approach chain equipercentile CE equate bidirection CE equate nearly equal group relatively unequal group onedirection CE equate new form link anchor sample examinee anchor link reference form sample bidirection CE equate anchor link new form sample examinee reference form sample approach evaluate comparison criterion equating function ie equivalent group equate index root expect square difference bias standard error equate root mean square error number gap bump overall result equate situation suggest CE equate approach produce similar result bidirection result slightly erratic smoother ie gap bump usually close criterion function variable © 2012 National Council,Comparison One BiDirection chain Equipercentile Equating,0.031242293979111737,0.02988043254631411,0.030452355738609285,0.8786266026608557,0.02979831507510923,0.0,0.0,0.0,0.1522244298528949,0.0
Guo H.; Oh H.J.; Eignor D.,Situations where it is appropriate to use frequency estimation equipercentile equating,2013,50,"In operational equating situations, frequency estimation equipercentile equating is considered only when the old and new groups have similar abilities. The frequency estimation assumptions are investigated in this study under various situations from both the levels of theoretical interest and practical use. It shows that frequency estimation equating can be used under circumstances when it is not normally used. To link theoretical results with practice, statistical methods are proposed for checking frequency estimation assumptions based on available data: observed-score distributions and item difficulty distributions of the forms. In addition to the conventional use of frequency estimation equating when the group abilities are similar, three situations are identified when the group abilities are dissimilar: (a) when the two forms and the observed conditional score distributions are similar the two forms and the observed conditional score distributions are similar (in this situation, the frequency estimation equating assumptions are likely to hold, and frequency estimation equating is appropriate); (b) when forms are similar but the observed conditional score distributions are not (in this situation, frequency estimation equating is not appropriate); and (c) when forms are not similar but the observed conditional score distributions are (frequency estimation equating is not appropriate). Statistical analysis procedures for comparing distributions are provided. Data from a large-scale test are used to illustrate the use of frequency estimation equating when the group difference in ability is large. © 2013 by the National Council on Measurement in Education.",Situations where it is appropriate to use frequency estimation equipercentile equating,"In operational equating situations, frequency estimation equipercentile equating is considered only when the old and new groups have similar abilities. The frequency estimation assumptions are investigated in this study under various situations from both the levels of theoretical interest and practical use. It shows that frequency estimation equating can be used under circumstances when it is not normally used. To link theoretical results with practice, statistical methods are proposed for checking frequency estimation assumptions based on available data: observed-score distributions and item difficulty distributions of the forms. In addition to the conventional use of frequency estimation equating when the group abilities are similar, three situations are identified when the group abilities are dissimilar: (a) when the two forms and the observed conditional score distributions are similar the two forms and the observed conditional score distributions are similar (in this situation, the frequency estimation equating assumptions are likely to hold, and frequency estimation equating is appropriate); (b) when forms are similar but the observed conditional score distributions are not (in this situation, frequency estimation equating is not appropriate); and (c) when forms are not similar but the observed conditional score distributions are (frequency estimation equating is not appropriate). Statistical analysis procedures for comparing distributions are provided. Data from a large-scale test are used to illustrate the use of frequency estimation equating when the group difference in ability is large. © 2013 by the National Council on Measurement in Education.","['operational', 'equate', 'situation', 'frequency', 'estimation', 'equipercentile', 'equating', 'consider', 'old', 'new', 'group', 'similar', 'ability', 'frequency', 'estimation', 'assumption', 'investigate', 'study', 'situation', 'level', 'theoretical', 'interest', 'practical', 'frequency', 'estimation', 'equating', 'circumstance', 'normally', 'link', 'theoretical', 'result', 'practice', 'statistical', 'method', 'propose', 'check', 'frequency', 'estimation', 'assumption', 'base', 'available', 'data', 'observedscore', 'distribution', 'item', 'difficulty', 'distribution', 'form', 'addition', 'conventional', 'frequency', 'estimation', 'equating', 'group', 'ability', 'similar', 'situation', 'identify', 'group', 'ability', 'dissimilar', 'form', 'observed', 'conditional', 'score', 'distribution', 'similar', 'form', 'observed', 'conditional', 'score', 'distribution', 'similar', 'situation', 'frequency', 'estimation', 'equate', 'assumption', 'likely', 'hold', 'frequency', 'estimation', 'equating', 'appropriate', 'b', 'form', 'similar', 'observed', 'conditional', 'score', 'distribution', 'situation', 'frequency', 'estimation', 'equating', 'appropriate', 'c', 'form', 'similar', 'observed', 'conditional', 'score', 'distribution', 'frequency', 'estimation', 'equating', 'appropriate', 'statistical', 'analysis', 'procedure', 'compare', 'distribution', 'provide', 'datum', 'largescale', 'test', 'illustrate', 'frequency', 'estimation', 'equating', 'group', 'difference', 'ability', 'large', '©', '2013', 'National', 'Council']","['situation', 'appropriate', 'frequency', 'estimation', 'equipercentile', 'equating']",operational equate situation frequency estimation equipercentile equating consider old new group similar ability frequency estimation assumption investigate study situation level theoretical interest practical frequency estimation equating circumstance normally link theoretical result practice statistical method propose check frequency estimation assumption base available data observedscore distribution item difficulty distribution form addition conventional frequency estimation equating group ability similar situation identify group ability dissimilar form observed conditional score distribution similar form observed conditional score distribution similar situation frequency estimation equate assumption likely hold frequency estimation equating appropriate b form similar observed conditional score distribution situation frequency estimation equating appropriate c form similar observed conditional score distribution frequency estimation equating appropriate statistical analysis procedure compare distribution provide datum largescale test illustrate frequency estimation equating group difference ability large © 2013 National Council,situation appropriate frequency estimation equipercentile equating,0.03378407480444791,0.03336687129335158,0.8658867397498173,0.03348213322191616,0.03348018093046684,0.014832320500104289,0.005069406722458934,0.0,0.10940487304405867,0.0
Samudra P.G.; Min I.; Cortina K.S.; Miller K.F.,No Second Chance to Make a First Impression: The “Thin-Slice” Effect on Instructor Ratings and Learning Outcomes in Higher Education,2016,53,"Prior research has found strong and persistent effects of instructor first impressions on student evaluations. Because these studies look at real classroom lessons, this finding fits two different interpretations: (1) first impressions may color student experience of instruction regardless of lesson quality, or (2) first impressions may provide valid evidence for instructional quality. By using scripted lessons, we experimentally investigated how first impression and instruction quality related to learning and evaluation of instruction among college students. Results from two studies indicate that quality of instruction is the strongest determinant of student factual and conceptual learning, but that both instructional quality and first impressions affect evaluations of the instructor. First impressions matter, but our findings suggest that lesson quality matters more. Copyright © 2016 by the National Council on Measurement in Education",No Second Chance to Make a First Impression: The “Thin-Slice” Effect on Instructor Ratings and Learning Outcomes in Higher Education,"Prior research has found strong and persistent effects of instructor first impressions on student evaluations. Because these studies look at real classroom lessons, this finding fits two different interpretations: (1) first impressions may color student experience of instruction regardless of lesson quality, or (2) first impressions may provide valid evidence for instructional quality. By using scripted lessons, we experimentally investigated how first impression and instruction quality related to learning and evaluation of instruction among college students. Results from two studies indicate that quality of instruction is the strongest determinant of student factual and conceptual learning, but that both instructional quality and first impressions affect evaluations of the instructor. First impressions matter, but our findings suggest that lesson quality matters more. Copyright © 2016 by the National Council on Measurement in Education","['prior', 'research', 'find', 'strong', 'persistent', 'effect', 'instructor', 'impression', 'student', 'evaluation', 'study', 'look', 'real', 'classroom', 'lesson', 'finding', 'fit', 'different', 'interpretation', '1', 'impression', 'color', 'student', 'experience', 'instruction', 'regardless', 'lesson', 'quality', '2', 'impression', 'provide', 'valid', 'evidence', 'instructional', 'quality', 'scripted', 'lesson', 'experimentally', 'investigate', 'impression', 'instruction', 'quality', 'relate', 'learning', 'evaluation', 'instruction', 'college', 'student', 'result', 'study', 'indicate', 'quality', 'instruction', 'strong', 'determinant', 'student', 'factual', 'conceptual', 'learning', 'instructional', 'quality', 'impression', 'affect', 'evaluation', 'instructor', 'impression', 'matter', 'finding', 'suggest', 'lesson', 'quality', 'matter', 'copyright', '©', '2016', 'National', 'Council']","['Second', 'Chance', 'Impression', '""', 'ThinSlice', '""', 'Effect', 'Instructor', 'Ratings', 'Learning', 'Outcomes', 'high']",prior research find strong persistent effect instructor impression student evaluation study look real classroom lesson finding fit different interpretation 1 impression color student experience instruction regardless lesson quality 2 impression provide valid evidence instructional quality scripted lesson experimentally investigate impression instruction quality relate learning evaluation instruction college student result study indicate quality instruction strong determinant student factual conceptual learning instructional quality impression affect evaluation instructor impression matter finding suggest lesson quality matter copyright © 2016 National Council,"Second Chance Impression "" ThinSlice "" Effect Instructor Ratings Learning Outcomes high",0.03410647216335202,0.034165014901553964,0.03412177955533421,0.8634777437174217,0.03412898966233819,0.0,0.0,0.058733554287501974,0.0,0.012848659061399483
Wang W.; Song L.; Chen P.; Meng Y.; Ding S.,Attribute-Level and Pattern-Level Classification Consistency and Accuracy Indices for Cognitive Diagnostic Assessment,2015,52,"Classification consistency and accuracy are viewed as important indicators for evaluating the reliability and validity of classification results in cognitive diagnostic assessment (CDA). Pattern-level classification consistency and accuracy indices were introduced by Cui, Gierl, and Chang. However, the indices at the attribute level have not yet been constructed. This study puts forward a simple approach to estimating the indices at both the attribute and the pattern level through one single test administration. Detailed elaboration is made on how the upper and lower bounds for the attribute-level accuracy can be derived from the variance of error of the attribute mastery probability estimate. In addition, based on Cui's pattern-level indices, an alternative approach to estimating the attribute-level indices is also proposed. Comparative analysis of simulation results indicate that the new indices are very desirable for evaluating test-retest consistency and correct classification rate. © 2015 by the National Council on Measurement in Education.",Attribute-Level and Pattern-Level Classification Consistency and Accuracy Indices for Cognitive Diagnostic Assessment,"Classification consistency and accuracy are viewed as important indicators for evaluating the reliability and validity of classification results in cognitive diagnostic assessment (CDA). Pattern-level classification consistency and accuracy indices were introduced by Cui, Gierl, and Chang. However, the indices at the attribute level have not yet been constructed. This study puts forward a simple approach to estimating the indices at both the attribute and the pattern level through one single test administration. Detailed elaboration is made on how the upper and lower bounds for the attribute-level accuracy can be derived from the variance of error of the attribute mastery probability estimate. In addition, based on Cui's pattern-level indices, an alternative approach to estimating the attribute-level indices is also proposed. Comparative analysis of simulation results indicate that the new indices are very desirable for evaluating test-retest consistency and correct classification rate. © 2015 by the National Council on Measurement in Education.","['classification', 'consistency', 'accuracy', 'view', 'important', 'indicator', 'evaluate', 'reliability', 'validity', 'classification', 'result', 'cognitive', 'diagnostic', 'assessment', 'CDA', 'Patternlevel', 'classification', 'consistency', 'accuracy', 'index', 'introduce', 'Cui', 'Gierl', 'Chang', 'index', 'attribute', 'level', 'construct', 'study', 'forward', 'simple', 'approach', 'estimate', 'index', 'attribute', 'pattern', 'level', 'single', 'test', 'administration', 'Detailed', 'elaboration', 'upper', 'low', 'bound', 'attributelevel', 'accuracy', 'derive', 'variance', 'error', 'attribute', 'mastery', 'probability', 'estimate', 'addition', 'base', 'Cuis', 'patternlevel', 'indice', 'alternative', 'approach', 'estimate', 'attributelevel', 'index', 'propose', 'comparative', 'analysis', 'simulation', 'result', 'indicate', 'new', 'index', 'desirable', 'evaluate', 'testret', 'consistency', 'correct', 'classification', 'rate', '©', '2015', 'National', 'Council']","['AttributeLevel', 'PatternLevel', 'Classification', 'Consistency', 'Accuracy', 'Indices', 'Cognitive', 'Diagnostic', 'Assessment']",classification consistency accuracy view important indicator evaluate reliability validity classification result cognitive diagnostic assessment CDA Patternlevel classification consistency accuracy index introduce Cui Gierl Chang index attribute level construct study forward simple approach estimate index attribute pattern level single test administration Detailed elaboration upper low bound attributelevel accuracy derive variance error attribute mastery probability estimate addition base Cuis patternlevel indice alternative approach estimate attributelevel index propose comparative analysis simulation result indicate new index desirable evaluate testret consistency correct classification rate © 2015 National Council,AttributeLevel PatternLevel Classification Consistency Accuracy Indices Cognitive Diagnostic Assessment,0.027205349039055048,0.026717328714245696,0.02681594050986844,0.02687787155026643,0.8923835101865644,0.04505695799523108,0.04392411971643363,0.01677838383078082,0.0,0.0
Belov D.I.,Robust Detection of Examinees With Aberrant Answer Changes,2015,52,"The statistical analysis of answer changes (ACs) has uncovered multiple testing irregularities on large-scale assessments and is now routinely performed at testing organizations. However, AC data has an uncertainty caused by technological or human factors. Therefore, existing statistics (e.g., number of wrong-to-right ACs) used to detect examinees with aberrant ACs capitalize on the uncertainty, which may result in a large Type I error. In this article, the information about ACs is used only for the partitioning of administered items into two disjoint subtests: items where ACs did not occur, and items where ACs did occur. A new statistic is based on the difference in performance between these subtests (measured as Kullback-Leibler divergence between corresponding posteriors of latent traits), where, in order to avoid the uncertainty, only final responses are used. One of the subtests can be filtered such that the asymptotic distribution of the statistic is chi-square with one degree of freedom. In computer simulations, the presented statistic demonstrated a strong robustness to the uncertainty and higher detection rates in contrast to two popular statistics based on wrong-to-right ACs. © 2015 by the National Council on Measurement in Education.",Robust Detection of Examinees With Aberrant Answer Changes,"The statistical analysis of answer changes (ACs) has uncovered multiple testing irregularities on large-scale assessments and is now routinely performed at testing organizations. However, AC data has an uncertainty caused by technological or human factors. Therefore, existing statistics (e.g., number of wrong-to-right ACs) used to detect examinees with aberrant ACs capitalize on the uncertainty, which may result in a large Type I error. In this article, the information about ACs is used only for the partitioning of administered items into two disjoint subtests: items where ACs did not occur, and items where ACs did occur. A new statistic is based on the difference in performance between these subtests (measured as Kullback-Leibler divergence between corresponding posteriors of latent traits), where, in order to avoid the uncertainty, only final responses are used. One of the subtests can be filtered such that the asymptotic distribution of the statistic is chi-square with one degree of freedom. In computer simulations, the presented statistic demonstrated a strong robustness to the uncertainty and higher detection rates in contrast to two popular statistics based on wrong-to-right ACs. © 2015 by the National Council on Measurement in Education.","['statistical', 'analysis', 'answer', 'change', 'ac', 'uncover', 'multiple', 'testing', 'irregularity', 'largescale', 'assessment', 'routinely', 'perform', 'testing', 'organization', 'However', 'AC', 'datum', 'uncertainty', 'cause', 'technological', 'human', 'factor', 'exist', 'statistic', 'eg', 'number', 'wrongtoright', 'ac', 'detect', 'examinee', 'aberrant', 'ac', 'capitalize', 'uncertainty', 'result', 'large', 'type', 'I', 'error', 'article', 'information', 'ac', 'partitioning', 'administer', 'item', 'disjoint', 'subtest', 'item', 'ac', 'occur', 'item', 'ac', 'occur', 'new', 'statistic', 'base', 'difference', 'performance', 'subtest', 'measure', 'KullbackLeibler', 'divergence', 'corresponding', 'posterior', 'latent', 'trait', 'order', 'avoid', 'uncertainty', 'final', 'response', 'subtest', 'filter', 'asymptotic', 'distribution', 'statistic', 'chisquare', 'degree', 'freedom', 'computer', 'simulation', 'present', 'statistic', 'demonstrate', 'strong', 'robustness', 'uncertainty', 'high', 'detection', 'rate', 'contrast', 'popular', 'statistic', 'base', 'wrongtoright', 'ac', '©', '2015', 'National', 'Council']","['robust', 'Detection', 'Examinees', 'Aberrant', 'Answer', 'change']",statistical analysis answer change ac uncover multiple testing irregularity largescale assessment routinely perform testing organization However AC datum uncertainty cause technological human factor exist statistic eg number wrongtoright ac detect examinee aberrant ac capitalize uncertainty result large type I error article information ac partitioning administer item disjoint subtest item ac occur item ac occur new statistic base difference performance subtest measure KullbackLeibler divergence corresponding posterior latent trait order avoid uncertainty final response subtest filter asymptotic distribution statistic chisquare degree freedom computer simulation present statistic demonstrate strong robustness uncertainty high detection rate contrast popular statistic base wrongtoright ac © 2015 National Council,robust Detection Examinees Aberrant Answer change,0.03128585243604332,0.03030063697651099,0.03039262898102203,0.8772812254943384,0.030739656112085217,0.03660298673583123,0.0,0.01207552460545034,0.0,0.0003492338333247511
Kim S.,Generalization of the Lord-Wingersky algorithm to computing the distribution of summed test scores based on real-number item scores,2013,50,"With known item response theory (IRT) item parameters, Lord and Wingersky provided a recursive algorithm for computing the conditional frequency distribution of number-correct test scores, given proficiency. This article presents a generalized algorithm for computing the conditional distribution of summed test scores involving real-number item scores. The generalized algorithm is distinct from the Lord-Wingersky algorithm in that it explicitly incorporates the task of figuring out all possible unique real-number test scores in each recursion. Some applications of the generalized recursive algorithm, such as IRT test score reliability estimation and IRT proficiency estimation based on summed test scores, are illustrated with a short test by varying scoring schemes for its items. © 2013 by the National Council on Measurement in Education.",Generalization of the Lord-Wingersky algorithm to computing the distribution of summed test scores based on real-number item scores,"With known item response theory (IRT) item parameters, Lord and Wingersky provided a recursive algorithm for computing the conditional frequency distribution of number-correct test scores, given proficiency. This article presents a generalized algorithm for computing the conditional distribution of summed test scores involving real-number item scores. The generalized algorithm is distinct from the Lord-Wingersky algorithm in that it explicitly incorporates the task of figuring out all possible unique real-number test scores in each recursion. Some applications of the generalized recursive algorithm, such as IRT test score reliability estimation and IRT proficiency estimation based on summed test scores, are illustrated with a short test by varying scoring schemes for its items. © 2013 by the National Council on Measurement in Education.","['know', 'item', 'response', 'theory', 'IRT', 'item', 'parameter', 'Lord', 'Wingersky', 'provide', 'recursive', 'algorithm', 'compute', 'conditional', 'frequency', 'distribution', 'numbercorrect', 'test', 'score', 'proficiency', 'article', 'present', 'generalized', 'algorithm', 'compute', 'conditional', 'distribution', 'sum', 'test', 'score', 'involve', 'realnumber', 'item', 'score', 'generalized', 'algorithm', 'distinct', 'LordWingersky', 'algorithm', 'explicitly', 'incorporate', 'task', 'figure', 'possible', 'unique', 'realnumber', 'test', 'score', 'recursion', 'application', 'generalized', 'recursive', 'algorithm', 'IRT', 'test', 'score', 'reliability', 'estimation', 'IRT', 'proficiency', 'estimation', 'base', 'sum', 'test', 'score', 'illustrate', 'short', 'test', 'vary', 'scoring', 'scheme', 'item', '©', '2013', 'National', 'Council']","['generalization', 'LordWingersky', 'algorithm', 'compute', 'distribution', 'sum', 'test', 'score', 'base', 'realnumber', 'item', 'score']",know item response theory IRT item parameter Lord Wingersky provide recursive algorithm compute conditional frequency distribution numbercorrect test score proficiency article present generalized algorithm compute conditional distribution sum test score involve realnumber item score generalized algorithm distinct LordWingersky algorithm explicitly incorporate task figure possible unique realnumber test score recursion application generalized recursive algorithm IRT test score reliability estimation IRT proficiency estimation base sum test score illustrate short test vary scoring scheme item © 2013 National Council,generalization LordWingersky algorithm compute distribution sum test score base realnumber item score,0.03021670028018012,0.8791690084069985,0.03051684681699752,0.030111428874025705,0.029986015621798184,0.04966325137737377,0.017726232772140124,0.016848531699018218,0.011598393074371513,0.0
Wiberg M.; Van Der Linden W.J.; Von Davier A.A.,Local observed-score kernel equating,2014,51,"Three local observed-score kernel equating methods that integrate methods from the local equating and kernel equating frameworks are proposed. The new methods were compared with their earlier counterparts with respect to such measures as bias-as defined by Lord's criterion of equity-and percent relative error. The local kernel item response theory observed-score equating method, which can be used for any of the common equating designs, had a small amount of bias, a low percent relative error, and a relatively low kernel standard error of equating, even when the accuracy of the test was reduced. The local kernel equating methods for the nonequivalent groups with anchor test generally had low bias and were quite stable against changes in the accuracy or length of the anchor test. Although all proposed methods showed small percent relative errors, the local kernel equating methods for the nonequivalent groups with anchor test design had somewhat larger standard error of equating than their kernel method counterparts. © 2014 by the National Council on Measurement in Education.",,"Three local observed-score kernel equating methods that integrate methods from the local equating and kernel equating frameworks are proposed. The new methods were compared with their earlier counterparts with respect to such measures as bias-as defined by Lord's criterion of equity-and percent relative error. The local kernel item response theory observed-score equating method, which can be used for any of the common equating designs, had a small amount of bias, a low percent relative error, and a relatively low kernel standard error of equating, even when the accuracy of the test was reduced. The local kernel equating methods for the nonequivalent groups with anchor test generally had low bias and were quite stable against changes in the accuracy or length of the anchor test. Although all proposed methods showed small percent relative errors, the local kernel equating methods for the nonequivalent groups with anchor test design had somewhat larger standard error of equating than their kernel method counterparts. © 2014 by the National Council on Measurement in Education.","['local', 'observedscore', 'kernel', 'equating', 'method', 'integrate', 'method', 'local', 'equating', 'kernel', 'equate', 'framework', 'propose', 'new', 'method', 'compare', 'early', 'counterpart', 'respect', 'measure', 'biasa', 'define', 'Lords', 'criterion', 'equityand', 'percent', 'relative', 'error', 'local', 'kernel', 'item', 'response', 'theory', 'observedscore', 'equate', 'method', 'common', 'equating', 'design', 'small', 'bias', 'low', 'percent', 'relative', 'error', 'relatively', 'low', 'kernel', 'standard', 'error', 'equate', 'accuracy', 'test', 'reduce', 'local', 'kernel', 'equate', 'method', 'nonequivalent', 'group', 'anchor', 'test', 'generally', 'low', 'bias', 'stable', 'change', 'accuracy', 'length', 'anchor', 'test', 'propose', 'method', 'small', 'percent', 'relative', 'error', 'local', 'kernel', 'equate', 'method', 'nonequivalent', 'group', 'anchor', 'test', 'design', 'somewhat', 'large', 'standard', 'error', 'equate', 'kernel', 'method', 'counterpart', '©', '2014', 'National', 'Council']",,local observedscore kernel equating method integrate method local equating kernel equate framework propose new method compare early counterpart respect measure biasa define Lords criterion equityand percent relative error local kernel item response theory observedscore equate method common equating design small bias low percent relative error relatively low kernel standard error equate accuracy test reduce local kernel equate method nonequivalent group anchor test generally low bias stable change accuracy length anchor test propose method small percent relative error local kernel equate method nonequivalent group anchor test design somewhat large standard error equate kernel method counterpart © 2014 National Council,,0.03393938122507656,0.03314446957116208,0.8661401346595378,0.03354050822163417,0.033235506322589585,0.005227891268005671,0.0,0.0,0.17462359544382455,0.0
Puhan G.,Choosing Among Tucker or Chained Linear Equating in Two Testing Situations: Rater Comparability Scoring and Randomly Equivalent Groups With an Anchor,2012,49,"Tucker and chained linear equatings were evaluated in two testing scenarios. In Scenario 1, referred to as rater comparability scoring and equating, the anchor-to-total correlation is often very high for the new form but moderate for the reference form. This may adversely affect the results of Tucker equating, especially if the new and reference form samples differ in ability. In Scenario 2, the new and reference form samples are randomly equivalent but the correlation between the anchor and total scores is low. When the correlation between the anchor and total scores is low, Tucker equating assumes that the new and reference form samples are similar in ability (which, with randomly equivalents groups, is the correct assumption). Thus Tucker equating should produce accurate results. Results indicated that in Scenario 1, the Tucker results were less accurate than the chained linear equating results. However, in Scenario 2, the Tucker results were more accurate than the chained linear equating results. Some implications are discussed. © 2012 by the National Council on Measurement in Education.",Choosing Among Tucker or Chained Linear Equating in Two Testing Situations: Rater Comparability Scoring and Randomly Equivalent Groups With an Anchor,"Tucker and chained linear equatings were evaluated in two testing scenarios. In Scenario 1, referred to as rater comparability scoring and equating, the anchor-to-total correlation is often very high for the new form but moderate for the reference form. This may adversely affect the results of Tucker equating, especially if the new and reference form samples differ in ability. In Scenario 2, the new and reference form samples are randomly equivalent but the correlation between the anchor and total scores is low. When the correlation between the anchor and total scores is low, Tucker equating assumes that the new and reference form samples are similar in ability (which, with randomly equivalents groups, is the correct assumption). Thus Tucker equating should produce accurate results. Results indicated that in Scenario 1, the Tucker results were less accurate than the chained linear equating results. However, in Scenario 2, the Tucker results were more accurate than the chained linear equating results. Some implications are discussed. © 2012 by the National Council on Measurement in Education.","['Tucker', 'chain', 'linear', 'equating', 'evaluate', 'testing', 'scenario', 'Scenario', '1', 'refer', 'rater', 'comparability', 'scoring', 'equate', 'anchortototal', 'correlation', 'high', 'new', 'form', 'moderate', 'reference', 'form', 'adversely', 'affect', 'result', 'Tucker', 'equate', 'especially', 'new', 'reference', 'form', 'sample', 'differ', 'ability', 'Scenario', '2', 'new', 'reference', 'form', 'sample', 'randomly', 'equivalent', 'correlation', 'anchor', 'total', 'score', 'low', 'correlation', 'anchor', 'total', 'score', 'low', 'Tucker', 'equating', 'assume', 'new', 'reference', 'form', 'sample', 'similar', 'ability', 'randomly', 'equivalent', 'group', 'correct', 'assumption', 'Thus', 'Tucker', 'equating', 'produce', 'accurate', 'result', 'result', 'indicate', 'Scenario', '1', 'Tucker', 'result', 'accurate', 'chain', 'linear', 'equate', 'result', 'Scenario', '2', 'Tucker', 'result', 'accurate', 'chain', 'linear', 'equating', 'result', 'implication', 'discuss', '©', '2012', 'National', 'Council']","['choose', 'Tucker', 'chain', 'Linear', 'Equating', 'Testing', 'Situations', 'Rater', 'Comparability', 'Scoring', 'Randomly', 'Equivalent', 'group', 'anchor']",Tucker chain linear equating evaluate testing scenario Scenario 1 refer rater comparability scoring equate anchortototal correlation high new form moderate reference form adversely affect result Tucker equate especially new reference form sample differ ability Scenario 2 new reference form sample randomly equivalent correlation anchor total score low correlation anchor total score low Tucker equating assume new reference form sample similar ability randomly equivalent group correct assumption Thus Tucker equating produce accurate result result indicate Scenario 1 Tucker result accurate chain linear equate result Scenario 2 Tucker result accurate chain linear equating result implication discuss © 2012 National Council,choose Tucker chain Linear Equating Testing Situations Rater Comparability Scoring Randomly Equivalent group anchor,0.036449183262071076,0.033696079647878595,0.8643853977487529,0.032692688610410504,0.03277665073088691,0.0,0.021767426524597275,0.0,0.11559201728674089,0.02511045959975134
French B.F.; Finch W.H.,Transforming SIBTEST to Account for Multilevel Data Structures,2015,52,"SIBTEST is a differential item functioning (DIF) detection method that is accurate and effective with small samples, in the presence of group mean differences, and for assessment of both uniform and nonuniform DIF. The presence of multilevel data with DIF detection has received increased attention. Ignoring such structure can inflate Type I error. This simulation study examines the performance of newly developed multilevel adaptations of SIBTEST in the presence of multilevel data. Data were simulated in a multilevel framework and both uniform and nonuniform DIF were assessed. Study results demonstrated that naïve SIBTEST and Crossing SIBTEST, ignoring the multilevel data structure, yield inflated Type I error rates, while certain multilevel extensions provided better error and accuracy control. © 2015 by the National Council on Measurement in Education.",Transforming SIBTEST to Account for Multilevel Data Structures,"SIBTEST is a differential item functioning (DIF) detection method that is accurate and effective with small samples, in the presence of group mean differences, and for assessment of both uniform and nonuniform DIF. The presence of multilevel data with DIF detection has received increased attention. Ignoring such structure can inflate Type I error. This simulation study examines the performance of newly developed multilevel adaptations of SIBTEST in the presence of multilevel data. Data were simulated in a multilevel framework and both uniform and nonuniform DIF were assessed. Study results demonstrated that naïve SIBTEST and Crossing SIBTEST, ignoring the multilevel data structure, yield inflated Type I error rates, while certain multilevel extensions provided better error and accuracy control. © 2015 by the National Council on Measurement in Education.","['SIBTEST', 'differential', 'item', 'function', 'dif', 'detection', 'method', 'accurate', 'effective', 'small', 'sample', 'presence', 'group', 'mean', 'difference', 'assessment', 'uniform', 'nonuniform', 'DIF', 'presence', 'multilevel', 'datum', 'dif', 'detection', 'receive', 'increase', 'attention', 'ignore', 'structure', 'inflate', 'type', 'I', 'error', 'simulation', 'study', 'examine', 'performance', 'newly', 'develop', 'multilevel', 'adaptation', 'SIBTEST', 'presence', 'multilevel', 'datum', 'Data', 'simulate', 'multilevel', 'framework', 'uniform', 'nonuniform', 'DIF', 'assess', 'Study', 'result', 'demonstrate', 'naïve', 'SIBTEST', 'Crossing', 'SIBTEST', 'ignore', 'multilevel', 'datum', 'structure', 'yield', 'inflate', 'Type', 'I', 'error', 'rate', 'certain', 'multilevel', 'extension', 'provide', 'error', 'accuracy', 'control', '©', '2015', 'National', 'Council']","['transform', 'SIBTEST', 'Account', 'Multilevel', 'Data', 'Structures']",SIBTEST differential item function dif detection method accurate effective small sample presence group mean difference assessment uniform nonuniform DIF presence multilevel datum dif detection receive increase attention ignore structure inflate type I error simulation study examine performance newly develop multilevel adaptation SIBTEST presence multilevel datum Data simulate multilevel framework uniform nonuniform DIF assess Study result demonstrate naïve SIBTEST Crossing SIBTEST ignore multilevel datum structure yield inflate Type I error rate certain multilevel extension provide error accuracy control © 2015 National Council,transform SIBTEST Account Multilevel Data Structures,0.8755796017208746,0.03106012226470525,0.03096117476196253,0.031198026567438614,0.031201074685018912,0.06017918233502059,0.0,0.003975473118249063,0.001381032373658842,0.003720323039698343
Chen H.,A Comparison Between Linear IRT Observed-Score Equating and Levine Observed-Score Equating Under the Generalized Kernel Equating Framework,2012,49,"In this article, linear item response theory (IRT) observed-score equating is compared under a generalized kernel equating framework with Levine observed-score equating for nonequivalent groups with anchor test design. Interestingly, these two equating methods are closely related despite being based on different methodologies. Specifically, when using data from IRT models, linear IRT observed-score equating is virtually identical to Levine observed-score equating. This leads to the conclusion that poststratification equating based on true anchor scores can be viewed as the curvilinear Levine observed-score equating. © 2012 by the National Council on Measurement in Education.",A Comparison Between Linear IRT Observed-Score Equating and Levine Observed-Score Equating Under the Generalized Kernel Equating Framework,"In this article, linear item response theory (IRT) observed-score equating is compared under a generalized kernel equating framework with Levine observed-score equating for nonequivalent groups with anchor test design. Interestingly, these two equating methods are closely related despite being based on different methodologies. Specifically, when using data from IRT models, linear IRT observed-score equating is virtually identical to Levine observed-score equating. This leads to the conclusion that poststratification equating based on true anchor scores can be viewed as the curvilinear Levine observed-score equating. © 2012 by the National Council on Measurement in Education.","['article', 'linear', 'item', 'response', 'theory', 'IRT', 'observedscore', 'equating', 'compare', 'generalized', 'kernel', 'equate', 'framework', 'Levine', 'observedscore', 'equating', 'nonequivalent', 'group', 'anchor', 'test', 'design', 'interestingly', 'equate', 'method', 'closely', 'relate', 'despite', 'base', 'different', 'methodology', 'specifically', 'datum', 'IRT', 'linear', 'IRT', 'observedscore', 'equating', 'virtually', 'identical', 'Levine', 'observedscore', 'equate', 'lead', 'conclusion', 'poststratification', 'equating', 'base', 'true', 'anchor', 'score', 'view', 'curvilinear', 'Levine', 'observedscore', 'equating', '©', '2012', 'National', 'Council']","['Comparison', 'Linear', 'IRT', 'ObservedScore', 'Equating', 'Levine', 'ObservedScore', 'Equating', 'Generalized', 'Kernel', 'Equating', 'Framework']",article linear item response theory IRT observedscore equating compare generalized kernel equate framework Levine observedscore equating nonequivalent group anchor test design interestingly equate method closely relate despite base different methodology specifically datum IRT linear IRT observedscore equating virtually identical Levine observedscore equate lead conclusion poststratification equating base true anchor score view curvilinear Levine observedscore equating © 2012 National Council,Comparison Linear IRT ObservedScore Equating Levine ObservedScore Equating Generalized Kernel Equating Framework,0.036578302510245374,0.035384405583453446,0.035806392569069284,0.8565236284685498,0.03570727086868204,0.0,0.0,0.0,0.18897192396069823,0.0
Andersson B.; von Davier A.A.,Improving the bandwidth selection in kernel equating,2014,51,"We investigate the current bandwidth selection methods in kernel equating and propose a method based on Silverman's rule of thumb for selecting the bandwidth parameters. In kernel equating, the bandwidth parameters have previously been obtained by minimizing a penalty function. This minimization process has been criticized by practitioners for being too complex and that it does not offer sufficient smoothing in certain cases. In addition, the bandwidth parameters have been treated as constants in the derivation of the standard error of equating even when they were selected by considering the observed data. Here, the bandwidth selection is simplified, and modified standard errors of equating (SEEs) that reflect the bandwidth selection method are derived. The method is illustrated with real data examples and simulated data. © 2014 by the National Council on Measurement in Education.",Improving the bandwidth selection in kernel equating,"We investigate the current bandwidth selection methods in kernel equating and propose a method based on Silverman's rule of thumb for selecting the bandwidth parameters. In kernel equating, the bandwidth parameters have previously been obtained by minimizing a penalty function. This minimization process has been criticized by practitioners for being too complex and that it does not offer sufficient smoothing in certain cases. In addition, the bandwidth parameters have been treated as constants in the derivation of the standard error of equating even when they were selected by considering the observed data. Here, the bandwidth selection is simplified, and modified standard errors of equating (SEEs) that reflect the bandwidth selection method are derived. The method is illustrated with real data examples and simulated data. © 2014 by the National Council on Measurement in Education.","['investigate', 'current', 'bandwidth', 'selection', 'method', 'kernel', 'equating', 'propose', 'method', 'base', 'Silvermans', 'rule', 'thumb', 'select', 'bandwidth', 'parameter', 'kernel', 'equate', 'bandwidth', 'parameter', 'previously', 'obtain', 'minimize', 'penalty', 'function', 'minimization', 'process', 'criticize', 'practitioner', 'complex', 'offer', 'sufficient', 'smoothing', 'certain', 'case', 'addition', 'bandwidth', 'parameter', 'treat', 'constant', 'derivation', 'standard', 'error', 'equate', 'select', 'consider', 'observed', 'datum', 'bandwidth', 'selection', 'simplify', 'modify', 'standard', 'error', 'equate', 'SEEs', 'reflect', 'bandwidth', 'selection', 'method', 'derive', 'method', 'illustrate', 'real', 'datum', 'example', 'simulated', 'datum', '©', '2014', 'National', 'Council']","['improve', 'bandwidth', 'selection', 'kernel', 'equating']",investigate current bandwidth selection method kernel equating propose method base Silvermans rule thumb select bandwidth parameter kernel equate bandwidth parameter previously obtain minimize penalty function minimization process criticize practitioner complex offer sufficient smoothing certain case addition bandwidth parameter treat constant derivation standard error equate select consider observed datum bandwidth selection simplify modify standard error equate SEEs reflect bandwidth selection method derive method illustrate real datum example simulated datum © 2014 National Council,improve bandwidth selection kernel equating,0.03283644655602192,0.03228254088086007,0.0322010655642396,0.8702278052722516,0.032452141726626936,0.017724384622683605,0.0,0.0,0.08537705800383798,0.0001295028593258701
Li F.; Cohen A.; Shen L.,Investigating the Effect of Item Position in Computer-Based Tests,2012,49,"Computer-based tests (CBTs) often use random ordering of items in order to minimize item exposure and reduce the potential for answer copying. Little research has been done, however, to examine item position effects for these tests. In this study, different versions of a Rasch model and different response time models were examined and applied to data from a CBT administration of a medical licensure examination. The models specifically were used to investigate whether item position affected item difficulty and item intensity estimates. Results indicated that the position effect was negligible. © 2012 by the National Council on Measurement in Education.",Investigating the Effect of Item Position in Computer-Based Tests,"Computer-based tests (CBTs) often use random ordering of items in order to minimize item exposure and reduce the potential for answer copying. Little research has been done, however, to examine item position effects for these tests. In this study, different versions of a Rasch model and different response time models were examined and applied to data from a CBT administration of a medical licensure examination. The models specifically were used to investigate whether item position affected item difficulty and item intensity estimates. Results indicated that the position effect was negligible. © 2012 by the National Council on Measurement in Education.","['computerbased', 'test', 'cbt', 'random', 'ordering', 'item', 'order', 'minimize', 'item', 'exposure', 'reduce', 'potential', 'answer', 'copy', 'little', 'research', 'examine', 'item', 'position', 'effect', 'test', 'study', 'different', 'version', 'Rasch', 'different', 'response', 'time', 'examine', 'apply', 'datum', 'CBT', 'administration', 'medical', 'licensure', 'examination', 'specifically', 'investigate', 'item', 'position', 'affect', 'item', 'difficulty', 'item', 'intensity', 'estimate', 'result', 'indicate', 'position', 'effect', 'negligible', '©', '2012', 'National', 'Council']","['investigate', 'Effect', 'Item', 'Position', 'ComputerBased', 'Tests']",computerbased test cbt random ordering item order minimize item exposure reduce potential answer copy little research examine item position effect test study different version Rasch different response time examine apply datum CBT administration medical licensure examination specifically investigate item position affect item difficulty item intensity estimate result indicate position effect negligible © 2012 National Council,investigate Effect Item Position ComputerBased Tests,0.03167096689315381,0.03119797816697427,0.8743464818946136,0.03108795009689399,0.03169662294836426,0.08192264000384178,0.0,0.0,0.0,0.006260224193216829
Sachse K.A.; Roppelt A.; Haag N.,A Comparison of Linking Methods for Estimating National Trends in International Comparative Large-Scale Assessments in the Presence of Cross-National DIF,2016,53,"Trend estimation in international comparative large-scale assessments relies on measurement invariance between countries. However, cross-national differential item functioning (DIF) has been repeatedly documented. We ran a simulation study using national item parameters, which required trends to be computed separately for each country, to compare trend estimation performances to two linking methods employing international item parameters across several conditions. The trend estimates based on the national item parameters were more accurate than the trend estimates based on the international item parameters when cross-national DIF was present. Moreover, the use of fixed common item parameter calibrations led to biased trend estimates. The detection and elimination of DIF can reduce this bias but is also likely to increase the total error. Copyright © 2016 by the National Council on Measurement in Education",A Comparison of Linking Methods for Estimating National Trends in International Comparative Large-Scale Assessments in the Presence of Cross-National DIF,"Trend estimation in international comparative large-scale assessments relies on measurement invariance between countries. However, cross-national differential item functioning (DIF) has been repeatedly documented. We ran a simulation study using national item parameters, which required trends to be computed separately for each country, to compare trend estimation performances to two linking methods employing international item parameters across several conditions. The trend estimates based on the national item parameters were more accurate than the trend estimates based on the international item parameters when cross-national DIF was present. Moreover, the use of fixed common item parameter calibrations led to biased trend estimates. The detection and elimination of DIF can reduce this bias but is also likely to increase the total error. Copyright © 2016 by the National Council on Measurement in Education","['trend', 'estimation', 'international', 'comparative', 'largescale', 'assessment', 'rely', 'invariance', 'country', 'crossnational', 'differential', 'item', 'function', 'DIF', 'repeatedly', 'document', 'run', 'simulation', 'study', 'national', 'item', 'parameter', 'require', 'trend', 'compute', 'separately', 'country', 'compare', 'trend', 'estimation', 'performance', 'link', 'method', 'employ', 'international', 'item', 'parameter', 'condition', 'trend', 'estimate', 'base', 'national', 'item', 'parameter', 'accurate', 'trend', 'estimate', 'base', 'international', 'item', 'parameter', 'crossnational', 'DIF', 'present', 'fix', 'common', 'item', 'parameter', 'calibration', 'lead', 'biased', 'trend', 'estimate', 'detection', 'elimination', 'DIF', 'reduce', 'bias', 'likely', 'increase', 'total', 'error', 'copyright', '©', '2016', 'National', 'Council']","['Comparison', 'Linking', 'Methods', 'estimate', 'National', 'Trends', 'International', 'Comparative', 'LargeScale', 'assessment', 'Presence', 'CrossNational', 'DIF']",trend estimation international comparative largescale assessment rely invariance country crossnational differential item function DIF repeatedly document run simulation study national item parameter require trend compute separately country compare trend estimation performance link method employ international item parameter condition trend estimate base national item parameter accurate trend estimate base international item parameter crossnational DIF present fix common item parameter calibration lead biased trend estimate detection elimination DIF reduce bias likely increase total error copyright © 2016 National Council,Comparison Linking Methods estimate National Trends International Comparative LargeScale assessment Presence CrossNational DIF,0.033232582254334564,0.03307115520313498,0.8673556380670627,0.03299653123831301,0.03334409323715462,0.07684817059018953,0.0,0.0,0.0,0.0
Shu L.; Schwarz R.D.,IRT-estimated reliability for tests containing mixed item formats,2014,51,"As a global measure of precision, item response theory (IRT) estimated reliability is derived for four coefficients (Cronbach's α, Feldt-Raju, stratified α, and marginal reliability). Models with different underlying assumptions concerning test-part similarity are discussed. A detailed computational example is presented for the targeted coefficients. A comparison of the IRT model-derived coefficients is made and the impact of varying ability distributions is evaluated. The advantages of IRT-derived reliability coefficients for problems such as automated test form assembly and vertical scaling are discussed. © 2014 by the National Council on Measurement in Education.",IRT-estimated reliability for tests containing mixed item formats,"As a global measure of precision, item response theory (IRT) estimated reliability is derived for four coefficients (Cronbach's α, Feldt-Raju, stratified α, and marginal reliability). Models with different underlying assumptions concerning test-part similarity are discussed. A detailed computational example is presented for the targeted coefficients. A comparison of the IRT model-derived coefficients is made and the impact of varying ability distributions is evaluated. The advantages of IRT-derived reliability coefficients for problems such as automated test form assembly and vertical scaling are discussed. © 2014 by the National Council on Measurement in Education.","['global', 'measure', 'precision', 'item', 'response', 'theory', 'IRT', 'estimate', 'reliability', 'derive', 'coefficient', 'Cronbachs', 'α', 'FeldtRaju', 'stratify', 'α', 'marginal', 'reliability', 'Models', 'different', 'underlying', 'assumption', 'concern', 'testpart', 'similarity', 'discuss', 'detailed', 'computational', 'example', 'present', 'target', 'coefficient', 'comparison', 'IRT', 'modelderive', 'coefficient', 'impact', 'vary', 'ability', 'distribution', 'evaluate', 'advantage', 'irtderive', 'reliability', 'coefficient', 'problem', 'automate', 'test', 'form', 'assembly', 'vertical', 'scaling', 'discuss', '©', '2014', 'National', 'Council']","['irtestimate', 'reliability', 'test', 'contain', 'mixed', 'item', 'format']",global measure precision item response theory IRT estimate reliability derive coefficient Cronbachs α FeldtRaju stratify α marginal reliability Models different underlying assumption concern testpart similarity discuss detailed computational example present target coefficient comparison IRT modelderive coefficient impact vary ability distribution evaluate advantage irtderive reliability coefficient problem automate test form assembly vertical scaling discuss © 2014 National Council,irtestimate reliability test contain mixed item format,0.02977899753318613,0.029732748064719806,0.881208596157835,0.029685320507172083,0.029594337737087074,0.04022298810710483,0.01675985910040202,0.00833876394334697,0.002223751924000917,0.0
Haberman S.; Yao L.,Repeater Analysis for Combining Information From Different Assessments,2015,52,"Admission decisions frequently rely on multiple assessments. As a consequence, it is important to explore rational approaches to combine the information from different educational tests. For example, U.S. graduate schools usually receive both TOEFL iBT® scores and GRE® General scores of foreign applicants for admission; however, little guidance has been given to combine information from these two assessments, even though the relationships between such sections as GRE Verbal and TOEFL iBT Reading are obvious. In this study, principles are provided to explore the extent to which different assessments complement one another and are distinguishable. Augmentation approaches developed for individual tests are applied to provide an accurate evaluation of combined assessments. Because augmentation methods require estimates of measurement error and internal reliability data are unavailable, required estimates of measurement error are obtained from repeaters, examinees who took the same test more than once. Because repeaters are not representative of all examinees in typical assessments, minimum discriminant information adjustment techniques are applied to the available sample of repeaters to treat the effect of selection bias. To illustrate methodology, combining information from TOEFL iBT scores and GRE General scores is examined. Analysis suggests that information from the GRE General and TOEFL iBT assessments is complementary but not redundant, indicating that the two tests measure related but somewhat different constructs. The proposed methodology can be readily applied to other situations where multiple assessments are needed. © 2015 by the National Council on Measurement in Education.",Repeater Analysis for Combining Information From Different Assessments,"Admission decisions frequently rely on multiple assessments. As a consequence, it is important to explore rational approaches to combine the information from different educational tests. For example, U.S. graduate schools usually receive both TOEFL iBT® scores and GRE® General scores of foreign applicants for admission; however, little guidance has been given to combine information from these two assessments, even though the relationships between such sections as GRE Verbal and TOEFL iBT Reading are obvious. In this study, principles are provided to explore the extent to which different assessments complement one another and are distinguishable. Augmentation approaches developed for individual tests are applied to provide an accurate evaluation of combined assessments. Because augmentation methods require estimates of measurement error and internal reliability data are unavailable, required estimates of measurement error are obtained from repeaters, examinees who took the same test more than once. Because repeaters are not representative of all examinees in typical assessments, minimum discriminant information adjustment techniques are applied to the available sample of repeaters to treat the effect of selection bias. To illustrate methodology, combining information from TOEFL iBT scores and GRE General scores is examined. Analysis suggests that information from the GRE General and TOEFL iBT assessments is complementary but not redundant, indicating that the two tests measure related but somewhat different constructs. The proposed methodology can be readily applied to other situations where multiple assessments are needed. © 2015 by the National Council on Measurement in Education.","['admission', 'decision', 'frequently', 'rely', 'multiple', 'assessment', 'consequence', 'important', 'explore', 'rational', 'approach', 'combine', 'information', 'different', 'educational', 'test', 'example', 'US', 'graduate', 'school', 'usually', 'receive', 'toefl', 'iBT', '®', 'score', 'GRE', '®', 'General', 'score', 'foreign', 'applicant', 'admission', 'little', 'guidance', 'combine', 'information', 'assessment', 'relationship', 'section', 'GRE', 'Verbal', 'toefl', 'iBT', 'reading', 'obvious', 'study', 'principle', 'provide', 'explore', 'extent', 'different', 'assessment', 'complement', 'distinguishable', 'Augmentation', 'approach', 'develop', 'individual', 'test', 'apply', 'provide', 'accurate', 'evaluation', 'combine', 'assessment', 'augmentation', 'method', 'require', 'estimate', 'error', 'internal', 'reliability', 'datum', 'unavailable', 'require', 'estimate', 'error', 'obtain', 'repeater', 'examinee', 'test', 'repeater', 'representative', 'examinee', 'typical', 'assessment', 'minimum', 'discriminant', 'information', 'adjustment', 'technique', 'apply', 'available', 'sample', 'repeater', 'treat', 'effect', 'selection', 'bias', 'illustrate', 'methodology', 'combine', 'information', 'TOEFL', 'iBT', 'score', 'GRE', 'General', 'score', 'examine', 'Analysis', 'suggest', 'information', 'GRE', 'General', 'TOEFL', 'iBT', 'assessment', 'complementary', 'redundant', 'indicate', 'test', 'measure', 'relate', 'somewhat', 'different', 'construct', 'propose', 'methodology', 'readily', 'apply', 'situation', 'multiple', 'assessment', 'need', '©', '2015', 'National', 'Council']","['Repeater', 'Analysis', 'combine', 'Information', 'different', 'assessment']",admission decision frequently rely multiple assessment consequence important explore rational approach combine information different educational test example US graduate school usually receive toefl iBT ® score GRE ® General score foreign applicant admission little guidance combine information assessment relationship section GRE Verbal toefl iBT reading obvious study principle provide explore extent different assessment complement distinguishable Augmentation approach develop individual test apply provide accurate evaluation combine assessment augmentation method require estimate error internal reliability datum unavailable require estimate error obtain repeater examinee test repeater representative examinee typical assessment minimum discriminant information adjustment technique apply available sample repeater treat effect selection bias illustrate methodology combine information TOEFL iBT score GRE General score examine Analysis suggest information GRE General TOEFL iBT assessment complementary redundant indicate test measure relate somewhat different construct propose methodology readily apply situation multiple assessment need © 2015 National Council,Repeater Analysis combine Information different assessment,0.023930423300209662,0.023999965746264656,0.024023881559191634,0.9039417655567553,0.024103963837578932,0.0210002836930543,0.004766858850518956,0.0723512699073577,0.0063566113499894375,0.003586075376137647
Wang C.; Zheng C.; Chang H.-H.,An Enhanced Approach to Combine Item Response Theory With Cognitive Diagnosis in Adaptive Testing,2014,51,"Computerized adaptive testing offers the possibility of gaining information on both the overall ability and cognitive profile in a single assessment administration. Some algorithms aiming for these dual purposes have been proposed, including the shadow test approach, the dual information method (DIM), and the constraint weighted method. The current study proposed two new methods, aggregate ranked information index (ARI) and aggregate standardized information index (ASI), which appropriately addressed the noncompatibility issue inherent in the original DIM method. More flexible weighting schemes that put different emphasis on information about general ability (i.e., θ in item response theory) and information about cognitive profile (i.e., α in cognitive diagnostic modeling) were also explored. Two simulation studies were carried out to investigate the effectiveness of the new methods and weighting schemes. Results showed that the new methods with the flexible weighting schemes could produce more accurate estimation of both overall ability and cognitive profile than the original DIM. Among them, the ASI with both empirical and theoretical weights is recommended, and attribute-level weighting scheme is preferred if some attributes are considered more important from a substantive perspective. © 2014 by the National Council on Measurement in Education.",An Enhanced Approach to Combine Item Response Theory With Cognitive Diagnosis in Adaptive Testing,"Computerized adaptive testing offers the possibility of gaining information on both the overall ability and cognitive profile in a single assessment administration. Some algorithms aiming for these dual purposes have been proposed, including the shadow test approach, the dual information method (DIM), and the constraint weighted method. The current study proposed two new methods, aggregate ranked information index (ARI) and aggregate standardized information index (ASI), which appropriately addressed the noncompatibility issue inherent in the original DIM method. More flexible weighting schemes that put different emphasis on information about general ability (i.e., θ in item response theory) and information about cognitive profile (i.e., α in cognitive diagnostic modeling) were also explored. Two simulation studies were carried out to investigate the effectiveness of the new methods and weighting schemes. Results showed that the new methods with the flexible weighting schemes could produce more accurate estimation of both overall ability and cognitive profile than the original DIM. Among them, the ASI with both empirical and theoretical weights is recommended, and attribute-level weighting scheme is preferred if some attributes are considered more important from a substantive perspective. © 2014 by the National Council on Measurement in Education.","['computerized', 'adaptive', 'testing', 'offer', 'possibility', 'gain', 'information', 'overall', 'ability', 'cognitive', 'profile', 'single', 'assessment', 'administration', 'algorithm', 'aim', 'dual', 'purpose', 'propose', 'include', 'shadow', 'test', 'approach', 'dual', 'information', 'method', 'DIM', 'constraint', 'weight', 'method', 'current', 'study', 'propose', 'new', 'method', 'aggregate', 'rank', 'information', 'index', 'ARI', 'aggregate', 'standardized', 'information', 'index', 'ASI', 'appropriately', 'address', 'noncompatibility', 'issue', 'inherent', 'original', 'DIM', 'method', 'flexible', 'weighting', 'scheme', 'different', 'emphasis', 'information', 'general', 'ability', 'ie', 'θ', 'item', 'response', 'theory', 'information', 'cognitive', 'profile', 'ie', 'α', 'cognitive', 'diagnostic', 'modeling', 'explore', 'simulation', 'study', 'carry', 'investigate', 'effectiveness', 'new', 'method', 'weighting', 'scheme', 'result', 'new', 'method', 'flexible', 'weighting', 'scheme', 'produce', 'accurate', 'estimation', 'overall', 'ability', 'cognitive', 'profile', 'original', 'DIM', 'ASI', 'empirical', 'theoretical', 'weight', 'recommend', 'attributelevel', 'weighting', 'scheme', 'prefer', 'attribute', 'consider', 'important', 'substantive', 'perspective', '©', '2014', 'National', 'Council']","['enhanced', 'Approach', 'Combine', 'Item', 'Response', 'Theory', 'cognitive', 'Diagnosis', 'Adaptive', 'Testing']",computerized adaptive testing offer possibility gain information overall ability cognitive profile single assessment administration algorithm aim dual purpose propose include shadow test approach dual information method DIM constraint weight method current study propose new method aggregate rank information index ARI aggregate standardized information index ASI appropriately address noncompatibility issue inherent original DIM method flexible weighting scheme different emphasis information general ability ie θ item response theory information cognitive profile ie α cognitive diagnostic modeling explore simulation study carry investigate effectiveness new method weighting scheme result new method flexible weighting scheme produce accurate estimation overall ability cognitive profile original DIM ASI empirical theoretical weight recommend attributelevel weighting scheme prefer attribute consider important substantive perspective © 2014 National Council,enhanced Approach Combine Item Response Theory cognitive Diagnosis Adaptive Testing,0.02537816901727639,0.8990491776515415,0.025077229060108005,0.025101340467035358,0.02539408380403871,0.04420627553642252,0.009434702255558194,0.019710029395072187,0.009693965336262057,0.0
Debeer D.; Janssen R.,Modeling item-position effects within an IRT framework,2013,50,"Changing the order of items between alternate test forms to prevent copying and to enhance test security is a common practice in achievement testing. However, these changes in item order may affect item and test characteristics. Several procedures have been proposed for studying these item-order effects. The present study explores the use of descriptive and explanatory models from item response theory for detecting and modeling these effects in a one-step procedure. The framework also allows for consideration of the impact of individual differences in position effect on item difficulty. A simulation was conducted to investigate the impact of a position effect on parameter recovery in a Rasch model. As an illustration, the framework was applied to a listening comprehension test for French as a foreign language and to data from the PISA 2006 assessment.Copyright © 2013 by the National Council on Measurement in Education.",Modeling item-position effects within an IRT framework,"Changing the order of items between alternate test forms to prevent copying and to enhance test security is a common practice in achievement testing. However, these changes in item order may affect item and test characteristics. Several procedures have been proposed for studying these item-order effects. The present study explores the use of descriptive and explanatory models from item response theory for detecting and modeling these effects in a one-step procedure. The framework also allows for consideration of the impact of individual differences in position effect on item difficulty. A simulation was conducted to investigate the impact of a position effect on parameter recovery in a Rasch model. As an illustration, the framework was applied to a listening comprehension test for French as a foreign language and to data from the PISA 2006 assessment.Copyright © 2013 by the National Council on Measurement in Education.","['change', 'order', 'item', 'alternate', 'test', 'form', 'prevent', 'copying', 'enhance', 'test', 'security', 'common', 'practice', 'achievement', 'testing', 'change', 'item', 'order', 'affect', 'item', 'test', 'characteristic', 'procedure', 'propose', 'study', 'itemorder', 'effect', 'present', 'study', 'explore', 'descriptive', 'explanatory', 'item', 'response', 'theory', 'detect', 'effect', 'onestep', 'procedure', 'framework', 'allow', 'consideration', 'impact', 'individual', 'difference', 'position', 'effect', 'item', 'difficulty', 'simulation', 'conduct', 'investigate', 'impact', 'position', 'effect', 'parameter', 'recovery', 'Rasch', 'illustration', 'framework', 'apply', 'listening', 'comprehension', 'test', 'French', 'foreign', 'language', 'datum', 'PISA', '2006', 'assessmentcopyright', '©', '2013', 'National', 'Council']","['itemposition', 'effect', 'IRT', 'framework']",change order item alternate test form prevent copying enhance test security common practice achievement testing change item order affect item test characteristic procedure propose study itemorder effect present study explore descriptive explanatory item response theory detect effect onestep procedure framework allow consideration impact individual difference position effect item difficulty simulation conduct investigate impact position effect parameter recovery Rasch illustration framework apply listening comprehension test French foreign language datum PISA 2006 assessmentcopyright © 2013 National Council,itemposition effect IRT framework,0.0256937791695928,0.02552125215359607,0.02564492543245357,0.02557753005921265,0.8975625131851449,0.08731620298076462,0.0,0.0011685035427507434,0.0,0.008848356629892112
Kane M.T.,"Validation as a Pragmatic, Scientific Activity",2013,50,"This response to the comments contains three main sections, each addressing a subset of the comments. In the first section, I will respond to the comments by Brennan, Haertel, and Moss. All of these comments suggest ways in which my presentation could be extended or improved; I generally agree with their suggestions, so my response to their comments is brief. In the second section, I will respond to suggestions by Newton and Sireci that my framework be simplified by employing only one kind of argument, a validity argument, and dropping the interpretation/use argument (IUA); I am sympathetic to their desire for greater simplicity, but I see considerable value in keeping the IUA as a framework for the validation effort and will argue for keeping both the IUA and the validity argument. In the third section, I will respond to Borsboom and Markus, who raise a fundamental objection to my approach to validation, suggesting that I give too much attention to justification and too little to truth as a criterion for validity; I don't accept their proposed conception of validity, and I will indicate why. © 2013 by the National Council on Measurement in Education.",,"This response to the comments contains three main sections, each addressing a subset of the comments. In the first section, I will respond to the comments by Brennan, Haertel, and Moss. All of these comments suggest ways in which my presentation could be extended or improved; I generally agree with their suggestions, so my response to their comments is brief. In the second section, I will respond to suggestions by Newton and Sireci that my framework be simplified by employing only one kind of argument, a validity argument, and dropping the interpretation/use argument (IUA); I am sympathetic to their desire for greater simplicity, but I see considerable value in keeping the IUA as a framework for the validation effort and will argue for keeping both the IUA and the validity argument. In the third section, I will respond to Borsboom and Markus, who raise a fundamental objection to my approach to validation, suggesting that I give too much attention to justification and too little to truth as a criterion for validity; I don't accept their proposed conception of validity, and I will indicate why. © 2013 by the National Council on Measurement in Education.","['response', 'comment', 'contain', 'main', 'section', 'address', 'subset', 'comment', 'section', 'I', 'respond', 'comment', 'Brennan', 'Haertel', 'Moss', 'comment', 'suggest', 'way', 'presentation', 'extend', 'improve', 'I', 'generally', 'agree', 'suggestion', 'response', 'comment', 'brief', 'second', 'section', 'I', 'respond', 'suggestion', 'Newton', 'Sireci', 'framework', 'simplify', 'employ', 'kind', 'argument', 'validity', 'argument', 'drop', 'interpretationuse', 'argument', 'IUA', 'I', 'sympathetic', 'desire', 'great', 'simplicity', 'I', 'considerable', 'value', 'IUA', 'framework', 'validation', 'effort', 'argue', 'IUA', 'validity', 'argument', 'section', 'I', 'respond', 'Borsboom', 'Markus', 'raise', 'fundamental', 'objection', 'approach', 'validation', 'suggest', 'I', 'attention', 'justification', 'little', 'truth', 'criterion', 'validity', 'I', 'accept', 'propose', 'conception', 'validity', 'I', 'indicate', '©', '2013', 'National', 'Council']",,response comment contain main section address subset comment section I respond comment Brennan Haertel Moss comment suggest way presentation extend improve I generally agree suggestion response comment brief second section I respond suggestion Newton Sireci framework simplify employ kind argument validity argument drop interpretationuse argument IUA I sympathetic desire great simplicity I considerable value IUA framework validation effort argue IUA validity argument section I respond Borsboom Markus raise fundamental objection approach validation suggest I attention justification little truth criterion validity I accept propose conception validity I indicate © 2013 National Council,,0.028950814773590055,0.8843543367184958,0.028801971369986263,0.028745959339997268,0.029146917797930768,1.8548729418841092e-05,0.009920119111860125,0.05675840196619428,0.0,0.0
Ranger J.; Kuhn J.-T.,Assessing Fit of Item Response Models Using the Information Matrix Test,2012,49,"The information matrix can equivalently be determined via the expectation of the Hessian matrix or the expectation of the outer product of the score vector. The identity of these two matrices, however, is only valid in case of a correctly specified model. Therefore, differences between the two versions of the observed information matrix indicate model misfit. The equality of both matrices can be tested with the so-called information matrix test as a general test of misspecification. This test can be adapted to item response models in order to evaluate the fit of single items and the fit of the whole scale. The performance of different versions of the test is compared in a simulation study with existing tests of model fit, among them thetest of Orlando and Thissen, the score test of local independence due to Glas and Suarez-Falcon, and the limited information approach of Maydeu-Olivares and Joe. In general, the different versions of the information matrix test adhere to the nominal Type I error rate and have high power for detecting misspecified item characteristic curves. Additionally, some versions of the test can be used in order to detect violations of the local independence assumption. © 2012 by the National Council on Measurement in Education.",Assessing Fit of Item Response Models Using the Information Matrix Test,"The information matrix can equivalently be determined via the expectation of the Hessian matrix or the expectation of the outer product of the score vector. The identity of these two matrices, however, is only valid in case of a correctly specified model. Therefore, differences between the two versions of the observed information matrix indicate model misfit. The equality of both matrices can be tested with the so-called information matrix test as a general test of misspecification. This test can be adapted to item response models in order to evaluate the fit of single items and the fit of the whole scale. The performance of different versions of the test is compared in a simulation study with existing tests of model fit, among them thetest of Orlando and Thissen, the score test of local independence due to Glas and Suarez-Falcon, and the limited information approach of Maydeu-Olivares and Joe. In general, the different versions of the information matrix test adhere to the nominal Type I error rate and have high power for detecting misspecified item characteristic curves. Additionally, some versions of the test can be used in order to detect violations of the local independence assumption. © 2012 by the National Council on Measurement in Education.","['information', 'matrix', 'equivalently', 'determine', 'expectation', 'hessian', 'matrix', 'expectation', 'outer', 'product', 'score', 'vector', 'identity', 'matrix', 'valid', 'case', 'correctly', 'specify', 'difference', 'version', 'observe', 'information', 'matrix', 'indicate', 'misfit', 'equality', 'matrix', 'test', 'socalled', 'information', 'matrix', 'test', 'general', 'test', 'misspecification', 'test', 'adapt', 'item', 'response', 'order', 'evaluate', 'fit', 'single', 'item', 'fit', 'scale', 'performance', 'different', 'version', 'test', 'compare', 'simulation', 'study', 'exist', 'test', 'fit', 'thetest', 'Orlando', 'Thissen', 'score', 'test', 'local', 'independence', 'Glas', 'SuarezFalcon', 'limited', 'information', 'approach', 'MaydeuOlivares', 'Joe', 'general', 'different', 'version', 'information', 'matrix', 'test', 'adhere', 'nominal', 'Type', 'I', 'error', 'rate', 'high', 'power', 'detect', 'misspecified', 'item', 'characteristic', 'curve', 'additionally', 'version', 'test', 'order', 'detect', 'violation', 'local', 'independence', 'assumption', '©', '2012', 'National', 'Council']","['assess', 'Fit', 'Item', 'Response', 'Models', 'Information', 'Matrix', 'test']",information matrix equivalently determine expectation hessian matrix expectation outer product score vector identity matrix valid case correctly specify difference version observe information matrix indicate misfit equality matrix test socalled information matrix test general test misspecification test adapt item response order evaluate fit single item fit scale performance different version test compare simulation study exist test fit thetest Orlando Thissen score test local independence Glas SuarezFalcon limited information approach MaydeuOlivares Joe general different version information matrix test adhere nominal Type I error rate high power detect misspecified item characteristic curve additionally version test order detect violation local independence assumption © 2012 National Council,assess Fit Item Response Models Information Matrix test,0.02821775208064937,0.887304693526674,0.02809563255568232,0.028367244942200213,0.028014676894794057,0.05501394292692876,0.005768920264978134,0.005262206726410507,0.003873302156092522,0.0
Moses T.,Relationships of measurement error and prediction error in observed-score regression,2012,49,The focus of this paper is assessing the impact of measurement errors on the prediction error of an observed-score regression. Measures are presented and described for decomposing the linear regression's prediction error variance into parts attributable to the true score variance and the error variances of the dependent variable and the predictor variable(s). These measures are demonstrated for regression situations reflecting a range of true score correlations and reliabilities and using one and two predictors. Simulation results also are presented which show that the measures of prediction error variance and its parts are generally well estimated for the considered ranges of true score correlations and reliabilities and for homoscedastic and heteroscedastic data. The final discussion considers how the decomposition might be useful for addressing additional questions about regression functions' prediction error variances. © 2012 by the National Council on Measurement in Education.,Relationships of measurement error and prediction error in observed-score regression,The focus of this paper is assessing the impact of measurement errors on the prediction error of an observed-score regression. Measures are presented and described for decomposing the linear regression's prediction error variance into parts attributable to the true score variance and the error variances of the dependent variable and the predictor variable(s). These measures are demonstrated for regression situations reflecting a range of true score correlations and reliabilities and using one and two predictors. Simulation results also are presented which show that the measures of prediction error variance and its parts are generally well estimated for the considered ranges of true score correlations and reliabilities and for homoscedastic and heteroscedastic data. The final discussion considers how the decomposition might be useful for addressing additional questions about regression functions' prediction error variances. © 2012 by the National Council on Measurement in Education.,"['focus', 'paper', 'assess', 'impact', 'error', 'prediction', 'error', 'observedscore', 'regression', 'measure', 'present', 'describe', 'decompose', 'linear', 'regression', 'prediction', 'error', 'variance', 'attributable', 'true', 'score', 'variance', 'error', 'variance', 'dependent', 'variable', 'predictor', 'variable', 'measure', 'demonstrate', 'regression', 'situation', 'reflect', 'range', 'true', 'score', 'correlation', 'reliability', 'predictor', 'Simulation', 'result', 'present', 'measure', 'prediction', 'error', 'variance', 'generally', 'estimate', 'consider', 'range', 'true', 'score', 'correlation', 'reliability', 'homoscedastic', 'heteroscedastic', 'datum', 'final', 'discussion', 'consider', 'decomposition', 'useful', 'address', 'additional', 'question', 'regression', 'function', 'prediction', 'error', 'variance', '©', '2012', 'National', 'Council']","['relationship', 'error', 'prediction', 'error', 'observedscore', 'regression']",focus paper assess impact error prediction error observedscore regression measure present describe decompose linear regression prediction error variance attributable true score variance error variance dependent variable predictor variable measure demonstrate regression situation reflect range true score correlation reliability predictor Simulation result present measure prediction error variance generally estimate consider range true score correlation reliability homoscedastic heteroscedastic datum final discussion consider decomposition useful address additional question regression function prediction error variance © 2012 National Council,relationship error prediction error observedscore regression,0.03138658312316868,0.031177028992064166,0.03123940635522551,0.8748339105558445,0.0313630709736972,0.01650750550531079,0.042890763102542036,0.02383832546397238,0.02528429599160901,0.0
Belov D.I.,Detection of test collusion via Kullback-Leibler divergence,2013,50,"The development of statistical methods for detecting test collusion is a new research direction in the area of test security. Test collusion may be described as large-scale sharing of test materials, including answers to test items. Current methods of detecting test collusion are based on statistics also used in answer-copying detection. Therefore, in computerized adaptive testing (CAT) these methods lose power because the actual test varies across examinees. This article addresses that problem by introducing a new approach that works in two stages: in Stage 1, test centers with an unusual distribution of a person-fit statistic are identified via Kullback-Leibler divergence; in Stage 2, examinees from identified test centers are analyzed further using the person-fit statistic, where the critical value is computed without data from the identified test centers. The approach is extremely flexible. One can employ any existing person-fit statistic. The approach can be applied to all major testing programs: paper-and-pencil testing (P&P), computer-based testing (CBT), multiple-stage testing (MST), and CAT. Also, the definition of test center is not limited by the geographic location (room, class, college) and can be extended to support various relations between examinees (from the same undergraduate college, from the same test-prep center, from the same group at a social network). The suggested approach was found to be effective in CAT for detecting groups of examinees with item pre-knowledge, meaning those with access (possibly unknown to us) to one or more subsets of items prior to the exam. © 2013 by the National Council on Measurement in Education.",Detection of test collusion via Kullback-Leibler divergence,"The development of statistical methods for detecting test collusion is a new research direction in the area of test security. Test collusion may be described as large-scale sharing of test materials, including answers to test items. Current methods of detecting test collusion are based on statistics also used in answer-copying detection. Therefore, in computerized adaptive testing (CAT) these methods lose power because the actual test varies across examinees. This article addresses that problem by introducing a new approach that works in two stages: in Stage 1, test centers with an unusual distribution of a person-fit statistic are identified via Kullback-Leibler divergence; in Stage 2, examinees from identified test centers are analyzed further using the person-fit statistic, where the critical value is computed without data from the identified test centers. The approach is extremely flexible. One can employ any existing person-fit statistic. The approach can be applied to all major testing programs: paper-and-pencil testing (P&P), computer-based testing (CBT), multiple-stage testing (MST), and CAT. Also, the definition of test center is not limited by the geographic location (room, class, college) and can be extended to support various relations between examinees (from the same undergraduate college, from the same test-prep center, from the same group at a social network). The suggested approach was found to be effective in CAT for detecting groups of examinees with item pre-knowledge, meaning those with access (possibly unknown to us) to one or more subsets of items prior to the exam. © 2013 by the National Council on Measurement in Education.","['development', 'statistical', 'method', 'detect', 'test', 'collusion', 'new', 'research', 'direction', 'area', 'test', 'security', 'Test', 'collusion', 'describe', 'largescale', 'sharing', 'test', 'material', 'include', 'answer', 'test', 'item', 'current', 'method', 'detect', 'test', 'collusion', 'base', 'statistic', 'answercopye', 'detection', 'computerized', 'adaptive', 'testing', 'CAT', 'method', 'lose', 'power', 'actual', 'test', 'vary', 'examine', 'article', 'address', 'problem', 'introduce', 'new', 'approach', 'work', 'stage', 'stage', '1', 'test', 'center', 'unusual', 'distribution', 'personfit', 'statistic', 'identify', 'KullbackLeibler', 'divergence', 'stage', '2', 'examine', 'identify', 'test', 'center', 'analyze', 'far', 'personfit', 'statistic', 'critical', 'value', 'compute', 'datum', 'identify', 'test', 'center', 'approach', 'extremely', 'flexible', 'employ', 'exist', 'personfit', 'statistic', 'approach', 'apply', 'major', 'testing', 'program', 'paperandpencil', 'testing', 'PP', 'computerbase', 'test', 'CBT', 'multiplestage', 'test', 'MST', 'CAT', 'definition', 'test', 'center', 'limit', 'geographic', 'location', 'room', 'class', 'college', 'extend', 'support', 'relation', 'examinee', 'undergraduate', 'college', 'testprep', 'center', 'group', 'social', 'network', 'suggest', 'approach', 'find', 'effective', 'CAT', 'detect', 'group', 'examinee', 'item', 'preknowledge', 'mean', 'access', 'possibly', 'unknown', 'subset', 'item', 'prior', 'exam', '©', '2013', 'National', 'Council']","['detection', 'test', 'collusion', 'KullbackLeibler', 'divergence']",development statistical method detect test collusion new research direction area test security Test collusion describe largescale sharing test material include answer test item current method detect test collusion base statistic answercopye detection computerized adaptive testing CAT method lose power actual test vary examine article address problem introduce new approach work stage stage 1 test center unusual distribution personfit statistic identify KullbackLeibler divergence stage 2 examine identify test center analyze far personfit statistic critical value compute datum identify test center approach extremely flexible employ exist personfit statistic approach apply major testing program paperandpencil testing PP computerbase test CBT multiplestage test MST CAT definition test center limit geographic location room class college extend support relation examinee undergraduate college testprep center group social network suggest approach find effective CAT detect group examinee item preknowledge mean access possibly unknown subset item prior exam © 2013 National Council,detection test collusion KullbackLeibler divergence,0.9082896308883268,0.022690988632479713,0.02300133734670735,0.022816598855301847,0.023201444277184093,0.06050301773248432,0.005593633499870964,0.025859246217029806,0.0023746330937487055,0.0
Li X.; Wang W.-C.,Assessment of differential item functioning under cognitive diagnosis models: The DINA model example,2015,52,"The assessment of differential item functioning (DIF) is routinely conducted to ensure test fairness and validity. Although many DIF assessment methods have been developed in the context of classical test theory and item response theory, they are not applicable for cognitive diagnosis models (CDMs), as the underlying latent attributes of CDMs are multidimensional and binary. This study proposes a very general DIF assessment method in the CDM framework which is applicable for various CDMs, more than two groups of examinees, and multiple grouping variables that are categorical, continuous, observed, or latent. The parameters can be estimated with Markov chain Monte Carlo algorithms implemented in the freeware WinBUGS. Simulation results demonstrated a good parameter recovery and advantages in DIF assessment for the new method over the Wald method. © 2015 by the National Council on Measurement in Education.",Assessment of differential item functioning under cognitive diagnosis models: The DINA model example,"The assessment of differential item functioning (DIF) is routinely conducted to ensure test fairness and validity. Although many DIF assessment methods have been developed in the context of classical test theory and item response theory, they are not applicable for cognitive diagnosis models (CDMs), as the underlying latent attributes of CDMs are multidimensional and binary. This study proposes a very general DIF assessment method in the CDM framework which is applicable for various CDMs, more than two groups of examinees, and multiple grouping variables that are categorical, continuous, observed, or latent. The parameters can be estimated with Markov chain Monte Carlo algorithms implemented in the freeware WinBUGS. Simulation results demonstrated a good parameter recovery and advantages in DIF assessment for the new method over the Wald method. © 2015 by the National Council on Measurement in Education.","['assessment', 'differential', 'item', 'function', 'DIF', 'routinely', 'conduct', 'ensure', 'test', 'fairness', 'validity', 'dif', 'assessment', 'method', 'develop', 'context', 'classical', 'test', 'theory', 'item', 'response', 'theory', 'applicable', 'cognitive', 'diagnosis', 'CDMs', 'underlie', 'latent', 'attribute', 'CDMs', 'multidimensional', 'binary', 'study', 'propose', 'general', 'DIF', 'assessment', 'method', 'CDM', 'framework', 'applicable', 'cdm', 'group', 'examinee', 'multiple', 'grouping', 'variable', 'categorical', 'continuous', 'observed', 'latent', 'parameter', 'estimate', 'Markov', 'chain', 'Monte', 'Carlo', 'algorithm', 'implement', 'freeware', 'WinBUGS', 'Simulation', 'result', 'demonstrate', 'good', 'parameter', 'recovery', 'advantage', 'dif', 'assessment', 'new', 'method', 'Wald', 'method', '©', '2015', 'National', 'Council']","['assessment', 'differential', 'item', 'function', 'cognitive', 'diagnosis', 'DINA', 'example']",assessment differential item function DIF routinely conduct ensure test fairness validity dif assessment method develop context classical test theory item response theory applicable cognitive diagnosis CDMs underlie latent attribute CDMs multidimensional binary study propose general DIF assessment method CDM framework applicable cdm group examinee multiple grouping variable categorical continuous observed latent parameter estimate Markov chain Monte Carlo algorithm implement freeware WinBUGS Simulation result demonstrate good parameter recovery advantage dif assessment new method Wald method © 2015 National Council,assessment differential item function cognitive diagnosis DINA example,0.026484321972571768,0.0263535144847063,0.026110094118487442,0.026663185895739308,0.8943888835284952,0.08847096390541095,0.0,0.02133952796385983,0.0,0.0
Puhan G.,Rater comparability scoring and equating: Does choice of target population weights matter in this context?,2013,50,"When a constructed-response test form is reused, raw scores from the two administrations of the form may not be comparable. The solution to this problem requires a rescoring, at the current administration, of examinee responses from the previous administration. The scores from this ""rescoring"" can be used as an anchor for equating. In this equating, the choice of weights for combining the samples to define the target population can be critical. In rescored data, the anchor usually correlates very strongly with the new form but only moderately with the reference form. This difference has a predictable impact: the equating results are most accurate when the target population is the reference form sample, least accurate when the target population is the new form sample, and somewhere in the middle when the new form and reference form samples are equally weighted in forming the target population. © 2013 by the National Council on Measurement in Education.",Rater comparability scoring and equating: Does choice of target population weights matter in this context?,"When a constructed-response test form is reused, raw scores from the two administrations of the form may not be comparable. The solution to this problem requires a rescoring, at the current administration, of examinee responses from the previous administration. The scores from this ""rescoring"" can be used as an anchor for equating. In this equating, the choice of weights for combining the samples to define the target population can be critical. In rescored data, the anchor usually correlates very strongly with the new form but only moderately with the reference form. This difference has a predictable impact: the equating results are most accurate when the target population is the reference form sample, least accurate when the target population is the new form sample, and somewhere in the middle when the new form and reference form samples are equally weighted in forming the target population. © 2013 by the National Council on Measurement in Education.","['constructedresponse', 'test', 'form', 'reuse', 'raw', 'score', 'administration', 'form', 'comparable', 'solution', 'problem', 'require', 'rescoring', 'current', 'administration', 'examinee', 'response', 'previous', 'administration', 'score', 'rescoring', 'anchor', 'equate', 'equate', 'choice', 'weight', 'combine', 'sample', 'define', 'target', 'population', 'critical', 'rescore', 'datum', 'anchor', 'usually', 'correlate', 'strongly', 'new', 'form', 'moderately', 'reference', 'form', 'difference', 'predictable', 'impact', 'equate', 'result', 'accurate', 'target', 'population', 'reference', 'form', 'sample', 'accurate', 'target', 'population', 'new', 'form', 'sample', 'middle', 'new', 'form', 'reference', 'form', 'sample', 'equally', 'weight', 'form', 'target', 'population', '©', '2013', 'National', 'Council']","['Rater', 'comparability', 'scoring', 'equating', 'choice', 'target', 'population', 'weight', 'matter', 'context']",constructedresponse test form reuse raw score administration form comparable solution problem require rescoring current administration examinee response previous administration score rescoring anchor equate equate choice weight combine sample define target population critical rescore datum anchor usually correlate strongly new form moderately reference form difference predictable impact equate result accurate target population reference form sample accurate target population new form sample middle new form reference form sample equally weight form target population © 2013 National Council,Rater comparability scoring equating choice target population weight matter context,0.03386033868155003,0.03284550361021933,0.8672020447710853,0.03311876802125818,0.03297334491588712,0.008899081556121085,0.01681718891207136,0.0,0.097702916506742,0.0031901336498251303
Albano A.D.,A general linear method for equating with small samples,2015,52,"Research on equating with small samples has shown that methods with stronger assumptions and fewer statistical estimates can lead to decreased error in the estimated equating function. This article introduces a new approach to linear observed-score equating, one which provides flexible control over how form difficulty is assumed versus estimated to change across the score scale. A general linear method is presented as an extension of traditional linear methods. The general method is then compared to other linear and nonlinear methods in terms of accuracy in estimating a criterion equating function. Results from two parametric bootstrapping studies based on real data demonstrate the usefulness of the general linear method. © 2015 by the National Council on Measurement in Education.",A general linear method for equating with small samples,"Research on equating with small samples has shown that methods with stronger assumptions and fewer statistical estimates can lead to decreased error in the estimated equating function. This article introduces a new approach to linear observed-score equating, one which provides flexible control over how form difficulty is assumed versus estimated to change across the score scale. A general linear method is presented as an extension of traditional linear methods. The general method is then compared to other linear and nonlinear methods in terms of accuracy in estimating a criterion equating function. Results from two parametric bootstrapping studies based on real data demonstrate the usefulness of the general linear method. © 2015 by the National Council on Measurement in Education.","['research', 'equate', 'small', 'sample', 'method', 'strong', 'assumption', 'statistical', 'estimate', 'lead', 'decrease', 'error', 'estimate', 'equating', 'function', 'article', 'introduce', 'new', 'approach', 'linear', 'observedscore', 'equate', 'provide', 'flexible', 'control', 'form', 'difficulty', 'assume', 'versus', 'estimate', 'change', 'score', 'scale', 'general', 'linear', 'method', 'present', 'extension', 'traditional', 'linear', 'method', 'general', 'method', 'compare', 'linear', 'nonlinear', 'method', 'term', 'accuracy', 'estimate', 'criterion', 'equating', 'function', 'result', 'parametric', 'bootstrappe', 'study', 'base', 'real', 'datum', 'demonstrate', 'usefulness', 'general', 'linear', 'method', '©', '2015', 'National', 'Council']","['general', 'linear', 'method', 'equate', 'small', 'sample']",research equate small sample method strong assumption statistical estimate lead decrease error estimate equating function article introduce new approach linear observedscore equate provide flexible control form difficulty assume versus estimate change score scale general linear method present extension traditional linear method general method compare linear nonlinear method term accuracy estimate criterion equating function result parametric bootstrappe study base real datum demonstrate usefulness general linear method © 2015 National Council,general linear method equate small sample,0.030488700391438667,0.02937301929149372,0.02962581402356737,0.02997461399701092,0.8805378522964894,0.032123960271761606,0.004046702389271515,0.006349963971043176,0.12781355223905488,0.0
Li Z.,Power and Sample Size Calculations for Logistic Regression Tests for Differential Item Functioning,2014,51,"Logistic regression is a popular method for detecting uniform and nonuniform differential item functioning (DIF) effects. Theoretical formulas for the power and sample size calculations are derived for likelihood ratio tests and Wald tests based on the asymptotic distribution of the maximum likelihood estimators for the logistic regression model. The power is related to the item response function (IRF) for the studied item, the latent trait distributions, and the sample sizes for the reference and focal groups. Simulation studies show that the theoretical values calculated from the formulas derived in the article are close to what are observed in the simulated data when the assumptions are satisfied. The robustness of the power formulas are studied with simulations when the assumptions are violated. © 2014 by the National Council on Measurement in Education .",Power and Sample Size Calculations for Logistic Regression Tests for Differential Item Functioning,"Logistic regression is a popular method for detecting uniform and nonuniform differential item functioning (DIF) effects. Theoretical formulas for the power and sample size calculations are derived for likelihood ratio tests and Wald tests based on the asymptotic distribution of the maximum likelihood estimators for the logistic regression model. The power is related to the item response function (IRF) for the studied item, the latent trait distributions, and the sample sizes for the reference and focal groups. Simulation studies show that the theoretical values calculated from the formulas derived in the article are close to what are observed in the simulated data when the assumptions are satisfied. The robustness of the power formulas are studied with simulations when the assumptions are violated. © 2014 by the National Council on Measurement in Education .","['logistic', 'regression', 'popular', 'method', 'detect', 'uniform', 'nonuniform', 'differential', 'item', 'function', 'dif', 'effect', 'theoretical', 'formula', 'power', 'sample', 'size', 'calculation', 'derive', 'likelihood', 'ratio', 'test', 'Wald', 'test', 'base', 'asymptotic', 'distribution', 'maximum', 'likelihood', 'estimator', 'logistic', 'regression', 'power', 'relate', 'item', 'response', 'function', 'IRF', 'study', 'item', 'latent', 'trait', 'distribution', 'sample', 'size', 'reference', 'focal', 'group', 'Simulation', 'study', 'theoretical', 'value', 'calculate', 'formula', 'derive', 'article', 'close', 'observe', 'simulated', 'datum', 'assumption', 'satisfied', 'robustness', 'power', 'formula', 'study', 'simulation', 'assumption', 'violate', '©', '2014', 'National', 'Council']","['power', 'Sample', 'Size', 'Calculations', 'Logistic', 'regression', 'test', 'Differential', 'Item', 'Functioning']",logistic regression popular method detect uniform nonuniform differential item function dif effect theoretical formula power sample size calculation derive likelihood ratio test Wald test base asymptotic distribution maximum likelihood estimator logistic regression power relate item response function IRF study item latent trait distribution sample size reference focal group Simulation study theoretical value calculate formula derive article close observe simulated datum assumption satisfied robustness power formula study simulation assumption violate © 2014 National Council,power Sample Size Calculations Logistic regression test Differential Item Functioning,0.8896984366467462,0.027696258339992407,0.02743792561168328,0.02759566446621612,0.02757171493536209,0.07236960673714371,0.001061379986257223,0.0,0.015294548463585524,0.0
Sireci S.G.,Agreeing on Validity Arguments,2013,50,"Kane (this issue) presents a comprehensive review of validity theory and reminds us that the focus of validation is on test score interpretations and use. In reacting to his article, I support the argument-based approach to validity and all of the major points regarding validation made by Dr. Kane. In addition, I call for a simpler, three-step method for developing validity arguments, one that focuses on explicit testing purposes, as suggested by the Standards for Educational and Psychological Testing. If testing purposes are appropriately articulated, the process of developing an interpretive argument becomes unnecessary and validation can directly address intended interpretations and uses. © 2013 by the National Council on Measurement in Education.",,"Kane (this issue) presents a comprehensive review of validity theory and reminds us that the focus of validation is on test score interpretations and use. In reacting to his article, I support the argument-based approach to validity and all of the major points regarding validation made by Dr. Kane. In addition, I call for a simpler, three-step method for developing validity arguments, one that focuses on explicit testing purposes, as suggested by the Standards for Educational and Psychological Testing. If testing purposes are appropriately articulated, the process of developing an interpretive argument becomes unnecessary and validation can directly address intended interpretations and uses. © 2013 by the National Council on Measurement in Education.","['Kane', 'issue', 'present', 'comprehensive', 'review', 'validity', 'theory', 'remind', 'focus', 'validation', 'test', 'score', 'interpretation', 'react', 'article', 'I', 'support', 'argumentbased', 'approach', 'validity', 'major', 'point', 'regard', 'validation', 'Dr', 'Kane', 'addition', 'I', 'simple', 'threestep', 'method', 'develop', 'validity', 'argument', 'focus', 'explicit', 'testing', 'purpose', 'suggest', 'Standards', 'Educational', 'Psychological', 'Testing', 'testing', 'purpose', 'appropriately', 'articulate', 'process', 'develop', 'interpretive', 'argument', 'unnecessary', 'validation', 'directly', 'address', 'intend', 'interpretation', '©', '2013', 'National', 'Council']",,Kane issue present comprehensive review validity theory remind focus validation test score interpretation react article I support argumentbased approach validity major point regard validation Dr Kane addition I simple threestep method develop validity argument focus explicit testing purpose suggest Standards Educational Psychological Testing testing purpose appropriately articulate process develop interpretive argument unnecessary validation directly address intend interpretation © 2013 National Council,,0.8853288614798079,0.028585551116419042,0.028341957311410156,0.028662027241516383,0.02908160285084647,0.0,0.0,0.11865473620501123,0.0,0.0
Liang T.; Wells C.S.; Hambleton R.K.,An assessment of the nonparametric approach for evaluating the fit of item response models,2014,51,"As item response theory has been more widely applied, investigating the fit of a parametric model becomes an important part of the measurement process. There is a lack of promising solutions to the detection of model misfit in IRT. Douglas and Cohen introduced a general nonparametric approach, RISE (Root Integrated Squared Error), for detecting model misfit. The purposes of this study were to extend the use of RISE to more general and comprehensive applications by manipulating a variety of factors (e.g., test length, sample size, IRT models, ability distribution). The results from the simulation study demonstrated that RISE outperformed G2 and S-X2 in that it controlled Type I error rates and provided adequate power under the studied conditions. In the empirical study, RISE detected reasonable numbers of misfitting items compared to G2 and S-X2, and RISE gave a much clearer picture of the location and magnitude of misfit for each misfitting item. In addition, there was no practical consequence to classification before and after replacement of misfitting items detected by three fit statistics. © 2014 by the National Council on Measurement in Education.",An assessment of the nonparametric approach for evaluating the fit of item response models,"As item response theory has been more widely applied, investigating the fit of a parametric model becomes an important part of the measurement process. There is a lack of promising solutions to the detection of model misfit in IRT. Douglas and Cohen introduced a general nonparametric approach, RISE (Root Integrated Squared Error), for detecting model misfit. The purposes of this study were to extend the use of RISE to more general and comprehensive applications by manipulating a variety of factors (e.g., test length, sample size, IRT models, ability distribution). The results from the simulation study demonstrated that RISE outperformed G2 and S-X2 in that it controlled Type I error rates and provided adequate power under the studied conditions. In the empirical study, RISE detected reasonable numbers of misfitting items compared to G2 and S-X2, and RISE gave a much clearer picture of the location and magnitude of misfit for each misfitting item. In addition, there was no practical consequence to classification before and after replacement of misfitting items detected by three fit statistics. © 2014 by the National Council on Measurement in Education.","['item', 'response', 'theory', 'widely', 'apply', 'investigate', 'fit', 'parametric', 'important', 'process', 'lack', 'promise', 'solution', 'detection', 'misfit', 'IRT', 'Douglas', 'Cohen', 'introduce', 'general', 'nonparametric', 'approach', 'RISE', 'Root', 'Integrated', 'Squared', 'Error', 'detect', 'misfit', 'purpose', 'study', 'extend', 'rise', 'general', 'comprehensive', 'application', 'manipulate', 'variety', 'factor', 'eg', 'test', 'length', 'sample', 'size', 'IRT', 'ability', 'distribution', 'result', 'simulation', 'study', 'demonstrate', 'rise', 'outperform', 'G2', 'SX2', 'control', 'Type', 'I', 'error', 'rate', 'provide', 'adequate', 'power', 'study', 'condition', 'empirical', 'study', 'rise', 'detect', 'reasonable', 'number', 'misfitting', 'item', 'compare', 'G2', 'SX2', 'RISE', 'clear', 'picture', 'location', 'magnitude', 'misfit', 'misfitting', 'item', 'addition', 'practical', 'consequence', 'classification', 'replacement', 'misfitting', 'item', 'detect', 'fit', 'statistic', '©', '2014', 'National', 'Council']","['assessment', 'nonparametric', 'approach', 'evaluate', 'fit', 'item', 'response']",item response theory widely apply investigate fit parametric important process lack promise solution detection misfit IRT Douglas Cohen introduce general nonparametric approach RISE Root Integrated Squared Error detect misfit purpose study extend rise general comprehensive application manipulate variety factor eg test length sample size IRT ability distribution result simulation study demonstrate rise outperform G2 SX2 control Type I error rate provide adequate power study condition empirical study rise detect reasonable number misfitting item compare G2 SX2 RISE clear picture location magnitude misfit misfitting item addition practical consequence classification replacement misfitting item detect fit statistic © 2014 National Council,assessment nonparametric approach evaluate fit item response,0.026193080709543622,0.026272117941640652,0.8948122385971209,0.026501103299899284,0.026221459451795433,0.05864701697247253,0.0,0.0061554997832954025,0.0005893339818886207,0.0
Chang Y.-W.; Huang W.-K.; Tsai R.-C.,DIF Detection Using Multiple-Group Categorical CFA With Minimum Free Baseline Approach,2015,52,"The aim of this study is to assess the efficiency of using the multiple-group categorical confirmatory factor analysis (MCCFA) and the robust chi-square difference test in differential item functioning (DIF) detection for polytomous items under the minimum free baseline strategy. While testing for DIF items, despite the strong assumption that all but the examined item are set to be DIF-free, MCCFA with such a constrained baseline approach is commonly used in the literature. The present study relaxes this strong assumption and adopts the minimum free baseline approach where, aside from those parameters constrained for identification purpose, parameters of all but the examined item are allowed to differ among groups. Based on the simulation results, the robust chi-square difference test statistic with the mean and variance adjustment is shown to be efficient in detecting DIF for polytomous items in terms of the empirical power and Type I error rates. To sum up, MCCFA under the minimum free baseline strategy is useful for DIF detection for polytomous items. © 2015 by the National Council on Measurement in Education.",DIF Detection Using Multiple-Group Categorical CFA With Minimum Free Baseline Approach,"The aim of this study is to assess the efficiency of using the multiple-group categorical confirmatory factor analysis (MCCFA) and the robust chi-square difference test in differential item functioning (DIF) detection for polytomous items under the minimum free baseline strategy. While testing for DIF items, despite the strong assumption that all but the examined item are set to be DIF-free, MCCFA with such a constrained baseline approach is commonly used in the literature. The present study relaxes this strong assumption and adopts the minimum free baseline approach where, aside from those parameters constrained for identification purpose, parameters of all but the examined item are allowed to differ among groups. Based on the simulation results, the robust chi-square difference test statistic with the mean and variance adjustment is shown to be efficient in detecting DIF for polytomous items in terms of the empirical power and Type I error rates. To sum up, MCCFA under the minimum free baseline strategy is useful for DIF detection for polytomous items. © 2015 by the National Council on Measurement in Education.","['aim', 'study', 'assess', 'efficiency', 'multiplegroup', 'categorical', 'confirmatory', 'factor', 'analysis', 'MCCFA', 'robust', 'chisquare', 'difference', 'test', 'differential', 'item', 'function', 'dif', 'detection', 'polytomous', 'item', 'minimum', 'free', 'baseline', 'strategy', 'test', 'dif', 'item', 'despite', 'strong', 'assumption', 'examine', 'item', 'set', 'DIFfree', 'MCCFA', 'constrained', 'baseline', 'approach', 'commonly', 'literature', 'present', 'study', 'relax', 'strong', 'assumption', 'adopt', 'minimum', 'free', 'baseline', 'approach', 'aside', 'parameter', 'constrain', 'identification', 'purpose', 'parameter', 'examine', 'item', 'allow', 'differ', 'group', 'base', 'simulation', 'result', 'robust', 'chisquare', 'difference', 'test', 'statistic', 'mean', 'variance', 'adjustment', 'efficient', 'detect', 'DIF', 'polytomous', 'item', 'term', 'empirical', 'power', 'Type', 'I', 'error', 'rate', 'sum', 'MCCFA', 'minimum', 'free', 'baseline', 'strategy', 'useful', 'dif', 'detection', 'polytomous', 'item', '©', '2015', 'National', 'Council']","['DIF', 'Detection', 'MultipleGroup', 'Categorical', 'CFA', 'Minimum', 'Free', 'Baseline', 'Approach']",aim study assess efficiency multiplegroup categorical confirmatory factor analysis MCCFA robust chisquare difference test differential item function dif detection polytomous item minimum free baseline strategy test dif item despite strong assumption examine item set DIFfree MCCFA constrained baseline approach commonly literature present study relax strong assumption adopt minimum free baseline approach aside parameter constrain identification purpose parameter examine item allow differ group base simulation result robust chisquare difference test statistic mean variance adjustment efficient detect DIF polytomous item term empirical power Type I error rate sum MCCFA minimum free baseline strategy useful dif detection polytomous item © 2015 National Council,DIF Detection MultipleGroup Categorical CFA Minimum Free Baseline Approach,0.027929170726544227,0.027573784152574984,0.8893043667312235,0.02765326044154317,0.02753941794811413,0.0774556582391805,0.0,0.0,0.0,0.0
Van der Linden W.J.,Some conceptual issues in observed-score equating,2013,50,"In spite of all of the technical progress in observed-score equating, several of the more conceptual aspects of the process still are not well understood. As a result, the equating literature struggles with rather complex criteria of equating, lack of a test-theoretic foundation, confusing terminology, and ad hoc analyses. A return to Lord's foundational criterion of equity of equating, a derivation of the true equating transformation from it, and mainstream statistical treatment of the problem of estimating the transformation for various data-collection designs exist as a solution to the problem. © 2013 by the National Council on Measurement in Education.",Some conceptual issues in observed-score equating,"In spite of all of the technical progress in observed-score equating, several of the more conceptual aspects of the process still are not well understood. As a result, the equating literature struggles with rather complex criteria of equating, lack of a test-theoretic foundation, confusing terminology, and ad hoc analyses. A return to Lord's foundational criterion of equity of equating, a derivation of the true equating transformation from it, and mainstream statistical treatment of the problem of estimating the transformation for various data-collection designs exist as a solution to the problem. © 2013 by the National Council on Measurement in Education.","['spite', 'technical', 'progress', 'observedscore', 'equate', 'conceptual', 'aspect', 'process', 'understand', 'result', 'equate', 'literature', 'struggle', 'complex', 'criterion', 'equate', 'lack', 'testtheoretic', 'foundation', 'confuse', 'terminology', 'ad', 'hoc', 'analyse', 'return', 'Lords', 'foundational', 'criterion', 'equity', 'equate', 'derivation', 'true', 'equating', 'transformation', 'mainstream', 'statistical', 'treatment', 'problem', 'estimate', 'transformation', 'datacollection', 'design', 'exist', 'solution', 'problem', '©', '2013', 'National', 'Council']","['conceptual', 'issue', 'observedscore', 'equate']",spite technical progress observedscore equate conceptual aspect process understand result equate literature struggle complex criterion equate lack testtheoretic foundation confuse terminology ad hoc analyse return Lords foundational criterion equity equate derivation true equating transformation mainstream statistical treatment problem estimate transformation datacollection design exist solution problem © 2013 National Council,conceptual issue observedscore equate,0.030765771225402586,0.8781950411934957,0.030086423783219133,0.03133694659581205,0.02961581720207051,0.0,0.0,0.0,0.13198665704916995,0.0
Sinharay S.; Wan P.; Whitaker M.; Kim D.-I.; Zhang L.; Choi S.W.,Determining the Overall Impact of Interruptions During Online Testing,2014,51,"With an increase in the number of online tests, interruptions during testing due to unexpected technical issues seem unavoidable. For example, interruptions occurred during several recent state tests. When interruptions occur, it is important to determine the extent of their impact on the examinees' scores. There is a lack of research on this topic due to the novelty of the problem. This article is an attempt to fill that void. Several methods, primarily based on propensity score matching, linear regression, and item response theory, were suggested to determine the overall impact of the interruptions on the examinees' scores. A realistic simulation study shows that the suggested methods have satisfactory Type I error rate and power. Then the methods were applied to data from the Indiana Statewide Testing for Educational Progress-Plus (ISTEP+) test that experienced interruptions in 2013. The results indicate that the interruptions did not have a significant overall impact on the student scores for the ISTEP+ test. © 2014 by the National Council on Measurement in Education.",Determining the Overall Impact of Interruptions During Online Testing,"With an increase in the number of online tests, interruptions during testing due to unexpected technical issues seem unavoidable. For example, interruptions occurred during several recent state tests. When interruptions occur, it is important to determine the extent of their impact on the examinees' scores. There is a lack of research on this topic due to the novelty of the problem. This article is an attempt to fill that void. Several methods, primarily based on propensity score matching, linear regression, and item response theory, were suggested to determine the overall impact of the interruptions on the examinees' scores. A realistic simulation study shows that the suggested methods have satisfactory Type I error rate and power. Then the methods were applied to data from the Indiana Statewide Testing for Educational Progress-Plus (ISTEP+) test that experienced interruptions in 2013. The results indicate that the interruptions did not have a significant overall impact on the student scores for the ISTEP+ test. © 2014 by the National Council on Measurement in Education.","['increase', 'number', 'online', 'test', 'interruption', 'testing', 'unexpected', 'technical', 'issue', 'unavoidable', 'example', 'interruption', 'occur', 'recent', 'state', 'test', 'interruption', 'occur', 'important', 'determine', 'extent', 'impact', 'examinees', 'score', 'lack', 'research', 'topic', 'novelty', 'problem', 'article', 'attempt', 'fill', 'void', 'method', 'primarily', 'base', 'propensity', 'score', 'match', 'linear', 'regression', 'item', 'response', 'theory', 'suggest', 'determine', 'overall', 'impact', 'interruption', 'examinee', 'score', 'realistic', 'simulation', 'study', 'suggest', 'method', 'satisfactory', 'Type', 'I', 'error', 'rate', 'power', 'method', 'apply', 'datum', 'Indiana', 'Statewide', 'Testing', 'Educational', 'ProgressPlus', 'istep', 'test', 'experience', 'interruption', '2013', 'result', 'indicate', 'interruption', 'significant', 'overall', 'impact', 'student', 'score', 'istep', 'test', '©', '2014', 'National', 'Council']","['determine', 'overall', 'Impact', 'interruption', 'Online', 'Testing']",increase number online test interruption testing unexpected technical issue unavoidable example interruption occur recent state test interruption occur important determine extent impact examinees score lack research topic novelty problem article attempt fill void method primarily base propensity score match linear regression item response theory suggest determine overall impact interruption examinee score realistic simulation study suggest method satisfactory Type I error rate power method apply datum Indiana Statewide Testing Educational ProgressPlus istep test experience interruption 2013 result indicate interruption significant overall impact student score istep test © 2014 National Council,determine overall Impact interruption Online Testing,0.8865192964845182,0.02841117540944514,0.02825602132422528,0.028439459335237942,0.028374047446573455,0.02500755702795531,0.021083478625349955,0.05455954934243965,0.01430620591456254,0.0
Schroeders U.; Robitzsch A.; Schipolowski S.,A Comparison of Different Psychometric Approaches to Modeling Testlet Structures: An Example with C-Tests,2014,51,"C-tests are a specific variant of cloze tests that are considered time-efficient, valid indicators of general language proficiency. They are commonly analyzed with models of item response theory assuming local item independence. In this article we estimated local interdependencies for 12 C-tests and compared the changes in item difficulties, reliability estimates, and person parameter estimates for different modeling approaches: (a) Rasch, (b) testlet, (c) partial credit, and (d) copula models. The results are complemented with findings of a simulation study in which sample size, number of testlets, and strength of residual correlations between items were systematically manipulated. Results are discussed with regard to the pivotal question whether residual dependencies between items are an artifact or part of the construct. © 2014 by the National Council on Measurement in Education .",A Comparison of Different Psychometric Approaches to Modeling Testlet Structures: An Example with C-Tests,"C-tests are a specific variant of cloze tests that are considered time-efficient, valid indicators of general language proficiency. They are commonly analyzed with models of item response theory assuming local item independence. In this article we estimated local interdependencies for 12 C-tests and compared the changes in item difficulties, reliability estimates, and person parameter estimates for different modeling approaches: (a) Rasch, (b) testlet, (c) partial credit, and (d) copula models. The results are complemented with findings of a simulation study in which sample size, number of testlets, and strength of residual correlations between items were systematically manipulated. Results are discussed with regard to the pivotal question whether residual dependencies between items are an artifact or part of the construct. © 2014 by the National Council on Measurement in Education .","['ctest', 'specific', 'variant', 'cloze', 'test', 'consider', 'timeefficient', 'valid', 'indicator', 'general', 'language', 'proficiency', 'commonly', 'analyze', 'item', 'response', 'theory', 'assume', 'local', 'item', 'independence', 'article', 'estimate', 'local', 'interdependency', '12', 'Ctests', 'compare', 'change', 'item', 'difficulty', 'reliability', 'estimate', 'person', 'parameter', 'estimate', 'different', 'modeling', 'approach', 'Rasch', 'b', 'testlet', 'c', 'partial', 'credit', 'd', 'copula', 'result', 'complement', 'finding', 'simulation', 'study', 'sample', 'size', 'number', 'testlet', 'strength', 'residual', 'correlation', 'item', 'systematically', 'manipulate', 'result', 'discuss', 'regard', 'pivotal', 'question', 'residual', 'dependency', 'item', 'artifact', 'construct', '©', '2014', 'National', 'Council']","['Comparison', 'Different', 'Psychometric', 'Approaches', 'Testlet', 'Structures', 'example', 'ctest']",ctest specific variant cloze test consider timeefficient valid indicator general language proficiency commonly analyze item response theory assume local item independence article estimate local interdependency 12 Ctests compare change item difficulty reliability estimate person parameter estimate different modeling approach Rasch b testlet c partial credit d copula result complement finding simulation study sample size number testlet strength residual correlation item systematically manipulate result discuss regard pivotal question residual dependency item artifact construct © 2014 National Council,Comparison Different Psychometric Approaches Testlet Structures example ctest,0.024744104429011197,0.9010536830017875,0.02467905185258408,0.024744588824463884,0.024778571892153423,0.0675655109857542,0.0,0.0,0.0,0.014102541764705578
Mislevy R.J.,How Developments in Psychology and Technology Challenge Validity Argumentation,2016,53,"Validity is the sine qua non of properties of educational assessment. While a theory of validity and a practical framework for validation has emerged over the past decades, most of the discussion has addressed familiar forms of assessment and psychological framings. Advances in digital technologies and in cognitive and social psychology have expanded the range of purposes, targets of inference, contexts of use, forms of activity, and sources of evidence we now see in educational assessment. This article discusses some of these developments and how concepts and representations that are employed to design and use assessments, hence to frame validity arguments, can be extended accordingly. Ideas are illustrated with a variety of examples, with an emphasis on assessment in higher education. Copyright © 2016 by the National Council on Measurement in Education",How Developments in Psychology and Technology Challenge Validity Argumentation,"Validity is the sine qua non of properties of educational assessment. While a theory of validity and a practical framework for validation has emerged over the past decades, most of the discussion has addressed familiar forms of assessment and psychological framings. Advances in digital technologies and in cognitive and social psychology have expanded the range of purposes, targets of inference, contexts of use, forms of activity, and sources of evidence we now see in educational assessment. This article discusses some of these developments and how concepts and representations that are employed to design and use assessments, hence to frame validity arguments, can be extended accordingly. Ideas are illustrated with a variety of examples, with an emphasis on assessment in higher education. Copyright © 2016 by the National Council on Measurement in Education","['validity', 'sine', 'qua', 'non', 'property', 'educational', 'assessment', 'theory', 'validity', 'practical', 'framework', 'validation', 'emerge', 'past', 'decade', 'discussion', 'address', 'familiar', 'form', 'assessment', 'psychological', 'framing', 'advance', 'digital', 'technology', 'cognitive', 'social', 'psychology', 'expand', 'range', 'purpose', 'target', 'inference', 'context', 'form', 'activity', 'source', 'evidence', 'educational', 'assessment', 'article', 'discuss', 'development', 'concept', 'representation', 'employ', 'design', 'assessment', 'frame', 'validity', 'argument', 'extend', 'accordingly', 'idea', 'illustrate', 'variety', 'example', 'emphasis', 'assessment', 'high', 'Copyright', '©', '2016', 'National', 'Council']","['Developments', 'psychology', 'Technology', 'Challenge', 'Validity', 'Argumentation']",validity sine qua non property educational assessment theory validity practical framework validation emerge past decade discussion address familiar form assessment psychological framing advance digital technology cognitive social psychology expand range purpose target inference context form activity source evidence educational assessment article discuss development concept representation employ design assessment frame validity argument extend accordingly idea illustrate variety example emphasis assessment high Copyright © 2016 National Council,Developments psychology Technology Challenge Validity Argumentation,0.025414385873327304,0.8978426748215931,0.02553020347386759,0.025553223984917696,0.02565951184629438,0.0,0.0,0.12326599905100072,0.0,0.0
Bolt D.M.; Deng S.; Lee S.,IRT model misspecification and measurement of growth in vertical scaling,2014,51,"Functional form misfit is frequently a concern in item response theory (IRT), although the practical implications of misfit are often difficult to evaluate. In this article, we illustrate how seemingly negligible amounts of functional form misfit, when systematic, can be associated with significant distortions of the score metric in vertical scaling contexts. Our analysis uses two- and three-parameter versions of Samejima's logistic positive exponent model (LPE) as a data generating model. Consistent with prior work, we find LPEs generally provide a better comparative fit to real item response data than traditional IRT models (2PL, 3PL). Further, our simulation results illustrate how 2PL- or 3PL-based vertical scaling in the presence of LPE-induced misspecification leads to an artificial growth deceleration across grades, consistent with that commonly seen in vertical scaling studies. The results raise further concerns about the use of standard IRT models in measuring growth, even apart from the frequently cited concerns of construct shift/multidimensionality across grades. © 2014 by the National Council on Measurement in Education.",IRT model misspecification and measurement of growth in vertical scaling,"Functional form misfit is frequently a concern in item response theory (IRT), although the practical implications of misfit are often difficult to evaluate. In this article, we illustrate how seemingly negligible amounts of functional form misfit, when systematic, can be associated with significant distortions of the score metric in vertical scaling contexts. Our analysis uses two- and three-parameter versions of Samejima's logistic positive exponent model (LPE) as a data generating model. Consistent with prior work, we find LPEs generally provide a better comparative fit to real item response data than traditional IRT models (2PL, 3PL). Further, our simulation results illustrate how 2PL- or 3PL-based vertical scaling in the presence of LPE-induced misspecification leads to an artificial growth deceleration across grades, consistent with that commonly seen in vertical scaling studies. The results raise further concerns about the use of standard IRT models in measuring growth, even apart from the frequently cited concerns of construct shift/multidimensionality across grades. © 2014 by the National Council on Measurement in Education.","['functional', 'form', 'misfit', 'frequently', 'concern', 'item', 'response', 'theory', 'IRT', 'practical', 'implication', 'misfit', 'difficult', 'evaluate', 'article', 'illustrate', 'seemingly', 'negligible', 'functional', 'form', 'misfit', 'systematic', 'associate', 'significant', 'distortion', 'score', 'metric', 'vertical', 'scaling', 'context', 'analysis', 'threeparameter', 'version', 'Samejimas', 'logistic', 'positive', 'exponent', 'LPE', 'datum', 'generating', 'Consistent', 'prior', 'work', 'find', 'LPEs', 'generally', 'provide', 'comparative', 'fit', 'real', 'item', 'response', 'datum', 'traditional', 'IRT', '2PL', '3PL', 'Further', 'simulation', 'result', 'illustrate', '2PL', '3plbased', 'vertical', 'scaling', 'presence', 'lpeinduce', 'misspecification', 'lead', 'artificial', 'growth', 'deceleration', 'grade', 'consistent', 'commonly', 'vertical', 'scaling', 'study', 'result', 'raise', 'concern', 'standard', 'IRT', 'measure', 'growth', 'apart', 'frequently', 'cite', 'concern', 'construct', 'shiftmultidimensionality', 'grade', '©', '2014', 'National', 'Council']","['IRT', 'misspecification', 'growth', 'vertical', 'scaling']",functional form misfit frequently concern item response theory IRT practical implication misfit difficult evaluate article illustrate seemingly negligible functional form misfit systematic associate significant distortion score metric vertical scaling context analysis threeparameter version Samejimas logistic positive exponent LPE datum generating Consistent prior work find LPEs generally provide comparative fit real item response datum traditional IRT 2PL 3PL Further simulation result illustrate 2PL 3plbased vertical scaling presence lpeinduce misspecification lead artificial growth deceleration grade consistent commonly vertical scaling study result raise concern standard IRT measure growth apart frequently cite concern construct shiftmultidimensionality grade © 2014 National Council,IRT misspecification growth vertical scaling,0.024740463884815093,0.024773132089056658,0.024875642181045644,0.900743748012198,0.02486701383288447,0.03204529709056127,0.0016992123895020522,0.029027868852505054,0.003004365290790405,0.006580791145632012
Guo R.; Zheng Y.; Chang H.-H.,A Stepwise Test Characteristic Curve Method to Detect Item Parameter Drift,2015,52,"An important assumption of item response theory is item parameter invariance. Sometimes, however, item parameters are not invariant across different test administrations due to factors other than sampling error; this phenomenon is termed item parameter drift. Several methods have been developed to detect drifted items. However, most of the existing methods were designed to detect drifts in individual items, which may not be adequate for test characteristic curve-based linking or equating. One example is the item response theory-based true score equating, whose goal is to generate a conversion table to relate number-correct scores on two forms based on their test characteristic curves. This article introduces a stepwise test characteristic curve method to detect item parameter drift iteratively based on test characteristic curves without needing to set any predetermined critical values. Comparisons are made between the proposed method and two existing methods under the three-parameter logistic item response model through simulation and real data analysis. Results show that the proposed method produces a small difference in test characteristic curves between administrations, an accurate conversion table, and a good classification of drifted and nondrifted items and at the same time keeps a large amount of linking items. © 2015 by the National Council on Measurement in Education.",A Stepwise Test Characteristic Curve Method to Detect Item Parameter Drift,"An important assumption of item response theory is item parameter invariance. Sometimes, however, item parameters are not invariant across different test administrations due to factors other than sampling error; this phenomenon is termed item parameter drift. Several methods have been developed to detect drifted items. However, most of the existing methods were designed to detect drifts in individual items, which may not be adequate for test characteristic curve-based linking or equating. One example is the item response theory-based true score equating, whose goal is to generate a conversion table to relate number-correct scores on two forms based on their test characteristic curves. This article introduces a stepwise test characteristic curve method to detect item parameter drift iteratively based on test characteristic curves without needing to set any predetermined critical values. Comparisons are made between the proposed method and two existing methods under the three-parameter logistic item response model through simulation and real data analysis. Results show that the proposed method produces a small difference in test characteristic curves between administrations, an accurate conversion table, and a good classification of drifted and nondrifted items and at the same time keeps a large amount of linking items. © 2015 by the National Council on Measurement in Education.","['important', 'assumption', 'item', 'response', 'theory', 'item', 'parameter', 'invariance', 'item', 'parameter', 'invariant', 'different', 'test', 'administration', 'factor', 'sample', 'error', 'phenomenon', 'term', 'item', 'parameter', 'drift', 'method', 'develop', 'detect', 'drift', 'item', 'exist', 'method', 'design', 'detect', 'drift', 'individual', 'item', 'adequate', 'test', 'characteristic', 'curvebase', 'linking', 'equate', 'example', 'item', 'response', 'theorybase', 'true', 'score', 'equate', 'goal', 'generate', 'conversion', 'table', 'relate', 'numbercorrect', 'score', 'form', 'base', 'test', 'characteristic', 'curve', 'article', 'introduce', 'stepwise', 'test', 'characteristic', 'curve', 'method', 'detect', 'item', 'parameter', 'drift', 'iteratively', 'base', 'test', 'characteristic', 'curve', 'need', 'set', 'predetermine', 'critical', 'value', 'comparison', 'propose', 'method', 'exist', 'method', 'threeparameter', 'logistic', 'item', 'response', 'simulation', 'real', 'datum', 'analysis', 'result', 'propose', 'method', 'produce', 'small', 'difference', 'test', 'characteristic', 'curve', 'administration', 'accurate', 'conversion', 'table', 'good', 'classification', 'drift', 'nondrifte', 'item', 'time', 'large', 'link', 'item', '©', '2015', 'National', 'Council']","['Stepwise', 'Test', 'Characteristic', 'Curve', 'Method', 'detect', 'Item', 'Parameter', 'Drift']",important assumption item response theory item parameter invariance item parameter invariant different test administration factor sample error phenomenon term item parameter drift method develop detect drift item exist method design detect drift individual item adequate test characteristic curvebase linking equate example item response theorybase true score equate goal generate conversion table relate numbercorrect score form base test characteristic curve article introduce stepwise test characteristic curve method detect item parameter drift iteratively base test characteristic curve need set predetermine critical value comparison propose method exist method threeparameter logistic item response simulation real datum analysis result propose method produce small difference test characteristic curve administration accurate conversion table good classification drift nondrifte item time large link item © 2015 National Council,Stepwise Test Characteristic Curve Method detect Item Parameter Drift,0.02762578154762264,0.02734732692023205,0.8897959227232054,0.027402595817961967,0.027828372990977932,0.08575757031198636,0.0083106788503898,0.0,0.03782821529355196,0.0
Bridgeman B.,A simple answer to a simple question on changing answers,2012,49,"In an article in the Winter 2011 issue of the Journal of Educational Measurement, van der Linden, Jeon, and Ferrara suggested that ""test takers should trust their initial instincts and retain their initial responses when they have the opportunity to review test items."" They presented a complex IRT model that appeared to show that students would be worse off by changing answers. As noted in a subsequent erratum, this conclusion was based on flawed data, and that the correct data could not be analyzed by their method because the model failed to converge. This left their basic question on the value of answer changing unanswered. A much more direct approach is to simply count the number of examinees whose scores after an opportunity to change answers are higher, lower, or the same as their initial scores. Using the same data set as the original article, an overwhelming majority of the students received higher scores after the opportunity to change answers. © 2012 by the National Council on Measurement in Education.",A simple answer to a simple question on changing answers,"In an article in the Winter 2011 issue of the Journal of Educational Measurement, van der Linden, Jeon, and Ferrara suggested that ""test takers should trust their initial instincts and retain their initial responses when they have the opportunity to review test items."" They presented a complex IRT model that appeared to show that students would be worse off by changing answers. As noted in a subsequent erratum, this conclusion was based on flawed data, and that the correct data could not be analyzed by their method because the model failed to converge. This left their basic question on the value of answer changing unanswered. A much more direct approach is to simply count the number of examinees whose scores after an opportunity to change answers are higher, lower, or the same as their initial scores. Using the same data set as the original article, an overwhelming majority of the students received higher scores after the opportunity to change answers. © 2012 by the National Council on Measurement in Education.","['article', 'Winter', '2011', 'issue', 'Journal', 'Educational', 'van', 'der', 'Linden', 'Jeon', 'Ferrara', 'suggest', 'test', 'taker', 'trust', 'initial', 'instinct', 'retain', 'initial', 'response', 'opportunity', 'review', 'test', 'item', 'present', 'complex', 'IRT', 'appear', 'student', 'bad', 'change', 'answer', 'note', 'subsequent', 'erratum', 'conclusion', 'base', 'flawed', 'datum', 'correct', 'datum', 'analyze', 'method', 'fail', 'converge', 'leave', 'basic', 'question', 'value', 'answer', 'change', 'unanswered', 'direct', 'approach', 'simply', 'count', 'number', 'examinee', 'score', 'opportunity', 'change', 'answer', 'higher', 'low', 'initial', 'score', 'datum', 'set', 'original', 'article', 'overwhelming', 'majority', 'student', 'receive', 'high', 'score', 'opportunity', 'change', 'answer', '©', '2012', 'National', 'Council']","['simple', 'answer', 'simple', 'question', 'change', 'answer']",article Winter 2011 issue Journal Educational van der Linden Jeon Ferrara suggest test taker trust initial instinct retain initial response opportunity review test item present complex IRT appear student bad change answer note subsequent erratum conclusion base flawed datum correct datum analyze method fail converge leave basic question value answer change unanswered direct approach simply count number examinee score opportunity change answer higher low initial score datum set original article overwhelming majority student receive high score opportunity change answer © 2012 National Council,simple answer simple question change answer,0.025645662453652383,0.02562233072322238,0.02560242350232569,0.02585205610018741,0.897277527220612,0.021066791708633677,0.01921150394051949,0.05336669437105101,0.00859301589388416,0.0
Kahraman N.,Unidimensional interpretations for multidimensional test items,2013,50,"This article considers potential problems that can arise in estimating a unidimensional item response theory (IRT) model when some test items are multidimensional (i.e., show a complex factorial structure). More specifically, this study examines (1) the consequences of model misfit on IRT item parameter estimates due to unintended minor item-level multidimensionality, and (2) whether a Projection IRT model can provide a useful remedy. A real-data example is used to illustrate the problem and also is used as a base model for a simulation study. The results suggest that ignoring item-level multidimensionality might lead to inflated item discrimination parameter estimates when the proportion of multidimensional test items to unidimensional test items is as low as 1:5. The Projection IRT model appears to be a useful tool for updating unidimensional item parameter estimates of multidimensional test items for a purified unidimensional interpretation. Copyright © 2013 by the National Council on Measurement in Education.",Unidimensional interpretations for multidimensional test items,"This article considers potential problems that can arise in estimating a unidimensional item response theory (IRT) model when some test items are multidimensional (i.e., show a complex factorial structure). More specifically, this study examines (1) the consequences of model misfit on IRT item parameter estimates due to unintended minor item-level multidimensionality, and (2) whether a Projection IRT model can provide a useful remedy. A real-data example is used to illustrate the problem and also is used as a base model for a simulation study. The results suggest that ignoring item-level multidimensionality might lead to inflated item discrimination parameter estimates when the proportion of multidimensional test items to unidimensional test items is as low as 1:5. The Projection IRT model appears to be a useful tool for updating unidimensional item parameter estimates of multidimensional test items for a purified unidimensional interpretation. Copyright © 2013 by the National Council on Measurement in Education.","['article', 'consider', 'potential', 'problem', 'arise', 'estimate', 'unidimensional', 'item', 'response', 'theory', 'IRT', 'test', 'item', 'multidimensional', 'ie', 'complex', 'factorial', 'structure', 'specifically', 'study', 'examine', '1', 'consequence', 'misfit', 'IRT', 'item', 'parameter', 'estimate', 'unintended', 'minor', 'itemlevel', 'multidimensionality', '2', 'Projection', 'IRT', 'provide', 'useful', 'remedy', 'realdata', 'example', 'illustrate', 'problem', 'base', 'simulation', 'study', 'result', 'suggest', 'ignore', 'itemlevel', 'multidimensionality', 'lead', 'inflated', 'item', 'discrimination', 'parameter', 'estimate', 'proportion', 'multidimensional', 'test', 'item', 'unidimensional', 'test', 'item', 'low', '15', 'Projection', 'IRT', 'appear', 'useful', 'tool', 'update', 'unidimensional', 'item', 'parameter', 'estimate', 'multidimensional', 'test', 'item', 'purified', 'unidimensional', 'interpretation', 'Copyright', '©', '2013', 'National', 'Council']","['unidimensional', 'interpretation', 'multidimensional', 'test', 'item']",article consider potential problem arise estimate unidimensional item response theory IRT test item multidimensional ie complex factorial structure specifically study examine 1 consequence misfit IRT item parameter estimate unintended minor itemlevel multidimensionality 2 Projection IRT provide useful remedy realdata example illustrate problem base simulation study result suggest ignore itemlevel multidimensionality lead inflated item discrimination parameter estimate proportion multidimensional test item unidimensional test item low 15 Projection IRT appear useful tool update unidimensional item parameter estimate multidimensional test item purified unidimensional interpretation Copyright © 2013 National Council,unidimensional interpretation multidimensional test item,0.8895826389509901,0.027451581442221255,0.027721822225115076,0.027672005169995153,0.02757195221167849,0.08911026607980468,0.0,0.0,0.0,0.0
Tendeiro J.N.; Meijer R.R.,Detection of invalid test scores: The usefulness of simple nonparametric statistics,2014,51,In recent guidelines for fair educational testing it is advised to check the validity of individual test scores through the use of person-fit statistics. For practitioners it is unclear on the basis of the existing literature which statistic to use. An overview of relatively simple existing nonparametric approaches to identify atypical response patterns is provided. A simulation study was conducted to compare the different approaches and on the basis of the literature review and the simulation study guidelines for the use of person-fit approaches are given. © 2014 by the National Council on Measurement in Education.,Detection of invalid test scores: The usefulness of simple nonparametric statistics,In recent guidelines for fair educational testing it is advised to check the validity of individual test scores through the use of person-fit statistics. For practitioners it is unclear on the basis of the existing literature which statistic to use. An overview of relatively simple existing nonparametric approaches to identify atypical response patterns is provided. A simulation study was conducted to compare the different approaches and on the basis of the literature review and the simulation study guidelines for the use of person-fit approaches are given. © 2014 by the National Council on Measurement in Education.,"['recent', 'guideline', 'fair', 'educational', 'testing', 'advise', 'check', 'validity', 'individual', 'test', 'score', 'personfit', 'statistic', 'practitioner', 'unclear', 'basis', 'exist', 'literature', 'statistic', 'overview', 'relatively', 'simple', 'exist', 'nonparametric', 'approach', 'identify', 'atypical', 'response', 'pattern', 'provide', 'simulation', 'study', 'conduct', 'compare', 'different', 'approach', 'basis', 'literature', 'review', 'simulation', 'study', 'guideline', 'personfit', 'approach', '©', '2014', 'National', 'Council']","['detection', 'invalid', 'test', 'score', 'usefulness', 'simple', 'nonparametric', 'statistic']",recent guideline fair educational testing advise check validity individual test score personfit statistic practitioner unclear basis exist literature statistic overview relatively simple exist nonparametric approach identify atypical response pattern provide simulation study conduct compare different approach basis literature review simulation study guideline personfit approach © 2014 National Council,detection invalid test score usefulness simple nonparametric statistic,0.03255238264643502,0.8716092110748592,0.031710010754869414,0.0319772426247508,0.03215115289908552,0.026283432896359482,0.0,0.050918419347365386,0.0,0.0
Rutkowski L.; Zhou Y.,Correcting Measurement Error in Latent Regression Covariates via the MC-SIMEX Method,2015,52,"Given the importance of large-scale assessments to educational policy conversations, it is critical that subpopulation achievement is estimated reliably and with sufficient precision. Despite this importance, biased subpopulation estimates have been found to occur when variables in the conditioning model side of a latent regression model contain measurement error. As such, this article proposes a method to correct for misclassification in the conditioning model by way of the misclassification simulation extrapolation (MC-SIMEX) method. Although the proposed method is computationally intensive, results from a simulation study show that the MC-SIMEX method improves latent regression coefficients and associated subpopulation achievement estimates. The method is demonstrated with PIRLS 2006 data. The importance of collecting high-priority, policy-relevant contextual data from at least two sources is emphasized and practical applications are discussed. © 2015 by the National Council on Measurement in Education.",Correcting Measurement Error in Latent Regression Covariates via the MC-SIMEX Method,"Given the importance of large-scale assessments to educational policy conversations, it is critical that subpopulation achievement is estimated reliably and with sufficient precision. Despite this importance, biased subpopulation estimates have been found to occur when variables in the conditioning model side of a latent regression model contain measurement error. As such, this article proposes a method to correct for misclassification in the conditioning model by way of the misclassification simulation extrapolation (MC-SIMEX) method. Although the proposed method is computationally intensive, results from a simulation study show that the MC-SIMEX method improves latent regression coefficients and associated subpopulation achievement estimates. The method is demonstrated with PIRLS 2006 data. The importance of collecting high-priority, policy-relevant contextual data from at least two sources is emphasized and practical applications are discussed. © 2015 by the National Council on Measurement in Education.","['importance', 'largescale', 'assessment', 'educational', 'policy', 'conversation', 'critical', 'subpopulation', 'achievement', 'estimate', 'reliably', 'sufficient', 'precision', 'despite', 'importance', 'biased', 'subpopulation', 'estimate', 'find', 'occur', 'variable', 'conditioning', 'latent', 'regression', 'contain', 'error', 'article', 'propose', 'method', 'correct', 'misclassification', 'conditioning', 'way', 'misclassification', 'simulation', 'extrapolation', 'mcsimex', 'method', 'propose', 'method', 'computationally', 'intensive', 'result', 'simulation', 'study', 'mcsimex', 'method', 'improve', 'latent', 'regression', 'coefficient', 'associate', 'subpopulation', 'achievement', 'estimate', 'method', 'demonstrate', 'PIRLS', '2006', 'datum', 'importance', 'collect', 'highpriority', 'policyrelevant', 'contextual', 'datum', 'source', 'emphasize', 'practical', 'application', 'discuss', '©', '2015', 'National', 'Council']","['correct', 'Error', 'Latent', 'Regression', 'Covariates', 'MCSIMEX', 'Method']",importance largescale assessment educational policy conversation critical subpopulation achievement estimate reliably sufficient precision despite importance biased subpopulation estimate find occur variable conditioning latent regression contain error article propose method correct misclassification conditioning way misclassification simulation extrapolation mcsimex method propose method computationally intensive result simulation study mcsimex method improve latent regression coefficient associate subpopulation achievement estimate method demonstrate PIRLS 2006 datum importance collect highpriority policyrelevant contextual datum source emphasize practical application discuss © 2015 National Council,correct Error Latent Regression Covariates MCSIMEX Method,0.027249925945774543,0.027335866028214275,0.027244228139055054,0.027408397802875805,0.8907615820840803,0.030045531332060314,0.0015533434041015437,0.0294685465087136,0.010226396552574828,0.0
Borsboom D.; Markus K.A.,Truth and Evidence in Validity Theory,2013,50,"According to Kane (this issue), ""the validity of a proposed interpretation or use depends on how well the evidence supports"" the claims being made. Because truth and evidence are distinct, this means that the validity of a test score interpretation could be high even though the interpretation is false. As an illustration, we discuss the case of phlogiston measurement as it existed in the 18th century. At face value, Kane's theory would seem to imply that interpretations of phlogiston measurement were valid in the 18th century (because the evidence for them was strong), even though amounts of phlogiston do not exist and hence cannot be measured. We suggest that this neglects an important aspect of validity and suggest various ways in which Kane's theory could meet this challenge. © 2013 by the National Council on Measurement in Education.",,"According to Kane (this issue), ""the validity of a proposed interpretation or use depends on how well the evidence supports"" the claims being made. Because truth and evidence are distinct, this means that the validity of a test score interpretation could be high even though the interpretation is false. As an illustration, we discuss the case of phlogiston measurement as it existed in the 18th century. At face value, Kane's theory would seem to imply that interpretations of phlogiston measurement were valid in the 18th century (because the evidence for them was strong), even though amounts of phlogiston do not exist and hence cannot be measured. We suggest that this neglects an important aspect of validity and suggest various ways in which Kane's theory could meet this challenge. © 2013 by the National Council on Measurement in Education.","['accord', 'Kane', 'issue', 'validity', 'propose', 'interpretation', 'depend', 'evidence', 'support', 'claim', 'truth', 'evidence', 'distinct', 'mean', 'validity', 'test', 'score', 'interpretation', 'high', 'interpretation', 'false', 'illustration', 'discuss', 'case', 'phlogiston', 'exist', '18th', 'century', 'face', 'value', 'Kanes', 'theory', 'imply', 'interpretation', 'phlogiston', 'valid', '18th', 'century', 'evidence', 'strong', 'phlogiston', 'exist', 'measure', 'suggest', 'neglect', 'important', 'aspect', 'validity', 'suggest', 'way', 'Kanes', 'theory', 'meet', 'challenge', '©', '2013', 'National', 'Council']",,accord Kane issue validity propose interpretation depend evidence support claim truth evidence distinct mean validity test score interpretation high interpretation false illustration discuss case phlogiston exist 18th century face value Kanes theory imply interpretation phlogiston valid 18th century evidence strong phlogiston exist measure suggest neglect important aspect validity suggest way Kanes theory meet challenge © 2013 National Council,,0.8693508163898348,0.032463093472608215,0.032316937150571164,0.033073621650235675,0.0327955313367501,0.0,0.007925065797196816,0.09310685848123361,0.0,0.0
Newton P.E.,Two Kinds of Argument?,2013,50,"Kane distinguishes between two kinds of argument: the interpretation/use argument and the validity argument. This commentary considers whether there really are two kinds of argument, two arguments, or just one. It concludes that there is just one argument: the validity argument. © 2013 by the National Council on Measurement in Education.",,"Kane distinguishes between two kinds of argument: the interpretation/use argument and the validity argument. This commentary considers whether there really are two kinds of argument, two arguments, or just one. It concludes that there is just one argument: the validity argument. © 2013 by the National Council on Measurement in Education.","['Kane', 'distinguishe', 'kind', 'argument', 'interpretationuse', 'argument', 'validity', 'argument', 'commentary', 'consider', 'kind', 'argument', 'argument', 'conclude', 'argument', 'validity', 'argument', '©', '2013', 'National', 'Council']",,Kane distinguishe kind argument interpretationuse argument validity argument commentary consider kind argument argument conclude argument validity argument © 2013 National Council,,0.06011708251083731,0.060333718402983176,0.059660619842842995,0.06016465130400207,0.7597239279393344,0.0,0.0,0.056610481410233955,0.0,0.0
Zhu M.; Shu Z.; von Davier A.A.,Using Networks to Visualize and Analyze Process Data for Educational Assessment,2016,53,"New technology enables interactive and adaptive scenario-based tasks (SBTs) to be adopted in educational measurement. At the same time, it is a challenging problem to build appropriate psychometric models to analyze data collected from these tasks, due to the complexity of the data. This study focuses on process data collected from SBTs. We explore the potential of using concepts and methods from social network analysis to represent and analyze process data. Empirical data were collected from the assessment of Technology and Engineering Literacy, conducted as part of the National Assessment of Educational Progress. For the activity sequences in the process data, we created a transition network using weighted directed networks, with nodes representing actions and directed links connecting two actions only if the first action is followed by the second action in the sequence. This study shows how visualization of the transition networks represents process data and provides insights for item design. This study also explores how network measures are related to existing scoring rubrics and how detailed network measures can be used to make intergroup comparisons. Copyright © 2016 by the National Council on Measurement in Education",Using Networks to Visualize and Analyze Process Data for Educational Assessment,"New technology enables interactive and adaptive scenario-based tasks (SBTs) to be adopted in educational measurement. At the same time, it is a challenging problem to build appropriate psychometric models to analyze data collected from these tasks, due to the complexity of the data. This study focuses on process data collected from SBTs. We explore the potential of using concepts and methods from social network analysis to represent and analyze process data. Empirical data were collected from the assessment of Technology and Engineering Literacy, conducted as part of the National Assessment of Educational Progress. For the activity sequences in the process data, we created a transition network using weighted directed networks, with nodes representing actions and directed links connecting two actions only if the first action is followed by the second action in the sequence. This study shows how visualization of the transition networks represents process data and provides insights for item design. This study also explores how network measures are related to existing scoring rubrics and how detailed network measures can be used to make intergroup comparisons. Copyright © 2016 by the National Council on Measurement in Education","['new', 'technology', 'enable', 'interactive', 'adaptive', 'scenariobase', 'task', 'sbt', 'adopt', 'educational', 'time', 'challenging', 'problem', 'build', 'appropriate', 'psychometric', 'analyze', 'datum', 'collect', 'task', 'complexity', 'datum', 'study', 'focus', 'process', 'datum', 'collect', 'SBTs', 'explore', 'potential', 'concept', 'method', 'social', 'network', 'analysis', 'represent', 'analyze', 'process', 'datum', 'empirical', 'datum', 'collect', 'assessment', 'Technology', 'Engineering', 'Literacy', 'conduct', 'National', 'Assessment', 'Educational', 'Progress', 'activity', 'sequence', 'process', 'datum', 'create', 'transition', 'network', 'weight', 'directed', 'network', 'node', 'represent', 'action', 'direct', 'link', 'connect', 'action', 'action', 'follow', 'second', 'action', 'sequence', 'study', 'visualization', 'transition', 'network', 'represent', 'process', 'datum', 'provide', 'insight', 'item', 'design', 'study', 'explore', 'network', 'measure', 'relate', 'exist', 'scoring', 'rubric', 'detailed', 'network', 'measure', 'intergroup', 'comparison', 'Copyright', '©', '2016', 'National', 'Council']","['network', 'Visualize', 'Analyze', 'Process', 'Data', 'Educational', 'Assessment']",new technology enable interactive adaptive scenariobase task sbt adopt educational time challenging problem build appropriate psychometric analyze datum collect task complexity datum study focus process datum collect SBTs explore potential concept method social network analysis represent analyze process datum empirical datum collect assessment Technology Engineering Literacy conduct National Assessment Educational Progress activity sequence process datum create transition network weight directed network node represent action direct link connect action action follow second action sequence study visualization transition network represent process datum provide insight item design study explore network measure relate exist scoring rubric detailed network measure intergroup comparison Copyright © 2016 National Council,network Visualize Analyze Process Data Educational Assessment,0.02697898941575364,0.027158418003012087,0.027003125890496908,0.027028632287083982,0.8918308344036533,0.004426392211496797,0.0,0.08384419969478756,0.0003160388637901901,0.00641615576492448
Pohl S.,Longitudinal multistage testing,2013,50,"This article introduces longitudinal multistage testing (lMST), a special form of multistage testing (MST), as a method for adaptive testing in longitudinal large-scale studies. In lMST designs, test forms of different difficulty levels are used, whereas the values on a pretest determine the routing to these test forms. Since lMST allows for testing in paper and pencil mode, lMST may represent an alternative to conventional testing (CT) in assessments for which other adaptive testing designs are not applicable. In this article the performance of lMST is compared to CT in terms of test targeting as well as bias and efficiency of ability and change estimates. Using a simulation study, the effect of the stability of ability across waves, the difficulty level of the different test forms, and the number of link items between the test forms were investigated. © 2013 by the National Council on Measurement in Education.",,"This article introduces longitudinal multistage testing (lMST), a special form of multistage testing (MST), as a method for adaptive testing in longitudinal large-scale studies. In lMST designs, test forms of different difficulty levels are used, whereas the values on a pretest determine the routing to these test forms. Since lMST allows for testing in paper and pencil mode, lMST may represent an alternative to conventional testing (CT) in assessments for which other adaptive testing designs are not applicable. In this article the performance of lMST is compared to CT in terms of test targeting as well as bias and efficiency of ability and change estimates. Using a simulation study, the effect of the stability of ability across waves, the difficulty level of the different test forms, and the number of link items between the test forms were investigated. © 2013 by the National Council on Measurement in Education.","['article', 'introduce', 'longitudinal', 'multistage', 'testing', 'lmst', 'special', 'form', 'multistage', 'testing', 'mst', 'method', 'adaptive', 'testing', 'longitudinal', 'largescale', 'study', 'lmst', 'design', 'test', 'form', 'different', 'difficulty', 'level', 'value', 'pret', 'determine', 'routing', 'test', 'form', 'lmst', 'allow', 'test', 'paper', 'pencil', 'mode', 'lMST', 'represent', 'alternative', 'conventional', 'testing', 'CT', 'assessment', 'adaptive', 'testing', 'design', 'applicable', 'article', 'performance', 'lMST', 'compare', 'CT', 'term', 'test', 'target', 'bias', 'efficiency', 'ability', 'change', 'estimate', 'simulation', 'study', 'effect', 'stability', 'ability', 'wave', 'difficulty', 'level', 'different', 'test', 'form', 'number', 'link', 'item', 'test', 'form', 'investigate', '©', '2013', 'National', 'Council']",,article introduce longitudinal multistage testing lmst special form multistage testing mst method adaptive testing longitudinal largescale study lmst design test form different difficulty level value pret determine routing test form lmst allow test paper pencil mode lMST represent alternative conventional testing CT assessment adaptive testing design applicable article performance lMST compare CT term test target bias efficiency ability change estimate simulation study effect stability ability wave difficulty level different test form number link item test form investigate © 2013 National Council,,0.03229468838120599,0.03144515834685935,0.03189010510379694,0.031175695468299538,0.8731943526998381,0.0507232008452457,0.01545553243615166,0.009416740867714718,0.013682240390831626,0.0
Brinkhuis M.J.S.; Bakker M.; Maris G.,Filtering Data for Detecting Differential Development,2015,52,"The amount of data available in the context of educational measurement has vastly increased in recent years. Such data are often incomplete, involve tests administered at different time points and during the course of many years, and can therefore be quite challenging to model. In addition, intermediate results like grades or report cards being available to pupils, teachers, parents, and policy makers might influence future performance, which adds to the modeling difficulties. We propose the use of simple data filters to obtain a reduced set of relevant data, which allows for simple checks on the relative development of persons, items, or both. © 2015 by the National Council on Measurement in Education.",Filtering Data for Detecting Differential Development,"The amount of data available in the context of educational measurement has vastly increased in recent years. Such data are often incomplete, involve tests administered at different time points and during the course of many years, and can therefore be quite challenging to model. In addition, intermediate results like grades or report cards being available to pupils, teachers, parents, and policy makers might influence future performance, which adds to the modeling difficulties. We propose the use of simple data filters to obtain a reduced set of relevant data, which allows for simple checks on the relative development of persons, items, or both. © 2015 by the National Council on Measurement in Education.","['datum', 'available', 'context', 'educational', 'vastly', 'increase', 'recent', 'year', 'datum', 'incomplete', 'involve', 'test', 'administer', 'different', 'time', 'point', 'course', 'year', 'challenging', 'addition', 'intermediate', 'result', 'like', 'grade', 'report', 'card', 'available', 'pupil', 'teacher', 'parent', 'policy', 'maker', 'influence', 'future', 'performance', 'add', 'modeling', 'difficulty', 'propose', 'simple', 'datum', 'filter', 'obtain', 'reduce', 'set', 'relevant', 'datum', 'allow', 'simple', 'check', 'relative', 'development', 'person', 'item', '©', '2015', 'National', 'Council']","['Filtering', 'Data', 'Detecting', 'Differential', 'Development']",datum available context educational vastly increase recent year datum incomplete involve test administer different time point course year challenging addition intermediate result like grade report card available pupil teacher parent policy maker influence future performance add modeling difficulty propose simple datum filter obtain reduce set relevant datum allow simple check relative development person item © 2015 National Council,Filtering Data Detecting Differential Development,0.02661029128441218,0.0265088268861846,0.026621384409312224,0.026670700952677623,0.8935887964674134,0.023253581926931267,0.015438638004576915,0.04706713921275913,0.0,0.0005800975144033344
Harrison G.M.,Non-numeric Intrajudge Consistency Feedback in an Angoff Procedure,2015,52,"The credibility of standard-setting cut scores depends in part on two sources of consistency evidence: intrajudge and interjudge consistency. Although intrajudge consistency feedback has often been provided to Angoff judges in practice, more evidence is needed to determine whether it achieves its intended effect. In this randomized experiment with 36 judges, non-numeric item-level intrajudge consistency feedback was provided to treatment-group judges after the first and second rounds of Angoff ratings. Compared to the judges in the control condition, those receiving the feedback significantly improved their intrajudge consistency, with the effect being stronger after the first round than after the second round. To examine whether this feedback has deleterious effects on between-judge consistency, I also examined interjudge consistency at the cut score level and the item level using generalizability theory. The results showed that without the feedback, cut score variability worsened; with the feedback, idiosyncratic item-level variability improved. These results suggest that non-numeric intrajudge consistency feedback achieves its intended effect and potentially improves interjudge consistency. The findings contribute to standard-setting feedback research and provide empirical evidence for practitioners planning Angoff procedures. © 2015 by the National Council on Measurement in Education.",Non-numeric Intrajudge Consistency Feedback in an Angoff Procedure,"The credibility of standard-setting cut scores depends in part on two sources of consistency evidence: intrajudge and interjudge consistency. Although intrajudge consistency feedback has often been provided to Angoff judges in practice, more evidence is needed to determine whether it achieves its intended effect. In this randomized experiment with 36 judges, non-numeric item-level intrajudge consistency feedback was provided to treatment-group judges after the first and second rounds of Angoff ratings. Compared to the judges in the control condition, those receiving the feedback significantly improved their intrajudge consistency, with the effect being stronger after the first round than after the second round. To examine whether this feedback has deleterious effects on between-judge consistency, I also examined interjudge consistency at the cut score level and the item level using generalizability theory. The results showed that without the feedback, cut score variability worsened; with the feedback, idiosyncratic item-level variability improved. These results suggest that non-numeric intrajudge consistency feedback achieves its intended effect and potentially improves interjudge consistency. The findings contribute to standard-setting feedback research and provide empirical evidence for practitioners planning Angoff procedures. © 2015 by the National Council on Measurement in Education.","['credibility', 'standardsette', 'cut', 'score', 'depend', 'source', 'consistency', 'evidence', 'intrajudge', 'interjudge', 'consistency', 'intrajudge', 'consistency', 'feedback', 'provide', 'Angoff', 'judge', 'practice', 'evidence', 'need', 'determine', 'achieve', 'intend', 'effect', 'randomize', 'experiment', '36', 'judge', 'nonnumeric', 'itemlevel', 'intrajudge', 'consistency', 'feedback', 'provide', 'treatmentgroup', 'judge', 'second', 'round', 'Angoff', 'rating', 'compare', 'judge', 'control', 'condition', 'receive', 'feedback', 'significantly', 'improve', 'intrajudge', 'consistency', 'effect', 'strong', 'round', 'second', 'round', 'examine', 'feedback', 'deleterious', 'effect', 'betweenjudge', 'consistency', 'I', 'examine', 'interjudge', 'consistency', 'cut', 'score', 'level', 'item', 'level', 'generalizability', 'theory', 'result', 'feedback', 'cut', 'score', 'variability', 'worsen', 'feedback', 'idiosyncratic', 'itemlevel', 'variability', 'improve', 'result', 'suggest', 'nonnumeric', 'intrajudge', 'consistency', 'feedback', 'achieve', 'intend', 'effect', 'potentially', 'improve', 'interjudge', 'consistency', 'finding', 'contribute', 'standardsette', 'feedback', 'research', 'provide', 'empirical', 'evidence', 'practitioner', 'plan', 'Angoff', 'procedure', '©', '2015', 'National', 'Council']","['Nonnumeric', 'Intrajudge', 'Consistency', 'Feedback', 'Angoff', 'Procedure']",credibility standardsette cut score depend source consistency evidence intrajudge interjudge consistency intrajudge consistency feedback provide Angoff judge practice evidence need determine achieve intend effect randomize experiment 36 judge nonnumeric itemlevel intrajudge consistency feedback provide treatmentgroup judge second round Angoff rating compare judge control condition receive feedback significantly improve intrajudge consistency effect strong round second round examine feedback deleterious effect betweenjudge consistency I examine interjudge consistency cut score level item level generalizability theory result feedback cut score variability worsen feedback idiosyncratic itemlevel variability improve result suggest nonnumeric intrajudge consistency feedback achieve intend effect potentially improve interjudge consistency finding contribute standardsette feedback research provide empirical evidence practitioner plan Angoff procedure © 2015 National Council,Nonnumeric Intrajudge Consistency Feedback Angoff Procedure,0.031827517317246354,0.03199629158249821,0.03147048025252276,0.8728156523981779,0.03189005844955484,0.005161931524503033,0.011448247099992795,0.03920331008746573,0.0,0.017780292058993945
Bradlow E.T.,"Comments on ""Some conceptual issues in observed-score equating"" by Wim J. van der Linden",2013,50,"The van der Linden article (this issue) provides a roadmap for future research in equating. My belief is that the roadmap begins and ends with collecting auxiliary data that can be utilized to provide improved equating, especially when data are sparse or equating beyond simple moments is desired. © 2013 by the National Council on Measurement in Education.","Comments on ""Some conceptual issues in observed-score equating"" by Wim J. van der Linden","The van der Linden article (this issue) provides a roadmap for future research in equating. My belief is that the roadmap begins and ends with collecting auxiliary data that can be utilized to provide improved equating, especially when data are sparse or equating beyond simple moments is desired. © 2013 by the National Council on Measurement in Education.","['van', 'der', 'Linden', 'article', 'issue', 'provide', 'roadmap', 'future', 'research', 'equate', 'belief', 'roadmap', 'begin', 'end', 'collect', 'auxiliary', 'datum', 'utilize', 'provide', 'improve', 'equate', 'especially', 'datum', 'sparse', 'equate', 'simple', 'moment', 'desire', '©', '2013', 'National', 'Council']","['comment', 'conceptual', 'issue', 'observedscore', 'equate', 'Wim', 'J', 'van', 'der', 'Linden']",van der Linden article issue provide roadmap future research equate belief roadmap begin end collect auxiliary datum utilize provide improve equate especially datum sparse equate simple moment desire © 2013 National Council,comment conceptual issue observedscore equate Wim J van der Linden,0.03604838743119126,0.03591650126326652,0.03604912813005198,0.03682428937075471,0.8551616938047356,0.0,0.0,0.014127051419767922,0.09726291275633672,0.0
Häggström J.; Wiberg M.,Optimal bandwidth selection in observed-score kernel equating,2014,51,"The selection of bandwidth in kernel equating is important because it has a direct impact on the equated test scores. The aim of this article is to examine the use of double smoothing when selecting bandwidths in kernel equating and to compare double smoothing with the commonly used penalty method. This comparison was made using both an equivalent groups design and a nonequivalent group with anchor test design. The performance of the methods was evaluated through simulation studies using both symmetric and skewed score distributions. In addition, the bandwidth selection methods were applied to real data from a college admissions test. The results show that the traditional penalty method works well although double smoothing is a viable alternative because it performs reasonably well compared to the traditional method. © 2014 by the National Council on Measurement in Education.",Optimal bandwidth selection in observed-score kernel equating,"The selection of bandwidth in kernel equating is important because it has a direct impact on the equated test scores. The aim of this article is to examine the use of double smoothing when selecting bandwidths in kernel equating and to compare double smoothing with the commonly used penalty method. This comparison was made using both an equivalent groups design and a nonequivalent group with anchor test design. The performance of the methods was evaluated through simulation studies using both symmetric and skewed score distributions. In addition, the bandwidth selection methods were applied to real data from a college admissions test. The results show that the traditional penalty method works well although double smoothing is a viable alternative because it performs reasonably well compared to the traditional method. © 2014 by the National Council on Measurement in Education.","['selection', 'bandwidth', 'kernel', 'equating', 'important', 'direct', 'impact', 'equate', 'test', 'score', 'aim', 'article', 'examine', 'double', 'smoothing', 'select', 'bandwidth', 'kernel', 'equating', 'compare', 'double', 'smoothing', 'commonly', 'penalty', 'method', 'comparison', 'equivalent', 'group', 'design', 'nonequivalent', 'group', 'anchor', 'test', 'design', 'performance', 'method', 'evaluate', 'simulation', 'study', 'symmetric', 'skewed', 'score', 'distribution', 'addition', 'bandwidth', 'selection', 'method', 'apply', 'real', 'datum', 'college', 'admission', 'test', 'result', 'traditional', 'penalty', 'method', 'work', 'double', 'smoothing', 'viable', 'alternative', 'perform', 'reasonably', 'compare', 'traditional', 'method', '©', '2014', 'National', 'Council']","['optimal', 'bandwidth', 'selection', 'observedscore', 'kernel', 'equating']",selection bandwidth kernel equating important direct impact equate test score aim article examine double smoothing select bandwidth kernel equating compare double smoothing commonly penalty method comparison equivalent group design nonequivalent group anchor test design performance method evaluate simulation study symmetric skewed score distribution addition bandwidth selection method apply real datum college admission test result traditional penalty method work double smoothing viable alternative perform reasonably compare traditional method © 2014 National Council,optimal bandwidth selection observedscore kernel equating,0.030879044938924637,0.03028337335459536,0.030399204626183934,0.878064907409883,0.030373469670413066,0.016369836399561938,0.0016154530079713083,0.014729087124703544,0.09276670752580106,0.0
Albano A.D.,Multilevel modeling of item position effects,2013,50,"In many testing programs it is assumed that the context or position in which an item is administered does not have a differential effect on examinee responses to the item. Violations of this assumption may bias item response theory estimates of item and person parameters. This study examines the potentially biasing effects of item position. A hierarchical generalized linear model is formulated for estimating item-position effects. The model is demonstrated using data from a pilot administration of the GRE wherein the same items appeared in different positions across the test form. Methods for detecting and assessing position effects are discussed, as are applications of the model in the contexts of test development and item analysis. © 2013 by the National Council on Measurement in Education.",,"In many testing programs it is assumed that the context or position in which an item is administered does not have a differential effect on examinee responses to the item. Violations of this assumption may bias item response theory estimates of item and person parameters. This study examines the potentially biasing effects of item position. A hierarchical generalized linear model is formulated for estimating item-position effects. The model is demonstrated using data from a pilot administration of the GRE wherein the same items appeared in different positions across the test form. Methods for detecting and assessing position effects are discussed, as are applications of the model in the contexts of test development and item analysis. © 2013 by the National Council on Measurement in Education.","['testing', 'program', 'assume', 'context', 'position', 'item', 'administer', 'differential', 'effect', 'examinee', 'response', 'item', 'violation', 'assumption', 'bias', 'item', 'response', 'theory', 'estimate', 'item', 'person', 'parameter', 'study', 'examine', 'potentially', 'bias', 'effect', 'item', 'position', 'hierarchical', 'generalize', 'linear', 'formulate', 'estimate', 'itemposition', 'effect', 'demonstrate', 'datum', 'pilot', 'administration', 'GRE', 'item', 'appear', 'different', 'position', 'test', 'form', 'Methods', 'detect', 'assess', 'position', 'effect', 'discuss', 'application', 'context', 'test', 'development', 'item', 'analysis', '©', '2013', 'National', 'Council']",,testing program assume context position item administer differential effect examinee response item violation assumption bias item response theory estimate item person parameter study examine potentially bias effect item position hierarchical generalize linear formulate estimate itemposition effect demonstrate datum pilot administration GRE item appear different position test form Methods detect assess position effect discuss application context test development item analysis © 2013 National Council,,0.8750048751549021,0.031124081110544342,0.03108433649583036,0.031211393400725426,0.03157531383799761,0.09649745090724171,0.0,0.0,0.0,0.012722897151457026
Schmidt S.; Zlatkin-Troitschanskaia O.; Fox J.-P.,Pretest-Posttest-Posttest Multilevel IRT Modeling of Competence Growth of Students in Higher Education in Germany,2016,53,"Longitudinal research in higher education faces several challenges. Appropriate methods of analyzing competence growth of students are needed to deal with those challenges and thereby obtain valid results. In this article, a pretest-posttest-posttest multivariate multilevel IRT model for repeated measures is introduced which is designed to address educational research questions according to a German research project. In this model, dependencies between repeated observations of the same students are considered not, as usual, by clustering observations within participants but rather by clustering observations within semesters. Estimation of the model is conducted within a Bayesian framework. Results indicate that competences grew over time. Gender, intelligence, motivation, and prior education could explain differences in the level of competence among business and economics students. Copyright © 2016 by the National Council on Measurement in Education",Pretest-Posttest-Posttest Multilevel IRT Modeling of Competence Growth of Students in Higher Education in Germany,"Longitudinal research in higher education faces several challenges. Appropriate methods of analyzing competence growth of students are needed to deal with those challenges and thereby obtain valid results. In this article, a pretest-posttest-posttest multivariate multilevel IRT model for repeated measures is introduced which is designed to address educational research questions according to a German research project. In this model, dependencies between repeated observations of the same students are considered not, as usual, by clustering observations within participants but rather by clustering observations within semesters. Estimation of the model is conducted within a Bayesian framework. Results indicate that competences grew over time. Gender, intelligence, motivation, and prior education could explain differences in the level of competence among business and economics students. Copyright © 2016 by the National Council on Measurement in Education","['longitudinal', 'research', 'high', 'face', 'challenge', 'appropriate', 'method', 'analyze', 'competence', 'growth', 'student', 'need', 'deal', 'challenge', 'obtain', 'valid', 'result', 'article', 'pretestposttestposttest', 'multivariate', 'multilevel', 'IRT', 'repeat', 'measure', 'introduce', 'design', 'address', 'educational', 'research', 'question', 'accord', 'german', 'research', 'project', 'dependency', 'repeat', 'observation', 'student', 'consider', 'usual', 'cluster', 'observation', 'participant', 'cluster', 'observation', 'semester', 'Estimation', 'conduct', 'bayesian', 'framework', 'result', 'indicate', 'competence', 'grow', 'time', 'Gender', 'intelligence', 'motivation', 'prior', 'explain', 'difference', 'level', 'competence', 'business', 'economics', 'student', 'Copyright', '©', '2016', 'National', 'Council']","['pretestposttestposttest', 'Multilevel', 'IRT', 'modeling', 'Competence', 'Growth', 'Students', 'high', 'Germany']",longitudinal research high face challenge appropriate method analyze competence growth student need deal challenge obtain valid result article pretestposttestposttest multivariate multilevel IRT repeat measure introduce design address educational research question accord german research project dependency repeat observation student consider usual cluster observation participant cluster observation semester Estimation conduct bayesian framework result indicate competence grow time Gender intelligence motivation prior explain difference level competence business economics student Copyright © 2016 National Council,pretestposttestposttest Multilevel IRT modeling Competence Growth Students high Germany,0.026795119876302096,0.02699335151121973,0.026737643994839008,0.8923200099379734,0.027153874679665724,0.0070784258684669634,0.0,0.07202850595057579,0.0015501590360414278,0.0
Hou L.; La Torre J.D.; Nandakumar R.,Differential item functioning assessment in cognitive diagnostic modeling: Application of the wald test to investigate DIF in the DINA model,2014,51,"Analyzing examinees' responses using cognitive diagnostic models (CDMs) has the advantage of providing diagnostic information. To ensure the validity of the results from these models, differential item functioning (DIF) in CDMs needs to be investigated. In this article, the Wald test is proposed to examine DIF in the context of CDMs. This study explored the effectiveness of the Wald test in detecting both uniform and nonuniform DIF in the DINA model through a simulation study. Results of this study suggest that for relatively discriminating items, the Wald test had Type I error rates close to the nominal level. Moreover, its viability was underscored by the medium to high power rates for most investigated DIF types when DIF size was large. Furthermore, the performance of the Wald test in detecting uniform DIF was compared to that of the traditional Mantel-Haenszel (MH) and SIBTEST procedures. The results of the comparison study showed that the Wald test was comparable to or outperformed the MH and SIBTEST procedures. Finally, the strengths and limitations of the proposed method and suggestions for future studies are discussed. © 2014 by the National Council on Measurement in Education.",Differential item functioning assessment in cognitive diagnostic modeling: Application of the wald test to investigate DIF in the DINA model,"Analyzing examinees' responses using cognitive diagnostic models (CDMs) has the advantage of providing diagnostic information. To ensure the validity of the results from these models, differential item functioning (DIF) in CDMs needs to be investigated. In this article, the Wald test is proposed to examine DIF in the context of CDMs. This study explored the effectiveness of the Wald test in detecting both uniform and nonuniform DIF in the DINA model through a simulation study. Results of this study suggest that for relatively discriminating items, the Wald test had Type I error rates close to the nominal level. Moreover, its viability was underscored by the medium to high power rates for most investigated DIF types when DIF size was large. Furthermore, the performance of the Wald test in detecting uniform DIF was compared to that of the traditional Mantel-Haenszel (MH) and SIBTEST procedures. The results of the comparison study showed that the Wald test was comparable to or outperformed the MH and SIBTEST procedures. Finally, the strengths and limitations of the proposed method and suggestions for future studies are discussed. © 2014 by the National Council on Measurement in Education.","['analyzing', 'examine', 'response', 'cognitive', 'diagnostic', 'CDMs', 'advantage', 'provide', 'diagnostic', 'information', 'ensure', 'validity', 'result', 'differential', 'item', 'function', 'DIF', 'CDMs', 'need', 'investigate', 'article', 'Wald', 'test', 'propose', 'examine', 'DIF', 'context', 'CDMs', 'study', 'explore', 'effectiveness', 'Wald', 'test', 'detect', 'uniform', 'nonuniform', 'DIF', 'DINA', 'simulation', 'study', 'result', 'study', 'suggest', 'relatively', 'discriminate', 'item', 'Wald', 'test', 'Type', 'I', 'error', 'rate', 'close', 'nominal', 'level', 'viability', 'underscore', 'medium', 'high', 'power', 'rate', 'investigate', 'dif', 'type', 'DIF', 'size', 'large', 'furthermore', 'performance', 'Wald', 'test', 'detect', 'uniform', 'DIF', 'compare', 'traditional', 'MantelHaenszel', 'MH', 'SIBTEST', 'procedures', 'result', 'comparison', 'study', 'Wald', 'test', 'comparable', 'outperform', 'MH', 'SIBTEST', 'procedure', 'finally', 'strength', 'limitation', 'propose', 'method', 'suggestion', 'future', 'study', 'discuss', '©', '2014', 'National', 'Council']","['differential', 'item', 'functioning', 'assessment', 'cognitive', 'diagnostic', 'modeling', 'application', 'wald', 'test', 'investigate', 'DIF', 'DINA']",analyzing examine response cognitive diagnostic CDMs advantage provide diagnostic information ensure validity result differential item function DIF CDMs need investigate article Wald test propose examine DIF context CDMs study explore effectiveness Wald test detect uniform nonuniform DIF DINA simulation study result study suggest relatively discriminate item Wald test Type I error rate close nominal level viability underscore medium high power rate investigate dif type DIF size large furthermore performance Wald test detect uniform DIF compare traditional MantelHaenszel MH SIBTEST procedures result comparison study Wald test comparable outperform MH SIBTEST procedure finally strength limitation propose method suggestion future study discuss © 2014 National Council,differential item functioning assessment cognitive diagnostic modeling application wald test investigate DIF DINA,0.02709588033476358,0.8922059551512243,0.02666364274929119,0.02700038115150987,0.027034140613211133,0.09094126549465704,0.0,0.0,0.0,0.0
Kane M.T.,Validating the Interpretations and Uses of Test Scores,2013,50,"To validate an interpretation or use of test scores is to evaluate the plausibility of the claims based on the scores. An argument-based approach to validation suggests that the claims based on the test scores be outlined as an argument that specifies the inferences and supporting assumptions needed to get from test responses to score-based interpretations and uses. Validation then can be thought of as an evaluation of the coherence and completeness of this interpretation/use argument and of the plausibility of its inferences and assumptions. In outlining the argument-based approach to validation, this paper makes eight general points. First, it is the proposed score interpretations and uses that are validated and not the test or the test scores. Second, the validity of a proposed interpretation or use depends on how well the evidence supports the claims being made. Third, more-ambitious claims require more support than less-ambitious claims. Fourth, more-ambitious claims (e.g., construct interpretations) tend to be more useful than less-ambitious claims, but they are also harder to validate. Fifth, interpretations and uses can change over time in response to new needs and new understandings leading to changes in the evidence needed for validation. Sixth, the evaluation of score uses requires an evaluation of the consequences of the proposed uses; negative consequences can render a score use unacceptable. Seventh, the rejection of a score use does not necessarily invalidate a prior, underlying score interpretation. Eighth, the validation of the score interpretation on which a score use is based does not validate the score use. © 2013 by the National Council on Measurement in Education.",Validating the Interpretations and Uses of Test Scores,"To validate an interpretation or use of test scores is to evaluate the plausibility of the claims based on the scores. An argument-based approach to validation suggests that the claims based on the test scores be outlined as an argument that specifies the inferences and supporting assumptions needed to get from test responses to score-based interpretations and uses. Validation then can be thought of as an evaluation of the coherence and completeness of this interpretation/use argument and of the plausibility of its inferences and assumptions. In outlining the argument-based approach to validation, this paper makes eight general points. First, it is the proposed score interpretations and uses that are validated and not the test or the test scores. Second, the validity of a proposed interpretation or use depends on how well the evidence supports the claims being made. Third, more-ambitious claims require more support than less-ambitious claims. Fourth, more-ambitious claims (e.g., construct interpretations) tend to be more useful than less-ambitious claims, but they are also harder to validate. Fifth, interpretations and uses can change over time in response to new needs and new understandings leading to changes in the evidence needed for validation. Sixth, the evaluation of score uses requires an evaluation of the consequences of the proposed uses; negative consequences can render a score use unacceptable. Seventh, the rejection of a score use does not necessarily invalidate a prior, underlying score interpretation. Eighth, the validation of the score interpretation on which a score use is based does not validate the score use. © 2013 by the National Council on Measurement in Education.","['validate', 'interpretation', 'test', 'score', 'evaluate', 'plausibility', 'claim', 'base', 'score', 'argumentbased', 'approach', 'validation', 'suggest', 'claim', 'base', 'test', 'score', 'outline', 'argument', 'specify', 'inference', 'support', 'assumption', 'need', 'test', 'response', 'scorebased', 'interpretation', 'validation', 'think', 'evaluation', 'coherence', 'completeness', 'interpretationuse', 'argument', 'plausibility', 'inference', 'assumption', 'outline', 'argumentbased', 'approach', 'validation', 'paper', 'general', 'point', 'First', 'propose', 'score', 'interpretation', 'validate', 'test', 'test', 'score', 'Second', 'validity', 'propose', 'interpretation', 'depend', 'evidence', 'support', 'claim', 'moreambitious', 'claim', 'require', 'support', 'lessambitious', 'claim', 'Fourth', 'moreambitious', 'claim', 'eg', 'construct', 'interpretation', 'tend', 'useful', 'lessambitious', 'claim', 'hard', 'validate', 'fifth', 'interpretation', 'change', 'time', 'response', 'new', 'need', 'new', 'understanding', 'lead', 'change', 'evidence', 'need', 'validation', 'Sixth', 'evaluation', 'score', 'require', 'evaluation', 'consequence', 'propose', 'negative', 'consequence', 'render', 'score', 'unacceptable', 'Seventh', 'rejection', 'score', 'necessarily', 'invalidate', 'prior', 'underlying', 'score', 'interpretation', 'Eighth', 'validation', 'score', 'interpretation', 'score', 'base', 'validate', 'score', '©', '2013', 'National', 'Council']","['validate', 'Interpretations', 'Uses', 'Test', 'Scores']",validate interpretation test score evaluate plausibility claim base score argumentbased approach validation suggest claim base test score outline argument specify inference support assumption need test response scorebased interpretation validation think evaluation coherence completeness interpretationuse argument plausibility inference assumption outline argumentbased approach validation paper general point First propose score interpretation validate test test score Second validity propose interpretation depend evidence support claim moreambitious claim require support lessambitious claim Fourth moreambitious claim eg construct interpretation tend useful lessambitious claim hard validate fifth interpretation change time response new need new understanding lead change evidence need validation Sixth evaluation score require evaluation consequence propose negative consequence render score unacceptable Seventh rejection score necessarily invalidate prior underlying score interpretation Eighth validation score interpretation score base validate score © 2013 National Council,validate Interpretations Uses Test Scores,0.028112178567414093,0.028017357093867913,0.02795817590511212,0.8877854458444953,0.02812684258911058,0.0,0.011217675681842698,0.11387308003126709,0.0038670739161589802,0.0
Moss P.A.,Validity in Action: Lessons From Studies of Data Use,2013,50,Studies of data use illuminate ways in which education professionals have used test scores and other evidence relevant to students' learning-in action in their own contexts of work-to make decisions about their practice. These studies raise instructive challenges for a validity theory that focuses on intended interpretations and uses of test scores as Kane's (this issue) does. This commentary explores implications of data use studies for elaborating Kane's approach to validation to accommodate the ways test scores are used with other sources of evidence to address users' questions. © 2013 by the National Council on Measurement in Education.,Validity in Action: Lessons From Studies of Data Use,Studies of data use illuminate ways in which education professionals have used test scores and other evidence relevant to students' learning-in action in their own contexts of work-to make decisions about their practice. These studies raise instructive challenges for a validity theory that focuses on intended interpretations and uses of test scores as Kane's (this issue) does. This commentary explores implications of data use studies for elaborating Kane's approach to validation to accommodate the ways test scores are used with other sources of evidence to address users' questions. © 2013 by the National Council on Measurement in Education.,"['study', 'datum', 'illuminate', 'way', 'professional', 'test', 'score', 'evidence', 'relevant', 'student', 'learningin', 'action', 'context', 'workto', 'decision', 'practice', 'study', 'raise', 'instructive', 'challenge', 'validity', 'theory', 'focus', 'intend', 'interpretation', 'test', 'score', 'Kanes', 'issue', 'commentary', 'explore', 'implication', 'datum', 'study', 'elaborate', 'Kanes', 'approach', 'validation', 'accommodate', 'way', 'test', 'score', 'source', 'evidence', 'address', 'user', 'question', '©', '2013', 'National', 'Council']","['validity', 'Action', 'Lessons', 'Studies', 'Data', 'Use']",study datum illuminate way professional test score evidence relevant student learningin action context workto decision practice study raise instructive challenge validity theory focus intend interpretation test score Kanes issue commentary explore implication datum study elaborate Kanes approach validation accommodate way test score source evidence address user question © 2013 National Council,validity Action Lessons Studies Data Use,0.029336551650319374,0.02925303166532847,0.029083404492860277,0.029364337091648977,0.8829626750998429,0.0,0.008141737442097318,0.11760811234658773,0.004173327362620038,0.0
Chalmers R.P.,Extended Mixed-Effects Item Response Models With the MH-RM Algorithm,2015,52,"A mixed-effects item response theory (IRT) model is presented as a logical extension of the generalized linear mixed-effects modeling approach to formulating explanatory IRT models. Fixed and random coefficients in the extended model are estimated using a Metropolis-Hastings Robbins-Monro (MH-RM) stochastic imputation algorithm to accommodate for increased dimensionality due to modeling multiple design- and trait-based random effects. As a consequence of using this algorithm, more flexible explanatory IRT models, such as the multidimensional four-parameter logistic model, are easily organized and efficiently estimated for unidimensional and multidimensional tests. Rasch versions of the linear latent trait and latent regression model, along with their extensions, are presented and discussed, Monte Carlo simulations are conducted to determine the efficiency of parameter recovery of the MH-RM algorithm, and an empirical example using the extended mixed-effects IRT model is presented. © 2015 by the National Council on Measurement in Education.",Extended Mixed-Effects Item Response Models With the MH-RM Algorithm,"A mixed-effects item response theory (IRT) model is presented as a logical extension of the generalized linear mixed-effects modeling approach to formulating explanatory IRT models. Fixed and random coefficients in the extended model are estimated using a Metropolis-Hastings Robbins-Monro (MH-RM) stochastic imputation algorithm to accommodate for increased dimensionality due to modeling multiple design- and trait-based random effects. As a consequence of using this algorithm, more flexible explanatory IRT models, such as the multidimensional four-parameter logistic model, are easily organized and efficiently estimated for unidimensional and multidimensional tests. Rasch versions of the linear latent trait and latent regression model, along with their extensions, are presented and discussed, Monte Carlo simulations are conducted to determine the efficiency of parameter recovery of the MH-RM algorithm, and an empirical example using the extended mixed-effects IRT model is presented. © 2015 by the National Council on Measurement in Education.","['mixedeffect', 'item', 'response', 'theory', 'IRT', 'present', 'logical', 'extension', 'generalized', 'linear', 'mixedeffect', 'modeling', 'approach', 'formulate', 'explanatory', 'IRT', 'fix', 'random', 'coefficient', 'extended', 'estimate', 'MetropolisHastings', 'RobbinsMonro', 'MHRM', 'stochastic', 'imputation', 'algorithm', 'accommodate', 'increase', 'dimensionality', 'multiple', 'design', 'traitbase', 'random', 'effect', 'consequence', 'algorithm', 'flexible', 'explanatory', 'IRT', 'multidimensional', 'fourparameter', 'logistic', 'easily', 'organize', 'efficiently', 'estimate', 'unidimensional', 'multidimensional', 'test', 'Rasch', 'version', 'linear', 'latent', 'trait', 'latent', 'regression', 'extension', 'present', 'discuss', 'Monte', 'Carlo', 'simulation', 'conduct', 'determine', 'efficiency', 'parameter', 'recovery', 'MHRM', 'algorithm', 'empirical', 'example', 'extend', 'mixedeffect', 'IRT', 'present', '©', '2015', 'National', 'Council']","['Extended', 'MixedEffects', 'Item', 'Response', 'Models', 'mhrm', 'Algorithm']",mixedeffect item response theory IRT present logical extension generalized linear mixedeffect modeling approach formulate explanatory IRT fix random coefficient extended estimate MetropolisHastings RobbinsMonro MHRM stochastic imputation algorithm accommodate increase dimensionality multiple design traitbase random effect consequence algorithm flexible explanatory IRT multidimensional fourparameter logistic easily organize efficiently estimate unidimensional multidimensional test Rasch version linear latent trait latent regression extension present discuss Monte Carlo simulation conduct determine efficiency parameter recovery MHRM algorithm empirical example extend mixedeffect IRT present © 2015 National Council,Extended MixedEffects Item Response Models mhrm Algorithm,0.026451351961529236,0.026431654761174407,0.02639807573073486,0.026185747723507936,0.8945331698230535,0.055991602682110855,0.0,0.0,0.0,0.01305874872675803
Brückner S.; Pellegrino J.W.,Integrating the Analysis of Mental Operations Into Multilevel Models to Validate an Assessment of Higher Education Students’ Competency in Business and Economics,2016,53,"The Standards for Educational and Psychological Testing indicate that validation of assessments should include analyses of participants’ response processes. However, such analyses typically are conducted only to supplement quantitative field studies with qualitative data, and seldom are such data connected to quantitative data on student or item performance. This paper presents an example of how data from an analysis of mental operations collected using a sociocognitive approach can be quantitatively integrated with other data on student and item performance to validate in part an assessment of higher education students’ competency in business and economics. Evidence of forward reasoning and paraphrasing as mental operations is obtained using the think-aloud method. As part of the validity argument and to enhance credibility of the findings, the generalized linear models are expressed as multilevel models in which the analyses of response processes are aligned with quantitative findings from large-scale field studies. Copyright © 2016 by the National Council on Measurement in Education",Integrating the Analysis of Mental Operations Into Multilevel Models to Validate an Assessment of Higher Education Students’ Competency in Business and Economics,"The Standards for Educational and Psychological Testing indicate that validation of assessments should include analyses of participants’ response processes. However, such analyses typically are conducted only to supplement quantitative field studies with qualitative data, and seldom are such data connected to quantitative data on student or item performance. This paper presents an example of how data from an analysis of mental operations collected using a sociocognitive approach can be quantitatively integrated with other data on student and item performance to validate in part an assessment of higher education students’ competency in business and economics. Evidence of forward reasoning and paraphrasing as mental operations is obtained using the think-aloud method. As part of the validity argument and to enhance credibility of the findings, the generalized linear models are expressed as multilevel models in which the analyses of response processes are aligned with quantitative findings from large-scale field studies. Copyright © 2016 by the National Council on Measurement in Education","['Standards', 'Educational', 'Psychological', 'Testing', 'indicate', 'validation', 'assessment', 'include', 'analysis', 'participant', '’', 'response', 'process', 'analysis', 'typically', 'conduct', 'supplement', 'quantitative', 'field', 'study', 'qualitative', 'datum', 'seldom', 'datum', 'connect', 'quantitative', 'datum', 'student', 'item', 'performance', 'paper', 'present', 'example', 'datum', 'analysis', 'mental', 'operation', 'collect', 'sociocognitive', 'approach', 'quantitatively', 'integrate', 'datum', 'student', 'item', 'performance', 'validate', 'assessment', 'high', 'student', '’', 'competency', 'business', 'economic', 'Evidence', 'forward', 'reasoning', 'paraphrasing', 'mental', 'operation', 'obtain', 'thinkaloud', 'method', 'validity', 'argument', 'enhance', 'credibility', 'finding', 'generalized', 'linear', 'express', 'multilevel', 'analysis', 'response', 'process', 'align', 'quantitative', 'finding', 'largescale', 'field', 'study', 'Copyright', '©', '2016', 'National', 'Council']","['integrate', 'Analysis', 'Mental', 'Operations', 'Multilevel', 'Models', 'validate', 'Assessment', 'high', 'Students', ""'"", 'Competency', 'Business', 'Economics']",Standards Educational Psychological Testing indicate validation assessment include analysis participant ’ response process analysis typically conduct supplement quantitative field study qualitative datum seldom datum connect quantitative datum student item performance paper present example datum analysis mental operation collect sociocognitive approach quantitatively integrate datum student item performance validate assessment high student ’ competency business economic Evidence forward reasoning paraphrasing mental operation obtain thinkaloud method validity argument enhance credibility finding generalized linear express multilevel analysis response process align quantitative finding largescale field study Copyright © 2016 National Council,integrate Analysis Mental Operations Multilevel Models validate Assessment high Students ' Competency Business Economics,0.025010397085579658,0.0250598782844149,0.024956015024060932,0.02507928018777995,0.8998944294181646,0.007856564669476238,0.0,0.11680595664429583,0.0,0.0
"Raczynski K.R.; Cohen A.S.; Engelhard G., Jr.; Lu Z.",Comparing the Effectiveness of Self-Paced and Collaborative Frame-of-Reference Training on Rater Accuracy in a Large-Scale Writing Assessment,2015,52,"There is a large body of research on the effectiveness of rater training methods in the industrial and organizational psychology literature. Less has been reported in the measurement literature on large-scale writing assessments. This study compared the effectiveness of two widely used rater training methods-self-paced and collaborative frame-of-reference training-in the context of a large-scale writing assessment. Sixty-six raters were randomly assigned to the training methods. After training, all raters scored the same 50 representative essays prescored by a group of expert raters. A series of generalized linear mixed models were then fitted to the rating data. Results suggested that the self-paced method was equivalent in effectiveness to the more time-intensive and expensive collaborative method. Implications for large-scale writing assessments and suggestions for further research are discussed. © 2015 by the National Council on Measurement in Education.",Comparing the Effectiveness of Self-Paced and Collaborative Frame-of-Reference Training on Rater Accuracy in a Large-Scale Writing Assessment,"There is a large body of research on the effectiveness of rater training methods in the industrial and organizational psychology literature. Less has been reported in the measurement literature on large-scale writing assessments. This study compared the effectiveness of two widely used rater training methods-self-paced and collaborative frame-of-reference training-in the context of a large-scale writing assessment. Sixty-six raters were randomly assigned to the training methods. After training, all raters scored the same 50 representative essays prescored by a group of expert raters. A series of generalized linear mixed models were then fitted to the rating data. Results suggested that the self-paced method was equivalent in effectiveness to the more time-intensive and expensive collaborative method. Implications for large-scale writing assessments and suggestions for further research are discussed. © 2015 by the National Council on Measurement in Education.","['large', 'body', 'research', 'effectiveness', 'rater', 'training', 'method', 'industrial', 'organizational', 'psychology', 'literature', 'report', 'literature', 'largescale', 'writing', 'assessment', 'study', 'compare', 'effectiveness', 'widely', 'rater', 'training', 'methodsselfpace', 'collaborative', 'frameofreference', 'trainingin', 'context', 'largescale', 'writing', 'assessment', 'Sixtysix', 'rater', 'randomly', 'assign', 'training', 'method', 'train', 'rater', 'score', '50', 'representative', 'essay', 'prescore', 'group', 'expert', 'rater', 'series', 'generalized', 'linear', 'mixed', 'fit', 'rating', 'datum', 'result', 'suggest', 'selfpace', 'method', 'equivalent', 'effectiveness', 'timeintensive', 'expensive', 'collaborative', 'method', 'Implications', 'largescale', 'writing', 'assessment', 'suggestion', 'research', 'discuss', '©', '2015', 'National', 'Council']","['compare', 'Effectiveness', 'SelfPaced', 'Collaborative', 'FrameofReference', 'Training', 'Rater', 'Accuracy', 'LargeScale', 'Writing', 'Assessment']",large body research effectiveness rater training method industrial organizational psychology literature report literature largescale writing assessment study compare effectiveness widely rater training methodsselfpace collaborative frameofreference trainingin context largescale writing assessment Sixtysix rater randomly assign training method train rater score 50 representative essay prescore group expert rater series generalized linear mixed fit rating datum result suggest selfpace method equivalent effectiveness timeintensive expensive collaborative method Implications largescale writing assessment suggestion research discuss © 2015 National Council,compare Effectiveness SelfPaced Collaborative FrameofReference Training Rater Accuracy LargeScale Writing Assessment,0.028763523691546014,0.8857549189622271,0.028342278383504428,0.028388114275017726,0.028751164687704642,0.0,0.0,0.02789992701887504,0.012890355121717975,0.1690860620974293
Han K.T.,An Efficiency Balanced Information Criterion for Item Selection in Computerized Adaptive Testing,2012,49,"Successful administration of computerized adaptive testing (CAT) programs in educational settings requires that test security and item exposure control issues be taken seriously. Developing an item selection algorithm that strikes the right balance between test precision and level of item pool utilization is the key to successful implementation and long-term quality control of CAT. This study proposed a new item selection method using the ""efficiency balanced information"" criterion to address issues with the maximum Fisher information method and stratification methods. According to the simulation results, the new efficiency balanced information method had desirable advantages over the other studied item selection methods in terms of improving the optimality of CAT assembly and utilizing items with low a-values while eliminating the need for item pool stratification. © 2012 by the National Council on Measurement in Education.",An Efficiency Balanced Information Criterion for Item Selection in Computerized Adaptive Testing,"Successful administration of computerized adaptive testing (CAT) programs in educational settings requires that test security and item exposure control issues be taken seriously. Developing an item selection algorithm that strikes the right balance between test precision and level of item pool utilization is the key to successful implementation and long-term quality control of CAT. This study proposed a new item selection method using the ""efficiency balanced information"" criterion to address issues with the maximum Fisher information method and stratification methods. According to the simulation results, the new efficiency balanced information method had desirable advantages over the other studied item selection methods in terms of improving the optimality of CAT assembly and utilizing items with low a-values while eliminating the need for item pool stratification. © 2012 by the National Council on Measurement in Education.","['successful', 'administration', 'computerized', 'adaptive', 'testing', 'CAT', 'program', 'educational', 'setting', 'require', 'test', 'security', 'item', 'exposure', 'control', 'issue', 'seriously', 'develop', 'item', 'selection', 'algorithm', 'strike', 'right', 'balance', 'test', 'precision', 'level', 'item', 'pool', 'utilization', 'key', 'successful', 'implementation', 'longterm', 'quality', 'control', 'CAT', 'study', 'propose', 'new', 'item', 'selection', 'method', 'efficiency', 'balance', 'information', 'criterion', 'address', 'issue', 'maximum', 'Fisher', 'information', 'method', 'stratification', 'method', 'accord', 'simulation', 'result', 'new', 'efficiency', 'balanced', 'information', 'method', 'desirable', 'advantage', 'study', 'item', 'selection', 'method', 'term', 'improve', 'optimality', 'CAT', 'assembly', 'utilize', 'item', 'low', 'avalue', 'eliminate', 'need', 'item', 'pool', 'stratification', '©', '2012', 'National', 'Council']","['Efficiency', 'Balanced', 'Information', 'Criterion', 'Item', 'Selection', 'Computerized', 'Adaptive', 'Testing']",successful administration computerized adaptive testing CAT program educational setting require test security item exposure control issue seriously develop item selection algorithm strike right balance test precision level item pool utilization key successful implementation longterm quality control CAT study propose new item selection method efficiency balance information criterion address issue maximum Fisher information method stratification method accord simulation result new efficiency balanced information method desirable advantage study item selection method term improve optimality CAT assembly utilize item low avalue eliminate need item pool stratification © 2012 National Council,Efficiency Balanced Information Criterion Item Selection Computerized Adaptive Testing,0.02616441676186784,0.8962794179190425,0.02585518242495541,0.025828705044296022,0.025872277849838157,0.07820788507801263,0.0,0.0,0.0029084360636831073,0.0
Jiao H.; Wang S.; He W.,Estimation methods for one-parameter testlet models,2013,50,"This study demonstrated the equivalence between the Rasch testlet model and the three-level one-parameter testlet model and explored the Markov Chain Monte Carlo (MCMC) method for model parameter estimation in WINBUGS. The estimation accuracy from the MCMC method was compared with those from the marginalized maximum likelihood estimation (MMLE) with the expectation-maximization algorithm in ConQuest and the sixth-order Laplace approximation estimation in HLM6. The results indicated that the estimation methods had significant effects on the bias of the testlet variance and ability variance estimation, the random error in the ability parameter estimation, and the bias in the item difficulty parameter estimation. The Laplace method best recovered the testlet variance while the MMLE best recovered the ability variance. The Laplace method resulted in the smallest random error in the ability parameter estimation while the MCMC method produced the smallest bias in item parameter estimates. Analyses of three real tests generally supported the findings from the simulation and indicated that the estimates for item difficulty and ability parameters were highly correlated across estimation methods. Copyright © 2013 by the National Council on Measurement in Education.",Estimation methods for one-parameter testlet models,"This study demonstrated the equivalence between the Rasch testlet model and the three-level one-parameter testlet model and explored the Markov Chain Monte Carlo (MCMC) method for model parameter estimation in WINBUGS. The estimation accuracy from the MCMC method was compared with those from the marginalized maximum likelihood estimation (MMLE) with the expectation-maximization algorithm in ConQuest and the sixth-order Laplace approximation estimation in HLM6. The results indicated that the estimation methods had significant effects on the bias of the testlet variance and ability variance estimation, the random error in the ability parameter estimation, and the bias in the item difficulty parameter estimation. The Laplace method best recovered the testlet variance while the MMLE best recovered the ability variance. The Laplace method resulted in the smallest random error in the ability parameter estimation while the MCMC method produced the smallest bias in item parameter estimates. Analyses of three real tests generally supported the findings from the simulation and indicated that the estimates for item difficulty and ability parameters were highly correlated across estimation methods. Copyright © 2013 by the National Council on Measurement in Education.","['study', 'demonstrate', 'equivalence', 'Rasch', 'testlet', 'threelevel', 'oneparameter', 'testlet', 'explore', 'Markov', 'Chain', 'Monte', 'Carlo', 'MCMC', 'method', 'parameter', 'estimation', 'WINBUGS', 'estimation', 'accuracy', 'MCMC', 'method', 'compare', 'marginalized', 'maximum', 'likelihood', 'estimation', 'MMLE', 'expectationmaximization', 'algorithm', 'ConQuest', 'sixthorder', 'Laplace', 'approximation', 'estimation', 'HLM6', 'result', 'indicate', 'estimation', 'method', 'significant', 'effect', 'bias', 'testlet', 'variance', 'ability', 'variance', 'estimation', 'random', 'error', 'ability', 'parameter', 'estimation', 'bias', 'item', 'difficulty', 'parameter', 'estimation', 'Laplace', 'method', 'recover', 'testlet', 'variance', 'MMLE', 'recover', 'ability', 'variance', 'Laplace', 'method', 'result', 'small', 'random', 'error', 'ability', 'parameter', 'estimation', 'MCMC', 'method', 'produce', 'small', 'bias', 'item', 'parameter', 'estimate', 'Analyses', 'real', 'test', 'generally', 'support', 'finding', 'simulation', 'indicate', 'estimate', 'item', 'difficulty', 'ability', 'parameter', 'highly', 'correlate', 'estimation', 'method', 'Copyright', '©', '2013', 'National', 'Council']","['estimation', 'method', 'oneparameter', 'testlet']",study demonstrate equivalence Rasch testlet threelevel oneparameter testlet explore Markov Chain Monte Carlo MCMC method parameter estimation WINBUGS estimation accuracy MCMC method compare marginalized maximum likelihood estimation MMLE expectationmaximization algorithm ConQuest sixthorder Laplace approximation estimation HLM6 result indicate estimation method significant effect bias testlet variance ability variance estimation random error ability parameter estimation bias item difficulty parameter estimation Laplace method recover testlet variance MMLE recover ability variance Laplace method result small random error ability parameter estimation MCMC method produce small bias item parameter estimate Analyses real test generally support finding simulation indicate estimate item difficulty ability parameter highly correlate estimation method Copyright © 2013 National Council,estimation method oneparameter testlet,0.880492686760633,0.02995130315342725,0.02966855105904497,0.02975808150374546,0.03012937752314938,0.06910903666336501,0.0,0.0,0.020861458664465163,0.0
Falk C.F.; Cai L.,Semiparametric Item Response Functions in the Context of Guessing,2016,53,"We present a logistic function of a monotonic polynomial with a lower asymptote, allowing additional flexibility beyond the three-parameter logistic model. We develop a maximum marginal likelihood-based approach to estimate the item parameters. The new item response model is demonstrated on math assessment data from a state, and a computationally efficient strategy for choosing the order of the polynomial is demonstrated. Finally, our approach is tested through simulations and compared to response function estimation using smoothed isotonic regression. Results indicate that our approach can result in small gains in item response function recovery and latent trait estimation. Copyright © 2016 by the National Council on Measurement in Education",Semiparametric Item Response Functions in the Context of Guessing,"We present a logistic function of a monotonic polynomial with a lower asymptote, allowing additional flexibility beyond the three-parameter logistic model. We develop a maximum marginal likelihood-based approach to estimate the item parameters. The new item response model is demonstrated on math assessment data from a state, and a computationally efficient strategy for choosing the order of the polynomial is demonstrated. Finally, our approach is tested through simulations and compared to response function estimation using smoothed isotonic regression. Results indicate that our approach can result in small gains in item response function recovery and latent trait estimation. Copyright © 2016 by the National Council on Measurement in Education","['present', 'logistic', 'function', 'monotonic', 'polynomial', 'low', 'asymptote', 'allow', 'additional', 'flexibility', 'threeparameter', 'logistic', 'develop', 'maximum', 'marginal', 'likelihoodbased', 'approach', 'estimate', 'item', 'parameter', 'new', 'item', 'response', 'demonstrate', 'math', 'assessment', 'datum', 'state', 'computationally', 'efficient', 'strategy', 'choose', 'order', 'polynomial', 'demonstrate', 'finally', 'approach', 'test', 'simulation', 'compare', 'response', 'function', 'estimation', 'smoothed', 'isotonic', 'regression', 'result', 'indicate', 'approach', 'result', 'small', 'gain', 'item', 'response', 'function', 'recovery', 'latent', 'trait', 'estimation', 'Copyright', '©', '2016', 'National', 'Council']","['Semiparametric', 'Item', 'Response', 'Functions', 'Context', 'Guessing']",present logistic function monotonic polynomial low asymptote allow additional flexibility threeparameter logistic develop maximum marginal likelihoodbased approach estimate item parameter new item response demonstrate math assessment datum state computationally efficient strategy choose order polynomial demonstrate finally approach test simulation compare response function estimation smoothed isotonic regression result indicate approach result small gain item response function recovery latent trait estimation Copyright © 2016 National Council,Semiparametric Item Response Functions Context Guessing,0.02745231352417248,0.027260234224427943,0.027253765888357838,0.02724641102822474,0.890787275334817,0.079860734863781,0.0,0.0010374819168645388,0.0,0.0
Von Davier M.; B. J.G.; von Davier A.A.,"Local equating using the rasch model, the OPLM, and the 2PL IRT model-or-what is it anyway if the model captures everything there is to know about the test takers?",2013,50,"Local equating (LE) is based on Lord's criterion of equity. It defines a family of true transformations that aim at the ideal of equitable equating. van der Linden (this issue) offers a detailed discussion of common issues in observed-score equating relative to this local approach. By assuming an underlying item response theory model, one of the main features of LE is that it adjusts the equated raw scores using conditional distributions of raw scores given an estimate of the ability of interest. In this article, we argue that this feature disappears when using a Rasch model for the estimation of the true transformation, while the one-parameter logistic model and the two-parameter logistic model do provide a local adjustment of the equated score. © 2013 by the National Council on Measurement in Education.","Local equating using the rasch model, the OPLM, and the 2PL IRT model-or-what is it anyway if the model captures everything there is to know about the test takers?","Local equating (LE) is based on Lord's criterion of equity. It defines a family of true transformations that aim at the ideal of equitable equating. van der Linden (this issue) offers a detailed discussion of common issues in observed-score equating relative to this local approach. By assuming an underlying item response theory model, one of the main features of LE is that it adjusts the equated raw scores using conditional distributions of raw scores given an estimate of the ability of interest. In this article, we argue that this feature disappears when using a Rasch model for the estimation of the true transformation, while the one-parameter logistic model and the two-parameter logistic model do provide a local adjustment of the equated score. © 2013 by the National Council on Measurement in Education.","['local', 'equating', 'LE', 'base', 'Lords', 'criterion', 'equity', 'define', 'family', 'true', 'transformation', 'aim', 'ideal', 'equitable', 'equate', 'van', 'der', 'Linden', 'issue', 'offer', 'detailed', 'discussion', 'common', 'issue', 'observedscore', 'equate', 'relative', 'local', 'approach', 'assume', 'underlying', 'item', 'response', 'theory', 'main', 'feature', 'LE', 'adjust', 'equate', 'raw', 'score', 'conditional', 'distribution', 'raw', 'score', 'estimate', 'ability', 'interest', 'article', 'argue', 'feature', 'disappear', 'Rasch', 'estimation', 'true', 'transformation', 'oneparameter', 'logistic', 'twoparameter', 'logistic', 'provide', 'local', 'adjustment', 'equate', 'score', '©', '2013', 'National', 'Council']","['local', 'equating', 'rasch', 'OPLM', '2PL', 'IRT', 'modelorwhat', 'capture', 'know', 'test', 'taker']",local equating LE base Lords criterion equity define family true transformation aim ideal equitable equate van der Linden issue offer detailed discussion common issue observedscore equate relative local approach assume underlying item response theory main feature LE adjust equate raw score conditional distribution raw score estimate ability interest article argue feature disappear Rasch estimation true transformation oneparameter logistic twoparameter logistic provide local adjustment equate score © 2013 National Council,local equating rasch OPLM 2PL IRT modelorwhat capture know test taker,0.027460145424793272,0.02757260791960024,0.8893928666827582,0.028138412165095352,0.027435967807752787,0.0021356161379183494,0.0,0.004921264236258782,0.13246542428498784,0.0026497055161717412
"Davison M.L.; Davenport E.C., Jr.; Chang Y.-F.; Vue K.; Su S.",Criterion-Related Validity: Assessing the Value of Subscores,2015,52,"Criterion-related profile analysis (CPA) can be used to assess whether subscores of a test or test battery account for more criterion variance than does a single total score. Application of CPA to subscore evaluation is described, compared to alternative procedures, and illustrated using SAT data. Considerations other than validity and reliability are discussed, including broad societal goals (e.g., affirmative action), fairness, and ties in expected criterion predictions. In simulation data, CPA results were sensitive to subscore correlations, sample size, and the proportion of criterion-related variance accounted for by the subscores. CPA can be a useful component in a thorough subscore evaluation encompassing subscore reliability, validity, distinctiveness, fairness, and broader societal goals. © 2015 by the National Council on Measurement in Education.",Criterion-Related Validity: Assessing the Value of Subscores,"Criterion-related profile analysis (CPA) can be used to assess whether subscores of a test or test battery account for more criterion variance than does a single total score. Application of CPA to subscore evaluation is described, compared to alternative procedures, and illustrated using SAT data. Considerations other than validity and reliability are discussed, including broad societal goals (e.g., affirmative action), fairness, and ties in expected criterion predictions. In simulation data, CPA results were sensitive to subscore correlations, sample size, and the proportion of criterion-related variance accounted for by the subscores. CPA can be a useful component in a thorough subscore evaluation encompassing subscore reliability, validity, distinctiveness, fairness, and broader societal goals. © 2015 by the National Council on Measurement in Education.","['criterionrelate', 'profile', 'analysis', 'cpa', 'assess', 'subscore', 'test', 'test', 'battery', 'account', 'criterion', 'variance', 'single', 'total', 'score', 'application', 'cpa', 'subscore', 'evaluation', 'describe', 'compare', 'alternative', 'procedure', 'illustrate', 'SAT', 'datum', 'consideration', 'validity', 'reliability', 'discuss', 'include', 'broad', 'societal', 'goal', 'eg', 'affirmative', 'action', 'fairness', 'tie', 'expect', 'criterion', 'prediction', 'simulation', 'datum', 'cpa', 'result', 'sensitive', 'subscore', 'correlation', 'sample', 'size', 'proportion', 'criterionrelated', 'variance', 'account', 'subscore', 'cpa', 'useful', 'component', 'thorough', 'subscore', 'evaluation', 'encompass', 'subscore', 'reliability', 'validity', 'distinctiveness', 'fairness', 'broad', 'societal', 'goal', '©', '2015', 'National', 'Council']","['CriterionRelated', 'Validity', 'assess', 'Value', 'Subscores']",criterionrelate profile analysis cpa assess subscore test test battery account criterion variance single total score application cpa subscore evaluation describe compare alternative procedure illustrate SAT datum consideration validity reliability discuss include broad societal goal eg affirmative action fairness tie expect criterion prediction simulation datum cpa result sensitive subscore correlation sample size proportion criterionrelated variance account subscore cpa useful component thorough subscore evaluation encompass subscore reliability validity distinctiveness fairness broad societal goal © 2015 National Council,CriterionRelated Validity assess Value Subscores,0.03067249894427858,0.031197700170336445,0.03070837761192974,0.8763712465878777,0.03105017668557754,0.0,0.23149472024728157,0.004133848156350084,0.0,0.0006126858585902563
Clauser J.C.; Margolis M.J.; Clauser B.E.,An examination of the replicability of angoff standard setting results within a generalizability theory framework,2014,51,"Evidence of stable standard setting results over panels or occasions is an important part of the validity argument for an established cut score. Unfortunately, due to the high cost of convening multiple panels of content experts, standards often are based on the recommendation from a single panel of judges. This approach implicitly assumes that the variability across panels will be modest, but little evidence is available to support this assertion. This article examines the stability of Angoff standard setting results across panels. Data were collected for six independent standard setting exercises, with three panels participating in each exercise. The results show that although in some cases the panel effect is negligible, for four of the six data sets the panel facet represented a large portion of the overall error variance. Ignoring the often hidden panel/occasion facet can result in artificially optimistic estimates of the cut score stability. Results based on a single panel should not be viewed as a reasonable estimate of the results that would be found over multiple panels. Instead, the variability seen in a single panel can best be viewed as a lower bound of the expected variability when the exercise is replicated. © 2014 by the National Board of Medical Examiners.",An examination of the replicability of angoff standard setting results within a generalizability theory framework,"Evidence of stable standard setting results over panels or occasions is an important part of the validity argument for an established cut score. Unfortunately, due to the high cost of convening multiple panels of content experts, standards often are based on the recommendation from a single panel of judges. This approach implicitly assumes that the variability across panels will be modest, but little evidence is available to support this assertion. This article examines the stability of Angoff standard setting results across panels. Data were collected for six independent standard setting exercises, with three panels participating in each exercise. The results show that although in some cases the panel effect is negligible, for four of the six data sets the panel facet represented a large portion of the overall error variance. Ignoring the often hidden panel/occasion facet can result in artificially optimistic estimates of the cut score stability. Results based on a single panel should not be viewed as a reasonable estimate of the results that would be found over multiple panels. Instead, the variability seen in a single panel can best be viewed as a lower bound of the expected variability when the exercise is replicated. © 2014 by the National Board of Medical Examiners.","['evidence', 'stable', 'standard', 'set', 'result', 'panel', 'occasion', 'important', 'validity', 'argument', 'establish', 'cut', 'score', 'unfortunately', 'high', 'cost', 'convene', 'multiple', 'panel', 'content', 'expert', 'standard', 'base', 'recommendation', 'single', 'panel', 'judge', 'approach', 'implicitly', 'assume', 'variability', 'panel', 'modest', 'little', 'evidence', 'available', 'support', 'assertion', 'article', 'examine', 'stability', 'Angoff', 'standard', 'set', 'result', 'panel', 'Data', 'collect', 'independent', 'standard', 'set', 'exercise', 'panel', 'participate', 'exercise', 'result', 'case', 'panel', 'effect', 'negligible', 'datum', 'set', 'panel', 'facet', 'represent', 'large', 'portion', 'overall', 'error', 'variance', 'ignore', 'hide', 'paneloccasion', 'facet', 'result', 'artificially', 'optimistic', 'estimate', 'cut', 'score', 'stability', 'result', 'base', 'single', 'panel', 'view', 'reasonable', 'estimate', 'result', 'find', 'multiple', 'panel', 'instead', 'variability', 'single', 'panel', 'view', 'lower', 'bind', 'expect', 'variability', 'exercise', 'replicate', '©', '2014', 'National', 'Board', 'Medical', 'Examiners']","['examination', 'replicability', 'angoff', 'standard', 'set', 'result', 'generalizability', 'theory', 'framework']",evidence stable standard set result panel occasion important validity argument establish cut score unfortunately high cost convene multiple panel content expert standard base recommendation single panel judge approach implicitly assume variability panel modest little evidence available support assertion article examine stability Angoff standard set result panel Data collect independent standard set exercise panel participate exercise result case panel effect negligible datum set panel facet represent large portion overall error variance ignore hide paneloccasion facet result artificially optimistic estimate cut score stability result base single panel view reasonable estimate result find multiple panel instead variability single panel view lower bind expect variability exercise replicate © 2014 National Board Medical Examiners,examination replicability angoff standard set result generalizability theory framework,0.030741122867546107,0.030742231054975287,0.8769868084521218,0.03044283061528657,0.031087007010070205,0.008470105382855064,0.012429259854251503,0.028507807030453094,0.008957304038656298,0.021504483291027658
Wang W.-C.; Jin K.-Y.; Qiu X.-L.; Wang L.,Item response models for examinee-selected items,2012,49,"In some tests, examinees are required to choose a fixed number of items from a set of given items to answer. This practice creates a challenge to standard item response models, because more capable examinees may have an advantage by making wiser choices. In this study, we developed a new class of item response models to account for the choice effect of examinee-selected items. The results of a series of simulation studies showed: (1) that the parameters of the new models were recovered well, (2) the parameter estimates were almost unbiased when the new models were fit to data that were simulated from standard item response models, (3) failing to consider the choice effect yielded shrunken parameter estimates for examinee-selected items, and (4) even when the missingness mechanism in examinee-selected items did not follow the item response functions specified in the new models, the new models still yielded a better fit than did standard item response models. An empirical example of a college entrance examination supported the use of the new models: in general, the higher the examinee's ability, the better his or her choice of items. © 2012 by the National Council on Measurement in Education.",,"In some tests, examinees are required to choose a fixed number of items from a set of given items to answer. This practice creates a challenge to standard item response models, because more capable examinees may have an advantage by making wiser choices. In this study, we developed a new class of item response models to account for the choice effect of examinee-selected items. The results of a series of simulation studies showed: (1) that the parameters of the new models were recovered well, (2) the parameter estimates were almost unbiased when the new models were fit to data that were simulated from standard item response models, (3) failing to consider the choice effect yielded shrunken parameter estimates for examinee-selected items, and (4) even when the missingness mechanism in examinee-selected items did not follow the item response functions specified in the new models, the new models still yielded a better fit than did standard item response models. An empirical example of a college entrance examination supported the use of the new models: in general, the higher the examinee's ability, the better his or her choice of items. © 2012 by the National Council on Measurement in Education.","['test', 'examinee', 'require', 'choose', 'fix', 'number', 'item', 'set', 'item', 'answer', 'practice', 'create', 'challenge', 'standard', 'item', 'response', 'capable', 'examinee', 'advantage', 'wise', 'choice', 'study', 'develop', 'new', 'class', 'item', 'response', 'account', 'choice', 'effect', 'examineeselecte', 'item', 'result', 'series', 'simulation', 'study', '1', 'parameter', 'new', 'recover', '2', 'parameter', 'estimate', 'unbiased', 'new', 'fit', 'datum', 'simulate', 'standard', 'item', 'response', '3', 'fail', 'consider', 'choice', 'effect', 'yield', 'shrunken', 'parameter', 'estimate', 'examineeselecte', 'item', '4', 'missingness', 'mechanism', 'examineeselecte', 'item', 'follow', 'item', 'response', 'function', 'specify', 'new', 'new', 'yield', 'fit', 'standard', 'item', 'response', 'empirical', 'example', 'college', 'entrance', 'examination', 'support', 'new', 'general', 'high', 'examinee', 'ability', 'choice', 'item', '©', '2012', 'National', 'Council']",,test examinee require choose fix number item set item answer practice create challenge standard item response capable examinee advantage wise choice study develop new class item response account choice effect examineeselecte item result series simulation study 1 parameter new recover 2 parameter estimate unbiased new fit datum simulate standard item response 3 fail consider choice effect yield shrunken parameter estimate examineeselecte item 4 missingness mechanism examineeselecte item follow item response function specify new new yield fit standard item response empirical example college entrance examination support new general high examinee ability choice item © 2012 National Council,,0.027888879843442172,0.0278205939589513,0.0278792593001288,0.888380825767949,0.028030441129528762,0.09578401233046989,0.0,0.0,0.0,0.0102124729295365
van der Palm D.W.; van der Ark L.A.; Sijtsma K.,A Flexible Latent Class Approach to Estimating Test-Score Reliability,2014,51,"The latent class reliability coefficient (LCRC) is improved by using the divisive latent class model instead of the unrestricted latent class model. This results in the divisive latent class reliability coefficient (DLCRC), which unlike LCRC avoids making subjective decisions about the best solution and thus avoids judgment error. A computational study using large numbers of items shows that DLCRC also is faster than LCRC and fast enough for practical purposes. Speed and objectivity render DLCRC superior to LCRC. A decisive feature of DLCRC is that it aims at closely approximating the multivariate distribution of item scores, which might render the method suited when test data are multidimensional. A simulation study focusing on multidimensionality shows that DLCRC in general has little bias relative to the true reliability and is relatively accurate compared to LCRC and classical lower bound methods coefficients α and λ2 and the greatest lower bound. © 2014 by the National Council on Measurement in Education.",A Flexible Latent Class Approach to Estimating Test-Score Reliability,"The latent class reliability coefficient (LCRC) is improved by using the divisive latent class model instead of the unrestricted latent class model. This results in the divisive latent class reliability coefficient (DLCRC), which unlike LCRC avoids making subjective decisions about the best solution and thus avoids judgment error. A computational study using large numbers of items shows that DLCRC also is faster than LCRC and fast enough for practical purposes. Speed and objectivity render DLCRC superior to LCRC. A decisive feature of DLCRC is that it aims at closely approximating the multivariate distribution of item scores, which might render the method suited when test data are multidimensional. A simulation study focusing on multidimensionality shows that DLCRC in general has little bias relative to the true reliability and is relatively accurate compared to LCRC and classical lower bound methods coefficients α and λ2 and the greatest lower bound. © 2014 by the National Council on Measurement in Education.","['latent', 'class', 'reliability', 'coefficient', 'LCRC', 'improve', 'divisive', 'latent', 'class', 'instead', 'unrestricted', 'latent', 'class', 'result', 'divisive', 'latent', 'class', 'reliability', 'coefficient', 'DLCRC', 'unlike', 'LCRC', 'avoid', 'subjective', 'decision', 'good', 'solution', 'avoid', 'judgment', 'error', 'computational', 'study', 'large', 'number', 'item', 'DLCRC', 'fast', 'LCRC', 'fast', 'practical', 'purpose', 'Speed', 'objectivity', 'render', 'DLCRC', 'superior', 'lcrc', 'decisive', 'feature', 'DLCRC', 'aim', 'closely', 'approximate', 'multivariate', 'distribution', 'item', 'score', 'render', 'method', 'suit', 'test', 'datum', 'multidimensional', 'simulation', 'study', 'focus', 'multidimensionality', 'DLCRC', 'general', 'little', 'bias', 'relative', 'true', 'reliability', 'relatively', 'accurate', 'compare', 'LCRC', 'classical', 'low', 'bind', 'method', 'coefficient', 'α', 'λ2', 'greatest', 'lower', 'bind', '©', '2014', 'National', 'Council']","['Flexible', 'Latent', 'Class', 'Approach', 'estimate', 'TestScore', 'Reliability']",latent class reliability coefficient LCRC improve divisive latent class instead unrestricted latent class result divisive latent class reliability coefficient DLCRC unlike LCRC avoid subjective decision good solution avoid judgment error computational study large number item DLCRC fast LCRC fast practical purpose Speed objectivity render DLCRC superior lcrc decisive feature DLCRC aim closely approximate multivariate distribution item score render method suit test datum multidimensional simulation study focus multidimensionality DLCRC general little bias relative true reliability relatively accurate compare LCRC classical low bind method coefficient α λ2 greatest lower bind © 2014 National Council,Flexible Latent Class Approach estimate TestScore Reliability,0.029152034046561196,0.02921742310932931,0.8831740121632823,0.029279372300480534,0.029177158380346794,0.030164826907386002,0.009266934138400329,0.0031476238074924314,0.008135166023368971,0.006483707519331211
Haertel E.,Getting the Help We Need,2013,50,"In validating uses of testing, it is helpful to distinguish those that rely directly on the information provided by scores or score distributions (direct uses and consequences) versus those that instead capitalize on the motivational effects of testing, or use testing and test reporting to shape public opinion (indirect uses and consequences). Some uses and consequences, both direct and indirect, are intended; others are unintended. Unintended consequences pose greater challenges in test validation because they must be identified before they can be investigated. Validation of uses and consequences can employ theories and methods from various social science disciplines. Educational measurement is most closely allied with psychology and statistics, but sociologists, anthropologists, economists, linguists, and others also could help in theorizing and investigating the consequences of test use, especially indirect and unintended consequences. © 2013 by the National Council on Measurement in Education.",,"In validating uses of testing, it is helpful to distinguish those that rely directly on the information provided by scores or score distributions (direct uses and consequences) versus those that instead capitalize on the motivational effects of testing, or use testing and test reporting to shape public opinion (indirect uses and consequences). Some uses and consequences, both direct and indirect, are intended; others are unintended. Unintended consequences pose greater challenges in test validation because they must be identified before they can be investigated. Validation of uses and consequences can employ theories and methods from various social science disciplines. Educational measurement is most closely allied with psychology and statistics, but sociologists, anthropologists, economists, linguists, and others also could help in theorizing and investigating the consequences of test use, especially indirect and unintended consequences. © 2013 by the National Council on Measurement in Education.","['validate', 'testing', 'helpful', 'distinguish', 'rely', 'directly', 'information', 'provide', 'score', 'score', 'distribution', 'direct', 'consequence', 'versus', 'instead', 'capitalize', 'motivational', 'effect', 'testing', 'testing', 'test', 'report', 'shape', 'public', 'opinion', 'indirect', 'consequence', 'consequence', 'direct', 'indirect', 'intend', 'unintended', 'unintended', 'consequence', 'pose', 'great', 'challenge', 'test', 'validation', 'identify', 'investigate', 'validation', 'consequence', 'employ', 'theory', 'method', 'social', 'science', 'discipline', 'educational', 'closely', 'ally', 'psychology', 'statistic', 'sociologist', 'anthropologist', 'economist', 'linguist', 'help', 'theorize', 'investigate', 'consequence', 'test', 'especially', 'indirect', 'unintended', 'consequence', '©', '2013', 'National', 'Council']",,validate testing helpful distinguish rely directly information provide score score distribution direct consequence versus instead capitalize motivational effect testing testing test report shape public opinion indirect consequence consequence direct indirect intend unintended unintended consequence pose great challenge test validation identify investigate validation consequence employ theory method social science discipline educational closely ally psychology statistic sociologist anthropologist economist linguist help theorize investigate consequence test especially indirect unintended consequence © 2013 National Council,,0.030365154630881398,0.03026353640262809,0.8788449438035627,0.030210346033098117,0.030316019129829747,0.010093831965943635,0.004779648994299879,0.043937282700168785,0.004519026178788797,0.001827714889681832
Cher Wong C.,Asymptotic standard errors for item response theory true score equating of polytomous items,2015,52,"Building on previous works by Lord and Ogasawara for dichotomous items, this article proposes an approach to derive the asymptotic standard errors of item response theory true score equating involving polytomous items, for equivalent and nonequivalent groups of examinees. This analytical approach could be used in place of empirical methods like the bootstrap method, to obtain standard errors of equated scores. Formulas are introduced to obtain the derivatives for computing the asymptotic standard errors. The approach was validated using mean-mean, mean-sigma, random-groups, or concurrent calibration equating of simulated samples, for tests modeled using the generalized partial credit model or the graded response model. © 2015 by the National Council on Measurement in Education.",Asymptotic standard errors for item response theory true score equating of polytomous items,"Building on previous works by Lord and Ogasawara for dichotomous items, this article proposes an approach to derive the asymptotic standard errors of item response theory true score equating involving polytomous items, for equivalent and nonequivalent groups of examinees. This analytical approach could be used in place of empirical methods like the bootstrap method, to obtain standard errors of equated scores. Formulas are introduced to obtain the derivatives for computing the asymptotic standard errors. The approach was validated using mean-mean, mean-sigma, random-groups, or concurrent calibration equating of simulated samples, for tests modeled using the generalized partial credit model or the graded response model. © 2015 by the National Council on Measurement in Education.","['build', 'previous', 'work', 'Lord', 'Ogasawara', 'dichotomous', 'item', 'article', 'propose', 'approach', 'derive', 'asymptotic', 'standard', 'error', 'item', 'response', 'theory', 'true', 'score', 'equating', 'involve', 'polytomous', 'item', 'equivalent', 'nonequivalent', 'group', 'examinee', 'analytical', 'approach', 'place', 'empirical', 'method', 'like', 'bootstrap', 'method', 'obtain', 'standard', 'error', 'equate', 'score', 'formula', 'introduce', 'obtain', 'derivative', 'compute', 'asymptotic', 'standard', 'error', 'approach', 'validate', 'meanmean', 'meansigma', 'randomgroup', 'concurrent', 'calibration', 'equating', 'simulate', 'sample', 'test', 'generalized', 'partial', 'credit', 'grade', 'response', '©', '2015', 'National', 'Council']","['asymptotic', 'standard', 'error', 'item', 'response', 'theory', 'true', 'score', 'equating', 'polytomous', 'item']",build previous work Lord Ogasawara dichotomous item article propose approach derive asymptotic standard error item response theory true score equating involve polytomous item equivalent nonequivalent group examinee analytical approach place empirical method like bootstrap method obtain standard error equate score formula introduce obtain derivative compute asymptotic standard error approach validate meanmean meansigma randomgroup concurrent calibration equating simulate sample test generalized partial credit grade response © 2015 National Council,asymptotic standard error item response theory true score equating polytomous item,0.026188575741978304,0.025827189391829836,0.8960152464080946,0.02597687907830558,0.025992109379791753,0.03469992786668467,0.0,0.018005274964549043,0.08154189081725427,0.0
Mislevy R.J.; Zwick R.,"Scaling, Linking, and Reporting a Periodic Assessment System",2012,49,"A new entry the testing lexicon is through-course summative assessment, a system consisting of components administered periodically during the academic year. As defined the Race to the Top program, these assessments are intended to yield a yearly summative score for accountability purposes. They must provide for both individual and group proficiency estimates and allow for the measurement of growth. They must accommodate students who vary their patterns of curricular exposure. Because they are meant to provide actionable information to teachers they must be instructionally sensitive, so item-operating characteristics can be expected to change relative to one another as a function of patterns of curricular exposure. This paper discusses methodology one can draw upon to tackle this ambitious collection of inferences. We consider a modeling framework that consists of an item response theory component and a population component, as the National Assessment of Educational Progress, and show how performance and growth could be expressed terms of expected performance on a market basket of tasks. We discuss conditions under which modeling simplifications might be possible and discuss studies that would be needed to fit models, estimate parameters, and evaluate data requirements. © 2012 by the National Council on Measurement in Education.","Scaling, Linking, and Reporting a Periodic Assessment System","A new entry the testing lexicon is through-course summative assessment, a system consisting of components administered periodically during the academic year. As defined the Race to the Top program, these assessments are intended to yield a yearly summative score for accountability purposes. They must provide for both individual and group proficiency estimates and allow for the measurement of growth. They must accommodate students who vary their patterns of curricular exposure. Because they are meant to provide actionable information to teachers they must be instructionally sensitive, so item-operating characteristics can be expected to change relative to one another as a function of patterns of curricular exposure. This paper discusses methodology one can draw upon to tackle this ambitious collection of inferences. We consider a modeling framework that consists of an item response theory component and a population component, as the National Assessment of Educational Progress, and show how performance and growth could be expressed terms of expected performance on a market basket of tasks. We discuss conditions under which modeling simplifications might be possible and discuss studies that would be needed to fit models, estimate parameters, and evaluate data requirements. © 2012 by the National Council on Measurement in Education.","['new', 'entry', 'testing', 'lexicon', 'throughcourse', 'summative', 'assessment', 'system', 'consist', 'component', 'administer', 'periodically', 'academic', 'year', 'define', 'race', 'program', 'assessment', 'intend', 'yield', 'yearly', 'summative', 'score', 'accountability', 'purpose', 'provide', 'individual', 'group', 'proficiency', 'estimate', 'allow', 'growth', 'accommodate', 'student', 'vary', 'pattern', 'curricular', 'exposure', 'mean', 'provide', 'actionable', 'information', 'teacher', 'instructionally', 'sensitive', 'itemoperate', 'characteristic', 'expect', 'change', 'relative', 'function', 'pattern', 'curricular', 'exposure', 'paper', 'discuss', 'methodology', 'draw', 'tackle', 'ambitious', 'collection', 'inference', 'consider', 'modeling', 'framework', 'consist', 'item', 'response', 'theory', 'component', 'population', 'component', 'National', 'Assessment', 'Educational', 'Progress', 'performance', 'growth', 'express', 'term', 'expect', 'performance', 'market', 'basket', 'task', 'discuss', 'condition', 'modeling', 'simplification', 'possible', 'discuss', 'study', 'need', 'fit', 'estimate', 'parameter', 'evaluate', 'data', 'requirement', '©', '2012', 'National', 'Council']","['scale', 'Linking', 'report', 'Periodic', 'Assessment', 'system']",new entry testing lexicon throughcourse summative assessment system consist component administer periodically academic year define race program assessment intend yield yearly summative score accountability purpose provide individual group proficiency estimate allow growth accommodate student vary pattern curricular exposure mean provide actionable information teacher instructionally sensitive itemoperate characteristic expect change relative function pattern curricular exposure paper discuss methodology draw tackle ambitious collection inference consider modeling framework consist item response theory component population component National Assessment Educational Progress performance growth express term expect performance market basket task discuss condition modeling simplification possible discuss study need fit estimate parameter evaluate data requirement © 2012 National Council,scale Linking report Periodic Assessment system,0.02212349480308307,0.022066932105951967,0.021819406975984643,0.9117959572918183,0.022194208823162163,0.02167154386543467,0.0,0.08332244858626163,0.0,0.0017297293628565891
Mittelhaëuser M.-A.; Béguin A.A.; Sijtsma K.,The Effect of Differential Motivation on IRT Linking,2015,52,"The purpose of this study was to investigate whether simulated differential motivation between the stakes for operational tests and anchor items produces an invalid linking result if the Rasch model is used to link the operational tests. This was done for an external anchor design and a variation of a pretest design. The study also investigated whether a constrained mixture Rasch model could identify latent classes in such a way that one latent class represented high-stakes responding while the other represented low-stakes responding. The results indicated that for an external anchor design, the Rasch linking result was only biased when the motivation level differed between the subpopulations to which the anchor items were administered. However, the mixture Rasch model did not identify the classes representing low-stakes and high-stakes responding. When a pretest design was used to link the operational tests by means of a Rasch model, the linking result was found to be biased in each condition. Bias increased as percentage of students showing low-stakes responding to the anchor items increased. The mixture Rasch model only identified the classes representing low-stakes and high-stakes responding under a limited number of conditions. © 2015 by the National Council on Measurement in Education.",The Effect of Differential Motivation on IRT Linking,"The purpose of this study was to investigate whether simulated differential motivation between the stakes for operational tests and anchor items produces an invalid linking result if the Rasch model is used to link the operational tests. This was done for an external anchor design and a variation of a pretest design. The study also investigated whether a constrained mixture Rasch model could identify latent classes in such a way that one latent class represented high-stakes responding while the other represented low-stakes responding. The results indicated that for an external anchor design, the Rasch linking result was only biased when the motivation level differed between the subpopulations to which the anchor items were administered. However, the mixture Rasch model did not identify the classes representing low-stakes and high-stakes responding. When a pretest design was used to link the operational tests by means of a Rasch model, the linking result was found to be biased in each condition. Bias increased as percentage of students showing low-stakes responding to the anchor items increased. The mixture Rasch model only identified the classes representing low-stakes and high-stakes responding under a limited number of conditions. © 2015 by the National Council on Measurement in Education.","['purpose', 'study', 'investigate', 'simulated', 'differential', 'motivation', 'stake', 'operational', 'test', 'anchor', 'item', 'produce', 'invalid', 'linking', 'result', 'Rasch', 'link', 'operational', 'test', 'external', 'anchor', 'design', 'variation', 'pret', 'design', 'study', 'investigate', 'constrained', 'mixture', 'Rasch', 'identify', 'latent', 'class', 'way', 'latent', 'class', 'represent', 'highstake', 'respond', 'represent', 'lowstake', 'respond', 'result', 'indicate', 'external', 'anchor', 'design', 'Rasch', 'link', 'result', 'bias', 'motivation', 'level', 'differ', 'subpopulation', 'anchor', 'item', 'administer', 'mixture', 'Rasch', 'identify', 'class', 'represent', 'lowstake', 'highstake', 'respond', 'pret', 'design', 'link', 'operational', 'test', 'mean', 'Rasch', 'link', 'result', 'find', 'bias', 'condition', 'Bias', 'increase', 'percentage', 'student', 'lowstake', 'respond', 'anchor', 'item', 'increase', 'mixture', 'Rasch', 'identify', 'class', 'represent', 'lowstake', 'highstake', 'respond', 'limited', 'number', 'condition', '©', '2015', 'National', 'Council']","['Effect', 'Differential', 'Motivation', 'IRT', 'link']",purpose study investigate simulated differential motivation stake operational test anchor item produce invalid linking result Rasch link operational test external anchor design variation pret design study investigate constrained mixture Rasch identify latent class way latent class represent highstake respond represent lowstake respond result indicate external anchor design Rasch link result bias motivation level differ subpopulation anchor item administer mixture Rasch identify class represent lowstake highstake respond pret design link operational test mean Rasch link result find bias condition Bias increase percentage student lowstake respond anchor item increase mixture Rasch identify class represent lowstake highstake respond limited number condition © 2015 National Council,Effect Differential Motivation IRT link,0.03179171141480231,0.03171515591218663,0.8729799830459537,0.031404916353448685,0.03210823327360855,0.03894732768808831,0.0020678642656699227,0.003967587318711656,0.02233198139280067,0.015583367660148603
Naumann A.; Hochweber J.; Hartig J.,Modeling Instructional Sensitivity Using a Longitudinal Multilevel Differential Item Functioning Approach,2014,51,"Students' performance in assessments is commonly attributed to more or less effective teaching. This implies that students' responses are significantly affected by instruction. However, the assumption that outcome measures indeed are instructionally sensitive is scarcely investigated empirically. In the present study, we propose a longitudinal multilevel-differential item functioning (DIF) model to combine two existing yet independent approaches to evaluate items' instructional sensitivity. The model permits for a more informative judgment of instructional sensitivity, allowing the distinction of global and differential sensitivity. Exemplarily, the model is applied to two empirical data sets, with classical indices (Pretest-Posttest Difference Index and posttest multilevel-DIF) computed for comparison. Results suggest that the approach works well in the application to empirical data, and may provide important information to test developers. © 2014 by the National Council on Measurement in Education.",Modeling Instructional Sensitivity Using a Longitudinal Multilevel Differential Item Functioning Approach,"Students' performance in assessments is commonly attributed to more or less effective teaching. This implies that students' responses are significantly affected by instruction. However, the assumption that outcome measures indeed are instructionally sensitive is scarcely investigated empirically. In the present study, we propose a longitudinal multilevel-differential item functioning (DIF) model to combine two existing yet independent approaches to evaluate items' instructional sensitivity. The model permits for a more informative judgment of instructional sensitivity, allowing the distinction of global and differential sensitivity. Exemplarily, the model is applied to two empirical data sets, with classical indices (Pretest-Posttest Difference Index and posttest multilevel-DIF) computed for comparison. Results suggest that the approach works well in the application to empirical data, and may provide important information to test developers. © 2014 by the National Council on Measurement in Education.","['student', 'performance', 'assessment', 'commonly', 'attribute', 'effective', 'teaching', 'imply', 'student', 'response', 'significantly', 'affect', 'instruction', 'assumption', 'outcome', 'measure', 'instructionally', 'sensitive', 'scarcely', 'investigate', 'empirically', 'present', 'study', 'propose', 'longitudinal', 'multileveldifferential', 'item', 'function', 'DIF', 'combine', 'exist', 'independent', 'approach', 'evaluate', 'item', 'instructional', 'sensitivity', 'permit', 'informative', 'judgment', 'instructional', 'sensitivity', 'allow', 'distinction', 'global', 'differential', 'sensitivity', 'exemplarily', 'apply', 'empirical', 'datum', 'set', 'classical', 'index', 'PretestPosttest', 'Difference', 'Index', 'postt', 'multilevelDIF', 'compute', 'comparison', 'result', 'suggest', 'approach', 'work', 'application', 'empirical', 'datum', 'provide', 'important', 'information', 'test', 'developer', '©', '2014', 'National', 'Council']","['Instructional', 'Sensitivity', 'Longitudinal', 'Multilevel', 'Differential', 'Item', 'Functioning', 'approach']",student performance assessment commonly attribute effective teaching imply student response significantly affect instruction assumption outcome measure instructionally sensitive scarcely investigate empirically present study propose longitudinal multileveldifferential item function DIF combine exist independent approach evaluate item instructional sensitivity permit informative judgment instructional sensitivity allow distinction global differential sensitivity exemplarily apply empirical datum set classical index PretestPosttest Difference Index postt multilevelDIF compute comparison result suggest approach work application empirical datum provide important information test developer © 2014 National Council,Instructional Sensitivity Longitudinal Multilevel Differential Item Functioning approach,0.02475768554245411,0.024462292410644605,0.024370709280805865,0.9017298745509198,0.024679438215175504,0.042260633957806534,0.0,0.055664814675316386,0.0,0.00035805665775337465
De la Torre J.; Lee Y.-S.,Evaluating the wald test for item-level comparison of saturated and reduced models in cognitive diagnosis,2013,50,"This article used the Wald test to evaluate the item-level fit of a saturated cognitive diagnosis model (CDM) relative to the fits of the reduced models it subsumes. A simulation study was carried out to examine the Type I error and power of the Wald test in the context of the G-DINA model. Results show that when the sample size is small and a larger number of attributes are required, the Type I error rate of the Wald test for the DINA and DINO models can be higher than the nominal significance levels, while the Type I error rate of the A-CDM is closer to the nominal significance levels. However, with larger sample sizes, the Type I error rates for the three models are closer to the nominal significance levels. In addition, the Wald test has excellent statistical power to detect when the true underlying model is none of the reduced models examined even for relatively small sample sizes. The performance of the Wald test was also examined with real data. With an increasing number of CDMs from which to choose, this article provides an important contribution toward advancing the use of CDMs in practical educational settings. © 2013 by the National Council on Measurement in Education.",Evaluating the wald test for item-level comparison of saturated and reduced models in cognitive diagnosis,"This article used the Wald test to evaluate the item-level fit of a saturated cognitive diagnosis model (CDM) relative to the fits of the reduced models it subsumes. A simulation study was carried out to examine the Type I error and power of the Wald test in the context of the G-DINA model. Results show that when the sample size is small and a larger number of attributes are required, the Type I error rate of the Wald test for the DINA and DINO models can be higher than the nominal significance levels, while the Type I error rate of the A-CDM is closer to the nominal significance levels. However, with larger sample sizes, the Type I error rates for the three models are closer to the nominal significance levels. In addition, the Wald test has excellent statistical power to detect when the true underlying model is none of the reduced models examined even for relatively small sample sizes. The performance of the Wald test was also examined with real data. With an increasing number of CDMs from which to choose, this article provides an important contribution toward advancing the use of CDMs in practical educational settings. © 2013 by the National Council on Measurement in Education.","['article', 'Wald', 'test', 'evaluate', 'itemlevel', 'fit', 'saturate', 'cognitive', 'diagnosis', 'CDM', 'relative', 'fit', 'reduce', 'subsume', 'simulation', 'study', 'carry', 'examine', 'type', 'I', 'error', 'power', 'Wald', 'test', 'context', 'GDINA', 'result', 'sample', 'size', 'small', 'large', 'number', 'attribute', 'require', 'Type', 'I', 'error', 'rate', 'Wald', 'test', 'DINA', 'DINO', 'high', 'nominal', 'significance', 'level', 'type', 'I', 'error', 'rate', 'acdm', 'close', 'nominal', 'significance', 'level', 'large', 'sample', 'size', 'Type', 'I', 'error', 'rate', 'close', 'nominal', 'significance', 'level', 'addition', 'Wald', 'test', 'excellent', 'statistical', 'power', 'detect', 'true', 'underlying', 'reduce', 'examine', 'relatively', 'small', 'sample', 'size', 'performance', 'Wald', 'test', 'examine', 'real', 'datum', 'increase', 'number', 'cdm', 'choose', 'article', 'provide', 'important', 'contribution', 'advance', 'CDMs', 'practical', 'educational', 'setting', '©', '2013', 'National', 'Council']","['evaluate', 'wald', 'test', 'itemlevel', 'comparison', 'saturated', 'reduce', 'cognitive', 'diagnosis']",article Wald test evaluate itemlevel fit saturate cognitive diagnosis CDM relative fit reduce subsume simulation study carry examine type I error power Wald test context GDINA result sample size small large number attribute require Type I error rate Wald test DINA DINO high nominal significance level type I error rate acdm close nominal significance level large sample size Type I error rate close nominal significance level addition Wald test excellent statistical power detect true underlying reduce examine relatively small sample size performance Wald test examine real datum increase number cdm choose article provide important contribution advance CDMs practical educational setting © 2013 National Council,evaluate wald test itemlevel comparison saturated reduce cognitive diagnosis,0.026928539534875402,0.026886698096318492,0.026671817676975153,0.8928582673386216,0.02665467735320926,0.05966210363842851,0.0029979856590171368,0.010375993689647797,0.011063262435276864,0.0
Levy R.; Xu Y.; Yel N.; Svetina D.,A Standardized Generalized Dimensionality Discrepancy Measure and a Standardized Model-Based Covariance for Dimensionality Assessment for Multidimensional Models,2015,52,"The standardized generalized dimensionality discrepancy measure and the standardized model-based covariance are introduced as tools to critique dimensionality assumptions in multidimensional item response models. These tools are grounded in a covariance theory perspective and associated connections between dimensionality and local independence. Relative to their precursors, they allow for dimensionality assessment in a more readily interpretable metric of correlations. A simulation study demonstrates the utility of the discrepancy measures' application at multiple levels of dimensionality analysis, and compares them to factor analytic and item response theoretic approaches. An example illustrates their use in practice. © 2015 by the National Council on Measurement in Education.",A Standardized Generalized Dimensionality Discrepancy Measure and a Standardized Model-Based Covariance for Dimensionality Assessment for Multidimensional Models,"The standardized generalized dimensionality discrepancy measure and the standardized model-based covariance are introduced as tools to critique dimensionality assumptions in multidimensional item response models. These tools are grounded in a covariance theory perspective and associated connections between dimensionality and local independence. Relative to their precursors, they allow for dimensionality assessment in a more readily interpretable metric of correlations. A simulation study demonstrates the utility of the discrepancy measures' application at multiple levels of dimensionality analysis, and compares them to factor analytic and item response theoretic approaches. An example illustrates their use in practice. © 2015 by the National Council on Measurement in Education.","['standardized', 'generalized', 'dimensionality', 'discrepancy', 'measure', 'standardized', 'modelbase', 'covariance', 'introduce', 'tool', 'critique', 'dimensionality', 'assumption', 'multidimensional', 'item', 'response', 'tool', 'ground', 'covariance', 'theory', 'perspective', 'associate', 'connection', 'dimensionality', 'local', 'independence', 'relative', 'precursor', 'allow', 'dimensionality', 'assessment', 'readily', 'interpretable', 'metric', 'correlation', 'simulation', 'study', 'demonstrate', 'utility', 'discrepancy', 'measure', 'application', 'multiple', 'level', 'dimensionality', 'analysis', 'compare', 'factor', 'analytic', 'item', 'response', 'theoretic', 'approach', 'example', 'illustrate', 'practice', '©', '2015', 'National', 'Council']","['Standardized', 'Generalized', 'Dimensionality', 'Discrepancy', 'measure', 'Standardized', 'ModelBased', 'Covariance', 'Dimensionality', 'Assessment', 'Multidimensional', 'Models']",standardized generalized dimensionality discrepancy measure standardized modelbase covariance introduce tool critique dimensionality assumption multidimensional item response tool ground covariance theory perspective associate connection dimensionality local independence relative precursor allow dimensionality assessment readily interpretable metric correlation simulation study demonstrate utility discrepancy measure application multiple level dimensionality analysis compare factor analytic item response theoretic approach example illustrate practice © 2015 National Council,Standardized Generalized Dimensionality Discrepancy measure Standardized ModelBased Covariance Dimensionality Assessment Multidimensional Models,0.032282690429798894,0.8704626278128842,0.03216391513766838,0.032214719085862695,0.032876047533785875,0.02994822147904752,0.0,0.01969200447993273,0.0,0.0027818344386734382
Keller L.A.; Hambleton R.K.,The Long-Term Sustainability of IRT Scaling Methods in Mixed-Format Tests,2013,50,"Due to recent research in equating methodologies indicating that some methods may be more susceptible to the accumulation of equating error over multiple administrations, the sustainability of several item response theory methods of equating over time was investigated. In particular, the paper is focused on two equating methodologies: fixed common item parameter scaling (with two variations, FCIP-1 and FCIP-2) and the Stocking and Lord characteristic curve scaling technique in the presence of nonequivalent groups. Results indicated that the improvements made to fixed common item parameter scaling in the FCIP-2 method were sustained over time. FCIP-2 and Stocking and Lord characteristic curve scaling performed similarly in many instances and produced more accurate results than FCIP-1. The relative performance of FCIP-2 and Stocking and Lord characteristic curve scaling depended on the nature of the change in the ability distribution: Stocking and Lord characteristic curve scaling captured the change in the distribution more accurately than FCIP-2 when the change was different across the ability distribution; FCIP-2 captured the changes more accurately when the change was consistent across the ability distribution. © 2013 by the National Council on Measurement in Education.",The Long-Term Sustainability of IRT Scaling Methods in Mixed-Format Tests,"Due to recent research in equating methodologies indicating that some methods may be more susceptible to the accumulation of equating error over multiple administrations, the sustainability of several item response theory methods of equating over time was investigated. In particular, the paper is focused on two equating methodologies: fixed common item parameter scaling (with two variations, FCIP-1 and FCIP-2) and the Stocking and Lord characteristic curve scaling technique in the presence of nonequivalent groups. Results indicated that the improvements made to fixed common item parameter scaling in the FCIP-2 method were sustained over time. FCIP-2 and Stocking and Lord characteristic curve scaling performed similarly in many instances and produced more accurate results than FCIP-1. The relative performance of FCIP-2 and Stocking and Lord characteristic curve scaling depended on the nature of the change in the ability distribution: Stocking and Lord characteristic curve scaling captured the change in the distribution more accurately than FCIP-2 when the change was different across the ability distribution; FCIP-2 captured the changes more accurately when the change was consistent across the ability distribution. © 2013 by the National Council on Measurement in Education.","['recent', 'research', 'equate', 'methodology', 'indicate', 'method', 'susceptible', 'accumulation', 'equate', 'error', 'multiple', 'administration', 'sustainability', 'item', 'response', 'theory', 'method', 'equate', 'time', 'investigate', 'particular', 'paper', 'focus', 'equate', 'methodology', 'fix', 'common', 'item', 'parameter', 'scale', 'variation', 'FCIP1', 'FCIP2', 'Stocking', 'Lord', 'characteristic', 'curve', 'scaling', 'technique', 'presence', 'nonequivalent', 'group', 'result', 'indicate', 'improvement', 'fix', 'common', 'item', 'parameter', 'scale', 'FCIP2', 'method', 'sustain', 'time', 'FCIP2', 'Stocking', 'Lord', 'characteristic', 'curve', 'scaling', 'perform', 'similarly', 'instance', 'produce', 'accurate', 'result', 'FCIP1', 'relative', 'performance', 'FCIP2', 'Stocking', 'Lord', 'characteristic', 'curve', 'scaling', 'depend', 'nature', 'change', 'ability', 'distribution', 'Stocking', 'Lord', 'characteristic', 'curve', 'scaling', 'capture', 'change', 'distribution', 'accurately', 'FCIP2', 'change', 'different', 'ability', 'distribution', 'FCIP2', 'capture', 'change', 'accurately', 'change', 'consistent', 'ability', 'distribution', '©', '2013', 'National', 'Council']","['LongTerm', 'Sustainability', 'IRT', 'Scaling', 'Methods', 'MixedFormat', 'test']",recent research equate methodology indicate method susceptible accumulation equate error multiple administration sustainability item response theory method equate time investigate particular paper focus equate methodology fix common item parameter scale variation FCIP1 FCIP2 Stocking Lord characteristic curve scaling technique presence nonequivalent group result indicate improvement fix common item parameter scale FCIP2 method sustain time FCIP2 Stocking Lord characteristic curve scaling perform similarly instance produce accurate result FCIP1 relative performance FCIP2 Stocking Lord characteristic curve scaling depend nature change ability distribution Stocking Lord characteristic curve scaling capture change distribution accurately FCIP2 change different ability distribution FCIP2 capture change accurately change consistent ability distribution © 2013 National Council,LongTerm Sustainability IRT Scaling Methods MixedFormat test,0.030661839632398123,0.02991658379694882,0.030070925210813573,0.8794045524602657,0.02994609889957384,0.028334699541292214,0.0,0.0011655779772005107,0.05770948312753098,0.0004115775004402076
Dorans N.J.,"On attempting to do what lord said was impossible: Commentary on van der Linden's ""Some Conceptual Issues in Observed-Score Equating""",2013,50,"van der Linden (this issue) uses words differently than Holland and Dorans. This difference in language usage is a source of some confusion in van der Linden's critique of what he calls equipercentile equating. I address these differences in language. van der Linden maintains that there are only two requirements for score equating. I maintain that the requirements he discards have practical utility and are testable. The score equity requirement proposed by Lord suggests that observed score equating was either unnecessary or impossible. Strong equity serves as the fulcrum for van der Linden's thesis. His proposed solution to the equity problem takes inequitable measures and aligns conditional error score distributions, resulting in a family of linking functions, one for each level of θ. In reality, θ is never known. Use of an anchor test as a proxy poses many practical problems, including defensibility. © 2013 by the National Council on Measurement in Education.","On attempting to do what lord said was impossible: Commentary on van der Linden's ""Some Conceptual Issues in Observed-Score Equating""","van der Linden (this issue) uses words differently than Holland and Dorans. This difference in language usage is a source of some confusion in van der Linden's critique of what he calls equipercentile equating. I address these differences in language. van der Linden maintains that there are only two requirements for score equating. I maintain that the requirements he discards have practical utility and are testable. The score equity requirement proposed by Lord suggests that observed score equating was either unnecessary or impossible. Strong equity serves as the fulcrum for van der Linden's thesis. His proposed solution to the equity problem takes inequitable measures and aligns conditional error score distributions, resulting in a family of linking functions, one for each level of θ. In reality, θ is never known. Use of an anchor test as a proxy poses many practical problems, including defensibility. © 2013 by the National Council on Measurement in Education.","['van', 'der', 'Linden', 'issue', 'word', 'differently', 'Holland', 'Dorans', 'difference', 'language', 'usage', 'source', 'confusion', 'van', 'der', 'Lindens', 'critique', 'equipercentile', 'equate', 'I', 'address', 'difference', 'language', 'van', 'der', 'Linden', 'maintain', 'requirement', 'score', 'equating', 'I', 'maintain', 'requirement', 'discard', 'practical', 'utility', 'testable', 'score', 'equity', 'requirement', 'propose', 'Lord', 'suggest', 'observe', 'score', 'equating', 'unnecessary', 'impossible', 'strong', 'equity', 'serve', 'fulcrum', 'van', 'der', 'Lindens', 'thesis', 'propose', 'solution', 'equity', 'problem', 'inequitable', 'measure', 'align', 'conditional', 'error', 'score', 'distribution', 'result', 'family', 'link', 'function', 'level', 'θ', 'reality', 'θ', 'know', 'Use', 'anchor', 'test', 'proxy', 'pose', 'practical', 'problem', 'include', 'defensibility', '©', '2013', 'National', 'Council']","['attempt', 'lord', 'impossible', 'Commentary', 'van', 'der', 'Lindens', 'Conceptual', 'Issues', 'ObservedScore', 'Equating']",van der Linden issue word differently Holland Dorans difference language usage source confusion van der Lindens critique equipercentile equate I address difference language van der Linden maintain requirement score equating I maintain requirement discard practical utility testable score equity requirement propose Lord suggest observe score equating unnecessary impossible strong equity serve fulcrum van der Lindens thesis propose solution equity problem inequitable measure align conditional error score distribution result family link function level θ reality θ know Use anchor test proxy pose practical problem include defensibility © 2013 National Council,attempt lord impossible Commentary van der Lindens Conceptual Issues ObservedScore Equating,0.02653579725128942,0.026580691032940827,0.026398897596387274,0.8940412218972664,0.026443392222116068,0.0,0.003467980291423758,0.02250131409285228,0.07611498470085262,1.9150334364107187e-05
Zhang J.; Li J.,Monitoring Items in Real Time to Enhance CAT Security,2016,53,"An IRT-based sequential procedure is developed to monitor items for enhancing test security. The procedure uses a series of statistical hypothesis tests to examine whether the statistical characteristics of each item under inspection have changed significantly during CAT administration. This procedure is compared with a previously developed CTT-based procedure through simulation studies. The results show that when the total number of examinees is fixed both procedures can control the rate of type I errors at any reasonable significance level by choosing an appropriate cutoff point and meanwhile maintain a low rate of type II errors. Further, the IRT-based method has a much lower type II error rate or more power than the CTT-based method when the number of compromised items is small (e.g., 5), which can be achieved if the IRT-based procedure can be applied in an active mode in the sense that flagged items can be replaced with new items. Copyright © 2016 by the National Council on Measurement in Education",Monitoring Items in Real Time to Enhance CAT Security,"An IRT-based sequential procedure is developed to monitor items for enhancing test security. The procedure uses a series of statistical hypothesis tests to examine whether the statistical characteristics of each item under inspection have changed significantly during CAT administration. This procedure is compared with a previously developed CTT-based procedure through simulation studies. The results show that when the total number of examinees is fixed both procedures can control the rate of type I errors at any reasonable significance level by choosing an appropriate cutoff point and meanwhile maintain a low rate of type II errors. Further, the IRT-based method has a much lower type II error rate or more power than the CTT-based method when the number of compromised items is small (e.g., 5), which can be achieved if the IRT-based procedure can be applied in an active mode in the sense that flagged items can be replaced with new items. Copyright © 2016 by the National Council on Measurement in Education","['irtbased', 'sequential', 'procedure', 'develop', 'monitor', 'item', 'enhance', 'test', 'security', 'procedure', 'series', 'statistical', 'hypothesis', 'test', 'examine', 'statistical', 'characteristic', 'item', 'inspection', 'change', 'significantly', 'CAT', 'administration', 'procedure', 'compare', 'previously', 'develop', 'cttbased', 'procedure', 'simulation', 'study', 'result', 'total', 'number', 'examinee', 'fix', 'procedure', 'control', 'rate', 'type', 'I', 'error', 'reasonable', 'significance', 'level', 'choose', 'appropriate', 'cutoff', 'point', 'maintain', 'low', 'rate', 'type', 'ii', 'error', 'far', 'irtbased', 'method', 'low', 'type', 'ii', 'error', 'rate', 'power', 'cttbased', 'method', 'number', 'compromise', 'item', 'small', 'eg', '5', 'achieve', 'irtbased', 'procedure', 'apply', 'active', 'mode', 'sense', 'flagged', 'item', 'replace', 'new', 'item', 'Copyright', '©', '2016', 'National', 'Council']","['monitor', 'Items', 'real', 'Time', 'enhance', 'CAT', 'Security']",irtbased sequential procedure develop monitor item enhance test security procedure series statistical hypothesis test examine statistical characteristic item inspection change significantly CAT administration procedure compare previously develop cttbased procedure simulation study result total number examinee fix procedure control rate type I error reasonable significance level choose appropriate cutoff point maintain low rate type ii error far irtbased method low type ii error rate power cttbased method number compromise item small eg 5 achieve irtbased procedure apply active mode sense flagged item replace new item Copyright © 2016 National Council,monitor Items real Time enhance CAT Security,0.025331277616489165,0.025278622955517246,0.8986998927113539,0.025303010458525465,0.025387196258114322,0.07594352595202404,0.009253563664100755,0.0,0.0,0.0
Jin K.-Y.; Wang W.-C.,Item response theory models for performance decline during testing,2014,51,"Sometimes, test-takers may not be able to attempt all items to the best of their ability (with full effort) due to personal factors (e.g., low motivation) or testing conditions (e.g., time limit), resulting in poor performances on certain items, especially those located toward the end of a test. Standard item response theory (IRT) models fail to consider such testing behaviors. In this study, a new class of mixture IRT models was developed to account for such testing behavior in dichotomous and polytomous items, by assuming test-takers were composed of multiple latent classes and by adding a decrement parameter to each latent class to describe performance decline. Parameter recovery, effect of model misspecification, and robustness of the linearity assumption in performance decline were evaluated using simulations. It was found that the parameters in the new models were recovered fairly well by using the freeware WinBUGS; the failure to account for such behavior by fitting standard IRT models resulted in overestimation of difficulty parameters on items located toward the end of the test and overestimation of test reliability; and the linearity assumption in performance decline was rather robust. An empirical example is provided to illustrate the applications and the implications of the new class of models. © 2014 by the National Council on Measurement in Education.",Item response theory models for performance decline during testing,"Sometimes, test-takers may not be able to attempt all items to the best of their ability (with full effort) due to personal factors (e.g., low motivation) or testing conditions (e.g., time limit), resulting in poor performances on certain items, especially those located toward the end of a test. Standard item response theory (IRT) models fail to consider such testing behaviors. In this study, a new class of mixture IRT models was developed to account for such testing behavior in dichotomous and polytomous items, by assuming test-takers were composed of multiple latent classes and by adding a decrement parameter to each latent class to describe performance decline. Parameter recovery, effect of model misspecification, and robustness of the linearity assumption in performance decline were evaluated using simulations. It was found that the parameters in the new models were recovered fairly well by using the freeware WinBUGS; the failure to account for such behavior by fitting standard IRT models resulted in overestimation of difficulty parameters on items located toward the end of the test and overestimation of test reliability; and the linearity assumption in performance decline was rather robust. An empirical example is provided to illustrate the applications and the implications of the new class of models. © 2014 by the National Council on Measurement in Education.","['testtaker', 'able', 'attempt', 'item', 'good', 'ability', 'effort', 'personal', 'factor', 'eg', 'low', 'motivation', 'testing', 'condition', 'eg', 'time', 'limit', 'result', 'poor', 'performance', 'certain', 'item', 'especially', 'locate', 'end', 'test', 'standard', 'item', 'response', 'theory', 'IRT', 'fail', 'consider', 'testing', 'behavior', 'study', 'new', 'class', 'mixture', 'IRT', 'develop', 'account', 'testing', 'behavior', 'dichotomous', 'polytomous', 'item', 'assume', 'testtaker', 'compose', 'multiple', 'latent', 'class', 'add', 'decrement', 'parameter', 'latent', 'class', 'describe', 'performance', 'decline', 'Parameter', 'recovery', 'effect', 'misspecification', 'robustness', 'linearity', 'assumption', 'performance', 'decline', 'evaluate', 'simulation', 'find', 'parameter', 'new', 'recover', 'fairly', 'freeware', 'WinBUGS', 'failure', 'account', 'behavior', 'fitting', 'standard', 'IRT', 'result', 'overestimation', 'difficulty', 'parameter', 'item', 'locate', 'end', 'test', 'overestimation', 'test', 'reliability', 'linearity', 'assumption', 'performance', 'decline', 'robust', 'empirical', 'example', 'provide', 'illustrate', 'application', 'implication', 'new', 'class', '©', '2014', 'National', 'Council']","['item', 'response', 'theory', 'performance', 'decline', 'testing']",testtaker able attempt item good ability effort personal factor eg low motivation testing condition eg time limit result poor performance certain item especially locate end test standard item response theory IRT fail consider testing behavior study new class mixture IRT develop account testing behavior dichotomous polytomous item assume testtaker compose multiple latent class add decrement parameter latent class describe performance decline Parameter recovery effect misspecification robustness linearity assumption performance decline evaluate simulation find parameter new recover fairly freeware WinBUGS failure account behavior fitting standard IRT result overestimation difficulty parameter item locate end test overestimation test reliability linearity assumption performance decline robust empirical example provide illustrate application implication new class © 2014 National Council,item response theory performance decline testing,0.02465964913195192,0.02416818733270054,0.9019249990343164,0.024353754092469183,0.02489341040856194,0.07927334468382626,0.004285615332566701,0.0,0.0,0.013923116706089207
Van der Linden W.J.,More issues in observed-score equating,2013,50,"This article is a response to the commentaries on the position paper on observed-score equating by van der Linden (this issue). The response focuses on the more general issues in these commentaries, such as the nature of the observed scores that are equated, the importance of test-theory assumptions in equating, the necessity to use multiple equating transformations, and the choice of conditioning variables in equating. © 2013 by the National Council on Measurement in Education.",,"This article is a response to the commentaries on the position paper on observed-score equating by van der Linden (this issue). The response focuses on the more general issues in these commentaries, such as the nature of the observed scores that are equated, the importance of test-theory assumptions in equating, the necessity to use multiple equating transformations, and the choice of conditioning variables in equating. © 2013 by the National Council on Measurement in Education.","['article', 'response', 'commentary', 'position', 'paper', 'observedscore', 'equate', 'van', 'der', 'Linden', 'issue', 'response', 'focus', 'general', 'issue', 'commentary', 'nature', 'observe', 'score', 'equate', 'importance', 'testtheory', 'assumption', 'equate', 'necessity', 'multiple', 'equate', 'transformation', 'choice', 'conditioning', 'variable', 'equate', '©', '2013', 'National', 'Council']",,article response commentary position paper observedscore equate van der Linden issue response focus general issue commentary nature observe score equate importance testtheory assumption equate necessity multiple equate transformation choice conditioning variable equate © 2013 National Council,,0.03653103809295326,0.03599576163362255,0.036235777168912554,0.8547430633114643,0.03649435979304721,0.0,0.0,0.002015183313952701,0.15743805094137883,0.0
Meng X.-B.; Tao J.; Chang H.-H.,A conditional joint modeling approach for locally dependent item responses and response times,2015,52,"The assumption of conditional independence between the responses and the response times (RTs) for a given person is common in RT modeling. However, when the speed of a test taker is not constant, this assumption will be violated. In this article we propose a conditional joint model for item responses and RTs, which incorporates a covariance structure to explain the local dependency between speed and accuracy. To obtain information about the population of test takers, the new model was embedded in the hierarchical framework proposed by van der Linden (). A fully Bayesian approach using a straightforward Markov chain Monte Carlo (MCMC) sampler was developed to estimate all parameters in the model. The deviance information criterion (DIC) and the Bayes factor (BF) were employed to compare the goodness of fit between the models with two different parameter structures. The Bayesian residual analysis method was also employed to evaluate the fit of the RT model. Based on the simulations, we conclude that (1) the new model noticeably improves the parameter recovery for both the item parameters and the examinees' latent traits when the assumptions of conditional independence between the item responses and the RTs are relaxed and (2) the proposed MCMC sampler adequately estimates the model parameters. The applicability of our approach is illustrated with an empirical example, and the model fit indices indicated a preference for the new model. © 2015 by the National Council on Measurement in Education.",A conditional joint modeling approach for locally dependent item responses and response times,"The assumption of conditional independence between the responses and the response times (RTs) for a given person is common in RT modeling. However, when the speed of a test taker is not constant, this assumption will be violated. In this article we propose a conditional joint model for item responses and RTs, which incorporates a covariance structure to explain the local dependency between speed and accuracy. To obtain information about the population of test takers, the new model was embedded in the hierarchical framework proposed by van der Linden (). A fully Bayesian approach using a straightforward Markov chain Monte Carlo (MCMC) sampler was developed to estimate all parameters in the model. The deviance information criterion (DIC) and the Bayes factor (BF) were employed to compare the goodness of fit between the models with two different parameter structures. The Bayesian residual analysis method was also employed to evaluate the fit of the RT model. Based on the simulations, we conclude that (1) the new model noticeably improves the parameter recovery for both the item parameters and the examinees' latent traits when the assumptions of conditional independence between the item responses and the RTs are relaxed and (2) the proposed MCMC sampler adequately estimates the model parameters. The applicability of our approach is illustrated with an empirical example, and the model fit indices indicated a preference for the new model. © 2015 by the National Council on Measurement in Education.","['assumption', 'conditional', 'independence', 'response', 'response', 'time', 'rt', 'person', 'common', 'RT', 'modeling', 'speed', 'test', 'taker', 'constant', 'assumption', 'violate', 'article', 'propose', 'conditional', 'joint', 'item', 'response', 'rt', 'incorporate', 'covariance', 'structure', 'explain', 'local', 'dependency', 'speed', 'accuracy', 'obtain', 'information', 'population', 'test', 'taker', 'new', 'embed', 'hierarchical', 'framework', 'propose', 'van', 'der', 'Linden', 'fully', 'bayesian', 'approach', 'straightforward', 'Markov', 'chain', 'Monte', 'Carlo', 'MCMC', 'sampler', 'develop', 'estimate', 'parameter', 'deviance', 'information', 'criterion', 'DIC', 'Bayes', 'factor', 'BF', 'employ', 'compare', 'goodness', 'fit', 'different', 'parameter', 'structure', 'bayesian', 'residual', 'analysis', 'method', 'employ', 'evaluate', 'fit', 'RT', 'base', 'simulation', 'conclude', '1', 'new', 'noticeably', 'improve', 'parameter', 'recovery', 'item', 'parameter', 'examinees', 'latent', 'trait', 'assumption', 'conditional', 'independence', 'item', 'response', 'rt', 'relaxed', '2', 'propose', 'MCMC', 'sampler', 'adequately', 'estimate', 'parameter', 'applicability', 'approach', 'illustrate', 'empirical', 'example', 'fit', 'index', 'indicate', 'preference', 'new', '©', '2015', 'National', 'Council']","['conditional', 'joint', 'modeling', 'approach', 'locally', 'dependent', 'item', 'response', 'response', 'time']",assumption conditional independence response response time rt person common RT modeling speed test taker constant assumption violate article propose conditional joint item response rt incorporate covariance structure explain local dependency speed accuracy obtain information population test taker new embed hierarchical framework propose van der Linden fully bayesian approach straightforward Markov chain Monte Carlo MCMC sampler develop estimate parameter deviance information criterion DIC Bayes factor BF employ compare goodness fit different parameter structure bayesian residual analysis method employ evaluate fit RT base simulation conclude 1 new noticeably improve parameter recovery item parameter examinees latent trait assumption conditional independence item response rt relaxed 2 propose MCMC sampler adequately estimate parameter applicability approach illustrate empirical example fit index indicate preference new © 2015 National Council,conditional joint modeling approach locally dependent item response response time,0.024305715216041267,0.9039557251642053,0.023673160533844426,0.023948158538496356,0.02411724054741242,0.07386097701768242,0.0,0.0,0.0009017206358117116,0.007670382217663023
Sinharay S.,Analysis of added value of subscores with respect to classification,2014,51,"Brennan noted that users of test scores often want (indeed, demand) that subscores be reported, along with total test scores, for diagnostic purposes. Haberman suggested a method based on classical test theory (CTT) to determine if subscores have added value over the total score. One way to interpret the method is that a subscore has added value only if it has a better agreement than the total score with the corresponding subscore on a parallel form. The focus of this article is on classification of the examinees into ""pass"" and ""fail"" (or master and nonmaster) categories based on subscores. A new CTT-based method is suggested to assess whether classification based on a subscore is in better agreement, than classification based on the total score, with classification based on the corresponding subscore on a parallel form. The method can be considered as an assessment of the added value of subscores with respect to classification. The suggested method is applied to data from several operational tests. The added value of subscores with respect to classification is found to be very similar, except at extreme cutscores, to their added value from a value-added analysis of Haberman. © 2014 by the National Council on Measurement in Education.",Analysis of added value of subscores with respect to classification,"Brennan noted that users of test scores often want (indeed, demand) that subscores be reported, along with total test scores, for diagnostic purposes. Haberman suggested a method based on classical test theory (CTT) to determine if subscores have added value over the total score. One way to interpret the method is that a subscore has added value only if it has a better agreement than the total score with the corresponding subscore on a parallel form. The focus of this article is on classification of the examinees into ""pass"" and ""fail"" (or master and nonmaster) categories based on subscores. A new CTT-based method is suggested to assess whether classification based on a subscore is in better agreement, than classification based on the total score, with classification based on the corresponding subscore on a parallel form. The method can be considered as an assessment of the added value of subscores with respect to classification. The suggested method is applied to data from several operational tests. The added value of subscores with respect to classification is found to be very similar, except at extreme cutscores, to their added value from a value-added analysis of Haberman. © 2014 by the National Council on Measurement in Education.","['Brennan', 'note', 'user', 'test', 'score', 'want', 'demand', 'subscore', 'report', 'total', 'test', 'score', 'diagnostic', 'purpose', 'Haberman', 'suggest', 'method', 'base', 'classical', 'test', 'theory', 'CTT', 'determine', 'subscore', 'add', 'value', 'total', 'score', 'way', 'interpret', 'method', 'subscore', 'add', 'value', 'agreement', 'total', 'score', 'corresponding', 'subscore', 'parallel', 'form', 'focus', 'article', 'classification', 'examinee', 'pass', 'fail', 'master', 'nonmaster', 'category', 'base', 'subscore', 'new', 'cttbased', 'method', 'suggest', 'assess', 'classification', 'base', 'subscore', 'agreement', 'classification', 'base', 'total', 'score', 'classification', 'base', 'corresponding', 'subscore', 'parallel', 'form', 'method', 'consider', 'assessment', 'add', 'value', 'subscore', 'respect', 'classification', 'suggest', 'method', 'apply', 'datum', 'operational', 'test', 'add', 'value', 'subscore', 'respect', 'classification', 'find', 'similar', 'extreme', 'cutscore', 'add', 'value', 'valueadde', 'analysis', 'Haberman', '©', '2014', 'National', 'Council']","['analysis', 'add', 'value', 'subscore', 'respect', 'classification']",Brennan note user test score want demand subscore report total test score diagnostic purpose Haberman suggest method base classical test theory CTT determine subscore add value total score way interpret method subscore add value agreement total score corresponding subscore parallel form focus article classification examinee pass fail master nonmaster category base subscore new cttbased method suggest assess classification base subscore agreement classification base total score classification base corresponding subscore parallel form method consider assessment add value subscore respect classification suggest method apply datum operational test add value subscore respect classification find similar extreme cutscore add value valueadde analysis Haberman © 2014 National Council,analysis add value subscore respect classification,0.03200083039243434,0.032297294914202086,0.03224310865327621,0.031968902212805975,0.8714898638272814,0.0,0.35741112170310635,0.0,0.0,0.0
Kim S.; Moses T.; Yoo H.,A comparison of IRT proficiency estimation methods under adaptive multistage testing,2015,52,"This inquiry is an investigation of item response theory (IRT) proficiency estimators' accuracy under multistage testing (MST). We chose a two-stage MST design that includes four modules (one at Stage 1, three at Stage 2) and three difficulty paths (low, middle, high). We assembled various two-stage MST panels (i.e., forms) by manipulating two assembly conditions in each module, such as difficulty level and module length. For each panel, we investigated the accuracy of examinees' proficiency levels derived from seven IRT proficiency estimators. The choice of Bayesian (prior) versus non-Bayesian (no prior) estimators was of more practical significance than the choice of number-correct versus item-pattern scoring estimators. The Bayesian estimators were slightly more efficient than the non-Bayesian estimators, resulting in smaller overall error. Possible score changes caused by the use of different proficiency estimators would be nonnegligible, particularly for low- and high-performing examinees. © 2015 by the National Council on Measurement in Education.",A comparison of IRT proficiency estimation methods under adaptive multistage testing,"This inquiry is an investigation of item response theory (IRT) proficiency estimators' accuracy under multistage testing (MST). We chose a two-stage MST design that includes four modules (one at Stage 1, three at Stage 2) and three difficulty paths (low, middle, high). We assembled various two-stage MST panels (i.e., forms) by manipulating two assembly conditions in each module, such as difficulty level and module length. For each panel, we investigated the accuracy of examinees' proficiency levels derived from seven IRT proficiency estimators. The choice of Bayesian (prior) versus non-Bayesian (no prior) estimators was of more practical significance than the choice of number-correct versus item-pattern scoring estimators. The Bayesian estimators were slightly more efficient than the non-Bayesian estimators, resulting in smaller overall error. Possible score changes caused by the use of different proficiency estimators would be nonnegligible, particularly for low- and high-performing examinees. © 2015 by the National Council on Measurement in Education.","['inquiry', 'investigation', 'item', 'response', 'theory', 'IRT', 'proficiency', 'estimator', 'accuracy', 'multistage', 'testing', 'MST', 'choose', 'twostage', 'mst', 'design', 'include', 'module', 'stage', '1', 'stage', '2', 'difficulty', 'path', 'low', 'middle', 'high', 'assemble', 'twostage', 'mst', 'panel', 'ie', 'form', 'manipulate', 'assembly', 'condition', 'module', 'difficulty', 'level', 'module', 'length', 'panel', 'investigate', 'accuracy', 'examinees', 'proficiency', 'level', 'derive', 'seven', 'IRT', 'proficiency', 'estimator', 'choice', 'Bayesian', 'prior', 'versus', 'nonBayesian', 'prior', 'estimator', 'practical', 'significance', 'choice', 'numbercorrect', 'versus', 'itempattern', 'scoring', 'estimator', 'bayesian', 'estimator', 'slightly', 'efficient', 'nonbayesian', 'estimator', 'result', 'small', 'overall', 'error', 'possible', 'score', 'change', 'cause', 'different', 'proficiency', 'estimator', 'nonnegligible', 'particularly', 'low', 'highperforming', 'examine', '©', '2015', 'National', 'Council']","['comparison', 'IRT', 'proficiency', 'estimation', 'method', 'adaptive', 'multistage', 'testing']",inquiry investigation item response theory IRT proficiency estimator accuracy multistage testing MST choose twostage mst design include module stage 1 stage 2 difficulty path low middle high assemble twostage mst panel ie form manipulate assembly condition module difficulty level module length panel investigate accuracy examinees proficiency level derive seven IRT proficiency estimator choice Bayesian prior versus nonBayesian prior estimator practical significance choice numbercorrect versus itempattern scoring estimator bayesian estimator slightly efficient nonbayesian estimator result small overall error possible score change cause different proficiency estimator nonnegligible particularly low highperforming examine © 2015 National Council,comparison IRT proficiency estimation method adaptive multistage testing,0.8881825723805842,0.027922098646078936,0.028197972933597576,0.027892314628801075,0.027805041410938237,0.031056096920695022,0.0,0.002785551961232975,0.02214295848178188,0.00729011764992451
Suh Y.; Cho S.-J.; Wollack J.A.,A Comparison of Item Calibration Procedures in the Presence of Test Speededness,2012,49,"In the presence of test speededness, the parameter estimates of item response theory models can be poorly estimated due to conditional dependencies among items, particularly for end-of-test items (i.e., speeded items). This article conducted a systematic comparison of five-item calibration procedures-a two-parameter logistic (2PL) model, a one-dimensional mixture model, a two-step strategy (a combination of the one-dimensional mixture and the 2PL), a two-dimensional mixture model, and a hybrid model--by examining how sample size, percentage of speeded examinees, percentage of missing responses, and way of scoring missing responses (incorrect vs. omitted) affect the item parameter estimation in speeded tests. For nonspeeded items, all five procedures showed similar results in recovering item parameters. For speeded items, the one-dimensional mixture model, the two-step strategy, and the two-dimensional mixture model provided largely similar results and performed better than the 2PL model and the hybrid model in calibrating slope parameters. However, those three procedures performed similarly to the hybrid model in estimating intercept parameters. As expected, the 2PL model did not appear to be as accurate as the other models in recovering item parameters, especially when there were large numbers of examinees showing speededness and a high percentage of missing responses with incorrect scoring. Real data analysis further described the similarities and differences between the five procedures. © 2012 by the National Council on Measurement in Education.",A Comparison of Item Calibration Procedures in the Presence of Test Speededness,"In the presence of test speededness, the parameter estimates of item response theory models can be poorly estimated due to conditional dependencies among items, particularly for end-of-test items (i.e., speeded items). This article conducted a systematic comparison of five-item calibration procedures-a two-parameter logistic (2PL) model, a one-dimensional mixture model, a two-step strategy (a combination of the one-dimensional mixture and the 2PL), a two-dimensional mixture model, and a hybrid model--by examining how sample size, percentage of speeded examinees, percentage of missing responses, and way of scoring missing responses (incorrect vs. omitted) affect the item parameter estimation in speeded tests. For nonspeeded items, all five procedures showed similar results in recovering item parameters. For speeded items, the one-dimensional mixture model, the two-step strategy, and the two-dimensional mixture model provided largely similar results and performed better than the 2PL model and the hybrid model in calibrating slope parameters. However, those three procedures performed similarly to the hybrid model in estimating intercept parameters. As expected, the 2PL model did not appear to be as accurate as the other models in recovering item parameters, especially when there were large numbers of examinees showing speededness and a high percentage of missing responses with incorrect scoring. Real data analysis further described the similarities and differences between the five procedures. © 2012 by the National Council on Measurement in Education.","['presence', 'test', 'speededness', 'parameter', 'estimate', 'item', 'response', 'theory', 'poorly', 'estimate', 'conditional', 'dependency', 'item', 'particularly', 'endoftest', 'item', 'ie', 'speed', 'item', 'article', 'conduct', 'systematic', 'comparison', 'fiveitem', 'calibration', 'proceduresa', 'twoparameter', 'logistic', '2PL', 'onedimensional', 'mixture', 'twostep', 'strategy', 'combination', 'onedimensional', 'mixture', '2PL', 'twodimensional', 'mixture', 'hybrid', 'modelby', 'examine', 'sample', 'size', 'percentage', 'speed', 'examine', 'percentage', 'miss', 'response', 'way', 'scoring', 'miss', 'response', 'incorrect', 'vs', 'omit', 'affect', 'item', 'parameter', 'estimation', 'speed', 'test', 'nonspeede', 'item', 'procedure', 'similar', 'result', 'recover', 'item', 'parameter', 'speed', 'item', 'onedimensional', 'mixture', 'twostep', 'strategy', 'twodimensional', 'mixture', 'provide', 'largely', 'similar', 'result', 'perform', '2PL', 'hybrid', 'calibrate', 'slope', 'parameter', 'procedure', 'perform', 'similarly', 'hybrid', 'estimate', 'intercept', 'parameter', 'expect', '2PL', 'appear', 'accurate', 'recover', 'item', 'parameter', 'especially', 'large', 'number', 'examinee', 'speededness', 'high', 'percentage', 'miss', 'response', 'incorrect', 'scoring', 'real', 'datum', 'analysis', 'far', 'describe', 'similarity', 'difference', 'procedure', '©', '2012', 'National', 'Council']","['Comparison', 'Item', 'Calibration', 'procedure', 'Presence', 'Test', 'Speededness']",presence test speededness parameter estimate item response theory poorly estimate conditional dependency item particularly endoftest item ie speed item article conduct systematic comparison fiveitem calibration proceduresa twoparameter logistic 2PL onedimensional mixture twostep strategy combination onedimensional mixture 2PL twodimensional mixture hybrid modelby examine sample size percentage speed examine percentage miss response way scoring miss response incorrect vs omit affect item parameter estimation speed test nonspeede item procedure similar result recover item parameter speed item onedimensional mixture twostep strategy twodimensional mixture provide largely similar result perform 2PL hybrid calibrate slope parameter procedure perform similarly hybrid estimate intercept parameter expect 2PL appear accurate recover item parameter especially large number examinee speededness high percentage miss response incorrect scoring real datum analysis far describe similarity difference procedure © 2012 National Council,Comparison Item Calibration procedure Presence Test Speededness,0.026075793010883683,0.8960066628020183,0.026267241898966762,0.02583151951447822,0.025818782773652956,0.07541619545834134,0.0,0.0,0.0,0.0
Lathrop Q.N.; Cheng Y.,A nonparametric approach to estimate classification accuracy and consistency,2014,51,"When cut scores for classifications occur on the total score scale, popular methods for estimating classification accuracy (CA) and classification consistency (CC) require assumptions about a parametric form of the test scores or about a parametric response model, such as item response theory (IRT). This article develops an approach to estimate CA and CC nonparametrically by replacing the role of the parametric IRT model in Lee's classification indices with a modified version of Ramsay's kernel-smoothed item response functions. The performance of the nonparametric CA and CC indices are tested in simulation studies in various conditions with different generating IRT models, test lengths, and ability distributions. The nonparametric approach to CA often outperforms Lee's method and Livingston and Lewis's method, showing robustness to nonnormality in the simulated ability. The nonparametric CC index performs similarly to Lee's method and outperforms Livingston and Lewis's method when the ability distributions are nonnormal. © 2014 by the National Council on Measurement in Education.",A nonparametric approach to estimate classification accuracy and consistency,"When cut scores for classifications occur on the total score scale, popular methods for estimating classification accuracy (CA) and classification consistency (CC) require assumptions about a parametric form of the test scores or about a parametric response model, such as item response theory (IRT). This article develops an approach to estimate CA and CC nonparametrically by replacing the role of the parametric IRT model in Lee's classification indices with a modified version of Ramsay's kernel-smoothed item response functions. The performance of the nonparametric CA and CC indices are tested in simulation studies in various conditions with different generating IRT models, test lengths, and ability distributions. The nonparametric approach to CA often outperforms Lee's method and Livingston and Lewis's method, showing robustness to nonnormality in the simulated ability. The nonparametric CC index performs similarly to Lee's method and outperforms Livingston and Lewis's method when the ability distributions are nonnormal. © 2014 by the National Council on Measurement in Education.","['cut', 'score', 'classification', 'occur', 'total', 'score', 'scale', 'popular', 'method', 'estimate', 'classification', 'accuracy', 'CA', 'classification', 'consistency', 'cc', 'require', 'assumption', 'parametric', 'form', 'test', 'score', 'parametric', 'response', 'item', 'response', 'theory', 'IRT', 'article', 'develop', 'approach', 'estimate', 'CA', 'CC', 'nonparametrically', 'replace', 'role', 'parametric', 'IRT', 'Lees', 'classification', 'index', 'modify', 'version', 'Ramsays', 'kernelsmoothe', 'item', 'response', 'function', 'performance', 'nonparametric', 'CA', 'CC', 'index', 'test', 'simulation', 'study', 'condition', 'different', 'generate', 'IRT', 'test', 'length', 'ability', 'distribution', 'nonparametric', 'approach', 'CA', 'outperform', 'Lees', 'method', 'Livingston', 'Lewiss', 'method', 'robustness', 'nonnormality', 'simulate', 'ability', 'nonparametric', 'CC', 'index', 'perform', 'similarly', 'Lees', 'method', 'outperform', 'Livingston', 'Lewiss', 'method', 'ability', 'distribution', 'nonnormal', '©', '2014', 'National', 'Council']","['nonparametric', 'approach', 'estimate', 'classification', 'accuracy', 'consistency']",cut score classification occur total score scale popular method estimate classification accuracy CA classification consistency cc require assumption parametric form test score parametric response item response theory IRT article develop approach estimate CA CC nonparametrically replace role parametric IRT Lees classification index modify version Ramsays kernelsmoothe item response function performance nonparametric CA CC index test simulation study condition different generate IRT test length ability distribution nonparametric approach CA outperform Lees method Livingston Lewiss method robustness nonnormality simulate ability nonparametric CC index perform similarly Lees method outperform Livingston Lewiss method ability distribution nonnormal © 2014 National Council,nonparametric approach estimate classification accuracy consistency,0.02844580288707591,0.028656894404516894,0.028367353904655276,0.028496073212745644,0.8860338755910062,0.05301850612694088,0.03964047726014308,0.007054876579103939,0.01566711287905749,0.0
Zu J.; Puhan G.,Preequating with empirical item characteristic curves: An observed-score preequating method,2014,51,"Preequating is in demand because it reduces score reporting time. In this article, we evaluated an observed-score preequating method: the empirical item characteristic curve (EICC) method, which makes preequating without item response theory (IRT) possible. EICC preequating results were compared with a criterion equating and with IRT true-score preequating conversions. Results suggested that the EICC preequating method worked well under the conditions considered in this study. The difference between the EICC preequating conversion and the criterion equating was smaller than .5 raw-score points (a practical criterion often used to evaluate equating quality) between the 5th and 95th percentiles of the new form total score distribution. EICC preequating also performed similarly or slightly better than IRT true-score preequating. © 2014 by the National Council on Measurement in Education.",Preequating with empirical item characteristic curves: An observed-score preequating method,"Preequating is in demand because it reduces score reporting time. In this article, we evaluated an observed-score preequating method: the empirical item characteristic curve (EICC) method, which makes preequating without item response theory (IRT) possible. EICC preequating results were compared with a criterion equating and with IRT true-score preequating conversions. Results suggested that the EICC preequating method worked well under the conditions considered in this study. The difference between the EICC preequating conversion and the criterion equating was smaller than .5 raw-score points (a practical criterion often used to evaluate equating quality) between the 5th and 95th percentiles of the new form total score distribution. EICC preequating also performed similarly or slightly better than IRT true-score preequating. © 2014 by the National Council on Measurement in Education.","['preequate', 'demand', 'reduce', 'score', 'reporting', 'time', 'article', 'evaluate', 'observedscore', 'preequating', 'method', 'empirical', 'item', 'characteristic', 'curve', 'EICC', 'method', 'preequate', 'item', 'response', 'theory', 'IRT', 'possible', 'eicc', 'preequate', 'result', 'compare', 'criterion', 'equating', 'IRT', 'truescore', 'preequate', 'conversion', 'Results', 'suggest', 'eicc', 'preequate', 'method', 'work', 'condition', 'consider', 'study', 'difference', 'EICC', 'preequate', 'conversion', 'criterion', 'equating', 'small', '5', 'rawscore', 'point', 'practical', 'criterion', 'evaluate', 'equate', 'quality', '5th', '95th', 'percentile', 'new', 'form', 'total', 'score', 'distribution', 'EICC', 'preequate', 'perform', 'similarly', 'slightly', 'IRT', 'truescore', 'preequate', '©', '2014', 'National', 'Council']","['preequate', 'empirical', 'item', 'characteristic', 'curve', 'observedscore', 'preequate', 'method']",preequate demand reduce score reporting time article evaluate observedscore preequating method empirical item characteristic curve EICC method preequate item response theory IRT possible eicc preequate result compare criterion equating IRT truescore preequate conversion Results suggest eicc preequate method work condition consider study difference EICC preequate conversion criterion equating small 5 rawscore point practical criterion evaluate equate quality 5th 95th percentile new form total score distribution EICC preequate perform similarly slightly IRT truescore preequate © 2014 National Council,preequate empirical item characteristic curve observedscore preequate method,0.039487285264504735,0.038905352565947904,0.03901210754633854,0.8438537436532733,0.03874151096993568,0.014151632044411352,0.011433137969421971,0.0,0.05633051580310928,0.0
Wiberg M.; van der Linden W.J.,Local linear observed-score equating,2011,48,"Two methods of local linear observed-score equating for use with anchor-test and single-group designs are introduced. In an empirical study, the two methods were compared with the current traditional linear methods for observed-score equating. As a criterion, the bias in the equated scores relative to true equating based on definition of equity was used. The local method for the anchor-test design yielded minimum bias, even for considerable variation of the relative difficulties of the two test forms and the length of the anchor test. Among the traditional methods, the method of chain equating performed best. The local method for single-group designs yielded equated scores with bias comparable to the traditional methods. This method, however, appears to be of theoretical interest because it forces us to rethink the relationship between score equating and regression. © 2011 by the National Council on Measurement in Education.",,"Two methods of local linear observed-score equating for use with anchor-test and single-group designs are introduced. In an empirical study, the two methods were compared with the current traditional linear methods for observed-score equating. As a criterion, the bias in the equated scores relative to true equating based on definition of equity was used. The local method for the anchor-test design yielded minimum bias, even for considerable variation of the relative difficulties of the two test forms and the length of the anchor test. Among the traditional methods, the method of chain equating performed best. The local method for single-group designs yielded equated scores with bias comparable to the traditional methods. This method, however, appears to be of theoretical interest because it forces us to rethink the relationship between score equating and regression. © 2011 by the National Council on Measurement in Education.","['method', 'local', 'linear', 'observedscore', 'equate', 'anchortest', 'singlegroup', 'design', 'introduce', 'empirical', 'study', 'method', 'compare', 'current', 'traditional', 'linear', 'method', 'observedscore', 'equate', 'criterion', 'bias', 'equate', 'score', 'relative', 'true', 'equating', 'base', 'definition', 'equity', 'local', 'method', 'anchortest', 'design', 'yield', 'minimum', 'bias', 'considerable', 'variation', 'relative', 'difficulty', 'test', 'form', 'length', 'anchor', 'test', 'traditional', 'method', 'method', 'chain', 'equating', 'perform', 'good', 'local', 'method', 'singlegroup', 'design', 'yield', 'equate', 'score', 'bias', 'comparable', 'traditional', 'method', 'method', 'appear', 'theoretical', 'interest', 'force', 'rethink', 'relationship', 'score', 'equating', 'regression', '©', '2011', 'National', 'Council']",,method local linear observedscore equate anchortest singlegroup design introduce empirical study method compare current traditional linear method observedscore equate criterion bias equate score relative true equating base definition equity local method anchortest design yield minimum bias considerable variation relative difficulty test form length anchor test traditional method method chain equating perform good local method singlegroup design yield equate score bias comparable traditional method method appear theoretical interest force rethink relationship score equating regression © 2011 National Council,,0.8777407469485228,0.030478970631165704,0.030821899767769598,0.030518650978123233,0.030439731674418782,0.004322964804356942,0.004134068456410017,0.0,0.1771650282850301,9.897474740470504e-05
Cui Y.; Gierl M.J.; Chang H.-H.,Estimating Classification Consistency and Accuracy for Cognitive Diagnostic Assessment,2012,49,"This article introduces procedures for the computation and asymptotic statistical inference for classification consistency and accuracy indices specifically designed for cognitive diagnostic assessments. The new classification indices can be used as important indicators of the reliability and validity of classification results produced by cognitive diagnostic assessments. For tests with known or previously calibrated item parameters, the sampling distributions of the two new indices are shown to be asymptotically normal. To illustrate the computations of the new indices, we apply them to the real diagnostic data from a fraction subtraction test (Tatsuoka). We also use simulated data to evaluate their performances and distributional properties. © 2012 by the National Council on Measurement in Education.",Estimating Classification Consistency and Accuracy for Cognitive Diagnostic Assessment,"This article introduces procedures for the computation and asymptotic statistical inference for classification consistency and accuracy indices specifically designed for cognitive diagnostic assessments. The new classification indices can be used as important indicators of the reliability and validity of classification results produced by cognitive diagnostic assessments. For tests with known or previously calibrated item parameters, the sampling distributions of the two new indices are shown to be asymptotically normal. To illustrate the computations of the new indices, we apply them to the real diagnostic data from a fraction subtraction test (Tatsuoka). We also use simulated data to evaluate their performances and distributional properties. © 2012 by the National Council on Measurement in Education.","['article', 'introduce', 'procedure', 'computation', 'asymptotic', 'statistical', 'inference', 'classification', 'consistency', 'accuracy', 'index', 'specifically', 'design', 'cognitive', 'diagnostic', 'assessment', 'new', 'classification', 'index', 'important', 'indicator', 'reliability', 'validity', 'classification', 'result', 'produce', 'cognitive', 'diagnostic', 'assessment', 'test', 'know', 'previously', 'calibrate', 'item', 'parameter', 'sample', 'distribution', 'new', 'index', 'asymptotically', 'normal', 'illustrate', 'computation', 'new', 'index', 'apply', 'real', 'diagnostic', 'datum', 'fraction', 'subtraction', 'test', 'Tatsuoka', 'simulate', 'datum', 'evaluate', 'performance', 'distributional', 'property', '©', '2012', 'National', 'Council']","['Estimating', 'Classification', 'Consistency', 'Accuracy', 'Cognitive', 'Diagnostic', 'Assessment']",article introduce procedure computation asymptotic statistical inference classification consistency accuracy index specifically design cognitive diagnostic assessment new classification index important indicator reliability validity classification result produce cognitive diagnostic assessment test know previously calibrate item parameter sample distribution new index asymptotically normal illustrate computation new index apply real diagnostic datum fraction subtraction test Tatsuoka simulate datum evaluate performance distributional property © 2012 National Council,Estimating Classification Consistency Accuracy Cognitive Diagnostic Assessment,0.02922094101071298,0.029326987513823462,0.029089497899048335,0.029351004060625813,0.8830115695157894,0.048980732457113735,0.0481140782598394,0.02264662768171331,0.0008185376880755782,0.0011183333615685928
Bränberg K.; Wiberg M.,Observed score linear equating with covariates,2011,48,"This paper examined observed score linear equating in two different data collection designs, the equivalent groups design and the nonequivalent groups design, when information from covariates (i.e., background variables correlated with the test scores) was included. The main purpose of the study was to examine the effect (i.e., bias, variance, and mean squared error) on the estimators of including this additional information. A model for observed score linear equating with covariates first was suggested. As a second step, the model was used in a simulation study to show that the use of covariates such as gender and education can increase the accuracy of an equating by reducing the mean squared error of the estimators. Finally, data from two administrations of the Swedish Scholastic Assessment Test were used to illustrate the use of the model. © 2011 by the National Council on Measurement in Education.",,"This paper examined observed score linear equating in two different data collection designs, the equivalent groups design and the nonequivalent groups design, when information from covariates (i.e., background variables correlated with the test scores) was included. The main purpose of the study was to examine the effect (i.e., bias, variance, and mean squared error) on the estimators of including this additional information. A model for observed score linear equating with covariates first was suggested. As a second step, the model was used in a simulation study to show that the use of covariates such as gender and education can increase the accuracy of an equating by reducing the mean squared error of the estimators. Finally, data from two administrations of the Swedish Scholastic Assessment Test were used to illustrate the use of the model. © 2011 by the National Council on Measurement in Education.","['paper', 'examine', 'observed', 'score', 'linear', 'equate', 'different', 'datum', 'collection', 'design', 'equivalent', 'group', 'design', 'nonequivalent', 'group', 'design', 'information', 'covariate', 'ie', 'background', 'variable', 'correlate', 'test', 'score', 'include', 'main', 'purpose', 'study', 'examine', 'effect', 'ie', 'bias', 'variance', 'mean', 'squared', 'error', 'estimator', 'include', 'additional', 'information', 'observed', 'score', 'linear', 'equate', 'covariate', 'suggest', 'second', 'step', 'simulation', 'study', 'covariate', 'gender', 'increase', 'accuracy', 'equating', 'reduce', 'mean', 'squared', 'error', 'estimator', 'finally', 'data', 'administration', 'Swedish', 'Scholastic', 'Assessment', 'Test', 'illustrate', '©', '2011', 'National', 'Council']",,paper examine observed score linear equate different datum collection design equivalent group design nonequivalent group design information covariate ie background variable correlate test score include main purpose study examine effect ie bias variance mean squared error estimator include additional information observed score linear equate covariate suggest second step simulation study covariate gender increase accuracy equating reduce mean squared error estimator finally data administration Swedish Scholastic Assessment Test illustrate © 2011 National Council,,0.8899888079906598,0.02747196484221057,0.027450478634259614,0.027505295175547395,0.027583453357322464,0.0133881879782079,0.012338578754129843,0.026990775068445813,0.09001527753571116,0.011269437213918968
Jiao H.; Kamata A.; Wang S.; Jin Y.,A Multilevel Testlet Model for Dual Local Dependence,2012,49,"The applications of item response theory (IRT) models assume local item independence and that examinees are independent of each other. When a representative sample for psychometric analysis is selected using a cluster sampling method in a testlet-based assessment, both local item dependence and local person dependence are likely to be induced. This study proposed a four-level IRT model to simultaneously account for dual local dependence due to item clustering and person clustering. Model parameter estimation was explored using the Markov Chain Monte Carlo method. Model parameter recovery was evaluated in a simulation study in comparison with three other related models: the Rasch model, the Rasch testlet model, and the three-level Rasch model for person clustering. In general, the proposed model recovered the item difficulty and person ability parameters with the least total error. The bias in both item and person parameter estimation was not affected but the standard error (SE) was affected. In some simulation conditions, the difference in classification accuracy between models could go up to 11%. The illustration using the real data generally supported model performance observed in the simulation study. © 2012 by the National Council on Measurement in Education.",A Multilevel Testlet Model for Dual Local Dependence,"The applications of item response theory (IRT) models assume local item independence and that examinees are independent of each other. When a representative sample for psychometric analysis is selected using a cluster sampling method in a testlet-based assessment, both local item dependence and local person dependence are likely to be induced. This study proposed a four-level IRT model to simultaneously account for dual local dependence due to item clustering and person clustering. Model parameter estimation was explored using the Markov Chain Monte Carlo method. Model parameter recovery was evaluated in a simulation study in comparison with three other related models: the Rasch model, the Rasch testlet model, and the three-level Rasch model for person clustering. In general, the proposed model recovered the item difficulty and person ability parameters with the least total error. The bias in both item and person parameter estimation was not affected but the standard error (SE) was affected. In some simulation conditions, the difference in classification accuracy between models could go up to 11%. The illustration using the real data generally supported model performance observed in the simulation study. © 2012 by the National Council on Measurement in Education.","['application', 'item', 'response', 'theory', 'IRT', 'assume', 'local', 'item', 'independence', 'examinee', 'independent', 'representative', 'sample', 'psychometric', 'analysis', 'select', 'cluster', 'sampling', 'method', 'testletbase', 'assessment', 'local', 'item', 'dependence', 'local', 'person', 'dependence', 'likely', 'induce', 'study', 'propose', 'fourlevel', 'IRT', 'simultaneously', 'account', 'dual', 'local', 'dependence', 'item', 'clustering', 'person', 'cluster', 'parameter', 'estimation', 'explore', 'Markov', 'Chain', 'Monte', 'Carlo', 'method', 'parameter', 'recovery', 'evaluate', 'simulation', 'study', 'comparison', 'relate', 'Rasch', 'Rasch', 'testlet', 'threelevel', 'Rasch', 'person', 'cluster', 'general', 'propose', 'recover', 'item', 'difficulty', 'person', 'ability', 'parameter', 'total', 'error', 'bias', 'item', 'person', 'parameter', 'estimation', 'affect', 'standard', 'error', 'SE', 'affect', 'simulation', 'condition', 'difference', 'classification', 'accuracy', '11', 'illustration', 'real', 'datum', 'generally', 'support', 'performance', 'observe', 'simulation', 'study', '©', '2012', 'National', 'Council']","['Multilevel', 'Testlet', 'Dual', 'Local', 'Dependence']",application item response theory IRT assume local item independence examinee independent representative sample psychometric analysis select cluster sampling method testletbase assessment local item dependence local person dependence likely induce study propose fourlevel IRT simultaneously account dual local dependence item clustering person cluster parameter estimation explore Markov Chain Monte Carlo method parameter recovery evaluate simulation study comparison relate Rasch Rasch testlet threelevel Rasch person cluster general propose recover item difficulty person ability parameter total error bias item person parameter estimation affect standard error SE affect simulation condition difference classification accuracy 11 illustration real datum generally support performance observe simulation study © 2012 National Council,Multilevel Testlet Dual Local Dependence,0.0263082606991175,0.026130903999220207,0.026057788311810555,0.02610300369905171,0.8954000432908,0.08319169743834512,0.0,0.0,0.008173745111669201,0.008477210016817482
French B.F.; Finch W.H.,Hierarchical logistic regression: Accounting for multilevel data in DIF detection,2010,47,"The purpose of this study was to examine the performance of differential item functioning (DIF) assessment in the presence of a multilevel structure that often underlies data from large-scale testing programs. Analyses were conducted using logistic regression (LR), a popular, flexible, and effective tool for DIF detection. Data were simulated using a hierarchical framework, such as might be seen when examinees are clustered in schools, for example. Both standard and hierarchical LR (accounting for multilevel data) approaches to DIF detection were employed. Results highlight the differences in DIF detection rates when the analytic strategy matches the data structure. Specifically, when the grouping variable was within clusters, LR and HLR performed similarly in terms of Type I error control and power. However, when the grouping variable was between clusters, LR failed to maintain the nominal Type I error rate of .05. HLR was able to maintain this rate. However, power for HLR tended to be low under many conditions in the between cluster variable case. © 2010 by the National Council on Measurement in Education.",Hierarchical logistic regression: Accounting for multilevel data in DIF detection,"The purpose of this study was to examine the performance of differential item functioning (DIF) assessment in the presence of a multilevel structure that often underlies data from large-scale testing programs. Analyses were conducted using logistic regression (LR), a popular, flexible, and effective tool for DIF detection. Data were simulated using a hierarchical framework, such as might be seen when examinees are clustered in schools, for example. Both standard and hierarchical LR (accounting for multilevel data) approaches to DIF detection were employed. Results highlight the differences in DIF detection rates when the analytic strategy matches the data structure. Specifically, when the grouping variable was within clusters, LR and HLR performed similarly in terms of Type I error control and power. However, when the grouping variable was between clusters, LR failed to maintain the nominal Type I error rate of .05. HLR was able to maintain this rate. However, power for HLR tended to be low under many conditions in the between cluster variable case. © 2010 by the National Council on Measurement in Education.","['purpose', 'study', 'examine', 'performance', 'differential', 'item', 'function', 'dif', 'assessment', 'presence', 'multilevel', 'structure', 'underlie', 'datum', 'largescale', 'testing', 'program', 'Analyses', 'conduct', 'logistic', 'regression', 'LR', 'popular', 'flexible', 'effective', 'tool', 'DIF', 'detection', 'Data', 'simulate', 'hierarchical', 'framework', 'examinee', 'cluster', 'school', 'example', 'standard', 'hierarchical', 'LR', 'accounting', 'multilevel', 'datum', 'approach', 'DIF', 'detection', 'employ', 'result', 'highlight', 'difference', 'dif', 'detection', 'rate', 'analytic', 'strategy', 'match', 'data', 'structure', 'specifically', 'group', 'variable', 'cluster', 'LR', 'HLR', 'perform', 'similarly', 'term', 'Type', 'I', 'error', 'control', 'power', 'group', 'variable', 'cluster', 'LR', 'fail', 'maintain', 'nominal', 'Type', 'I', 'error', 'rate', '05', 'HLR', 'able', 'maintain', 'rate', 'power', 'HLR', 'tend', 'low', 'condition', 'cluster', 'variable', 'case', '©', '2010', 'National', 'Council']","['hierarchical', 'logistic', 'regression', 'Accounting', 'multilevel', 'datum', 'DIF', 'detection']",purpose study examine performance differential item function dif assessment presence multilevel structure underlie datum largescale testing program Analyses conduct logistic regression LR popular flexible effective tool DIF detection Data simulate hierarchical framework examinee cluster school example standard hierarchical LR accounting multilevel datum approach DIF detection employ result highlight difference dif detection rate analytic strategy match data structure specifically group variable cluster LR HLR perform similarly term Type I error control power group variable cluster LR fail maintain nominal Type I error rate 05 HLR able maintain rate power HLR tend low condition cluster variable case © 2010 National Council,hierarchical logistic regression Accounting multilevel datum DIF detection,0.027326298519444348,0.8918914896840903,0.02687128067735208,0.026920779306520086,0.02699015181259328,0.0575165240335776,0.0,0.016273369061792733,0.0,0.0021691407504800127
Sinharay S.; Haberman S.J.; Lee Y.-H.,When does scale anchoring work? A case study,2011,48,"Providing information to test takers and test score users about the abilities of test takers at different score levels has been a persistent problem in educational and psychological measurement. Scale anchoring, a technique which describes what students at different points on a score scale know and can do, is a tool to provide such information. Scale anchoring for a test involves a substantial amount of work, both by the statistical analysts and test developers involved with the test. In addition, scale anchoring involves considerable use of subjective judgment, so its conclusions may be questionable. We describe statistical procedures that can be used to determine if scale anchoring is likely to be successful for a test. If these procedures indicate that scale anchoring is unlikely to be successful, then there is little reason to perform a detailed scale anchoring study. The procedures are applied to several data sets from a teachers' licensing test. © 2011 by the National Council on Measurement in Education.",,"Providing information to test takers and test score users about the abilities of test takers at different score levels has been a persistent problem in educational and psychological measurement. Scale anchoring, a technique which describes what students at different points on a score scale know and can do, is a tool to provide such information. Scale anchoring for a test involves a substantial amount of work, both by the statistical analysts and test developers involved with the test. In addition, scale anchoring involves considerable use of subjective judgment, so its conclusions may be questionable. We describe statistical procedures that can be used to determine if scale anchoring is likely to be successful for a test. If these procedures indicate that scale anchoring is unlikely to be successful, then there is little reason to perform a detailed scale anchoring study. The procedures are applied to several data sets from a teachers' licensing test. © 2011 by the National Council on Measurement in Education.","['provide', 'information', 'test', 'taker', 'test', 'score', 'user', 'ability', 'test', 'taker', 'different', 'score', 'level', 'persistent', 'problem', 'educational', 'psychological', 'Scale', 'anchor', 'technique', 'describe', 'student', 'different', 'point', 'score', 'scale', 'know', 'tool', 'provide', 'information', 'Scale', 'anchor', 'test', 'involve', 'substantial', 'work', 'statistical', 'analyst', 'test', 'developer', 'involve', 'test', 'addition', 'scale', 'anchoring', 'involve', 'considerable', 'subjective', 'judgment', 'conclusion', 'questionable', 'describe', 'statistical', 'procedure', 'determine', 'scale', 'anchoring', 'likely', 'successful', 'test', 'procedure', 'indicate', 'scale', 'anchoring', 'unlikely', 'successful', 'little', 'reason', 'perform', 'detailed', 'scale', 'anchoring', 'study', 'procedure', 'apply', 'datum', 'set', 'teacher', 'license', 'test', '©', '2011', 'National', 'Council']",,provide information test taker test score user ability test taker different score level persistent problem educational psychological Scale anchor technique describe student different point score scale know tool provide information Scale anchor test involve substantial work statistical analyst test developer involve test addition scale anchoring involve considerable subjective judgment conclusion questionable describe statistical procedure determine scale anchoring likely successful test procedure indicate scale anchoring unlikely successful little reason perform detailed scale anchoring study procedure apply datum set teacher license test © 2011 National Council,,0.02917638567313168,0.02931620901028945,0.02902622795277924,0.8830509770848078,0.029430200278991694,0.027308268635680078,0.02040614344108477,0.046340614794884626,0.02827616894513779,0.004161695517755575
Rijmen F.,"Formal relations and an empirical comparison among the bi-factor, the testlet, and a second-order multidimensional IRT model",2010,47,"Testlet effects can be taken into account by incorporating specific dimensions in addition to the general dimension into the item response theory model. Three such multidimensional models are described: the bi-factor model, the testlet model, and a second-order model. It is shown how the second-order model is formally equivalent to the testlet model. In turn, both models are constrained bi-factor models. Therefore, the efficient full maximum likelihood estimation method that has been established for the bi-factor model can be modified to estimate the parameters of the two other models. An application on a testlet-based international English assessment indicated that the bi-factor model was the preferred model for this particular data set. © 2010 by the National Council on Measurement in Education.","Formal relations and an empirical comparison among the bi-factor, the testlet, and a second-order multidimensional IRT model","Testlet effects can be taken into account by incorporating specific dimensions in addition to the general dimension into the item response theory model. Three such multidimensional models are described: the bi-factor model, the testlet model, and a second-order model. It is shown how the second-order model is formally equivalent to the testlet model. In turn, both models are constrained bi-factor models. Therefore, the efficient full maximum likelihood estimation method that has been established for the bi-factor model can be modified to estimate the parameters of the two other models. An application on a testlet-based international English assessment indicated that the bi-factor model was the preferred model for this particular data set. © 2010 by the National Council on Measurement in Education.","['testlet', 'effect', 'account', 'incorporate', 'specific', 'dimension', 'addition', 'general', 'dimension', 'item', 'response', 'theory', 'multidimensional', 'describe', 'bifactor', 'testlet', 'secondorder', 'secondorder', 'formally', 'equivalent', 'testlet', 'turn', 'constrain', 'bifactor', 'efficient', 'maximum', 'likelihood', 'estimation', 'method', 'establish', 'bifactor', 'modify', 'estimate', 'parameter', 'application', 'testletbased', 'international', 'english', 'assessment', 'indicate', 'bifactor', 'preferred', 'particular', 'datum', 'set', '©', '2010', 'National', 'Council']","['formal', 'relation', 'empirical', 'comparison', 'bifactor', 'testlet', 'secondorder', 'multidimensional', 'IRT']",testlet effect account incorporate specific dimension addition general dimension item response theory multidimensional describe bifactor testlet secondorder secondorder formally equivalent testlet turn constrain bifactor efficient maximum likelihood estimation method establish bifactor modify estimate parameter application testletbased international english assessment indicate bifactor preferred particular datum set © 2010 National Council,formal relation empirical comparison bifactor testlet secondorder multidimensional IRT,0.033821773331564886,0.03398518282852859,0.03362612935918244,0.8648078119703747,0.0337591025103493,0.03532160766707272,0.0,0.005346331820205635,0.0,0.010301383485689514
Sinharay S.; Haberman S.J.,Equating of augmented subscores,2011,48,"Recently, there has been an increasing level of interest in subscores for their potential diagnostic value. suggested reporting an augmented subscore that is a linear combination of a subscore and the total score. and showed that augmented subscores often lead to more accurate diagnostic information than subscores. In order to report augmented subscores operationally, they should be comparable across the different forms of a test. One way to achieve comparability is to equate them. We suggest several methods for equating augmented subscores. Results from several operational and simulated data sets show that the error in the equating of augmented subscores appears to be small in most practical situations. Copyright © 2011 by the National Council on Measurement in Education.",,"Recently, there has been an increasing level of interest in subscores for their potential diagnostic value. suggested reporting an augmented subscore that is a linear combination of a subscore and the total score. and showed that augmented subscores often lead to more accurate diagnostic information than subscores. In order to report augmented subscores operationally, they should be comparable across the different forms of a test. One way to achieve comparability is to equate them. We suggest several methods for equating augmented subscores. Results from several operational and simulated data sets show that the error in the equating of augmented subscores appears to be small in most practical situations. Copyright © 2011 by the National Council on Measurement in Education.","['recently', 'increase', 'level', 'interest', 'subscore', 'potential', 'diagnostic', 'value', 'suggest', 'report', 'augmented', 'subscore', 'linear', 'combination', 'subscore', 'total', 'score', 'augmented', 'subscore', 'lead', 'accurate', 'diagnostic', 'information', 'subscore', 'order', 'report', 'augment', 'subscore', 'operationally', 'comparable', 'different', 'form', 'test', 'way', 'achieve', 'comparability', 'equate', 'suggest', 'method', 'equate', 'augment', 'subscore', 'result', 'operational', 'simulated', 'data', 'set', 'error', 'equating', 'augmented', 'subscore', 'appear', 'small', 'practical', 'situation', 'copyright', '©', '2011', 'National', 'Council']",,recently increase level interest subscore potential diagnostic value suggest report augmented subscore linear combination subscore total score augmented subscore lead accurate diagnostic information subscore order report augment subscore operationally comparable different form test way achieve comparability equate suggest method equate augment subscore result operational simulated data set error equating augmented subscore appear small practical situation copyright © 2011 National Council,,0.04263425747139288,0.8359532908780912,0.04080445450871999,0.040271083838687605,0.0403369133031085,0.0,0.30205104939099026,0.0,0.01716821707071317,0.0
Baldwin P.,A Strategy for developing a common metric in item response theory when parameter posterior distributions are known,2011,48,"Growing interest in fully Bayesian item response models begs the question: To what extent can model parameter posterior draws enhance existing practices? One practice that has traditionally relied on model parameter point estimates but may be improved by using posterior draws is the development of a common metric for two independently calibrated test forms. Before parameter estimates from independently calibrated forms can be compared, at least one form's estimates must be adjusted such that both forms share a common metric. Because this adjustment is estimated, there is a propagation of error effect when it is applied. This effect is typically ignored, which leads to overconfidence in the adjusted estimates; yet, when model parameter posterior draws are available, it may be accounted for with a simple sampling strategy. In this paper, it is shown using simulated data that the proposed sampling strategy results in adjusted posteriors with superior coverage properties than those obtained using traditional point-estimate-based methods. © 2011 by the National Council on Measurement in Education.",A Strategy for developing a common metric in item response theory when parameter posterior distributions are known,"Growing interest in fully Bayesian item response models begs the question: To what extent can model parameter posterior draws enhance existing practices? One practice that has traditionally relied on model parameter point estimates but may be improved by using posterior draws is the development of a common metric for two independently calibrated test forms. Before parameter estimates from independently calibrated forms can be compared, at least one form's estimates must be adjusted such that both forms share a common metric. Because this adjustment is estimated, there is a propagation of error effect when it is applied. This effect is typically ignored, which leads to overconfidence in the adjusted estimates; yet, when model parameter posterior draws are available, it may be accounted for with a simple sampling strategy. In this paper, it is shown using simulated data that the proposed sampling strategy results in adjusted posteriors with superior coverage properties than those obtained using traditional point-estimate-based methods. © 2011 by the National Council on Measurement in Education.","['grow', 'interest', 'fully', 'bayesian', 'item', 'response', 'beg', 'question', 'extent', 'parameter', 'posterior', 'draw', 'enhance', 'exist', 'practice', 'practice', 'traditionally', 'rely', 'parameter', 'point', 'estimate', 'improve', 'posterior', 'draw', 'development', 'common', 'metric', 'independently', 'calibrate', 'test', 'form', 'parameter', 'estimate', 'independently', 'calibrate', 'form', 'compare', 'form', 'estimate', 'adjust', 'form', 'share', 'common', 'metric', 'adjustment', 'estimate', 'propagation', 'error', 'effect', 'apply', 'effect', 'typically', 'ignore', 'lead', 'overconfidence', 'adjust', 'estimate', 'parameter', 'posterior', 'draw', 'available', 'account', 'simple', 'sampling', 'strategy', 'paper', 'simulate', 'datum', 'propose', 'sampling', 'strategy', 'result', 'adjust', 'posterior', 'superior', 'coverage', 'property', 'obtain', 'traditional', 'pointestimatebase', 'method', '©', '2011', 'National', 'Council']","['strategy', 'develop', 'common', 'metric', 'item', 'response', 'theory', 'parameter', 'posterior', 'distribution', 'know']",grow interest fully bayesian item response beg question extent parameter posterior draw enhance exist practice practice traditionally rely parameter point estimate improve posterior draw development common metric independently calibrate test form parameter estimate independently calibrate form compare form estimate adjust form share common metric adjustment estimate propagation error effect apply effect typically ignore lead overconfidence adjust estimate parameter posterior draw available account simple sampling strategy paper simulate datum propose sampling strategy result adjust posterior superior coverage property obtain traditional pointestimatebase method © 2011 National Council,strategy develop common metric item response theory parameter posterior distribution know,0.028064026368379822,0.8886780774759021,0.027642712857131997,0.02758111527599322,0.0280340680225929,0.050261835764636976,0.0010195877697243355,0.00582986096918166,0.0045913943190399355,0.006902067244369908
Deng H.; Ansley T.; Chang H.-H.,Stratified and maximum information item selection procedures in computer adaptive testing,2010,47,"In this study we evaluated and compared three item selection procedures: the maximum Fisher information procedure (F), the a-stratified multistage computer adaptive testing (CAT) (STR), and a refined stratification procedure that allows more items to be selected from the high a strata and fewer items from the low a strata (USTR), along with completely random item selection (RAN). The comparisons were with respect to error variances, reliability of ability estimates and item usage through CATs simulated under nine test conditions of various practical constraints and item selection space. The results showed that F had an apparent precision advantage over STR and USTR under unconstrained item selection, but with very poor item usage. USTR reduced error variances for STR under various conditions, with small compromises in item usage. Compared to F, USTR enhanced item usage while achieving comparable precision in ability estimates; it achieved a precision level similar to F with improved item usage when items were selected under exposure control and with limited item selection space. The results provide implications for choosing an appropriate item selection procedure in applied settings. © 2010 by the National Council on Measurement in Education.",Stratified and maximum information item selection procedures in computer adaptive testing,"In this study we evaluated and compared three item selection procedures: the maximum Fisher information procedure (F), the a-stratified multistage computer adaptive testing (CAT) (STR), and a refined stratification procedure that allows more items to be selected from the high a strata and fewer items from the low a strata (USTR), along with completely random item selection (RAN). The comparisons were with respect to error variances, reliability of ability estimates and item usage through CATs simulated under nine test conditions of various practical constraints and item selection space. The results showed that F had an apparent precision advantage over STR and USTR under unconstrained item selection, but with very poor item usage. USTR reduced error variances for STR under various conditions, with small compromises in item usage. Compared to F, USTR enhanced item usage while achieving comparable precision in ability estimates; it achieved a precision level similar to F with improved item usage when items were selected under exposure control and with limited item selection space. The results provide implications for choosing an appropriate item selection procedure in applied settings. © 2010 by the National Council on Measurement in Education.","['study', 'evaluate', 'compare', 'item', 'selection', 'procedure', 'maximum', 'Fisher', 'information', 'procedure', 'F', 'astratifie', 'multistage', 'computer', 'adaptive', 'testing', 'CAT', 'STR', 'refined', 'stratification', 'procedure', 'allow', 'item', 'select', 'high', 'strata', 'item', 'low', 'strata', 'USTR', 'completely', 'random', 'item', 'selection', 'ran', 'comparison', 'respect', 'error', 'variance', 'reliability', 'ability', 'estimate', 'item', 'usage', 'CATs', 'simulate', 'test', 'condition', 'practical', 'constraint', 'item', 'selection', 'space', 'result', 'F', 'apparent', 'precision', 'advantage', 'STR', 'USTR', 'unconstrained', 'item', 'selection', 'poor', 'item', 'usage', 'USTR', 'reduce', 'error', 'variance', 'STR', 'condition', 'small', 'compromise', 'item', 'usage', 'compare', 'F', 'USTR', 'enhance', 'item', 'usage', 'achieve', 'comparable', 'precision', 'ability', 'estimate', 'achieve', 'precision', 'level', 'similar', 'F', 'improve', 'item', 'usage', 'item', 'select', 'exposure', 'control', 'limited', 'item', 'selection', 'space', 'result', 'provide', 'implication', 'choose', 'appropriate', 'item', 'selection', 'procedure', 'apply', 'setting', '©', '2010', 'National', 'Council']","['stratified', 'maximum', 'information', 'item', 'selection', 'procedure', 'computer', 'adaptive', 'testing']",study evaluate compare item selection procedure maximum Fisher information procedure F astratifie multistage computer adaptive testing CAT STR refined stratification procedure allow item select high strata item low strata USTR completely random item selection ran comparison respect error variance reliability ability estimate item usage CATs simulate test condition practical constraint item selection space result F apparent precision advantage STR USTR unconstrained item selection poor item usage USTR reduce error variance STR condition small compromise item usage compare F USTR enhance item usage achieve comparable precision ability estimate achieve precision level similar F improve item usage item select exposure control limited item selection space result provide implication choose appropriate item selection procedure apply setting © 2010 National Council,stratified maximum information item selection procedure computer adaptive testing,0.02902276748523222,0.028832036823205377,0.028886870910665628,0.8844576034369521,0.02880072134394466,0.07654264905706383,0.0,0.0,0.0,0.0
Alexeev N.; Templin J.; Cohen A.S.,Spurious latent classes in the mixture Rasch model,2011,48,"Mixture Rasch models have been used to study a number of psychometric issues such as goodness of fit, response strategy differences, strategy shifts, and multidimensionality. Although these models offer the potential for improving understanding of the latent variables being measured, under some conditions overextraction of latent classes may occur, potentially leading to misinterpretation of results. In this study, a mixture Rasch model was applied to data from a statewide test that was initially calibrated to conform to a 3-parameter logistic (3PL) model. Results suggested how latent classes could be explained and also suggested that these latent classes might be due to applying a mixture Rasch model to 3PL data. To support this latter conjecture, a simulation study was presented to demonstrate how data generated to fit a one-class 2-parameter logistic (2PL) model required more than one class when fit with a mixture Rasch model. © 2011 by the National Council on Measurement in Education.",Spurious latent classes in the mixture Rasch model,"Mixture Rasch models have been used to study a number of psychometric issues such as goodness of fit, response strategy differences, strategy shifts, and multidimensionality. Although these models offer the potential for improving understanding of the latent variables being measured, under some conditions overextraction of latent classes may occur, potentially leading to misinterpretation of results. In this study, a mixture Rasch model was applied to data from a statewide test that was initially calibrated to conform to a 3-parameter logistic (3PL) model. Results suggested how latent classes could be explained and also suggested that these latent classes might be due to applying a mixture Rasch model to 3PL data. To support this latter conjecture, a simulation study was presented to demonstrate how data generated to fit a one-class 2-parameter logistic (2PL) model required more than one class when fit with a mixture Rasch model. © 2011 by the National Council on Measurement in Education.","['Mixture', 'Rasch', 'study', 'number', 'psychometric', 'issue', 'goodness', 'fit', 'response', 'strategy', 'difference', 'strategy', 'shift', 'multidimensionality', 'offer', 'potential', 'improve', 'understanding', 'latent', 'variable', 'measure', 'condition', 'overextraction', 'latent', 'class', 'occur', 'potentially', 'lead', 'misinterpretation', 'result', 'study', 'mixture', 'Rasch', 'apply', 'datum', 'statewide', 'test', 'initially', 'calibrate', 'conform', '3parameter', 'logistic', '3pl', 'result', 'suggest', 'latent', 'class', 'explain', 'suggest', 'latent', 'class', 'apply', 'mixture', 'Rasch', '3pl', 'datum', 'support', 'conjecture', 'simulation', 'study', 'present', 'demonstrate', 'datum', 'generate', 'fit', 'oneclass', '2parameter', 'logistic', '2pl', 'require', 'class', 'fit', 'mixture', 'Rasch', '©', '2011', 'National', 'Council']","['spurious', 'latent', 'class', 'mixture', 'Rasch']",Mixture Rasch study number psychometric issue goodness fit response strategy difference strategy shift multidimensionality offer potential improve understanding latent variable measure condition overextraction latent class occur potentially lead misinterpretation result study mixture Rasch apply datum statewide test initially calibrate conform 3parameter logistic 3pl result suggest latent class explain suggest latent class apply mixture Rasch 3pl datum support conjecture simulation study present demonstrate datum generate fit oneclass 2parameter logistic 2pl require class fit mixture Rasch © 2011 National Council,spurious latent class mixture Rasch,0.028822973454747568,0.028722212628703538,0.028808541634652407,0.8848701689787091,0.028776103303187407,0.047343891916931675,0.0,0.0046490416702623895,0.0,0.022312746156600356
Omar M.H.,Statistical process control charts for measuring and monitoring temporal consistency of ratings,2010,47,"Methods of statistical process control were briefly investigated in the field of educational measurement as early as 1999. However, only the use of a cumulative sum chart was explored. In this article other methods of statistical quality control are introduced and explored. In particular, methods in the form of Shewhart mean and standard deviation charts are introduced as techniques for ensuring quality in a measurement process for rating performance items in operational assessments. Several strengths and weaknesses of the procedures are explored with illustrative real and simulated rating data. Further research directions are also suggested. © 2010 by the National Council on Measurement in Education.",Statistical process control charts for measuring and monitoring temporal consistency of ratings,"Methods of statistical process control were briefly investigated in the field of educational measurement as early as 1999. However, only the use of a cumulative sum chart was explored. In this article other methods of statistical quality control are introduced and explored. In particular, methods in the form of Shewhart mean and standard deviation charts are introduced as techniques for ensuring quality in a measurement process for rating performance items in operational assessments. Several strengths and weaknesses of the procedures are explored with illustrative real and simulated rating data. Further research directions are also suggested. © 2010 by the National Council on Measurement in Education.","['method', 'statistical', 'process', 'control', 'briefly', 'investigate', 'field', 'educational', 'early', '1999', 'cumulative', 'sum', 'chart', 'explore', 'article', 'method', 'statistical', 'quality', 'control', 'introduce', 'explore', 'particular', 'method', 'form', 'Shewhart', 'mean', 'standard', 'deviation', 'chart', 'introduce', 'technique', 'ensure', 'quality', 'process', 'rating', 'performance', 'item', 'operational', 'assessment', 'strength', 'weakness', 'procedure', 'explore', 'illustrative', 'real', 'simulated', 'rating', 'datum', 'research', 'direction', 'suggest', '©', '2010', 'National', 'Council']","['statistical', 'process', 'control', 'chart', 'measure', 'monitor', 'temporal', 'consistency', 'rating']",method statistical process control briefly investigate field educational early 1999 cumulative sum chart explore article method statistical quality control introduce explore particular method form Shewhart mean standard deviation chart introduce technique ensure quality process rating performance item operational assessment strength weakness procedure explore illustrative real simulated rating datum research direction suggest © 2010 National Council,statistical process control chart measure monitor temporal consistency rating,0.02971839325977804,0.029704238901297173,0.8809038781054747,0.029707123721594213,0.02996636601185591,0.018135748357229815,0.006817850822519035,0.03667258337836641,0.01488917350559366,0.03568804586718954
Jiang Y.; von Davier A.A.; Chen H.,Evaluating Equating Results: Percent Relative Error for Chained Kernel Equating,2012,49,"This article presents a method for evaluating equating results. Within the kernel equating framework, the percent relative error (PRE) for chained equipercentile equating was computed under the nonequivalent groups with anchor test (NEAT) design. The method was applied to two data sets to obtain the PRE, which can be used to measure equating effectiveness. The study compared the PRE results for chained and poststratification equating. The results indicated that the chained method transformed the new form score distribution to the reference form scale more effectively than the poststratification method. In addition, the study found that in chained equating, the population weight had impact on score distributions over the target population but not on the equating and PRE results. © 2012 by the National Council on Measurement in Education.",Evaluating Equating Results: Percent Relative Error for Chained Kernel Equating,"This article presents a method for evaluating equating results. Within the kernel equating framework, the percent relative error (PRE) for chained equipercentile equating was computed under the nonequivalent groups with anchor test (NEAT) design. The method was applied to two data sets to obtain the PRE, which can be used to measure equating effectiveness. The study compared the PRE results for chained and poststratification equating. The results indicated that the chained method transformed the new form score distribution to the reference form scale more effectively than the poststratification method. In addition, the study found that in chained equating, the population weight had impact on score distributions over the target population but not on the equating and PRE results. © 2012 by the National Council on Measurement in Education.","['article', 'present', 'method', 'evaluate', 'equate', 'result', 'kernel', 'equating', 'framework', 'percent', 'relative', 'error', 'PRE', 'chain', 'equipercentile', 'equating', 'compute', 'nonequivalent', 'group', 'anchor', 'test', 'NEAT', 'design', 'method', 'apply', 'datum', 'set', 'obtain', 'PRE', 'measure', 'equate', 'effectiveness', 'study', 'compare', 'pre', 'result', 'chain', 'poststratification', 'equate', 'result', 'indicate', 'chain', 'method', 'transform', 'new', 'form', 'score', 'distribution', 'reference', 'form', 'scale', 'effectively', 'poststratification', 'method', 'addition', 'study', 'find', 'chain', 'equate', 'population', 'weight', 'impact', 'score', 'distribution', 'target', 'population', 'equating', 'pre', 'result', '©', '2012', 'National', 'Council']","['evaluate', 'Equating', 'Results', 'Percent', 'Relative', 'Error', 'Chained', 'Kernel', 'Equating']",article present method evaluate equate result kernel equating framework percent relative error PRE chain equipercentile equating compute nonequivalent group anchor test NEAT design method apply datum set obtain PRE measure equate effectiveness study compare pre result chain poststratification equate result indicate chain method transform new form score distribution reference form scale effectively poststratification method addition study find chain equate population weight impact score distribution target population equating pre result © 2012 National Council,evaluate Equating Results Percent Relative Error Chained Kernel Equating,0.8721944818929622,0.031796936463433394,0.032161857356332174,0.031999791330898066,0.03184693295637399,0.0,0.003371403097571931,0.004229671436943555,0.1729505172084233,0.00047916681337058356
Petscher Y.; Schatschneider C.,A Simulation study on the performance of the simple difference and covariance-adjusted scores in randomized experimental designs,2011,48,"Research by demonstrated that the covariance-adjusted score is more powerful than the simple difference score, yet recent reviews indicate researchers are equally likely to use either score type in two-wave randomized experimental designs. A Monte Carlo simulation was conducted to examine the conditions under which the simple difference and covariance-adjusted scores were more or less powerful to detect treatment effects when relaxing certain assumptions made by Four factors were manipulated in the design including sample size, normality of the pretest and posttest distributions, the correlation between pretest and posttest, and posttest variance. A 5 × 5 × 4 × 3 mostly crossed design was run with 1,000 replications per condition, resulting in 226,000 unique samples. The gain score was nearly as powerful as the covariance-adjusted score when pretest and posttest variances were equal, and as powerful in fan-spread growth conditions; thus, under certain circumstances the gain score could be used in two-wave randomized experimental designs. © 2011 by the National Council on Measurement in Education.",A Simulation study on the performance of the simple difference and covariance-adjusted scores in randomized experimental designs,"Research by demonstrated that the covariance-adjusted score is more powerful than the simple difference score, yet recent reviews indicate researchers are equally likely to use either score type in two-wave randomized experimental designs. A Monte Carlo simulation was conducted to examine the conditions under which the simple difference and covariance-adjusted scores were more or less powerful to detect treatment effects when relaxing certain assumptions made by Four factors were manipulated in the design including sample size, normality of the pretest and posttest distributions, the correlation between pretest and posttest, and posttest variance. A 5 × 5 × 4 × 3 mostly crossed design was run with 1,000 replications per condition, resulting in 226,000 unique samples. The gain score was nearly as powerful as the covariance-adjusted score when pretest and posttest variances were equal, and as powerful in fan-spread growth conditions; thus, under certain circumstances the gain score could be used in two-wave randomized experimental designs. © 2011 by the National Council on Measurement in Education.","['research', 'demonstrate', 'covarianceadjuste', 'score', 'powerful', 'simple', 'difference', 'score', 'recent', 'review', 'indicate', 'researcher', 'equally', 'likely', 'score', 'type', 'twowave', 'randomize', 'experimental', 'design', 'A', 'Monte', 'Carlo', 'simulation', 'conduct', 'examine', 'condition', 'simple', 'difference', 'covarianceadjuste', 'score', 'powerful', 'detect', 'treatment', 'effect', 'relax', 'certain', 'assumption', 'factor', 'manipulate', 'design', 'include', 'sample', 'size', 'normality', 'pret', 'postt', 'distribution', 'correlation', 'pret', 'postt', 'postt', 'variance', '5', '×', '5', '×', '4', '×', '3', 'cross', 'design', 'run', '1000', 'replication', 'condition', 'result', '226000', 'unique', 'sample', 'gain', 'score', 'nearly', 'powerful', 'covarianceadjuste', 'score', 'pret', 'postt', 'variance', 'equal', 'powerful', 'fanspread', 'growth', 'condition', 'certain', 'circumstance', 'gain', 'score', 'twowave', 'randomize', 'experimental', 'design', '©', '2011', 'National', 'Council']","['Simulation', 'study', 'performance', 'simple', 'difference', 'covarianceadjuste', 'score', 'randomize', 'experimental', 'design']",research demonstrate covarianceadjuste score powerful simple difference score recent review indicate researcher equally likely score type twowave randomize experimental design A Monte Carlo simulation conduct examine condition simple difference covarianceadjuste score powerful detect treatment effect relax certain assumption factor manipulate design include sample size normality pret postt distribution correlation pret postt postt variance 5 × 5 × 4 × 3 cross design run 1000 replication condition result 226000 unique sample gain score nearly powerful covarianceadjuste score pret postt variance equal powerful fanspread growth condition certain circumstance gain score twowave randomize experimental design © 2011 National Council,Simulation study performance simple difference covarianceadjuste score randomize experimental design,0.8891614475370748,0.027780471568085597,0.027657771275972607,0.02767524523973131,0.027725064379135706,0.009921098259297469,0.011224718554126602,0.02664530317062618,0.02371576954842559,0.006983201731848374
Kim S.; Livingston S.A.,Comparisons among small sample equating methods in a common-item design,2010,47,"Score equating based on small samples of examinees is often inaccurate for the examinee populations. We conducted a series of resampling studies to investigate the accuracy of five methods of equating in a common-item design. The methods were chained equipercentile equating of smoothed distributions, chained linear equating, chained mean equating, the symmetric circle-arc method, and the simplified circle-arc method. Four operational test forms, each containing at least 110 items, were used for the equating, with new-form samples of 100, 50, 25, and 10 examinees and reference-form samples three times as large. Accuracy was described in terms of the root-mean-squared difference (over 1,000 replications) of the sample equatings from the criterion equating. Overall, chained mean equating produced the most accurate results for low scores, but the two circle-arc methods produced the most accurate results, particularly in the upper half of the score distribution. The difference in equating accuracy between the two circle-arc methods was negligible. © 2010 by the National Council on Measurement in Education.",Comparisons among small sample equating methods in a common-item design,"Score equating based on small samples of examinees is often inaccurate for the examinee populations. We conducted a series of resampling studies to investigate the accuracy of five methods of equating in a common-item design. The methods were chained equipercentile equating of smoothed distributions, chained linear equating, chained mean equating, the symmetric circle-arc method, and the simplified circle-arc method. Four operational test forms, each containing at least 110 items, were used for the equating, with new-form samples of 100, 50, 25, and 10 examinees and reference-form samples three times as large. Accuracy was described in terms of the root-mean-squared difference (over 1,000 replications) of the sample equatings from the criterion equating. Overall, chained mean equating produced the most accurate results for low scores, but the two circle-arc methods produced the most accurate results, particularly in the upper half of the score distribution. The difference in equating accuracy between the two circle-arc methods was negligible. © 2010 by the National Council on Measurement in Education.","['score', 'equate', 'base', 'small', 'sample', 'examinee', 'inaccurate', 'examinee', 'population', 'conduct', 'series', 'resample', 'study', 'investigate', 'accuracy', 'method', 'equate', 'commonitem', 'design', 'method', 'chain', 'equipercentile', 'equating', 'smoothed', 'distribution', 'chain', 'linear', 'equating', 'chain', 'mean', 'equate', 'symmetric', 'circlearc', 'method', 'simplified', 'circlearc', 'method', 'operational', 'test', 'form', 'contain', '110', 'item', 'equating', 'newform', 'sample', '100', '50', '25', '10', 'examinee', 'referenceform', 'sample', 'time', 'large', 'Accuracy', 'describe', 'term', 'rootmeansquare', 'difference', '1000', 'replication', 'sample', 'equating', 'criterion', 'equate', 'overall', 'chain', 'mean', 'equating', 'produce', 'accurate', 'result', 'low', 'score', 'circlearc', 'method', 'produce', 'accurate', 'result', 'particularly', 'upper', 'half', 'score', 'distribution', 'difference', 'equate', 'accuracy', 'circlearc', 'method', 'negligible', '©', '2010', 'National', 'Council']","['comparison', 'small', 'sample', 'equate', 'method', 'commonitem', 'design']",score equate base small sample examinee inaccurate examinee population conduct series resample study investigate accuracy method equate commonitem design method chain equipercentile equating smoothed distribution chain linear equating chain mean equate symmetric circlearc method simplified circlearc method operational test form contain 110 item equating newform sample 100 50 25 10 examinee referenceform sample time large Accuracy describe term rootmeansquare difference 1000 replication sample equating criterion equate overall chain mean equating produce accurate result low score circlearc method produce accurate result particularly upper half score distribution difference equate accuracy circlearc method negligible © 2010 National Council,comparison small sample equate method commonitem design,0.888085564871625,0.02790063788210624,0.028017919681568804,0.027961929902904647,0.028033947661795223,0.0,0.0,0.0,0.1995195700927129,0.0
Kahraman N.; Thompson T.,Relating unidimensional IRT parameters to a multidimensional response space: A review of two alternative projection IRT models for scoring subscales,2011,48,"A practical concern for many existing tests is that subscore test lengths are too short to provide reliable and meaningful measurement. A possible method of improving the subscale reliability and validity would be to make use of collateral information provided by items from other subscales of the same test. To this end, the purpose of this article is to compare two different formulations of an alternative Item Response Theory (IRT) model developed to parameterize unidimensional projections of multidimensional test items: Analytical and Empirical formulations. Two real data applications are provided to illustrate how the projection IRT model can be used in practice, as well as to further examine how ability estimates from the projection IRT model compare to external examinee measures. The results suggest that collateral information extracted by a projection IRT model can be used to improve reliability and validity of subscale scores, which in turn can be used to provide diagnostic information about strength and weaknesses of examinees helping stakeholders to link instruction or curriculum to assessment results. Copyright © 2011 by the National Council on Measurement in Education.",Relating unidimensional IRT parameters to a multidimensional response space: A review of two alternative projection IRT models for scoring subscales,"A practical concern for many existing tests is that subscore test lengths are too short to provide reliable and meaningful measurement. A possible method of improving the subscale reliability and validity would be to make use of collateral information provided by items from other subscales of the same test. To this end, the purpose of this article is to compare two different formulations of an alternative Item Response Theory (IRT) model developed to parameterize unidimensional projections of multidimensional test items: Analytical and Empirical formulations. Two real data applications are provided to illustrate how the projection IRT model can be used in practice, as well as to further examine how ability estimates from the projection IRT model compare to external examinee measures. The results suggest that collateral information extracted by a projection IRT model can be used to improve reliability and validity of subscale scores, which in turn can be used to provide diagnostic information about strength and weaknesses of examinees helping stakeholders to link instruction or curriculum to assessment results. Copyright © 2011 by the National Council on Measurement in Education.","['practical', 'concern', 'exist', 'test', 'subscore', 'test', 'length', 'short', 'provide', 'reliable', 'meaningful', 'possible', 'method', 'improve', 'subscale', 'reliability', 'validity', 'collateral', 'information', 'provide', 'item', 'subscale', 'test', 'end', 'purpose', 'article', 'compare', 'different', 'formulation', 'alternative', 'Item', 'Response', 'Theory', 'IRT', 'develop', 'parameterize', 'unidimensional', 'projection', 'multidimensional', 'test', 'item', 'Analytical', 'empirical', 'formulation', 'real', 'datum', 'application', 'provide', 'illustrate', 'projection', 'IRT', 'practice', 'far', 'examine', 'ability', 'estimate', 'projection', 'IRT', 'compare', 'external', 'examinee', 'measure', 'result', 'suggest', 'collateral', 'information', 'extract', 'projection', 'IRT', 'improve', 'reliability', 'validity', 'subscale', 'score', 'turn', 'provide', 'diagnostic', 'information', 'strength', 'weakness', 'examinee', 'help', 'stakeholder', 'link', 'instruction', 'curriculum', 'assessment', 'result', 'copyright', '©', '2011', 'National', 'Council']","['relate', 'unidimensional', 'IRT', 'parameter', 'multidimensional', 'response', 'space', 'review', 'alternative', 'projection', 'IRT', 'score', 'subscale']",practical concern exist test subscore test length short provide reliable meaningful possible method improve subscale reliability validity collateral information provide item subscale test end purpose article compare different formulation alternative Item Response Theory IRT develop parameterize unidimensional projection multidimensional test item Analytical empirical formulation real datum application provide illustrate projection IRT practice far examine ability estimate projection IRT compare external examinee measure result suggest collateral information extract projection IRT improve reliability validity subscale score turn provide diagnostic information strength weakness examinee help stakeholder link instruction curriculum assessment result copyright © 2011 National Council,relate unidimensional IRT parameter multidimensional response space review alternative projection IRT score subscale,0.026843307913382967,0.02701346502241644,0.892068320868711,0.02700727197121229,0.027067634224277264,0.04846921447482525,0.05815275101481231,0.027954705539310176,0.0,0.0
Van Der Linden W.J.; Diao Q.,Automated test-form generation,2011,48,"In automated test assembly (ATA), the methodology of mixed-integer programming is used to select test items from an item bank to meet the specifications for a desired test form and optimize its measurement accuracy. The same methodology can be used to automate the formatting of the set of selected items into the actual test form. Three different cases are discussed: (i) computerized test forms in which the items are presented on a screen one at a time and only their optimal order has to be determined; (ii) paper forms in which the items need to be ordered and paginated and the typical goal is to minimize paper use; and (iii) published test forms with the same requirements but a more sophisticated layout (e.g., double-column print). For each case, a menu of possible test-form specifications is identified, and it is shown how they can be modeled as linear constraints using 0-1 decision variables. The methodology is demonstrated using two empirical examples. © 2011 by the National Council on Measurement in Education.",,"In automated test assembly (ATA), the methodology of mixed-integer programming is used to select test items from an item bank to meet the specifications for a desired test form and optimize its measurement accuracy. The same methodology can be used to automate the formatting of the set of selected items into the actual test form. Three different cases are discussed: (i) computerized test forms in which the items are presented on a screen one at a time and only their optimal order has to be determined; (ii) paper forms in which the items need to be ordered and paginated and the typical goal is to minimize paper use; and (iii) published test forms with the same requirements but a more sophisticated layout (e.g., double-column print). For each case, a menu of possible test-form specifications is identified, and it is shown how they can be modeled as linear constraints using 0-1 decision variables. The methodology is demonstrated using two empirical examples. © 2011 by the National Council on Measurement in Education.","['automate', 'test', 'assembly', 'ATA', 'methodology', 'mixedinteger', 'programming', 'select', 'test', 'item', 'item', 'bank', 'meet', 'specification', 'desire', 'test', 'form', 'optimize', 'accuracy', 'methodology', 'automate', 'formatting', 'set', 'select', 'item', 'actual', 'test', 'form', 'different', 'case', 'discuss', 'I', 'computerize', 'test', 'form', 'item', 'present', 'screen', 'time', 'optimal', 'order', 'determine', 'ii', 'paper', 'form', 'item', 'need', 'order', 'paginate', 'typical', 'goal', 'minimize', 'paper', 'iii', 'publish', 'test', 'form', 'requirement', 'sophisticated', 'layout', 'eg', 'doublecolumn', 'print', 'case', 'menu', 'possible', 'testform', 'specification', 'identify', 'linear', 'constraint', '01', 'decision', 'variable', 'methodology', 'demonstrate', 'empirical', 'example', '©', '2011', 'National', 'Council']",,automate test assembly ATA methodology mixedinteger programming select test item item bank meet specification desire test form optimize accuracy methodology automate formatting set select item actual test form different case discuss I computerize test form item present screen time optimal order determine ii paper form item need order paginate typical goal minimize paper iii publish test form requirement sophisticated layout eg doublecolumn print case menu possible testform specification identify linear constraint 01 decision variable methodology demonstrate empirical example © 2011 National Council,,0.8992888410289065,0.02512984923254001,0.025083119303159797,0.025113537144350835,0.02538465329104291,0.05622688779541226,0.01784142880628688,0.0,0.005677123917087854,0.0
Robusto E.; Stefanutti L.; Anselmi P.,The gain-loss model: A probabilistic skill multimap model for assessing learning processes,2010,47,"Within the theoretical framework of knowledge space theory, a probabilistic skill multimap model for assessing learning processes is proposed. The learning process of a student is modeled as a function of the student's knowledge and of an educational intervention on the attainment of specific skills required to solve problems in a knowledge domain. Model parameters are initial probabilities of the skills, effects of learning objects on gaining and losing the skills, and careless error and lucky guess probabilities of the problems. An empirical application shows that the model is effective in assessing knowledge and effectiveness of educational intervention at both classroom and student levels. Practical implications for teaching and learning are discussed. © 2010 by the National Council on Measurement in Education.",The gain-loss model: A probabilistic skill multimap model for assessing learning processes,"Within the theoretical framework of knowledge space theory, a probabilistic skill multimap model for assessing learning processes is proposed. The learning process of a student is modeled as a function of the student's knowledge and of an educational intervention on the attainment of specific skills required to solve problems in a knowledge domain. Model parameters are initial probabilities of the skills, effects of learning objects on gaining and losing the skills, and careless error and lucky guess probabilities of the problems. An empirical application shows that the model is effective in assessing knowledge and effectiveness of educational intervention at both classroom and student levels. Practical implications for teaching and learning are discussed. © 2010 by the National Council on Measurement in Education.","['theoretical', 'framework', 'knowledge', 'space', 'theory', 'probabilistic', 'skill', 'multimap', 'assess', 'learning', 'process', 'propose', 'learn', 'process', 'student', 'function', 'student', 'knowledge', 'educational', 'intervention', 'attainment', 'specific', 'skill', 'require', 'solve', 'problem', 'knowledge', 'domain', 'parameter', 'initial', 'probability', 'skill', 'effect', 'learn', 'object', 'gain', 'lose', 'skill', 'careless', 'error', 'lucky', 'guess', 'probability', 'problem', 'empirical', 'application', 'effective', 'assess', 'knowledge', 'effectiveness', 'educational', 'intervention', 'classroom', 'student', 'level', 'practical', 'implication', 'teaching', 'learning', 'discuss', '©', '2010', 'National', 'Council']","['gainloss', 'probabilistic', 'skill', 'multimap', 'assess', 'learn', 'process']",theoretical framework knowledge space theory probabilistic skill multimap assess learning process propose learn process student function student knowledge educational intervention attainment specific skill require solve problem knowledge domain parameter initial probability skill effect learn object gain lose skill careless error lucky guess probability problem empirical application effective assess knowledge effectiveness educational intervention classroom student level practical implication teaching learning discuss © 2010 National Council,gainloss probabilistic skill multimap assess learn process,0.8782741922228513,0.030129772960486374,0.03055011877333413,0.030391964926923573,0.030653951116404673,0.0,0.0,0.0951772274031104,0.0,0.0
Suh Y.; Bolt D.M.,A nested logit approach for investigating distractors as causes of differential item functioning,2011,48,"In multiple-choice items, differential item functioning (DIF) in the correct response may or may not be caused by differentially functioning distractors. Identifying distractors as causes of DIF can provide valuable information for potential item revision or the design of new test items. In this paper, we examine a two-step approach based on application of a nested logit model for this purpose. The approach separates testing of differential distractor functioning (DDF) from DIF, thus allowing for clearer evaluations of where distractors may be responsible for DIF. The approach is contrasted against competing methods and evaluated in simulation and real data analyses. © 2011 by the National Council on Measurement in Education.",A nested logit approach for investigating distractors as causes of differential item functioning,"In multiple-choice items, differential item functioning (DIF) in the correct response may or may not be caused by differentially functioning distractors. Identifying distractors as causes of DIF can provide valuable information for potential item revision or the design of new test items. In this paper, we examine a two-step approach based on application of a nested logit model for this purpose. The approach separates testing of differential distractor functioning (DDF) from DIF, thus allowing for clearer evaluations of where distractors may be responsible for DIF. The approach is contrasted against competing methods and evaluated in simulation and real data analyses. © 2011 by the National Council on Measurement in Education.","['multiplechoice', 'item', 'differential', 'item', 'function', 'DIF', 'correct', 'response', 'cause', 'differentially', 'function', 'distractor', 'identify', 'distractor', 'cause', 'DIF', 'provide', 'valuable', 'information', 'potential', 'item', 'revision', 'design', 'new', 'test', 'item', 'paper', 'examine', 'twostep', 'approach', 'base', 'application', 'nest', 'logit', 'purpose', 'approach', 'separate', 'testing', 'differential', 'distractor', 'function', 'DDF', 'DIF', 'allow', 'clear', 'evaluation', 'distractor', 'responsible', 'DIF', 'approach', 'contrast', 'compete', 'method', 'evaluate', 'simulation', 'real', 'datum', 'analyse', '©', '2011', 'National', 'Council']","['nest', 'logit', 'approach', 'investigate', 'distractor', 'cause', 'differential', 'item', 'function']",multiplechoice item differential item function DIF correct response cause differentially function distractor identify distractor cause DIF provide valuable information potential item revision design new test item paper examine twostep approach base application nest logit purpose approach separate testing differential distractor function DDF DIF allow clear evaluation distractor responsible DIF approach contrast compete method evaluate simulation real datum analyse © 2011 National Council,nest logit approach investigate distractor cause differential item function,0.03244389921558404,0.031766668576787296,0.03166420186156453,0.03189150846534898,0.8722337218807151,0.07432067515241705,0.0,0.0,0.0,0.0
Wang C.; Gierl M.J.,Using the attribute hierarchy method to make diagnostic inferences about examinees' cognitive skills in critical reading,2011,48,"The purpose of this study is to apply the attribute hierarchy method (AHM) to a subset of SAT critical reading items and illustrate how the method can be used to promote cognitive diagnostic inferences. The AHM is a psychometric procedure for classifying examinees' test item responses into a set of attribute mastery patterns associated with different components from a cognitive model. The study was conducted in two steps. In step 1, three cognitive models were developed by reviewing selected literature in reading comprehension as well as research related to SAT Critical Reading. Then, the cognitive models were validated by having a sample of students think aloud as they solved each item. In step 2, psychometric analyses were conducted on the SAT critical reading cognitive models by evaluating the model-data fit between the expected and observed response patterns produced from two random samples of 2,000 examinees who wrote the items. The model that provided best data-model fit was then used to calculate attribute probabilities for 15 examinees to illustrate our diagnostic testing procedure. Copyright © 2011 by the National Council on Measurement in Education.",Using the attribute hierarchy method to make diagnostic inferences about examinees' cognitive skills in critical reading,"The purpose of this study is to apply the attribute hierarchy method (AHM) to a subset of SAT critical reading items and illustrate how the method can be used to promote cognitive diagnostic inferences. The AHM is a psychometric procedure for classifying examinees' test item responses into a set of attribute mastery patterns associated with different components from a cognitive model. The study was conducted in two steps. In step 1, three cognitive models were developed by reviewing selected literature in reading comprehension as well as research related to SAT Critical Reading. Then, the cognitive models were validated by having a sample of students think aloud as they solved each item. In step 2, psychometric analyses were conducted on the SAT critical reading cognitive models by evaluating the model-data fit between the expected and observed response patterns produced from two random samples of 2,000 examinees who wrote the items. The model that provided best data-model fit was then used to calculate attribute probabilities for 15 examinees to illustrate our diagnostic testing procedure. Copyright © 2011 by the National Council on Measurement in Education.","['purpose', 'study', 'apply', 'attribute', 'hierarchy', 'method', 'AHM', 'subset', 'SAT', 'critical', 'reading', 'item', 'illustrate', 'method', 'promote', 'cognitive', 'diagnostic', 'inference', 'AHM', 'psychometric', 'procedure', 'classify', 'examine', 'test', 'item', 'response', 'set', 'attribute', 'mastery', 'pattern', 'associate', 'different', 'component', 'cognitive', 'study', 'conduct', 'step', 'step', '1', 'cognitive', 'develop', 'review', 'select', 'literature', 'read', 'comprehension', 'research', 'relate', 'SAT', 'Critical', 'reading', 'cognitive', 'validate', 'sample', 'student', 'think', 'aloud', 'solve', 'item', 'step', '2', 'psychometric', 'analysis', 'conduct', 'SAT', 'critical', 'read', 'cognitive', 'evaluate', 'modeldata', 'fit', 'expect', 'observe', 'response', 'pattern', 'produce', 'random', 'sample', '2000', 'examinee', 'write', 'item', 'provide', 'good', 'datamodel', 'fit', 'calculate', 'attribute', 'probability', '15', 'examinee', 'illustrate', 'diagnostic', 'testing', 'procedure', 'copyright', '©', '2011', 'National', 'Council']","['attribute', 'hierarchy', 'method', 'diagnostic', 'inference', 'examine', 'cognitive', 'skill', 'critical', 'reading']",purpose study apply attribute hierarchy method AHM subset SAT critical reading item illustrate method promote cognitive diagnostic inference AHM psychometric procedure classify examine test item response set attribute mastery pattern associate different component cognitive study conduct step step 1 cognitive develop review select literature read comprehension research relate SAT Critical reading cognitive validate sample student think aloud solve item step 2 psychometric analysis conduct SAT critical read cognitive evaluate modeldata fit expect observe response pattern produce random sample 2000 examinee write item provide good datamodel fit calculate attribute probability 15 examinee illustrate diagnostic testing procedure copyright © 2011 National Council,attribute hierarchy method diagnostic inference examine cognitive skill critical reading,0.025109999108269592,0.025204230696272593,0.02506330679044881,0.025218710101415062,0.8994037533035938,0.04704732238998781,0.0042873999354381,0.03741149504046061,0.0,0.0
Sinharay S.,How often do subscores have added value? Results from operational and simulated data,2010,47,"Recently, there has been an increasing level of interest in subscores for their potential diagnostic value. Haberman suggested a method based on classical test theory to determine whether subscores have added value over total scores. In this article I first provide a rich collection of results regarding when subscores were found to have added value for several operational data sets. Following that I provide results from a detailed simulation study that examines what properties subscores should possess in order to have added value. The results indicate that subscores have to satisfy strict standards of reliability and correlation to have added value. A weighted average of the subscore and the total score was found to have added value more often. © 2010 by the National Council on Measurement in Education.",How often do subscores have added value? Results from operational and simulated data,"Recently, there has been an increasing level of interest in subscores for their potential diagnostic value. Haberman suggested a method based on classical test theory to determine whether subscores have added value over total scores. In this article I first provide a rich collection of results regarding when subscores were found to have added value for several operational data sets. Following that I provide results from a detailed simulation study that examines what properties subscores should possess in order to have added value. The results indicate that subscores have to satisfy strict standards of reliability and correlation to have added value. A weighted average of the subscore and the total score was found to have added value more often. © 2010 by the National Council on Measurement in Education.","['recently', 'increase', 'level', 'interest', 'subscore', 'potential', 'diagnostic', 'value', 'Haberman', 'suggest', 'method', 'base', 'classical', 'test', 'theory', 'determine', 'subscore', 'add', 'value', 'total', 'score', 'article', 'I', 'provide', 'rich', 'collection', 'result', 'regard', 'subscore', 'find', 'add', 'value', 'operational', 'data', 'set', 'follow', 'I', 'provide', 'result', 'detailed', 'simulation', 'study', 'examine', 'property', 'subscore', 'possess', 'order', 'add', 'value', 'result', 'indicate', 'subscore', 'satisfy', 'strict', 'standard', 'reliability', 'correlation', 'add', 'value', 'weighted', 'average', 'subscore', 'total', 'score', 'find', 'add', 'value', '©', '2010', 'National', 'Council']","['subscore', 'add', 'value', 'result', 'operational', 'simulated', 'datum']",recently increase level interest subscore potential diagnostic value Haberman suggest method base classical test theory determine subscore add value total score article I provide rich collection result regard subscore find add value operational data set follow I provide result detailed simulation study examine property subscore possess order add value result indicate subscore satisfy strict standard reliability correlation add value weighted average subscore total score find add value © 2010 National Council,subscore add value result operational simulated datum,0.034313334327702005,0.8624066717620646,0.034400273779650395,0.034231115890625524,0.034648604239957595,0.0,0.3348362109218164,0.0,0.0,0.00013758155838386227
de la Torre J.; Hong Y.; Deng W.,Factors affecting the item parameter estimation and classification accuracy of the DINA model,2010,47,"To better understand the statistical properties of the deterministic inputs, noisy "" and"" gate cognitive diagnosis (DINA) model, the impact of several factors on the quality of the item parameter estimates and classification accuracy was investigated. Results of the simulation study indicate that the fully Bayes approach is most accurate when the prior distribution matches the latent class structure. However, when the latent classes are of indefinite structure, the empirical Bayes method in conjunction with an unstructured prior distribution provides much better estimates and classification accuracy. Moreover, using empirical Bayes with an unstructured prior does not lead to extremely poor results as other prior-estimation method combinations do. The simulation results also show that increasing the sample size reduces the variability, and to some extent the bias, of item parameter estimates, whereas lower level of guessing and slip parameter is associated with higher quality item parameter estimation and classification accuracy. © 2010 by the National Council on Measurement in Education.",Factors affecting the item parameter estimation and classification accuracy of the DINA model,"To better understand the statistical properties of the deterministic inputs, noisy "" and"" gate cognitive diagnosis (DINA) model, the impact of several factors on the quality of the item parameter estimates and classification accuracy was investigated. Results of the simulation study indicate that the fully Bayes approach is most accurate when the prior distribution matches the latent class structure. However, when the latent classes are of indefinite structure, the empirical Bayes method in conjunction with an unstructured prior distribution provides much better estimates and classification accuracy. Moreover, using empirical Bayes with an unstructured prior does not lead to extremely poor results as other prior-estimation method combinations do. The simulation results also show that increasing the sample size reduces the variability, and to some extent the bias, of item parameter estimates, whereas lower level of guessing and slip parameter is associated with higher quality item parameter estimation and classification accuracy. © 2010 by the National Council on Measurement in Education.","['understand', 'statistical', 'property', 'deterministic', 'input', 'noisy', 'gate', 'cognitive', 'diagnosis', 'DINA', 'impact', 'factor', 'quality', 'item', 'parameter', 'estimate', 'classification', 'accuracy', 'investigate', 'result', 'simulation', 'study', 'indicate', 'fully', 'Bayes', 'approach', 'accurate', 'prior', 'distribution', 'match', 'latent', 'class', 'structure', 'latent', 'class', 'indefinite', 'structure', 'empirical', 'Bayes', 'method', 'conjunction', 'unstructured', 'prior', 'distribution', 'provide', 'estimate', 'classification', 'accuracy', 'empirical', 'Bayes', 'unstructured', 'prior', 'lead', 'extremely', 'poor', 'result', 'priorestimation', 'method', 'combination', 'simulation', 'result', 'increase', 'sample', 'size', 'reduce', 'variability', 'extent', 'bias', 'item', 'parameter', 'estimate', 'low', 'level', 'guessing', 'slip', 'parameter', 'associate', 'high', 'quality', 'item', 'parameter', 'estimation', 'classification', 'accuracy', '©', '2010', 'National', 'Council']","['factor', 'affect', 'item', 'parameter', 'estimation', 'classification', 'accuracy', 'DINA']",understand statistical property deterministic input noisy gate cognitive diagnosis DINA impact factor quality item parameter estimate classification accuracy investigate result simulation study indicate fully Bayes approach accurate prior distribution match latent class structure latent class indefinite structure empirical Bayes method conjunction unstructured prior distribution provide estimate classification accuracy empirical Bayes unstructured prior lead extremely poor result priorestimation method combination simulation result increase sample size reduce variability extent bias item parameter estimate low level guessing slip parameter associate high quality item parameter estimation classification accuracy © 2010 National Council,factor affect item parameter estimation classification accuracy DINA,0.025978988480446948,0.025879330376385817,0.025913838599516383,0.026016268173495262,0.8962115743701555,0.0739711826581679,0.010905448660691633,0.0,0.00368841810975355,0.007995988768640425
Livingston S.A.; Kim S.,Random-groups equating with samples of 50 to 400 test takers,2010,47,"Five methods for equating in a random groups design were investigated in a series of resampling studies with samples of 400, 200, 100, and 50 test takers. Six operational test forms, each taken by 9,000 or more test takers, were used as item pools to construct pairs of forms to be equated. The criterion equating was the direct equipercentile equating in the group of all test takers. Equating accuracy was indicated by the root-mean-squared deviation, over 1,000 replications, of the sample equatings from the criterion equating. The methods investigated were equipercentile equating of smoothed distributions, linear equating, mean equating, symmetric circle-arc equating, and simplified circle-arc equating. The circle-arc methods produced the most accurate results for all sample sizes investigated, particularly in the upper half of the score distribution. The difference in equating accuracy between the two circle-arc methods was negligible. © 2010 by the National Council on Measurement in Education.",Random-groups equating with samples of 50 to 400 test takers,"Five methods for equating in a random groups design were investigated in a series of resampling studies with samples of 400, 200, 100, and 50 test takers. Six operational test forms, each taken by 9,000 or more test takers, were used as item pools to construct pairs of forms to be equated. The criterion equating was the direct equipercentile equating in the group of all test takers. Equating accuracy was indicated by the root-mean-squared deviation, over 1,000 replications, of the sample equatings from the criterion equating. The methods investigated were equipercentile equating of smoothed distributions, linear equating, mean equating, symmetric circle-arc equating, and simplified circle-arc equating. The circle-arc methods produced the most accurate results for all sample sizes investigated, particularly in the upper half of the score distribution. The difference in equating accuracy between the two circle-arc methods was negligible. © 2010 by the National Council on Measurement in Education.","['method', 'equate', 'random', 'group', 'design', 'investigate', 'series', 'resample', 'study', 'sample', '400', '200', '100', '50', 'test', 'taker', 'operational', 'test', 'form', '9000', 'test', 'taker', 'item', 'pool', 'construct', 'pair', 'form', 'equate', 'criterion', 'equating', 'direct', 'equipercentile', 'equating', 'group', 'test', 'taker', 'Equating', 'accuracy', 'indicate', 'rootmeansquare', 'deviation', '1000', 'replication', 'sample', 'equating', 'criterion', 'equate', 'method', 'investigate', 'equipercentile', 'equating', 'smoothed', 'distribution', 'linear', 'equate', 'mean', 'equate', 'symmetric', 'circlearc', 'equate', 'simplify', 'circlearc', 'equate', 'circlearc', 'method', 'produce', 'accurate', 'result', 'sample', 'size', 'investigate', 'particularly', 'upper', 'half', 'score', 'distribution', 'difference', 'equate', 'accuracy', 'circlearc', 'method', 'negligible', '©', '2010', 'National', 'Council']","['randomgroup', 'equate', 'sample', '50', '400', 'test', 'taker']",method equate random group design investigate series resample study sample 400 200 100 50 test taker operational test form 9000 test taker item pool construct pair form equate criterion equating direct equipercentile equating group test taker Equating accuracy indicate rootmeansquare deviation 1000 replication sample equating criterion equate method investigate equipercentile equating smoothed distribution linear equate mean equate symmetric circlearc equate simplify circlearc equate circlearc method produce accurate result sample size investigate particularly upper half score distribution difference equate accuracy circlearc method negligible © 2010 National Council,randomgroup equate sample 50 400 test taker,0.8716791862765475,0.03197847602364506,0.03208310360638641,0.032195893899616315,0.03206334019380457,0.0,0.0,0.0,0.21532545678401072,0.0
Leckie G.; Baird J.-A.,"Rater effects on essay scoring: A multilevel analysis of severity drift, central tendency, and rater experience",2011,48,"This study examined rater effects on essay scoring in an operational monitoring system from England's 2008 national curriculum English writing test for 14-year-olds. We fitted two multilevel models and analyzed: (1) drift in rater severity effects over time; (2) rater central tendency effects; and (3) differences in rater severity and central tendency effects by raters' previous rating experience. We found no significant evidence of rater drift and, while raters with less experience appeared more severe than raters with more experience, this result also was not significant. However, we did find that there was a central tendency to raters' scoring. We also found that rater severity was significantly unstable over time. We discuss the theoretical and practical questions that our findings raise. © 2011 by the National Council on Measurement in Education.","Rater effects on essay scoring: A multilevel analysis of severity drift, central tendency, and rater experience","This study examined rater effects on essay scoring in an operational monitoring system from England's 2008 national curriculum English writing test for 14-year-olds. We fitted two multilevel models and analyzed: (1) drift in rater severity effects over time; (2) rater central tendency effects; and (3) differences in rater severity and central tendency effects by raters' previous rating experience. We found no significant evidence of rater drift and, while raters with less experience appeared more severe than raters with more experience, this result also was not significant. However, we did find that there was a central tendency to raters' scoring. We also found that rater severity was significantly unstable over time. We discuss the theoretical and practical questions that our findings raise. © 2011 by the National Council on Measurement in Education.","['study', 'examine', 'rater', 'effect', 'essay', 'scoring', 'operational', 'monitoring', 'system', 'Englands', '2008', 'national', 'curriculum', 'English', 'writing', 'test', '14yearold', 'fit', 'multilevel', 'analyze', '1', 'drift', 'rater', 'severity', 'effect', 'time', '2', 'rater', 'central', 'tendency', 'effect', '3', 'difference', 'rater', 'severity', 'central', 'tendency', 'effect', 'rater', 'previous', 'rating', 'experience', 'find', 'significant', 'evidence', 'rater', 'drift', 'rater', 'experience', 'appear', 'severe', 'rater', 'experience', 'result', 'significant', 'find', 'central', 'tendency', 'rater', 'score', 'find', 'rater', 'severity', 'significantly', 'unstable', 'time', 'discuss', 'theoretical', 'practical', 'question', 'finding', 'raise', '©', '2011', 'National', 'Council']","['Rater', 'effect', 'essay', 'scoring', 'multilevel', 'analysis', 'severity', 'drift', 'central', 'tendency', 'rater', 'experience']",study examine rater effect essay scoring operational monitoring system Englands 2008 national curriculum English writing test 14yearold fit multilevel analyze 1 drift rater severity effect time 2 rater central tendency effect 3 difference rater severity central tendency effect rater previous rating experience find significant evidence rater drift rater experience appear severe rater experience result significant find central tendency rater score find rater severity significantly unstable time discuss theoretical practical question finding raise © 2011 National Council,Rater effect essay scoring multilevel analysis severity drift central tendency rater experience,0.03444444463781722,0.8634478079490245,0.0340993155895537,0.033841617539329225,0.03416681428427546,0.0,0.0,0.0,0.0,0.2675034958567697
Wang C.; Chang H.-H.; Huebner A.,Restrictive stochastic item selection methods in cognitive diagnostic computerized adaptive testing,2011,48,"This paper proposes two new item selection methods for cognitive diagnostic computerized adaptive testing: the restrictive progressive method and the restrictive threshold method. They are built upon the posterior weighted Kullback-Leibler (KL) information index but include additional stochastic components either in the item selection index or in the item selection procedure. Simulation studies show that both methods are successful at simultaneously suppressing overexposed items and increasing the usage of underexposed items. Compared to item selection based upon (1) pure KL information and (2) the Sympson-Hetter method, the two new methods strike a better balance between item exposure control and measurement accuracy. The two new methods are also compared with progressive method and proportional method. © 2011 by the National Council on Measurement in Education.",Restrictive stochastic item selection methods in cognitive diagnostic computerized adaptive testing,"This paper proposes two new item selection methods for cognitive diagnostic computerized adaptive testing: the restrictive progressive method and the restrictive threshold method. They are built upon the posterior weighted Kullback-Leibler (KL) information index but include additional stochastic components either in the item selection index or in the item selection procedure. Simulation studies show that both methods are successful at simultaneously suppressing overexposed items and increasing the usage of underexposed items. Compared to item selection based upon (1) pure KL information and (2) the Sympson-Hetter method, the two new methods strike a better balance between item exposure control and measurement accuracy. The two new methods are also compared with progressive method and proportional method. © 2011 by the National Council on Measurement in Education.","['paper', 'propose', 'new', 'item', 'selection', 'method', 'cognitive', 'diagnostic', 'computerized', 'adaptive', 'testing', 'restrictive', 'progressive', 'method', 'restrictive', 'threshold', 'method', 'build', 'posterior', 'weight', 'KullbackLeibler', 'KL', 'information', 'index', 'include', 'additional', 'stochastic', 'component', 'item', 'selection', 'index', 'item', 'selection', 'procedure', 'Simulation', 'study', 'method', 'successful', 'simultaneously', 'suppress', 'overexposed', 'item', 'increase', 'usage', 'underexposed', 'item', 'compare', 'item', 'selection', 'base', '1', 'pure', 'KL', 'information', '2', 'SympsonHetter', 'method', 'new', 'method', 'strike', 'balance', 'item', 'exposure', 'control', 'accuracy', 'new', 'method', 'compare', 'progressive', 'method', 'proportional', 'method', '©', '2011', 'National', 'Council']","['restrictive', 'stochastic', 'item', 'selection', 'method', 'cognitive', 'diagnostic', 'computerized', 'adaptive', 'testing']",paper propose new item selection method cognitive diagnostic computerized adaptive testing restrictive progressive method restrictive threshold method build posterior weight KullbackLeibler KL information index include additional stochastic component item selection index item selection procedure Simulation study method successful simultaneously suppress overexposed item increase usage underexposed item compare item selection base 1 pure KL information 2 SympsonHetter method new method strike balance item exposure control accuracy new method compare progressive method proportional method © 2011 National Council,restrictive stochastic item selection method cognitive diagnostic computerized adaptive testing,0.8836088389896815,0.02932014974425963,0.028822533430467926,0.029068497997582477,0.029179979838008522,0.07323833526972699,0.005843020280870083,0.0,0.012168806746663673,0.0
Puhan G.,Futility of log-linear smoothing when equating with unrepresentative small samples,2011,48,"The impact of log-linear presmoothing on the accuracy of small sample chained equipercentile equating was evaluated under two conditions. In the first condition the small samples differed randomly in ability from the target population. In the second condition the small samples were systematically different from the target population. Results showed that equating with small samples (e.g., N < 25 or 50) using either raw or smoothed score distributions led to considerable large random equating error (although smoothing reduced random equating error). Moreover, when the small samples were not representative of the target population, the amount of equating bias also was quite large. It is concluded that although presmoothing can reduce random equating error, it is not likely to reduce equating bias caused by using an unrepresentative sample. Other alternatives to the small sample equating problem (e.g., the SiGNET design) which focus more on improving data collection are discussed. © 2011 by the National Council on Measurement in Education.",Futility of log-linear smoothing when equating with unrepresentative small samples,"The impact of log-linear presmoothing on the accuracy of small sample chained equipercentile equating was evaluated under two conditions. In the first condition the small samples differed randomly in ability from the target population. In the second condition the small samples were systematically different from the target population. Results showed that equating with small samples (e.g., N < 25 or 50) using either raw or smoothed score distributions led to considerable large random equating error (although smoothing reduced random equating error). Moreover, when the small samples were not representative of the target population, the amount of equating bias also was quite large. It is concluded that although presmoothing can reduce random equating error, it is not likely to reduce equating bias caused by using an unrepresentative sample. Other alternatives to the small sample equating problem (e.g., the SiGNET design) which focus more on improving data collection are discussed. © 2011 by the National Council on Measurement in Education.","['impact', 'loglinear', 'presmoothe', 'accuracy', 'small', 'sample', 'chain', 'equipercentile', 'equating', 'evaluate', 'condition', 'condition', 'small', 'sample', 'differ', 'randomly', 'ability', 'target', 'population', 'second', 'condition', 'small', 'sample', 'systematically', 'different', 'target', 'population', 'result', 'equate', 'small', 'sample', 'eg', 'N', '25', '50', 'raw', 'smoothed', 'score', 'distribution', 'lead', 'considerable', 'large', 'random', 'equating', 'error', 'smooth', 'reduce', 'random', 'equating', 'error', 'small', 'sample', 'representative', 'target', 'population', 'equate', 'bias', 'large', 'conclude', 'presmoothing', 'reduce', 'random', 'equating', 'error', 'likely', 'reduce', 'equate', 'bias', 'cause', 'unrepresentative', 'sample', 'alternative', 'small', 'sample', 'equating', 'problem', 'eg', 'SiGNET', 'design', 'focus', 'improve', 'datum', 'collection', 'discuss', '©', '2011', 'National', 'Council']","['futility', 'loglinear', 'smooth', 'equate', 'unrepresentative', 'small', 'sample']",impact loglinear presmoothe accuracy small sample chain equipercentile equating evaluate condition condition small sample differ randomly ability target population second condition small sample systematically different target population result equate small sample eg N 25 50 raw smoothed score distribution lead considerable large random equating error smooth reduce random equating error small sample representative target population equate bias large conclude presmoothing reduce random equating error likely reduce equate bias cause unrepresentative sample alternative small sample equating problem eg SiGNET design focus improve datum collection discuss © 2011 National Council,futility loglinear smooth equate unrepresentative small sample,0.8832444993580603,0.02904853308352372,0.029230465383175322,0.029221988369242415,0.029254513805998382,0.003579611488555783,0.0,0.0,0.12530316373492179,0.001158912627132117
Zwick R.; Himelfarb I.,The effect of high school socioeconomic status on the predictive validity of SAT scores and high school grade-point average,2011,48,"Research has often found that, when high school grades and SAT scores are used to predict first-year college grade-point average (FGPA) via regression analysis, African-American and Latino students, are, on average, predicted to earn higher FGPAs than they actually do. Under various plausible models, this phenomenon can be explained in terms of the unreliability of predictor variables. Attributing overprediction to measurement error, however, is not fully satisfactory: Might the measurement errors in the predictor variables be systematic in part, and could they be reduced? The research hypothesis in the current study was that the overprediction of Latino and African-American performance occurs, at least in part, because these students are more likely than White students to attend high schools with fewer resources. The study provided some support for this hypothesis and showed that the prediction of college grades can be improved using information about high school socioeconomic status. An interesting peripheral finding was that grades provided by students' high schools were stronger predictors of FGPA than were students' self-reported high school grades. Correlations between the two types of high school grades (computed for each of 18 colleges) ranged from .59 to .85. © 2011 by the National Council on Measurement in Education.",The effect of high school socioeconomic status on the predictive validity of SAT scores and high school grade-point average,"Research has often found that, when high school grades and SAT scores are used to predict first-year college grade-point average (FGPA) via regression analysis, African-American and Latino students, are, on average, predicted to earn higher FGPAs than they actually do. Under various plausible models, this phenomenon can be explained in terms of the unreliability of predictor variables. Attributing overprediction to measurement error, however, is not fully satisfactory: Might the measurement errors in the predictor variables be systematic in part, and could they be reduced? The research hypothesis in the current study was that the overprediction of Latino and African-American performance occurs, at least in part, because these students are more likely than White students to attend high schools with fewer resources. The study provided some support for this hypothesis and showed that the prediction of college grades can be improved using information about high school socioeconomic status. An interesting peripheral finding was that grades provided by students' high schools were stronger predictors of FGPA than were students' self-reported high school grades. Correlations between the two types of high school grades (computed for each of 18 colleges) ranged from .59 to .85. © 2011 by the National Council on Measurement in Education.","['research', 'find', 'high', 'school', 'grade', 'SAT', 'score', 'predict', 'firstyear', 'college', 'gradepoint', 'average', 'fgpa', 'regression', 'analysis', 'AfricanAmerican', 'latino', 'student', 'average', 'predict', 'earn', 'high', 'fgpa', 'actually', 'plausible', 'phenomenon', 'explain', 'term', 'unreliability', 'predictor', 'variable', 'attribute', 'overprediction', 'error', 'fully', 'satisfactory', 'error', 'predictor', 'variable', 'systematic', 'reduce', 'research', 'hypothesis', 'current', 'study', 'overprediction', 'Latino', 'africanamerican', 'performance', 'occur', 'student', 'likely', 'white', 'student', 'attend', 'high', 'school', 'resource', 'study', 'provide', 'support', 'hypothesis', 'prediction', 'college', 'grade', 'improve', 'information', 'high', 'school', 'socioeconomic', 'status', 'interesting', 'peripheral', 'finding', 'grade', 'provide', 'student', 'high', 'school', 'strong', 'predictor', 'FGPA', 'student', 'selfreporte', 'high', 'school', 'grade', 'Correlations', 'type', 'high', 'school', 'grade', 'compute', '18', 'college', 'range', '59', '85', '©', '2011', 'National', 'Council']","['effect', 'high', 'school', 'socioeconomic', 'status', 'predictive', 'validity', 'SAT', 'score', 'high', 'school', 'gradepoint', 'average']",research find high school grade SAT score predict firstyear college gradepoint average fgpa regression analysis AfricanAmerican latino student average predict earn high fgpa actually plausible phenomenon explain term unreliability predictor variable attribute overprediction error fully satisfactory error predictor variable systematic reduce research hypothesis current study overprediction Latino africanamerican performance occur student likely white student attend high school resource study provide support hypothesis prediction college grade improve information high school socioeconomic status interesting peripheral finding grade provide student high school strong predictor FGPA student selfreporte high school grade Correlations type high school grade compute 18 college range 59 85 © 2011 National Council,effect high school socioeconomic status predictive validity SAT score high school gradepoint average,0.027209612725022984,0.027198694658852655,0.889531952091425,0.02803295301404111,0.028026787510658197,0.0,0.00745509626100744,0.07226480273461133,0.0,0.0
Sinharay S.; Holland P.W.,A new approach to comparing several equating methods in the context of the NEAT design,2010,47,"The nonequivalent groups with anchor test (NEAT) design involves missing data that are missing by design. Three equating methods that can be used with a NEAT design are the frequency estimation equipercentile equating method, the chain equipercentile equating method, and the item-response-theory observed-score-equating method. We suggest an approach to perform a fair comparison of the three methods. The approach is then applied to compare the three equating methods using three data sets from operational tests. For each data set, we examine how the three equating methods perform when the missing data satisfy the assumptions made by only one of these equating methods. The chain equipercentile equating method is somewhat more satisfactory overall than the other methods. © 2010 by the National Council on Measurement in Education.",A new approach to comparing several equating methods in the context of the NEAT design,"The nonequivalent groups with anchor test (NEAT) design involves missing data that are missing by design. Three equating methods that can be used with a NEAT design are the frequency estimation equipercentile equating method, the chain equipercentile equating method, and the item-response-theory observed-score-equating method. We suggest an approach to perform a fair comparison of the three methods. The approach is then applied to compare the three equating methods using three data sets from operational tests. For each data set, we examine how the three equating methods perform when the missing data satisfy the assumptions made by only one of these equating methods. The chain equipercentile equating method is somewhat more satisfactory overall than the other methods. © 2010 by the National Council on Measurement in Education.","['nonequivalent', 'group', 'anchor', 'test', 'NEAT', 'design', 'involve', 'miss', 'datum', 'miss', 'design', 'equate', 'method', 'NEAT', 'design', 'frequency', 'estimation', 'equipercentile', 'equating', 'method', 'chain', 'equipercentile', 'equate', 'method', 'itemresponsetheory', 'observedscoreequate', 'method', 'suggest', 'approach', 'perform', 'fair', 'comparison', 'method', 'approach', 'apply', 'compare', 'equate', 'method', 'datum', 'set', 'operational', 'test', 'datum', 'set', 'examine', 'equate', 'method', 'perform', 'miss', 'datum', 'satisfy', 'assumption', 'equate', 'method', 'chain', 'equipercentile', 'equate', 'method', 'somewhat', 'satisfactory', 'overall', 'method', '©', '2010', 'National', 'Council']","['new', 'approach', 'compare', 'equate', 'method', 'context', 'NEAT', 'design']",nonequivalent group anchor test NEAT design involve miss datum miss design equate method NEAT design frequency estimation equipercentile equating method chain equipercentile equate method itemresponsetheory observedscoreequate method suggest approach perform fair comparison method approach apply compare equate method datum set operational test datum set examine equate method perform miss datum satisfy assumption equate method chain equipercentile equate method somewhat satisfactory overall method © 2010 National Council,new approach compare equate method context NEAT design,0.8583308971807833,0.03525676119929486,0.03565066851906403,0.035592442901831445,0.03516923019902657,0.004486983874682174,0.0,0.005205893146826667,0.18371106200433196,0.0
Zu J.; Yuan K.-H.,Standard Error of Linear Observed-Score Equating for the NEAT Design With Nonnormally Distributed Data,2012,49,"In the nonequivalent groups with anchor test (NEAT) design, the standard error of linear observed-score equating is commonly estimated by an estimator derived assuming multivariate normality. However, real data are seldom normally distributed, causing this normal estimator to be inconsistent. A general estimator, which does not rely on the normality assumption, would be preferred, because it is asymptotically accurate regardless of the distribution of the data. In this article, an analytical formula for the standard error of linear observed-score equating, which characterizes the effect of nonnormality, is obtained under elliptical distributions. Using three large-scale real data sets as the populations, resampling studies are conducted to empirically evaluate the normal and general estimators of the standard error of linear observed-score equating. The effect of sample size (50, 100, 250, or 500) and equating method (chained linear, Tucker, or Levine observed-score equating) are examined. Results suggest that the general estimator has smaller bias than the normal estimator in all 36 conditions; it has larger standard error when the sample size is at least 100; and it has smaller root mean squared error in all but one condition. An R program is also provided to facilitate the use of the general estimator. © 2012 by the National Council on Measurement in Education.",Standard Error of Linear Observed-Score Equating for the NEAT Design With Nonnormally Distributed Data,"In the nonequivalent groups with anchor test (NEAT) design, the standard error of linear observed-score equating is commonly estimated by an estimator derived assuming multivariate normality. However, real data are seldom normally distributed, causing this normal estimator to be inconsistent. A general estimator, which does not rely on the normality assumption, would be preferred, because it is asymptotically accurate regardless of the distribution of the data. In this article, an analytical formula for the standard error of linear observed-score equating, which characterizes the effect of nonnormality, is obtained under elliptical distributions. Using three large-scale real data sets as the populations, resampling studies are conducted to empirically evaluate the normal and general estimators of the standard error of linear observed-score equating. The effect of sample size (50, 100, 250, or 500) and equating method (chained linear, Tucker, or Levine observed-score equating) are examined. Results suggest that the general estimator has smaller bias than the normal estimator in all 36 conditions; it has larger standard error when the sample size is at least 100; and it has smaller root mean squared error in all but one condition. An R program is also provided to facilitate the use of the general estimator. © 2012 by the National Council on Measurement in Education.","['nonequivalent', 'group', 'anchor', 'test', 'NEAT', 'design', 'standard', 'error', 'linear', 'observedscore', 'equating', 'commonly', 'estimate', 'estimator', 'derive', 'assume', 'multivariate', 'normality', 'However', 'real', 'datum', 'seldom', 'normally', 'distribute', 'cause', 'normal', 'estimator', 'inconsistent', 'general', 'estimator', 'rely', 'normality', 'assumption', 'prefer', 'asymptotically', 'accurate', 'regardless', 'distribution', 'datum', 'article', 'analytical', 'formula', 'standard', 'error', 'linear', 'observedscore', 'equating', 'characterize', 'effect', 'nonnormality', 'obtain', 'elliptical', 'distribution', 'largescale', 'real', 'data', 'set', 'population', 'resample', 'study', 'conduct', 'empirically', 'evaluate', 'normal', 'general', 'estimator', 'standard', 'error', 'linear', 'observedscore', 'equate', 'effect', 'sample', 'size', '50', '100', '250', '500', 'equate', 'method', 'chain', 'linear', 'Tucker', 'Levine', 'observedscore', 'equating', 'examine', 'result', 'suggest', 'general', 'estimator', 'small', 'bias', 'normal', 'estimator', '36', 'condition', 'large', 'standard', 'error', 'sample', 'size', '100', 'small', 'root', 'mean', 'square', 'error', 'condition', 'r', 'program', 'provide', 'facilitate', 'general', 'estimator', '©', '2012', 'National', 'Council']","['Standard', 'Error', 'Linear', 'ObservedScore', 'Equating', 'NEAT', 'Design', 'Nonnormally', 'distribute', 'Data']",nonequivalent group anchor test NEAT design standard error linear observedscore equating commonly estimate estimator derive assume multivariate normality However real datum seldom normally distribute cause normal estimator inconsistent general estimator rely normality assumption prefer asymptotically accurate regardless distribution datum article analytical formula standard error linear observedscore equating characterize effect nonnormality obtain elliptical distribution largescale real data set population resample study conduct empirically evaluate normal general estimator standard error linear observedscore equate effect sample size 50 100 250 500 equate method chain linear Tucker Levine observedscore equating examine result suggest general estimator small bias normal estimator 36 condition large standard error sample size 100 small root mean square error condition r program provide facilitate general estimator © 2012 National Council,Standard Error Linear ObservedScore Equating NEAT Design Nonnormally distribute Data,0.8993088624849797,0.02502527798519925,0.025184180694302363,0.02531158804475069,0.02517009079076803,0.009707459920115385,0.0,0.0,0.12539214678732039,0.00692780181208052
Moses T.; Holland P.W.,The effects of selection strategies for bivariate loglinear smoothing models on neat equating Functions,2010,47,"In this study, eight statistical strategies were evaluated for selecting the parameterizations of loglinear models for smoothing the bivariate test score distributions used in nonequivalent groups with anchor test (NEAT) equating. Four of the strategies were based on significance tests of chi-square statistics (Likelihood Ratio, Pearson, Freeman-Tukey, and Cressie-Read) and four additional strategies were based on different evaluations of the Likelihood Ratio Chi-Square statistic (Akaike Information Criterion, Bayesian Information Criterion, Consistent Akaike Information Criterion, and an index traced to Goodman). The focus was the implications of the selection strategies' selection tendencies for the accuracy of chained and poststratification equating functions. The results differentiated the strategies in terms of their tendencies to select models with particular bivariate parameterizations and the implications of these tendencies for equating bias and variability. © 2010 by the National Council on Measurement in Education.",The effects of selection strategies for bivariate loglinear smoothing models on neat equating Functions,"In this study, eight statistical strategies were evaluated for selecting the parameterizations of loglinear models for smoothing the bivariate test score distributions used in nonequivalent groups with anchor test (NEAT) equating. Four of the strategies were based on significance tests of chi-square statistics (Likelihood Ratio, Pearson, Freeman-Tukey, and Cressie-Read) and four additional strategies were based on different evaluations of the Likelihood Ratio Chi-Square statistic (Akaike Information Criterion, Bayesian Information Criterion, Consistent Akaike Information Criterion, and an index traced to Goodman). The focus was the implications of the selection strategies' selection tendencies for the accuracy of chained and poststratification equating functions. The results differentiated the strategies in terms of their tendencies to select models with particular bivariate parameterizations and the implications of these tendencies for equating bias and variability. © 2010 by the National Council on Measurement in Education.","['study', 'statistical', 'strategy', 'evaluate', 'select', 'parameterization', 'loglinear', 'smooth', 'bivariate', 'test', 'score', 'distribution', 'nonequivalent', 'group', 'anchor', 'test', 'NEAT', 'equate', 'strategy', 'base', 'significance', 'test', 'chisquare', 'statistic', 'Likelihood', 'Ratio', 'Pearson', 'FreemanTukey', 'CressieRead', 'additional', 'strategy', 'base', 'different', 'evaluation', 'Likelihood', 'Ratio', 'ChiSquare', 'statistic', 'Akaike', 'Information', 'Criterion', 'Bayesian', 'Information', 'Criterion', 'Consistent', 'Akaike', 'Information', 'Criterion', 'index', 'trace', 'Goodman', 'focus', 'implication', 'selection', 'strategy', 'selection', 'tendency', 'accuracy', 'chain', 'poststratification', 'equating', 'function', 'result', 'differentiate', 'strategy', 'term', 'tendency', 'select', 'particular', 'bivariate', 'parameterization', 'implication', 'tendency', 'equate', 'bias', 'variability', '©', '2010', 'National', 'Council']","['effect', 'selection', 'strategy', 'bivariate', 'loglinear', 'smooth', 'neat', 'equate', 'function']",study statistical strategy evaluate select parameterization loglinear smooth bivariate test score distribution nonequivalent group anchor test NEAT equate strategy base significance test chisquare statistic Likelihood Ratio Pearson FreemanTukey CressieRead additional strategy base different evaluation Likelihood Ratio ChiSquare statistic Akaike Information Criterion Bayesian Information Criterion Consistent Akaike Information Criterion index trace Goodman focus implication selection strategy selection tendency accuracy chain poststratification equating function result differentiate strategy term tendency select particular bivariate parameterization implication tendency equate bias variability © 2010 National Council,effect selection strategy bivariate loglinear smooth neat equate function,0.8875017523745786,0.028101377888785722,0.028080873905172294,0.02830493445607716,0.028011061375386358,0.02433199169805955,0.0010056441338416953,0.0,0.06923116143184307,0.008654038307200735
Yao L.,Reporting valid and reliable overall scores and domain scores,2010,47,"In educational assessment, overall scores obtained by simply averaging a number of domain scores are sometimes reported. However, simply averaging the domain scores ignores the fact that different domains have different score points, that scores from those domains are related, and that at different score points the relationship between overall score and domain score may be different. To report reliable and valid overall scores and domain scores, I investigated the performance of four methods using both real and simulation data: (a) the unidimensional IRT model; (b) the higher-order IRT model, which simultaneously estimates the overall ability and domain abilities; (c) the multidimensional IRT (MIRT) model, which estimates domain abilities and uses the maximum information method to obtain the overall ability; and (d) the bifactor general model. My findings suggest that the MIRT model not only provides reliable domain scores, but also produces reliable overall scores. The overall score from the MIRT maximum information method has the smallest standard error of measurement. In addition, unlike the other models, there is no linear relationship assumed between overall score and domain scores. Recommendations for sizes of correlations between domains and the number of items needed for reporting purposes are provided. © 2010 by the National Council on Measurement in EducationNo claim to original US government works.",Reporting valid and reliable overall scores and domain scores,"In educational assessment, overall scores obtained by simply averaging a number of domain scores are sometimes reported. However, simply averaging the domain scores ignores the fact that different domains have different score points, that scores from those domains are related, and that at different score points the relationship between overall score and domain score may be different. To report reliable and valid overall scores and domain scores, I investigated the performance of four methods using both real and simulation data: (a) the unidimensional IRT model; (b) the higher-order IRT model, which simultaneously estimates the overall ability and domain abilities; (c) the multidimensional IRT (MIRT) model, which estimates domain abilities and uses the maximum information method to obtain the overall ability; and (d) the bifactor general model. My findings suggest that the MIRT model not only provides reliable domain scores, but also produces reliable overall scores. The overall score from the MIRT maximum information method has the smallest standard error of measurement. In addition, unlike the other models, there is no linear relationship assumed between overall score and domain scores. Recommendations for sizes of correlations between domains and the number of items needed for reporting purposes are provided. © 2010 by the National Council on Measurement in EducationNo claim to original US government works.","['educational', 'assessment', 'overall', 'score', 'obtain', 'simply', 'average', 'number', 'domain', 'score', 'report', 'simply', 'average', 'domain', 'score', 'ignore', 'fact', 'different', 'domain', 'different', 'score', 'point', 'score', 'domain', 'relate', 'different', 'score', 'point', 'relationship', 'overall', 'score', 'domain', 'score', 'different', 'report', 'reliable', 'valid', 'overall', 'score', 'domain', 'score', 'I', 'investigate', 'performance', 'method', 'real', 'simulation', 'datum', 'unidimensional', 'IRT', 'b', 'higherorder', 'IRT', 'simultaneously', 'estimate', 'overall', 'ability', 'domain', 'ability', 'c', 'multidimensional', 'IRT', 'MIRT', 'estimate', 'domain', 'ability', 'maximum', 'information', 'method', 'obtain', 'overall', 'ability', 'd', 'bifactor', 'general', 'finding', 'suggest', 'MIRT', 'provide', 'reliable', 'domain', 'score', 'produce', 'reliable', 'overall', 'score', 'overall', 'score', 'MIRT', 'maximum', 'information', 'method', 'small', 'standard', 'error', 'addition', 'unlike', 'linear', 'relationship', 'assume', 'overall', 'score', 'domain', 'score', 'Recommendations', 'size', 'correlation', 'domain', 'number', 'item', 'need', 'reporting', 'purpose', 'provide', '©', '2010', 'National', 'Council', 'EducationNo', 'claim', 'original', 'US', 'government', 'work']","['report', 'valid', 'reliable', 'overall', 'score', 'domain', 'score']",educational assessment overall score obtain simply average number domain score report simply average domain score ignore fact different domain different score point score domain relate different score point relationship overall score domain score different report reliable valid overall score domain score I investigate performance method real simulation datum unidimensional IRT b higherorder IRT simultaneously estimate overall ability domain ability c multidimensional IRT MIRT estimate domain ability maximum information method obtain overall ability d bifactor general finding suggest MIRT provide reliable domain score produce reliable overall score overall score MIRT maximum information method small standard error addition unlike linear relationship assume overall score domain score Recommendations size correlation domain number item need reporting purpose provide © 2010 National Council EducationNo claim original US government work,report valid reliable overall score domain score,0.032390099779117866,0.0323858170104146,0.8703262501808154,0.032631865546227344,0.03226596748342475,0.012310545530665429,0.0286523166128273,0.053286689876271276,0.02291512270752882,0.004005561833409643
Kim S.; Walker M.E.; McHale F.,Investigating the effectiveness of equating designs for constructed-response tests in large-scale assessments,2010,47,"Using data from a large-scale exam, in this study we compared various designs for equating constructed-response (CR) tests to determine which design was most effective in producing equivalent scores across the two tests to be equated. In the context of classical equating methods, four linking designs were examined: (a) an anchor set containing common CR items, (b) an anchor set incorporating common CR items rescored, (c) an external multiple-choice (MC) anchor test, and (d) an equivalent groups design incorporating rescored CR items (no anchor test). The use of CR items without rescoring resulted in much larger bias than the other designs. The use of an external MC anchor resulted in the next largest bias. The use of a rescored CR anchor and the equivalent groups design led to similar levels of equating error. © 2010 by the National Council on Measurement in Education.",Investigating the effectiveness of equating designs for constructed-response tests in large-scale assessments,"Using data from a large-scale exam, in this study we compared various designs for equating constructed-response (CR) tests to determine which design was most effective in producing equivalent scores across the two tests to be equated. In the context of classical equating methods, four linking designs were examined: (a) an anchor set containing common CR items, (b) an anchor set incorporating common CR items rescored, (c) an external multiple-choice (MC) anchor test, and (d) an equivalent groups design incorporating rescored CR items (no anchor test). The use of CR items without rescoring resulted in much larger bias than the other designs. The use of an external MC anchor resulted in the next largest bias. The use of a rescored CR anchor and the equivalent groups design led to similar levels of equating error. © 2010 by the National Council on Measurement in Education.","['datum', 'largescale', 'exam', 'study', 'compare', 'design', 'equate', 'constructedresponse', 'CR', 'test', 'determine', 'design', 'effective', 'produce', 'equivalent', 'score', 'test', 'equate', 'context', 'classical', 'equating', 'method', 'link', 'design', 'examine', 'anchor', 'set', 'contain', 'common', 'CR', 'item', 'b', 'anchor', 'set', 'incorporate', 'common', 'CR', 'item', 'rescore', 'c', 'external', 'multiplechoice', 'MC', 'anchor', 'test', 'd', 'equivalent', 'group', 'design', 'incorporate', 'rescore', 'CR', 'item', 'anchor', 'test', 'CR', 'item', 'rescore', 'result', 'large', 'bias', 'design', 'external', 'MC', 'anchor', 'result', 'large', 'bias', 'rescore', 'CR', 'anchor', 'equivalent', 'group', 'design', 'lead', 'similar', 'level', 'equate', 'error', '©', '2010', 'National', 'Council']","['investigate', 'effectiveness', 'equate', 'design', 'constructedresponse', 'test', 'largescale', 'assessment']",datum largescale exam study compare design equate constructedresponse CR test determine design effective produce equivalent score test equate context classical equating method link design examine anchor set contain common CR item b anchor set incorporate common CR item rescore c external multiplechoice MC anchor test d equivalent group design incorporate rescore CR item anchor test CR item rescore result large bias design external MC anchor result large bias rescore CR anchor equivalent group design lead similar level equate error © 2010 National Council,investigate effectiveness equate design constructedresponse test largescale assessment,0.8604250628807264,0.03520639837029682,0.03520274138100971,0.034575802826117094,0.03458999454184982,0.021749386139860195,0.0,0.0011914312144591763,0.09770113530160836,0.0011008660897854573
Paek I.,A Note on Three Statistical Tests in the Logistic Regression DIF Procedure,2012,49,"Although logistic regression became one of the well-known methods in detecting differential item functioning (DIF), its three statistical tests, the Wald, likelihood ratio (LR), and score tests, which are readily available under the maximum likelihood, do not seem to be consistently distinguished in DIF literature. This paper provides a clarifying note on those three tests when logistic regression is applied for DIF detection. © 2012 by the National Council on Measurement in Education.",A Note on Three Statistical Tests in the Logistic Regression DIF Procedure,"Although logistic regression became one of the well-known methods in detecting differential item functioning (DIF), its three statistical tests, the Wald, likelihood ratio (LR), and score tests, which are readily available under the maximum likelihood, do not seem to be consistently distinguished in DIF literature. This paper provides a clarifying note on those three tests when logistic regression is applied for DIF detection. © 2012 by the National Council on Measurement in Education.","['logistic', 'regression', 'wellknown', 'method', 'detect', 'differential', 'item', 'function', 'DIF', 'statistical', 'test', 'Wald', 'likelihood', 'ratio', 'LR', 'score', 'test', 'readily', 'available', 'maximum', 'likelihood', 'consistently', 'distinguish', 'DIF', 'literature', 'paper', 'provide', 'clarifying', 'note', 'test', 'logistic', 'regression', 'apply', 'dif', 'detection', '©', '2012', 'National', 'Council']","['note', 'statistical', 'Tests', 'Logistic', 'Regression', 'dif', 'procedure']",logistic regression wellknown method detect differential item function DIF statistical test Wald likelihood ratio LR score test readily available maximum likelihood consistently distinguish DIF literature paper provide clarifying note test logistic regression apply dif detection © 2012 National Council,note statistical Tests Logistic Regression dif procedure,0.03429750763048023,0.03433571721547086,0.03384048993679351,0.03426461722587508,0.8632616679913803,0.0720024442850843,0.0,0.0,0.0,0.0
Puhan G.,A comparison of chained linear and poststratification linear equating under different testing conditions,2010,47,"In this study I compared results of chained linear, Tucker, and Levine-observed score equatings under conditions where the new and old forms samples were similar in ability and also when they were different in ability. The length of the anchor test was also varied to examine its effect on the three different equating methods. The three equating methods were compared to a criterion equating to obtain estimates of random equating error, bias, and root mean squared error (RMSE). Results showed that, for most studied conditions, chained linear equating produced fairly good equating results in terms of low bias and RMSE. Levine equating also produced low bias and RMSE in some conditions. Although the Tucker method always produced the lowest random equating error, it produced a larger bias and RMSE than either of the other equating methods. As noted in the literature, these results also suggest that either chained linear or Levine equating be used when new and old form samples differ on ability and/or when the anchor-to-total correlation is not very high. Finally, by testing the missing data assumptions of the three equating methods, this study also shows empirically why an equating method is more or less accurate under certain conditions. © 2010 by the National Council on Measurement in Education.",A comparison of chained linear and poststratification linear equating under different testing conditions,"In this study I compared results of chained linear, Tucker, and Levine-observed score equatings under conditions where the new and old forms samples were similar in ability and also when they were different in ability. The length of the anchor test was also varied to examine its effect on the three different equating methods. The three equating methods were compared to a criterion equating to obtain estimates of random equating error, bias, and root mean squared error (RMSE). Results showed that, for most studied conditions, chained linear equating produced fairly good equating results in terms of low bias and RMSE. Levine equating also produced low bias and RMSE in some conditions. Although the Tucker method always produced the lowest random equating error, it produced a larger bias and RMSE than either of the other equating methods. As noted in the literature, these results also suggest that either chained linear or Levine equating be used when new and old form samples differ on ability and/or when the anchor-to-total correlation is not very high. Finally, by testing the missing data assumptions of the three equating methods, this study also shows empirically why an equating method is more or less accurate under certain conditions. © 2010 by the National Council on Measurement in Education.","['study', 'I', 'compare', 'result', 'chain', 'linear', 'Tucker', 'levineobserved', 'score', 'equating', 'condition', 'new', 'old', 'form', 'sample', 'similar', 'ability', 'different', 'ability', 'length', 'anchor', 'test', 'varied', 'examine', 'effect', 'different', 'equating', 'method', 'equate', 'method', 'compare', 'criterion', 'equate', 'obtain', 'estimate', 'random', 'equating', 'error', 'bias', 'root', 'mean', 'square', 'error', 'rmse', 'result', 'study', 'condition', 'chain', 'linear', 'equating', 'produce', 'fairly', 'good', 'equate', 'result', 'term', 'low', 'bias', 'rmse', 'Levine', 'equating', 'produce', 'low', 'bias', 'rmse', 'condition', 'Tucker', 'method', 'produce', 'low', 'random', 'equating', 'error', 'produce', 'large', 'bias', 'rmse', 'equate', 'method', 'note', 'literature', 'result', 'suggest', 'chain', 'linear', 'Levine', 'equating', 'new', 'old', 'form', 'sample', 'differ', 'ability', 'andor', 'anchortototal', 'correlation', 'high', 'finally', 'test', 'miss', 'data', 'assumption', 'equate', 'method', 'study', 'empirically', 'equate', 'method', 'accurate', 'certain', 'condition', '©', '2010', 'National', 'Council']","['comparison', 'chain', 'linear', 'poststratification', 'linear', 'equate', 'different', 'testing', 'condition']",study I compare result chain linear Tucker levineobserved score equating condition new old form sample similar ability different ability length anchor test varied examine effect different equating method equate method compare criterion equate obtain estimate random equating error bias root mean square error rmse result study condition chain linear equating produce fairly good equate result term low bias rmse Levine equating produce low bias rmse condition Tucker method produce low random equating error produce large bias rmse equate method note literature result suggest chain linear Levine equating new old form sample differ ability andor anchortototal correlation high finally test miss data assumption equate method study empirically equate method accurate certain condition © 2010 National Council,comparison chain linear poststratification linear equate different testing condition,0.8874266813601731,0.028024408012833398,0.028247093608673807,0.028141788975526976,0.02816002804279282,0.006296166863945587,0.0,0.0,0.1941534327532363,0.0
Rutkowski L.,The impact of missing background data on subpopulation estimation,2011,48,"Although population modeling methods are well established, a paucity of literature appears to exist regarding the effect of missing background data on subpopulation achievement estimates. Using simulated data that follows typical large-scale assessment designs with known parameters and a number of missing conditions, this paper examines the extent to which missing background data impacts subpopulation achievement estimates. In particular, the paper compares achievement estimates under a model with fully observed background data to achievement estimates for a variety of missing background data conditions. The findings suggest that sub-population differences are preserved under all analyzed conditions while point estimates for subpopulation achievement values are influenced by missing at random conditions. Implications for cross-population comparisons are discussed. © 2011 by the National Council on Measurement in Education.",The impact of missing background data on subpopulation estimation,"Although population modeling methods are well established, a paucity of literature appears to exist regarding the effect of missing background data on subpopulation achievement estimates. Using simulated data that follows typical large-scale assessment designs with known parameters and a number of missing conditions, this paper examines the extent to which missing background data impacts subpopulation achievement estimates. In particular, the paper compares achievement estimates under a model with fully observed background data to achievement estimates for a variety of missing background data conditions. The findings suggest that sub-population differences are preserved under all analyzed conditions while point estimates for subpopulation achievement values are influenced by missing at random conditions. Implications for cross-population comparisons are discussed. © 2011 by the National Council on Measurement in Education.","['population', 'modeling', 'method', 'establish', 'paucity', 'literature', 'appear', 'exist', 'regard', 'effect', 'miss', 'background', 'datum', 'subpopulation', 'achievement', 'estimate', 'simulated', 'datum', 'follow', 'typical', 'largescale', 'assessment', 'design', 'know', 'parameter', 'number', 'miss', 'condition', 'paper', 'examine', 'extent', 'miss', 'background', 'datum', 'impact', 'subpopulation', 'achievement', 'estimate', 'particular', 'paper', 'compare', 'achievement', 'estimate', 'fully', 'observed', 'background', 'datum', 'achievement', 'estimate', 'variety', 'miss', 'background', 'datum', 'condition', 'finding', 'suggest', 'subpopulation', 'difference', 'preserve', 'analyze', 'condition', 'point', 'estimate', 'subpopulation', 'achievement', 'value', 'influence', 'miss', 'random', 'condition', 'Implications', 'crosspopulation', 'comparison', 'discuss', '©', '2011', 'National', 'Council']","['impact', 'miss', 'background', 'datum', 'subpopulation', 'estimation']",population modeling method establish paucity literature appear exist regard effect miss background datum subpopulation achievement estimate simulated datum follow typical largescale assessment design know parameter number miss condition paper examine extent miss background datum impact subpopulation achievement estimate particular paper compare achievement estimate fully observed background datum achievement estimate variety miss background datum condition finding suggest subpopulation difference preserve analyze condition point estimate subpopulation achievement value influence miss random condition Implications crosspopulation comparison discuss © 2011 National Council,impact miss background datum subpopulation estimation,0.0328787963230418,0.03259523313916532,0.03243404318325357,0.03274286364218899,0.8693490637123502,0.027673851471417276,0.0022598092208388123,0.027662850157308877,0.0073712941171362985,0.007309345486687298
Van der Linden W.J.,Test design and speededness,2011,48,"A critical component of test speededness is the distribution of the test taker's total time on the test. A simple set of constraints on the item parameters in the lognormal model for response times is derived that can be used to control the distribution when assembling a new test form. As the constraints are linear in the item parameters, they can easily be included in a mixed integer programming model for test assembly. The use of the constraints is demonstrated for the problems of assembling a new test form to be equally speeded as a reference form, test assembly in which the impact of a change in the content specifications on speededness is to be neutralized, and the assembly of test forms with a revised level of speededness. © 2011 by the National Council on Measurement in Education.",,"A critical component of test speededness is the distribution of the test taker's total time on the test. A simple set of constraints on the item parameters in the lognormal model for response times is derived that can be used to control the distribution when assembling a new test form. As the constraints are linear in the item parameters, they can easily be included in a mixed integer programming model for test assembly. The use of the constraints is demonstrated for the problems of assembling a new test form to be equally speeded as a reference form, test assembly in which the impact of a change in the content specifications on speededness is to be neutralized, and the assembly of test forms with a revised level of speededness. © 2011 by the National Council on Measurement in Education.","['critical', 'component', 'test', 'speededness', 'distribution', 'test', 'taker', 'total', 'time', 'test', 'simple', 'set', 'constraint', 'item', 'parameter', 'lognormal', 'response', 'time', 'derive', 'control', 'distribution', 'assemble', 'new', 'test', 'form', 'constraint', 'linear', 'item', 'parameter', 'easily', 'include', 'mixed', 'integer', 'programming', 'test', 'assembly', 'constraint', 'demonstrate', 'problem', 'assemble', 'new', 'test', 'form', 'equally', 'speed', 'reference', 'form', 'test', 'assembly', 'impact', 'change', 'content', 'specification', 'speededness', 'neutralize', 'assembly', 'test', 'form', 'revise', 'level', 'speededness', '©', '2011', 'National', 'Council']",,critical component test speededness distribution test taker total time test simple set constraint item parameter lognormal response time derive control distribution assemble new test form constraint linear item parameter easily include mixed integer programming test assembly constraint demonstrate problem assemble new test form equally speed reference form test assembly impact change content specification speededness neutralize assembly test form revise level speededness © 2011 National Council,,0.03198157011743473,0.03213024947559232,0.8727591137452162,0.031264153431297494,0.03186491323045923,0.059661604113434824,0.024638217996091043,0.0,0.009016038871154144,0.0
Lee W.-C.,Classification consistency and accuracy for complex assessments using item response theory,2010,47,"In this article, procedures are described for estimating single-administration classification consistency and accuracy indices for complex assessments using item response theory (IRT). This IRT approach was applied to real test data comprising dichotomous and polytomous items. Several different IRT model combinations were considered. Comparisons were also made between the IRT approach and two non-IRT approaches including the Livingston-Lewis and compound multinomial procedures. Results for various IRT model combinations were not substantially different. The estimated classification consistency and accuracy indices for the non-IRT procedures were almost always lower than those for the IRT procedures. © 2010 by the National Council on Measurement in Education.",Classification consistency and accuracy for complex assessments using item response theory,"In this article, procedures are described for estimating single-administration classification consistency and accuracy indices for complex assessments using item response theory (IRT). This IRT approach was applied to real test data comprising dichotomous and polytomous items. Several different IRT model combinations were considered. Comparisons were also made between the IRT approach and two non-IRT approaches including the Livingston-Lewis and compound multinomial procedures. Results for various IRT model combinations were not substantially different. The estimated classification consistency and accuracy indices for the non-IRT procedures were almost always lower than those for the IRT procedures. © 2010 by the National Council on Measurement in Education.","['article', 'procedure', 'describe', 'estimate', 'singleadministration', 'classification', 'consistency', 'accuracy', 'index', 'complex', 'assessment', 'item', 'response', 'theory', 'IRT', 'IRT', 'approach', 'apply', 'real', 'test', 'datum', 'comprise', 'dichotomous', 'polytomous', 'item', 'different', 'IRT', 'combination', 'consider', 'Comparisons', 'IRT', 'approach', 'nonIRT', 'approach', 'include', 'LivingstonLewis', 'compound', 'multinomial', 'procedure', 'result', 'IRT', 'combination', 'substantially', 'different', 'estimate', 'classification', 'consistency', 'accuracy', 'index', 'nonIRT', 'procedure', 'low', 'IRT', 'procedure', '©', '2010', 'National', 'Council']","['classification', 'consistency', 'accuracy', 'complex', 'assessment', 'item', 'response', 'theory']",article procedure describe estimate singleadministration classification consistency accuracy index complex assessment item response theory IRT IRT approach apply real test datum comprise dichotomous polytomous item different IRT combination consider Comparisons IRT approach nonIRT approach include LivingstonLewis compound multinomial procedure result IRT combination substantially different estimate classification consistency accuracy index nonIRT procedure low IRT procedure © 2010 National Council,classification consistency accuracy complex assessment item response theory,0.03383021062311339,0.03368815132853798,0.8639191855647317,0.03376389593336032,0.03479855655025659,0.0676678046540615,0.02169795414331123,0.002802855614794972,0.0,0.0
Dorans N.J.; Middleton K.,Addressing the Extreme Assumptions of Presumed Linkings,2012,49,"The interpretability of score comparisons depends on the design and execution of a sound data collection plan and the establishment of linkings between these scores. When comparisons are made between scores from two or more assessments that are built to different specifications and are administered to different populations under different conditions, the validity of the comparisons hinges on untestable assumptions. For example, tests administered across different disability groups or tests administered to different language groups produce scores for which implicit linkings are presumed to hold. Presumed linking makes use of extreme assumptions to produce links between scores on tests in the absence of common test material or equivalent groups of test takers. These presumed linkings lead to dubious interpretations. This article suggests an approach that indirectly assesses the validity of these presumed linkings among scores on assessments that contain neither equivalent groups nor common anchor material. © 2012 by the National Council on Measurement in Education.",Addressing the Extreme Assumptions of Presumed Linkings,"The interpretability of score comparisons depends on the design and execution of a sound data collection plan and the establishment of linkings between these scores. When comparisons are made between scores from two or more assessments that are built to different specifications and are administered to different populations under different conditions, the validity of the comparisons hinges on untestable assumptions. For example, tests administered across different disability groups or tests administered to different language groups produce scores for which implicit linkings are presumed to hold. Presumed linking makes use of extreme assumptions to produce links between scores on tests in the absence of common test material or equivalent groups of test takers. These presumed linkings lead to dubious interpretations. This article suggests an approach that indirectly assesses the validity of these presumed linkings among scores on assessments that contain neither equivalent groups nor common anchor material. © 2012 by the National Council on Measurement in Education.","['interpretability', 'score', 'comparison', 'depend', 'design', 'execution', 'sound', 'datum', 'collection', 'plan', 'establishment', 'linking', 'score', 'comparison', 'score', 'assessment', 'build', 'different', 'specification', 'administer', 'different', 'population', 'different', 'condition', 'validity', 'comparison', 'hinge', 'untestable', 'assumption', 'example', 'test', 'administer', 'different', 'disability', 'group', 'test', 'administer', 'different', 'language', 'group', 'produce', 'score', 'implicit', 'linking', 'presume', 'hold', 'presumed', 'link', 'extreme', 'assumption', 'produce', 'link', 'score', 'test', 'absence', 'common', 'test', 'material', 'equivalent', 'group', 'test', 'taker', 'presume', 'linking', 'lead', 'dubious', 'interpretation', 'article', 'suggest', 'approach', 'indirectly', 'assess', 'validity', 'presume', 'linking', 'score', 'assessment', 'contain', 'equivalent', 'group', 'common', 'anchor', 'material', '©', '2012', 'National', 'Council']","['address', 'Extreme', 'Assumptions', 'Presumed', 'linking']",interpretability score comparison depend design execution sound datum collection plan establishment linking score comparison score assessment build different specification administer different population different condition validity comparison hinge untestable assumption example test administer different disability group test administer different language group produce score implicit linking presume hold presumed link extreme assumption produce link score test absence common test material equivalent group test taker presume linking lead dubious interpretation article suggest approach indirectly assess validity presume linking score assessment contain equivalent group common anchor material © 2012 National Council,address Extreme Assumptions Presumed linking,0.030069750859960463,0.881209995723686,0.029459839466013087,0.029735971614719855,0.02952444233562055,0.010494975246462038,0.011985395292146478,0.06152365975347057,0.03148366186365586,0.0
Kunina-Habenicht O.; Rupp A.A.; Wilhelm O.,The Impact of Model Misspecification on Parameter Estimation and Item-Fit Assessment in Log-Linear Diagnostic Classification Models,2012,49,"Using a complex simulation study we investigated parameter recovery, classification accuracy, and performance of two item-fit statistics for correct and misspecified diagnostic classification models within a log-linear modeling framework. The basic manipulated test design factors included the number of respondents (1,000 vs. 10,000), attributes (3 vs. 5), and items (25 vs. 50) as well as different attribute correlations (50 vs. 80) and marginal attribute difficulties (equal vs. different). We investigated misspecifications of interaction effect parameters under correct Q-matrix specification and two types of Q-matrix misspecification. While the misspecification of interaction effects had little impact on classification accuracy, invalid Q-matrix specifications led to notably decreased classification accuracy. Two proposed item-fit indexes were more strongly sensitive to overspecification of Q-matrix entries for items than to underspecification. Information-based fit indexes AIC and BIC were sensitive to both over- and underspecification. © 2012 by the National Council on Measurement in Education.",The Impact of Model Misspecification on Parameter Estimation and Item-Fit Assessment in Log-Linear Diagnostic Classification Models,"Using a complex simulation study we investigated parameter recovery, classification accuracy, and performance of two item-fit statistics for correct and misspecified diagnostic classification models within a log-linear modeling framework. The basic manipulated test design factors included the number of respondents (1,000 vs. 10,000), attributes (3 vs. 5), and items (25 vs. 50) as well as different attribute correlations (50 vs. 80) and marginal attribute difficulties (equal vs. different). We investigated misspecifications of interaction effect parameters under correct Q-matrix specification and two types of Q-matrix misspecification. While the misspecification of interaction effects had little impact on classification accuracy, invalid Q-matrix specifications led to notably decreased classification accuracy. Two proposed item-fit indexes were more strongly sensitive to overspecification of Q-matrix entries for items than to underspecification. Information-based fit indexes AIC and BIC were sensitive to both over- and underspecification. © 2012 by the National Council on Measurement in Education.","['complex', 'simulation', 'study', 'investigate', 'parameter', 'recovery', 'classification', 'accuracy', 'performance', 'itemfit', 'statistic', 'correct', 'misspecified', 'diagnostic', 'classification', 'loglinear', 'modeling', 'framework', 'basic', 'manipulate', 'test', 'design', 'factor', 'include', 'number', 'respondent', '1000', 'vs', '10000', 'attribute', '3', 'vs', '5', 'item', '25', 'vs', '50', 'different', 'attribute', 'correlation', '50', 'vs', '80', 'marginal', 'attribute', 'difficulty', 'equal', 'vs', 'different', 'investigate', 'misspecification', 'interaction', 'effect', 'parameter', 'correct', 'qmatrix', 'specification', 'type', 'qmatrix', 'misspecification', 'misspecification', 'interaction', 'effect', 'little', 'impact', 'classification', 'accuracy', 'invalid', 'Qmatrix', 'specification', 'lead', 'notably', 'decrease', 'classification', 'accuracy', 'propose', 'itemfit', 'index', 'strongly', 'sensitive', 'overspecification', 'qmatrix', 'entry', 'item', 'underspecification', 'Informationbased', 'fit', 'index', 'AIC', 'BIC', 'sensitive', 'underspecification', '©', '2012', 'National', 'Council']","['Impact', 'Misspecification', 'Parameter', 'Estimation', 'ItemFit', 'Assessment', 'LogLinear', 'Diagnostic', 'Classification', 'Models']",complex simulation study investigate parameter recovery classification accuracy performance itemfit statistic correct misspecified diagnostic classification loglinear modeling framework basic manipulate test design factor include number respondent 1000 vs 10000 attribute 3 vs 5 item 25 vs 50 different attribute correlation 50 vs 80 marginal attribute difficulty equal vs different investigate misspecification interaction effect parameter correct qmatrix specification type qmatrix misspecification misspecification interaction effect little impact classification accuracy invalid Qmatrix specification lead notably decrease classification accuracy propose itemfit index strongly sensitive overspecification qmatrix entry item underspecification Informationbased fit index AIC BIC sensitive underspecification © 2012 National Council,Impact Misspecification Parameter Estimation ItemFit Assessment LogLinear Diagnostic Classification Models,0.8919826677582535,0.026896288569290376,0.02677654040456393,0.0269728734761096,0.027371629791782575,0.04240760985798183,0.01993875059376467,0.0,0.0,0.004921639126127867
Frederickx S.; Tuerlinckx F.; De Boeck P.; Magis D.,RIM: A Random Item Mixture Model to Detect Differential Item Functioning,2010,47,"In this paper we present a new methodology for detecting differential item functioning (DIF). We introduce a DIF model, called the random item mixture (RIM), that is based on a Rasch model with random item difficulties (besides the common random person abilities). In addition, a mixture model is assumed for the item difficulties such that the items may belong to one of two classes: a DIF or a non-DIF class. The crucial difference between the DIF class and the non-DIF class is that the item difficulties in the DIF class may differ according to the observed person groups while they are equal across the person groups for the items from the non-DIF class. Statistical inference for the RIM is carried out in a Bayesian framework. The performance of the RIM is evaluated using a simulation study in which it is compared with traditional procedures, like the likelihood ratio test, the Mantel-Haenszel procedure and the standardized p -DIF procedure. In this comparison, the RIM performs better than the other methods. Finally, the usefulness of the model is also demonstrated on a real life data set. © 2010 by the National Council on Measurement in Education.",RIM: A Random Item Mixture Model to Detect Differential Item Functioning,"In this paper we present a new methodology for detecting differential item functioning (DIF). We introduce a DIF model, called the random item mixture (RIM), that is based on a Rasch model with random item difficulties (besides the common random person abilities). In addition, a mixture model is assumed for the item difficulties such that the items may belong to one of two classes: a DIF or a non-DIF class. The crucial difference between the DIF class and the non-DIF class is that the item difficulties in the DIF class may differ according to the observed person groups while they are equal across the person groups for the items from the non-DIF class. Statistical inference for the RIM is carried out in a Bayesian framework. The performance of the RIM is evaluated using a simulation study in which it is compared with traditional procedures, like the likelihood ratio test, the Mantel-Haenszel procedure and the standardized p -DIF procedure. In this comparison, the RIM performs better than the other methods. Finally, the usefulness of the model is also demonstrated on a real life data set. © 2010 by the National Council on Measurement in Education.","['paper', 'present', 'new', 'methodology', 'detect', 'differential', 'item', 'function', 'DIF', 'introduce', 'dif', 'random', 'item', 'mixture', 'rim', 'base', 'Rasch', 'random', 'item', 'difficulty', 'common', 'random', 'person', 'ability', 'addition', 'mixture', 'assume', 'item', 'difficulty', 'item', 'belong', 'class', 'DIF', 'nondif', 'class', 'crucial', 'difference', 'DIF', 'class', 'nonDIF', 'class', 'item', 'difficulty', 'DIF', 'class', 'differ', 'accord', 'observed', 'person', 'group', 'equal', 'person', 'group', 'item', 'nonDIF', 'class', 'statistical', 'inference', 'RIM', 'carry', 'bayesian', 'framework', 'performance', 'rim', 'evaluate', 'simulation', 'study', 'compare', 'traditional', 'procedure', 'like', 'likelihood', 'ratio', 'test', 'MantelHaenszel', 'procedure', 'standardized', 'p', 'dif', 'procedure', 'comparison', 'RIM', 'perform', 'method', 'finally', 'usefulness', 'demonstrate', 'real', 'life', 'datum', 'set', '©', '2010', 'National', 'Council']","['RIM', 'A', 'Random', 'Item', 'Mixture', 'detect', 'Differential', 'Item', 'Functioning']",paper present new methodology detect differential item function DIF introduce dif random item mixture rim base Rasch random item difficulty common random person ability addition mixture assume item difficulty item belong class DIF nondif class crucial difference DIF class nonDIF class item difficulty DIF class differ accord observed person group equal person group item nonDIF class statistical inference RIM carry bayesian framework performance rim evaluate simulation study compare traditional procedure like likelihood ratio test MantelHaenszel procedure standardized p dif procedure comparison RIM perform method finally usefulness demonstrate real life datum set © 2010 National Council,RIM A Random Item Mixture detect Differential Item Functioning,0.030394419298862427,0.02999962015942546,0.029915084583661003,0.8794625949984544,0.030228280959596697,0.08540271178522817,0.0,0.0,0.0,0.0
van der Linden W.J.,Linking response-time parameters onto a common scale,2010,47,"Although response times on test items are recorded on a natural scale, the scale for some of the parameters in the lognormal response-time model (van der Linden, 2006) is not fixed. As a result, when the model is used to periodically calibrate new items in a testing program, the parameter are not automatically mapped onto a common scale. Several combinations of linking designs and procedures for the lognormal model are examined that do map parameter estimates onto a common scale. For each of the designs, the standard error of linking is derived. The results are illustrated using examples with simulated data. © 2010 by the National Council on Measurement in Education.",Linking response-time parameters onto a common scale,"Although response times on test items are recorded on a natural scale, the scale for some of the parameters in the lognormal response-time model (van der Linden, 2006) is not fixed. As a result, when the model is used to periodically calibrate new items in a testing program, the parameter are not automatically mapped onto a common scale. Several combinations of linking designs and procedures for the lognormal model are examined that do map parameter estimates onto a common scale. For each of the designs, the standard error of linking is derived. The results are illustrated using examples with simulated data. © 2010 by the National Council on Measurement in Education.","['response', 'time', 'test', 'item', 'record', 'natural', 'scale', 'scale', 'parameter', 'lognormal', 'responsetime', 'van', 'der', 'Linden', '2006', 'fix', 'result', 'periodically', 'calibrate', 'new', 'item', 'testing', 'program', 'parameter', 'automatically', 'map', 'common', 'scale', 'combination', 'link', 'design', 'procedure', 'lognormal', 'examine', 'map', 'parameter', 'estimate', 'common', 'scale', 'design', 'standard', 'error', 'link', 'derive', 'result', 'illustrate', 'example', 'simulated', 'datum', '©', '2010', 'National', 'Council']","['link', 'responsetime', 'parameter', 'common', 'scale']",response time test item record natural scale scale parameter lognormal responsetime van der Linden 2006 fix result periodically calibrate new item testing program parameter automatically map common scale combination link design procedure lognormal examine map parameter estimate common scale design standard error link derive result illustrate example simulated datum © 2010 National Council,link responsetime parameter common scale,0.03138954717946675,0.8740541286760241,0.03130328395463166,0.03177198790538614,0.03148105228449147,0.06034125657404856,0.0,0.0,0.013197047335780775,0.01569091085147822
Zu J.; Liu J.,Observed Score Equating Using Discrete and Passage-Based Anchor Items,2010,47,"Equating of tests composed of both discrete and passage-based multiple choice items using the nonequivalent groups with anchor test design is popular in practice. In this study, we compared the effect of discrete and passage-based anchor items on observed score equating via simulation. Results suggested that an anchor with a larger proportion of passage-based items, more items in each passage, and/or a larger degree of local dependence among items within one passage produces larger equating errors, especially when the groups taking the new form and the reference form differ in ability. Our findings challenge the common belief that an anchor should be a miniature version of the tests to be equated. Suggestions to practitioners regarding anchor design are also given. © 2010 by the National Council on Measurement in Education.",Observed Score Equating Using Discrete and Passage-Based Anchor Items,"Equating of tests composed of both discrete and passage-based multiple choice items using the nonequivalent groups with anchor test design is popular in practice. In this study, we compared the effect of discrete and passage-based anchor items on observed score equating via simulation. Results suggested that an anchor with a larger proportion of passage-based items, more items in each passage, and/or a larger degree of local dependence among items within one passage produces larger equating errors, especially when the groups taking the new form and the reference form differ in ability. Our findings challenge the common belief that an anchor should be a miniature version of the tests to be equated. Suggestions to practitioners regarding anchor design are also given. © 2010 by the National Council on Measurement in Education.","['equating', 'test', 'compose', 'discrete', 'passagebase', 'multiple', 'choice', 'item', 'nonequivalent', 'group', 'anchor', 'test', 'design', 'popular', 'practice', 'study', 'compare', 'effect', 'discrete', 'passagebase', 'anchor', 'item', 'observe', 'score', 'equate', 'simulation', 'result', 'suggest', 'anchor', 'large', 'proportion', 'passagebase', 'item', 'item', 'passage', 'andor', 'large', 'degree', 'local', 'dependence', 'item', 'passage', 'produce', 'large', 'equate', 'error', 'especially', 'group', 'new', 'form', 'reference', 'form', 'differ', 'ability', 'finding', 'challenge', 'common', 'belief', 'anchor', 'miniature', 'version', 'test', 'equate', 'suggestion', 'practitioner', 'regard', 'anchor', 'design', '©', '2010', 'National', 'Council']","['observe', 'Score', 'Equating', 'Discrete', 'PassageBased', 'Anchor', 'Items']",equating test compose discrete passagebase multiple choice item nonequivalent group anchor test design popular practice study compare effect discrete passagebase anchor item observe score equate simulation result suggest anchor large proportion passagebase item item passage andor large degree local dependence item passage produce large equate error especially group new form reference form differ ability finding challenge common belief anchor miniature version test equate suggestion practitioner regard anchor design © 2010 National Council,observe Score Equating Discrete PassageBased Anchor Items,0.03074040803947997,0.0305063489377307,0.8777565676941549,0.030525466252809743,0.030471209075824707,0.025928088399840083,0.0,0.0,0.11871068513909358,0.005757500307041026
Attali Y.,An Analysis of Variance Approach for the Estimation of Response Time Distributions in Tests,2010,47,"Generalizability theory and analysis of variance methods are employed, together with the concept of objective time pressure, to estimate response time distributions and the degree of time pressure in timed tests. By estimating response time variance components due to person, item, and their interaction, and fixed effects due to item types and examinee time pressure, one can predict the distribution (mean and variance) of total response time for a population of examinees and a particular time limit. Furthermore, these variance components and fixed effects can be used in a simulation approach to estimate the distributions of time pressure during the test to help test developers evaluate the appropriateness of specific time limits. I present theoretical considerations and empirical results from two tests. © 2010 by the National Council on Measurement in Education.",An Analysis of Variance Approach for the Estimation of Response Time Distributions in Tests,"Generalizability theory and analysis of variance methods are employed, together with the concept of objective time pressure, to estimate response time distributions and the degree of time pressure in timed tests. By estimating response time variance components due to person, item, and their interaction, and fixed effects due to item types and examinee time pressure, one can predict the distribution (mean and variance) of total response time for a population of examinees and a particular time limit. Furthermore, these variance components and fixed effects can be used in a simulation approach to estimate the distributions of time pressure during the test to help test developers evaluate the appropriateness of specific time limits. I present theoretical considerations and empirical results from two tests. © 2010 by the National Council on Measurement in Education.","['generalizability', 'theory', 'analysis', 'variance', 'method', 'employ', 'concept', 'objective', 'time', 'pressure', 'estimate', 'response', 'time', 'distribution', 'degree', 'time', 'pressure', 'time', 'test', 'estimate', 'response', 'time', 'variance', 'component', 'person', 'item', 'interaction', 'fix', 'effect', 'item', 'type', 'examinee', 'time', 'pressure', 'predict', 'distribution', 'mean', 'variance', 'total', 'response', 'time', 'population', 'examinee', 'particular', 'time', 'limit', 'furthermore', 'variance', 'component', 'fix', 'effect', 'simulation', 'approach', 'estimate', 'distribution', 'time', 'pressure', 'test', 'help', 'test', 'developer', 'evaluate', 'appropriateness', 'specific', 'time', 'limit', 'I', 'present', 'theoretical', 'consideration', 'empirical', 'result', 'test', '©', '2010', 'National', 'Council']","['Analysis', 'Variance', 'Approach', 'Estimation', 'Response', 'Time', 'Distributions', 'test']",generalizability theory analysis variance method employ concept objective time pressure estimate response time distribution degree time pressure time test estimate response time variance component person item interaction fix effect item type examinee time pressure predict distribution mean variance total response time population examinee particular time limit furthermore variance component fix effect simulation approach estimate distribution time pressure test help test developer evaluate appropriateness specific time limit I present theoretical consideration empirical result test © 2010 National Council,Analysis Variance Approach Estimation Response Time Distributions test,0.03552939402536377,0.03528241224468121,0.03470368076495855,0.0351198243692058,0.8593646885957907,0.050247739501502305,0.010833226999337418,0.007694151817008138,0.0016573753945207457,0.005578333749442734
Penfield R.D.,Distinguishing between net and global DIF in polytomous items,2010,47,"In this article, I address two competing conceptions of differential item functioning (DIF) in polytomously scored items. The first conception, referred to as net DIF, concerns between-group differences in the conditional expected value of the polytomous response variable. The second conception, referred to as global DIF, concerns the conditional dependence of group membership and the polytomous response variable. The distinction between net and global DIF is important because different DIF evaluation methods are appropriate for net and global DIF; no currently available method is universally the best for detecting both net and global DIF. Net and global DIF definitions are presented under two different, yet compatible, modeling frameworks: a traditional item response theory (IRT) framework, and a differential step functioning (DSF) framework. The theoretical relationship between the IRT and DSF frameworks is presented. Available methods for evaluating net and global DIF are described, and an applied example of net and global DIF is presented. © 2010 by the National Council on Measurement in Education.",Distinguishing between net and global DIF in polytomous items,"In this article, I address two competing conceptions of differential item functioning (DIF) in polytomously scored items. The first conception, referred to as net DIF, concerns between-group differences in the conditional expected value of the polytomous response variable. The second conception, referred to as global DIF, concerns the conditional dependence of group membership and the polytomous response variable. The distinction between net and global DIF is important because different DIF evaluation methods are appropriate for net and global DIF; no currently available method is universally the best for detecting both net and global DIF. Net and global DIF definitions are presented under two different, yet compatible, modeling frameworks: a traditional item response theory (IRT) framework, and a differential step functioning (DSF) framework. The theoretical relationship between the IRT and DSF frameworks is presented. Available methods for evaluating net and global DIF are described, and an applied example of net and global DIF is presented. © 2010 by the National Council on Measurement in Education.","['article', 'I', 'address', 'compete', 'conception', 'differential', 'item', 'function', 'DIF', 'polytomously', 'score', 'item', 'conception', 'refer', 'net', 'DIF', 'concern', 'betweengroup', 'difference', 'conditional', 'expect', 'value', 'polytomous', 'response', 'variable', 'second', 'conception', 'refer', 'global', 'dif', 'concern', 'conditional', 'dependence', 'group', 'membership', 'polytomous', 'response', 'variable', 'distinction', 'net', 'global', 'DIF', 'important', 'different', 'DIF', 'evaluation', 'method', 'appropriate', 'net', 'global', 'DIF', 'currently', 'available', 'method', 'universally', 'good', 'detect', 'net', 'global', 'DIF', 'Net', 'global', 'DIF', 'definition', 'present', 'different', 'compatible', 'framework', 'traditional', 'item', 'response', 'theory', 'IRT', 'framework', 'differential', 'step', 'function', 'DSF', 'framework', 'theoretical', 'relationship', 'IRT', 'DSF', 'framework', 'present', 'available', 'method', 'evaluate', 'net', 'global', 'DIF', 'describe', 'apply', 'example', 'net', 'global', 'DIF', 'present', '©', '2010', 'National', 'Council']","['distinguish', 'net', 'global', 'DIF', 'polytomous', 'item']",article I address compete conception differential item function DIF polytomously score item conception refer net DIF concern betweengroup difference conditional expect value polytomous response variable second conception refer global dif concern conditional dependence group membership polytomous response variable distinction net global DIF important different DIF evaluation method appropriate net global DIF currently available method universally good detect net global DIF Net global DIF definition present different compatible framework traditional item response theory IRT framework differential step function DSF framework theoretical relationship IRT DSF framework present available method evaluate net global DIF describe apply example net global DIF present © 2010 National Council,distinguish net global DIF polytomous item,0.8626245662622746,0.034129397365320925,0.03438624295749869,0.03434023939336635,0.03451955402153956,0.055815725919865534,0.0,0.0,0.0,0.0
Liu J.; Sinharay S.; Holland P.W.; Curley E.; Feigenbaum M.,Test score equating using a mini-version anchor and a midi anchor: A case study using SAT® data,2011,48,"This study explores an anchor that is different from the traditional miniature anchor in test score equating. In contrast to a traditional ""mini"" anchor that has the same spread of item difficulties as the tests to be equated, the studied anchor, referred to as a ""midi"" anchor (Sinharay & Holland), has a smaller spread of item difficulties than the tests to be equated. Both anchors were administered in an operational SAT administration and the impact of anchor type on equating was evaluated with respect to systematic error or equating bias. Contradicting the popular belief that the mini anchor is best, the results showed that the mini anchor does not always produce more accurate equating functions than the midi anchor; the midi anchor was found to perform as well as or even better than the mini anchor. Because testing programs usually have more middle difficulty items and few very hard or very easy items, midi external anchors are operationally easier to build. Therefore, the results of our study provide evidence in favor of the midi anchor, the use of which will lead to cost saving with no reduction in equating quality. © 2011 by the National Council on Measurement in Education.",Test score equating using a mini-version anchor and a midi anchor: A case study using SAT® data,"This study explores an anchor that is different from the traditional miniature anchor in test score equating. In contrast to a traditional ""mini"" anchor that has the same spread of item difficulties as the tests to be equated, the studied anchor, referred to as a ""midi"" anchor (Sinharay & Holland), has a smaller spread of item difficulties than the tests to be equated. Both anchors were administered in an operational SAT administration and the impact of anchor type on equating was evaluated with respect to systematic error or equating bias. Contradicting the popular belief that the mini anchor is best, the results showed that the mini anchor does not always produce more accurate equating functions than the midi anchor; the midi anchor was found to perform as well as or even better than the mini anchor. Because testing programs usually have more middle difficulty items and few very hard or very easy items, midi external anchors are operationally easier to build. Therefore, the results of our study provide evidence in favor of the midi anchor, the use of which will lead to cost saving with no reduction in equating quality. © 2011 by the National Council on Measurement in Education.","['study', 'explore', 'anchor', 'different', 'traditional', 'miniature', 'anchor', 'test', 'score', 'equate', 'contrast', 'traditional', 'mini', 'anchor', 'spread', 'item', 'difficulty', 'test', 'equate', 'study', 'anchor', 'refer', 'midi', 'anchor', 'Sinharay', 'Holland', 'small', 'spread', 'item', 'difficulty', 'test', 'equate', 'anchor', 'administer', 'operational', 'SAT', 'administration', 'impact', 'anchor', 'type', 'equating', 'evaluate', 'respect', 'systematic', 'error', 'equate', 'bias', 'contradict', 'popular', 'belief', 'mini', 'anchor', 'good', 'result', 'mini', 'anchor', 'produce', 'accurate', 'equate', 'function', 'midi', 'anchor', 'midi', 'anchor', 'find', 'perform', 'mini', 'anchor', 'test', 'program', 'usually', 'middle', 'difficulty', 'item', 'hard', 'easy', 'item', 'midi', 'external', 'anchor', 'operationally', 'easy', 'build', 'result', 'study', 'provide', 'evidence', 'favor', 'midi', 'anchor', 'lead', 'cost', 'saving', 'reduction', 'equate', 'quality', '©', '2011', 'National', 'Council']","['test', 'score', 'equating', 'miniversion', 'anchor', 'midi', 'anchor', 'case', 'study', 'SAT', '®', 'datum']",study explore anchor different traditional miniature anchor test score equate contrast traditional mini anchor spread item difficulty test equate study anchor refer midi anchor Sinharay Holland small spread item difficulty test equate anchor administer operational SAT administration impact anchor type equating evaluate respect systematic error equate bias contradict popular belief mini anchor good result mini anchor produce accurate equate function midi anchor midi anchor find perform mini anchor test program usually middle difficulty item hard easy item midi external anchor operationally easy build result study provide evidence favor midi anchor lead cost saving reduction equate quality © 2011 National Council,test score equating miniversion anchor midi anchor case study SAT ® datum,0.03482963971614541,0.03366133498016731,0.03410597546248226,0.03419708190704361,0.8632059679341615,0.006979452233573641,0.004849833472661578,0.0,0.1062872125364014,0.005725171491100451
Seo M.; Roussos L.A.,Formulation of a DIMTEST Effect Size Measure (DESM) and Evaluation of the DESM Estimator Bias,2010,47,"DIMTEST is a widely used and studied method for testing the hypothesis of test unidimensionality as represented by local item independence. However, DIMTEST does not report the amount of multidimensionality that exists in data when rejecting its null. To provide more information regarding the degree to which data depart from unidimensionality, a DIMTEST-based Effect Size Measure (DESM) was formulated. In addition to detailing the development of the DESM estimate, the current study describes the theoretical formulation of a DESM parameter. To evaluate the efficacy of the DESM estimator according to test length, sample size, and correlations between dimensions, Monte Carlo simulations were conducted. The results of the simulation study indicated that the DESM estimator converged to its parameter as test length increased, and, as desired, its expected value did not increase with sample size (unlike the DIMTEST statistic in the case of multidimensionality). Also as desired, the standard error of DESM decreased as sample size increased. © 2010 by the National Council on Measurement in Education.",Formulation of a DIMTEST Effect Size Measure (DESM) and Evaluation of the DESM Estimator Bias,"DIMTEST is a widely used and studied method for testing the hypothesis of test unidimensionality as represented by local item independence. However, DIMTEST does not report the amount of multidimensionality that exists in data when rejecting its null. To provide more information regarding the degree to which data depart from unidimensionality, a DIMTEST-based Effect Size Measure (DESM) was formulated. In addition to detailing the development of the DESM estimate, the current study describes the theoretical formulation of a DESM parameter. To evaluate the efficacy of the DESM estimator according to test length, sample size, and correlations between dimensions, Monte Carlo simulations were conducted. The results of the simulation study indicated that the DESM estimator converged to its parameter as test length increased, and, as desired, its expected value did not increase with sample size (unlike the DIMTEST statistic in the case of multidimensionality). Also as desired, the standard error of DESM decreased as sample size increased. © 2010 by the National Council on Measurement in Education.","['DIMTEST', 'widely', 'study', 'method', 'test', 'hypothesis', 'test', 'unidimensionality', 'represent', 'local', 'item', 'independence', 'However', 'DIMTEST', 'report', 'multidimensionality', 'exist', 'datum', 'reject', 'null', 'provide', 'information', 'regard', 'degree', 'datum', 'depart', 'unidimensionality', 'dimtestbase', 'Effect', 'Size', 'measure', 'DESM', 'formulate', 'addition', 'detail', 'development', 'DESM', 'estimate', 'current', 'study', 'describe', 'theoretical', 'formulation', 'desm', 'parameter', 'evaluate', 'efficacy', 'DESM', 'estimator', 'accord', 'test', 'length', 'sample', 'size', 'correlation', 'dimension', 'Monte', 'Carlo', 'simulation', 'conduct', 'result', 'simulation', 'study', 'indicate', 'DESM', 'estimator', 'converge', 'parameter', 'test', 'length', 'increase', 'desire', 'expect', 'value', 'increase', 'sample', 'size', 'unlike', 'DIMTEST', 'statistic', 'case', 'multidimensionality', 'desire', 'standard', 'error', 'DESM', 'decrease', 'sample', 'size', 'increase', '©', '2010', 'National', 'Council']","['formulation', 'dimtest', 'Effect', 'Size', 'measure', 'DESM', 'Evaluation', 'DESM', 'Estimator', 'Bias']",DIMTEST widely study method test hypothesis test unidimensionality represent local item independence However DIMTEST report multidimensionality exist datum reject null provide information regard degree datum depart unidimensionality dimtestbase Effect Size measure DESM formulate addition detail development DESM estimate current study describe theoretical formulation desm parameter evaluate efficacy DESM estimator accord test length sample size correlation dimension Monte Carlo simulation conduct result simulation study indicate DESM estimator converge parameter test length increase desire expect value increase sample size unlike DIMTEST statistic case multidimensionality desire standard error DESM decrease sample size increase © 2010 National Council,formulation dimtest Effect Size measure DESM Evaluation DESM Estimator Bias,0.030047912401512732,0.029480012359720385,0.029393691058076682,0.02943683195920202,0.8816415522214881,0.04168360290618853,0.013501699366662692,0.00242197757251671,0.013826254120888616,0.0017735202611828557
DeCarlo L.T.; Kim Y.; Johnson M.S.,"A hierarchical rater model for constructed responses, with a signal detection rater model",2011,48,"The hierarchical rater model (HRM) re-cognizes the hierarchical structure of data that arises when raters score constructed response items. In this approach, raters' scores are not viewed as being direct indicators of examinee proficiency but rather as indicators of essay quality; the (latent categorical) quality of an examinee's essay in turn serves as an indicator of the examinee's proficiency, thus yielding a hierarchical structure. Here it is shown that a latent class model motivated by signal detection theory (SDT) is a natural candidate for the first level of the HRM, the rater model. The latent class SDT model provides measures of rater precision and various rater effects, above and beyond simply severity or leniency. The HRM-SDT model is applied to data from a large-scale assessment and is shown to provide a useful summary of various aspects of the raters' performance. © 2011 by the National Council on Measurement in Education.","A hierarchical rater model for constructed responses, with a signal detection rater model","The hierarchical rater model (HRM) re-cognizes the hierarchical structure of data that arises when raters score constructed response items. In this approach, raters' scores are not viewed as being direct indicators of examinee proficiency but rather as indicators of essay quality; the (latent categorical) quality of an examinee's essay in turn serves as an indicator of the examinee's proficiency, thus yielding a hierarchical structure. Here it is shown that a latent class model motivated by signal detection theory (SDT) is a natural candidate for the first level of the HRM, the rater model. The latent class SDT model provides measures of rater precision and various rater effects, above and beyond simply severity or leniency. The HRM-SDT model is applied to data from a large-scale assessment and is shown to provide a useful summary of various aspects of the raters' performance. © 2011 by the National Council on Measurement in Education.","['hierarchical', 'rater', 'HRM', 'recognize', 'hierarchical', 'structure', 'datum', 'arise', 'rater', 'score', 'construct', 'response', 'item', 'approach', 'rater', 'score', 'view', 'direct', 'indicator', 'examinee', 'proficiency', 'indicator', 'essay', 'quality', 'latent', 'categorical', 'quality', 'examinee', 'essay', 'turn', 'serve', 'indicator', 'examinees', 'proficiency', 'yield', 'hierarchical', 'structure', 'latent', 'class', 'motivate', 'signal', 'detection', 'theory', 'SDT', 'natural', 'candidate', 'level', 'HRM', 'rater', 'latent', 'class', 'SDT', 'provide', 'measure', 'rater', 'precision', 'rater', 'effect', 'simply', 'severity', 'leniency', 'HRMSDT', 'apply', 'datum', 'largescale', 'assessment', 'provide', 'useful', 'summary', 'aspect', 'rater', 'performance', '©', '2011', 'National', 'Council']","['hierarchical', 'rater', 'construct', 'response', 'signal', 'detection', 'rater']",hierarchical rater HRM recognize hierarchical structure datum arise rater score construct response item approach rater score view direct indicator examinee proficiency indicator essay quality latent categorical quality examinee essay turn serve indicator examinees proficiency yield hierarchical structure latent class motivate signal detection theory SDT natural candidate level HRM rater latent class SDT provide measure rater precision rater effect simply severity leniency HRMSDT apply datum largescale assessment provide useful summary aspect rater performance © 2011 National Council,hierarchical rater construct response signal detection rater,0.03086747647312981,0.03064552174007424,0.029830550860770737,0.8785987548814905,0.030057696044534715,0.0015279536306056101,0.0,0.0,0.0,0.23602277988758227
Mislevy J.L.; Rupp A.A.; Harring J.R.,Detecting Local Item Dependence in Polytomous Adaptive Data,2012,49,"A rapidly expanding arena for item response theory (IRT) is in attitudinal and health-outcomes survey applications, often with polytomous items. In particular, there is interest in computer adaptive testing (CAT). Meeting model assumptions is necessary to realize the benefits of IRT in this setting, however. Although initial investigations of local item dependence have been studied both for polytomous items in fixed-form settings and for dichotomous items in CAT settings, there have been no publications applying local item dependence detection methodology to polytomous items in CAT despite its central importance to these applications. The current research uses a simulation study to investigate the extension of widely used pairwise statistics, Yen's Q 3 Statistic and Pearson's Statistic X 2, in this context. The simulation design and results are contextualized throughout with a real item bank of this type from the Patient-Reported Outcomes Measurement Information System (PROMIS). © 2012 by the National Council on Measurement in Education.",Detecting Local Item Dependence in Polytomous Adaptive Data,"A rapidly expanding arena for item response theory (IRT) is in attitudinal and health-outcomes survey applications, often with polytomous items. In particular, there is interest in computer adaptive testing (CAT). Meeting model assumptions is necessary to realize the benefits of IRT in this setting, however. Although initial investigations of local item dependence have been studied both for polytomous items in fixed-form settings and for dichotomous items in CAT settings, there have been no publications applying local item dependence detection methodology to polytomous items in CAT despite its central importance to these applications. The current research uses a simulation study to investigate the extension of widely used pairwise statistics, Yen's Q 3 Statistic and Pearson's Statistic X 2, in this context. The simulation design and results are contextualized throughout with a real item bank of this type from the Patient-Reported Outcomes Measurement Information System (PROMIS). © 2012 by the National Council on Measurement in Education.","['rapidly', 'expand', 'arena', 'item', 'response', 'theory', 'IRT', 'attitudinal', 'healthoutcome', 'survey', 'application', 'polytomous', 'item', 'particular', 'interest', 'computer', 'adaptive', 'testing', 'CAT', 'Meeting', 'assumption', 'necessary', 'realize', 'benefit', 'IRT', 'setting', 'initial', 'investigation', 'local', 'item', 'dependence', 'study', 'polytomous', 'item', 'fixedform', 'setting', 'dichotomous', 'item', 'CAT', 'setting', 'publication', 'apply', 'local', 'item', 'dependence', 'detection', 'methodology', 'polytomous', 'item', 'CAT', 'despite', 'central', 'importance', 'application', 'current', 'research', 'simulation', 'study', 'investigate', 'extension', 'widely', 'pairwise', 'statistic', 'Yens', 'Q', '3', 'Statistic', 'Pearsons', 'Statistic', 'X', '2', 'context', 'simulation', 'design', 'result', 'contextualize', 'real', 'item', 'bank', 'type', 'PatientReported', 'Outcomes', 'Information', 'System', 'PROMIS', '©', '2012', 'National', 'Council']","['detect', 'Local', 'Item', 'Dependence', 'Polytomous', 'Adaptive', 'Data']",rapidly expand arena item response theory IRT attitudinal healthoutcome survey application polytomous item particular interest computer adaptive testing CAT Meeting assumption necessary realize benefit IRT setting initial investigation local item dependence study polytomous item fixedform setting dichotomous item CAT setting publication apply local item dependence detection methodology polytomous item CAT despite central importance application current research simulation study investigate extension widely pairwise statistic Yens Q 3 Statistic Pearsons Statistic X 2 context simulation design result contextualize real item bank type PatientReported Outcomes Information System PROMIS © 2012 National Council,detect Local Item Dependence Polytomous Adaptive Data,0.02432530090493757,0.024219310626508287,0.9026529441902171,0.024163791513324324,0.02463865276501243,0.07291466417381168,0.0,0.0,0.0,0.0049715859795000416
Li D.; Jiang Y.; von Davier A.A.,The Accuracy and Consistency of a Series of IRT True Score Equatings,2012,49,"This study investigates a sequence of item response theory (IRT) true score equatings based on various scale transformation approaches and evaluates equating accuracy and consistency over time. The results show that the biases and sample variances for the IRT true score equating (both direct and indirect) are quite small (except for the mean/sigma method). The biases and sample variances for the equating functions based on the characteristic curve methods and concurrent calibrations for adjacent forms are smaller than the biases and variances for the equating functions based on the moment methods. In addition, the IRT true score equating is also compared to the chained equipercentile equating, and we observe that the sample variances for the chained equipercentile equating are much smaller than the variances for the IRT true score equating with an exception at the low scores. © 2012 by the National Council on Measurement in Education.",The Accuracy and Consistency of a Series of IRT True Score Equatings,"This study investigates a sequence of item response theory (IRT) true score equatings based on various scale transformation approaches and evaluates equating accuracy and consistency over time. The results show that the biases and sample variances for the IRT true score equating (both direct and indirect) are quite small (except for the mean/sigma method). The biases and sample variances for the equating functions based on the characteristic curve methods and concurrent calibrations for adjacent forms are smaller than the biases and variances for the equating functions based on the moment methods. In addition, the IRT true score equating is also compared to the chained equipercentile equating, and we observe that the sample variances for the chained equipercentile equating are much smaller than the variances for the IRT true score equating with an exception at the low scores. © 2012 by the National Council on Measurement in Education.","['study', 'investigate', 'sequence', 'item', 'response', 'theory', 'IRT', 'true', 'score', 'equating', 'base', 'scale', 'transformation', 'approach', 'evaluate', 'equate', 'accuracy', 'consistency', 'time', 'result', 'bias', 'sample', 'variance', 'IRT', 'true', 'score', 'equate', 'direct', 'indirect', 'small', 'meansigma', 'method', 'bias', 'sample', 'variance', 'equate', 'function', 'base', 'characteristic', 'curve', 'method', 'concurrent', 'calibration', 'adjacent', 'form', 'small', 'bias', 'variance', 'equate', 'function', 'base', 'moment', 'method', 'addition', 'IRT', 'true', 'score', 'equating', 'compare', 'chain', 'equipercentile', 'equating', 'observe', 'sample', 'variance', 'chain', 'equipercentile', 'equating', 'small', 'variance', 'IRT', 'true', 'score', 'equate', 'exception', 'low', 'score', '©', '2012', 'National', 'Council']","['Accuracy', 'Consistency', 'Series', 'IRT', 'true', 'Score', 'equating']",study investigate sequence item response theory IRT true score equating base scale transformation approach evaluate equate accuracy consistency time result bias sample variance IRT true score equate direct indirect small meansigma method bias sample variance equate function base characteristic curve method concurrent calibration adjacent form small bias variance equate function base moment method addition IRT true score equating compare chain equipercentile equating observe sample variance chain equipercentile equating small variance IRT true score equate exception low score © 2012 National Council,Accuracy Consistency Series IRT true Score equating,0.8712641486342426,0.03171035627997591,0.032728143145706566,0.03224356804895932,0.03205378389111553,0.005323934560094165,0.009584436014314674,0.0,0.186176610986982,0.0
Zhu X.; Stone C.A.,Assessing fit of unidimensional graded response models using bayesian methods,2011,48,"The posterior predictive model checking method is a flexible Bayesian model-checking tool and has recently been used to assess fit of dichotomous IRT models. This paper extended previous research to polytomous IRT models. A simulation study was conducted to explore the performance of posterior predictive model checking in evaluating different aspects of fit for unidimensional graded response models. A variety of discrepancy measures (test-level, item-level, and pair-wise measures) that reflected different threats to applications of graded IRT models to performance assessments were considered. Results showed that posterior predictive model checking exhibited adequate power in detecting different aspects of misfit for graded IRT models when appropriate discrepancy measures were used. Pair-wise measures were found more powerful in detecting violations of the unidimensionality and local independence assumptions. © 2011 by the National Council on Measurement in Education.",Assessing fit of unidimensional graded response models using bayesian methods,"The posterior predictive model checking method is a flexible Bayesian model-checking tool and has recently been used to assess fit of dichotomous IRT models. This paper extended previous research to polytomous IRT models. A simulation study was conducted to explore the performance of posterior predictive model checking in evaluating different aspects of fit for unidimensional graded response models. A variety of discrepancy measures (test-level, item-level, and pair-wise measures) that reflected different threats to applications of graded IRT models to performance assessments were considered. Results showed that posterior predictive model checking exhibited adequate power in detecting different aspects of misfit for graded IRT models when appropriate discrepancy measures were used. Pair-wise measures were found more powerful in detecting violations of the unidimensionality and local independence assumptions. © 2011 by the National Council on Measurement in Education.","['posterior', 'predictive', 'checking', 'method', 'flexible', 'bayesian', 'modelchecking', 'tool', 'recently', 'assess', 'fit', 'dichotomous', 'IRT', 'paper', 'extend', 'previous', 'research', 'polytomous', 'IRT', 'simulation', 'study', 'conduct', 'explore', 'performance', 'posterior', 'predictive', 'check', 'evaluate', 'different', 'aspect', 'fit', 'unidimensional', 'grade', 'response', 'variety', 'discrepancy', 'measure', 'testlevel', 'itemlevel', 'pairwise', 'measure', 'reflect', 'different', 'threat', 'application', 'grade', 'IRT', 'performance', 'assessment', 'consider', 'result', 'posterior', 'predictive', 'checking', 'exhibit', 'adequate', 'power', 'detect', 'different', 'aspect', 'misfit', 'grade', 'IRT', 'appropriate', 'discrepancy', 'measure', 'pairwise', 'measure', 'find', 'powerful', 'detect', 'violation', 'unidimensionality', 'local', 'independence', 'assumption', '©', '2011', 'National', 'Council']","['assess', 'fit', 'unidimensional', 'grade', 'response', 'bayesian', 'method']",posterior predictive checking method flexible bayesian modelchecking tool recently assess fit dichotomous IRT paper extend previous research polytomous IRT simulation study conduct explore performance posterior predictive check evaluate different aspect fit unidimensional grade response variety discrepancy measure testlevel itemlevel pairwise measure reflect different threat application grade IRT performance assessment consider result posterior predictive checking exhibit adequate power detect different aspect misfit grade IRT appropriate discrepancy measure pairwise measure find powerful detect violation unidimensionality local independence assumption © 2011 National Council,assess fit unidimensional grade response bayesian method,0.027666286556144212,0.027770971002964603,0.889093325299805,0.027880947968956966,0.02758846917212917,0.03341276922448346,0.0,0.02598201702935502,0.002364330067908398,0.01182577858059493
Chon K.H.; Lee W.-C.; Dunbar S.B.,A comparison of item fit statistics for mixed IRT models,2010,47,"In this study we examined procedures for assessing model-data fit of item response theory (IRT) models for mixed format data. The model fit indices used in this study include PARSCALE's G2, Orlando and Thissen's S - X2 and S - G2, and Stone's χ2* and G2*. To investigate the relative performance of the fit statistics at the item level, we conducted two simulation studies: Type I error and power studies. We evaluated the performance of the item fit indices for various conditions of test length, sample size, and IRT models. Among the competing measures, the summed score-based indices S - X2 and S - G2 were found to be the sensible and efficient choice for assessing model fit for mixed format data. These indices performed well, particularly with short tests. The pseudo-observed score indices, χ2* and G2*, showed inflated Type I error rates in some simulation conditions. Consistent with the findings of current literature, the PARSCALE's G2 index was rarely useful, although it provided reasonable results for long tests. © 2010 by the National Council on Measurement in Education.",A comparison of item fit statistics for mixed IRT models,"In this study we examined procedures for assessing model-data fit of item response theory (IRT) models for mixed format data. The model fit indices used in this study include PARSCALE's G2, Orlando and Thissen's S - X2 and S - G2, and Stone's χ2* and G2*. To investigate the relative performance of the fit statistics at the item level, we conducted two simulation studies: Type I error and power studies. We evaluated the performance of the item fit indices for various conditions of test length, sample size, and IRT models. Among the competing measures, the summed score-based indices S - X2 and S - G2 were found to be the sensible and efficient choice for assessing model fit for mixed format data. These indices performed well, particularly with short tests. The pseudo-observed score indices, χ2* and G2*, showed inflated Type I error rates in some simulation conditions. Consistent with the findings of current literature, the PARSCALE's G2 index was rarely useful, although it provided reasonable results for long tests. © 2010 by the National Council on Measurement in Education.","['study', 'examine', 'procedure', 'assess', 'modeldata', 'fit', 'item', 'response', 'theory', 'IRT', 'mixed', 'format', 'datum', 'fit', 'index', 'study', 'include', 'PARSCALEs', 'G2', 'Orlando', 'Thissens', 'S', 'x2', 'S', 'G2', 'Stones', 'χ2', 'G2', 'investigate', 'relative', 'performance', 'fit', 'statistic', 'item', 'level', 'conduct', 'simulation', 'study', 'Type', 'I', 'error', 'power', 'study', 'evaluate', 'performance', 'item', 'fit', 'index', 'condition', 'test', 'length', 'sample', 'size', 'IRT', 'compete', 'measure', 'sum', 'scorebased', 'index', 'S', 'x2', 'S', 'G2', 'find', 'sensible', 'efficient', 'choice', 'assess', 'fit', 'mixed', 'format', 'datum', 'index', 'perform', 'particularly', 'short', 'test', 'pseudoobserved', 'score', 'indice', 'χ2', 'G2', 'inflated', 'Type', 'I', 'error', 'rate', 'simulation', 'condition', 'consistent', 'finding', 'current', 'literature', 'PARSCALEs', 'G2', 'index', 'rarely', 'useful', 'provide', 'reasonable', 'result', 'long', 'test', '©', '2010', 'National', 'Council']","['comparison', 'item', 'fit', 'statistic', 'mixed', 'IRT']",study examine procedure assess modeldata fit item response theory IRT mixed format datum fit index study include PARSCALEs G2 Orlando Thissens S x2 S G2 Stones χ2 G2 investigate relative performance fit statistic item level conduct simulation study Type I error power study evaluate performance item fit index condition test length sample size IRT compete measure sum scorebased index S x2 S G2 find sensible efficient choice assess fit mixed format datum index perform particularly short test pseudoobserved score indice χ2 G2 inflated Type I error rate simulation condition consistent finding current literature PARSCALEs G2 index rarely useful provide reasonable result long test © 2010 National Council,comparison item fit statistic mixed IRT,0.028419529333330338,0.02839052090648045,0.02827987698825294,0.8865850816161827,0.028324991155753584,0.05247005694160027,0.005539060799415938,0.005144307525621136,0.0004497338820257561,0.004763023709153083
de la Torre J.; Lee Y.-S.,A note on the invariance of the dina model parameters,2010,47,"Cognitive diagnosis models (CDMs), as alternative approaches to unidimensional item response models, have received increasing attention in recent years. CDMs are developed for the purpose of identifying the mastery or nonmastery of multiple fine-grained attributes or skills required for solving problems in a domain. For CDMs to receive wider use, researchers and practitioners need to understand the basic properties of these models. The article focuses on one CDM, the deterministic inputs, noisy "" and"" gate (DINA) model, and the invariance property of its parameters. Using simulated data involving different attribute distributions, the article demonstrates that the DINA model parameters are absolutely invariant when the model perfectly fits the data. An additional example involving different ability groups illustrates how noise in real data can contribute to the lack of invariance in these parameters. Some practical implications of these findings are discussed. © 2010 by the National Council on Measurement in Education.",A note on the invariance of the dina model parameters,"Cognitive diagnosis models (CDMs), as alternative approaches to unidimensional item response models, have received increasing attention in recent years. CDMs are developed for the purpose of identifying the mastery or nonmastery of multiple fine-grained attributes or skills required for solving problems in a domain. For CDMs to receive wider use, researchers and practitioners need to understand the basic properties of these models. The article focuses on one CDM, the deterministic inputs, noisy "" and"" gate (DINA) model, and the invariance property of its parameters. Using simulated data involving different attribute distributions, the article demonstrates that the DINA model parameters are absolutely invariant when the model perfectly fits the data. An additional example involving different ability groups illustrates how noise in real data can contribute to the lack of invariance in these parameters. Some practical implications of these findings are discussed. © 2010 by the National Council on Measurement in Education.","['cognitive', 'diagnosis', 'cdm', 'alternative', 'approach', 'unidimensional', 'item', 'response', 'receive', 'increase', 'attention', 'recent', 'year', 'CDMs', 'develop', 'purpose', 'identify', 'mastery', 'nonmastery', 'multiple', 'finegraine', 'attribute', 'skill', 'require', 'solve', 'problem', 'domain', 'cdm', 'receive', 'wide', 'researcher', 'practitioner', 'need', 'understand', 'basic', 'property', 'article', 'focus', 'cdm', 'deterministic', 'input', 'noisy', 'gate', 'DINA', 'invariance', 'property', 'parameter', 'simulated', 'datum', 'involve', 'different', 'attribute', 'distribution', 'article', 'demonstrate', 'DINA', 'parameter', 'absolutely', 'invariant', 'perfectly', 'fit', 'datum', 'additional', 'example', 'involve', 'different', 'ability', 'group', 'illustrate', 'noise', 'real', 'datum', 'contribute', 'lack', 'invariance', 'parameter', 'practical', 'implication', 'finding', 'discuss', '©', '2010', 'National', 'Council']","['note', 'invariance', 'dina', 'parameter']",cognitive diagnosis cdm alternative approach unidimensional item response receive increase attention recent year CDMs develop purpose identify mastery nonmastery multiple finegraine attribute skill require solve problem domain cdm receive wide researcher practitioner need understand basic property article focus cdm deterministic input noisy gate DINA invariance property parameter simulated datum involve different attribute distribution article demonstrate DINA parameter absolutely invariant perfectly fit datum additional example involve different ability group illustrate noise real datum contribute lack invariance parameter practical implication finding discuss © 2010 National Council,note invariance dina parameter,0.023787932308645898,0.023707037836933866,0.023770618826850283,0.9048632714072833,0.023871139620286707,0.04676795239342218,0.0,0.039409624583747895,0.0,0.002340368047858464
Kane M.,The errors of our ways,2011,48,"Errors don't exist in our data, but they serve a vital function. Reality is complicated, but our models need to be simple in order to be manageable. We assume that attributes are invariant over some conditions of observation, and once we do that we need some way of accounting for the variability in observed scores over these conditions of observation. We relegate this inconvenient variability to errors of measurement. The seriousness of errors of measurement depends on the intended interpretations and uses of the scores and the context in which they are used. Errors are too large if they interfere with the intended interpretations and uses, and otherwise are acceptable. The errors of measurement have to be small compared to the tolerance for error, and errors that are too large have to be controlled in some way. We have several ways of doing this. We can redefine the attribute of interest, we can standardize the assessments and leave the attribute alone, and/or we can sample the relevant performance domain more thoroughly. It is particularly important to control the larger sources of error. If a source of error (systematic or random) is small compared to the dominant sources of error for a testing procedure, it can generally be ignored. © 2011 by the National Council on Measurement in Education.",,"Errors don't exist in our data, but they serve a vital function. Reality is complicated, but our models need to be simple in order to be manageable. We assume that attributes are invariant over some conditions of observation, and once we do that we need some way of accounting for the variability in observed scores over these conditions of observation. We relegate this inconvenient variability to errors of measurement. The seriousness of errors of measurement depends on the intended interpretations and uses of the scores and the context in which they are used. Errors are too large if they interfere with the intended interpretations and uses, and otherwise are acceptable. The errors of measurement have to be small compared to the tolerance for error, and errors that are too large have to be controlled in some way. We have several ways of doing this. We can redefine the attribute of interest, we can standardize the assessments and leave the attribute alone, and/or we can sample the relevant performance domain more thoroughly. It is particularly important to control the larger sources of error. If a source of error (systematic or random) is small compared to the dominant sources of error for a testing procedure, it can generally be ignored. © 2011 by the National Council on Measurement in Education.","['error', 'exist', 'datum', 'serve', 'vital', 'function', 'reality', 'complicated', 'need', 'simple', 'order', 'manageable', 'assume', 'attribute', 'invariant', 'condition', 'observation', 'need', 'way', 'account', 'variability', 'observe', 'score', 'condition', 'observation', 'relegate', 'inconvenient', 'variability', 'error', 'seriousness', 'error', 'depend', 'intend', 'interpretation', 'score', 'context', 'error', 'large', 'interfere', 'intend', 'interpretation', 'acceptable', 'error', 'small', 'compare', 'tolerance', 'error', 'error', 'large', 'control', 'way', 'way', 'redefine', 'attribute', 'interest', 'standardize', 'assessment', 'leave', 'attribute', 'andor', 'sample', 'relevant', 'performance', 'domain', 'thoroughly', 'particularly', 'important', 'control', 'large', 'source', 'error', 'source', 'error', 'systematic', 'random', 'small', 'compare', 'dominant', 'source', 'error', 'testing', 'procedure', 'generally', 'ignore', '©', '2011', 'National', 'Council']",,error exist datum serve vital function reality complicated need simple order manageable assume attribute invariant condition observation need way account variability observe score condition observation relegate inconvenient variability error seriousness error depend intend interpretation score context error large interfere intend interpretation acceptable error small compare tolerance error error large control way way redefine attribute interest standardize assessment leave attribute andor sample relevant performance domain thoroughly particularly important control large source error source error systematic random small compare dominant source error testing procedure generally ignore © 2011 National Council,,0.026801204525495333,0.026584304211976936,0.026710786146530857,0.0266965312032939,0.893207173912703,0.015318799152239793,0.007785678816105519,0.04576750464270544,0.023043173902768853,0.0016380180334924846
Kim S.; Walker M.E.; McHale F.,Comparisons among designs for equating mixed-format tests in large-scale assessments,2010,47,"In this study we examined variations of the nonequivalent groups equating design for tests containing both multiple-choice (MC) and constructed-response (CR) items to determine which design was most effective in producing equivalent scores across the two tests to be equated. Using data from a large-scale exam, this study investigated the use of anchor CR item rescoring (known as trend scoring) in the context of classical equating methods. Four linking designs were examined: an anchor with only MC items, a mixed-format anchor test containing both MC and CR items; a mixed-format anchor test incorporating common CR item rescoring; and an equivalent groups (EG) design with CR item rescoring, thereby avoiding the need for an anchor test. Designs using either MC items alone or a mixed anchor without CR item rescoring resulted in much larger bias than the other two designs. The EG design with trend scoring resulted in the smallest bias, leading to the smallest root mean squared error value. © 2010 by the National Council on Measurement in Education.",Comparisons among designs for equating mixed-format tests in large-scale assessments,"In this study we examined variations of the nonequivalent groups equating design for tests containing both multiple-choice (MC) and constructed-response (CR) items to determine which design was most effective in producing equivalent scores across the two tests to be equated. Using data from a large-scale exam, this study investigated the use of anchor CR item rescoring (known as trend scoring) in the context of classical equating methods. Four linking designs were examined: an anchor with only MC items, a mixed-format anchor test containing both MC and CR items; a mixed-format anchor test incorporating common CR item rescoring; and an equivalent groups (EG) design with CR item rescoring, thereby avoiding the need for an anchor test. Designs using either MC items alone or a mixed anchor without CR item rescoring resulted in much larger bias than the other two designs. The EG design with trend scoring resulted in the smallest bias, leading to the smallest root mean squared error value. © 2010 by the National Council on Measurement in Education.","['study', 'examine', 'variation', 'nonequivalent', 'group', 'equate', 'design', 'test', 'contain', 'multiplechoice', 'MC', 'constructedresponse', 'CR', 'item', 'determine', 'design', 'effective', 'produce', 'equivalent', 'score', 'test', 'equate', 'datum', 'largescale', 'exam', 'study', 'investigate', 'anchor', 'CR', 'item', 'rescore', 'know', 'trend', 'scoring', 'context', 'classical', 'equating', 'method', 'linking', 'design', 'examine', 'anchor', 'MC', 'item', 'mixedformat', 'anchor', 'test', 'contain', 'MC', 'CR', 'item', 'mixedformat', 'anchor', 'test', 'incorporate', 'common', 'cr', 'item', 'rescoring', 'equivalent', 'group', 'eg', 'design', 'CR', 'item', 'rescore', 'avoid', 'need', 'anchor', 'test', 'design', 'MC', 'item', 'mixed', 'anchor', 'CR', 'item', 'rescoring', 'result', 'large', 'bias', 'design', 'EG', 'design', 'trend', 'scoring', 'result', 'small', 'bias', 'lead', 'small', 'root', 'mean', 'square', 'error', 'value', '©', '2010', 'National', 'Council']","['comparison', 'design', 'equate', 'mixedformat', 'test', 'largescale', 'assessment']",study examine variation nonequivalent group equate design test contain multiplechoice MC constructedresponse CR item determine design effective produce equivalent score test equate datum largescale exam study investigate anchor CR item rescore know trend scoring context classical equating method linking design examine anchor MC item mixedformat anchor test contain MC CR item mixedformat anchor test incorporate common cr item rescoring equivalent group eg design CR item rescore avoid need anchor test design MC item mixed anchor CR item rescoring result large bias design EG design trend scoring result small bias lead small root mean square error value © 2010 National Council,comparison design equate mixedformat test largescale assessment,0.8743251591971908,0.031501978997253884,0.031631079654085674,0.03128790240212589,0.03125387974934376,0.03644124522414421,0.0027057937911396297,0.0,0.08079746154261618,0.0008004668729298224
Wang W.-C.; Wu S.-L.,The random-effect generalized rating scale model,2011,48,"Rating scale items have been widely used in educational and psychological tests. These items require people to make subjective judgments, and these subjective judgments usually involve randomness. To account for this randomness, Wang, Wilson, and Shih proposed the random-effect rating scale model in which the threshold parameters are treated as random effects rather than fixed effects. In the present study, the Wang et al. model was further extended to incorporate slope parameters and embed the new model within the framework of multilevel nonlinear mixed-effect models. This was done so that (1) no efforts are needed to derive parameter estimation procedures, and (2) existing computer programs can be applied directly. A brief simulation study was conducted to ascertain parameter recovery using the SAS NLMIXED procedure. An empirical example regarding students' interest in learning science is presented to demonstrate the implications and applications of the new model. © 2011 by the National Council on Measurement in Education.",,"Rating scale items have been widely used in educational and psychological tests. These items require people to make subjective judgments, and these subjective judgments usually involve randomness. To account for this randomness, Wang, Wilson, and Shih proposed the random-effect rating scale model in which the threshold parameters are treated as random effects rather than fixed effects. In the present study, the Wang et al. model was further extended to incorporate slope parameters and embed the new model within the framework of multilevel nonlinear mixed-effect models. This was done so that (1) no efforts are needed to derive parameter estimation procedures, and (2) existing computer programs can be applied directly. A brief simulation study was conducted to ascertain parameter recovery using the SAS NLMIXED procedure. An empirical example regarding students' interest in learning science is presented to demonstrate the implications and applications of the new model. © 2011 by the National Council on Measurement in Education.","['rating', 'scale', 'item', 'widely', 'educational', 'psychological', 'test', 'item', 'require', 'people', 'subjective', 'judgment', 'subjective', 'judgment', 'usually', 'involve', 'randomness', 'account', 'randomness', 'Wang', 'Wilson', 'Shih', 'propose', 'randomeffect', 'rating', 'scale', 'threshold', 'parameter', 'treat', 'random', 'effect', 'fix', 'effect', 'present', 'study', 'Wang', 'et', 'al', 'far', 'extend', 'incorporate', 'slope', 'parameter', 'embed', 'new', 'framework', 'multilevel', 'nonlinear', 'mixedeffect', '1', 'effort', 'need', 'derive', 'parameter', 'estimation', 'procedure', '2', 'exist', 'computer', 'program', 'apply', 'directly', 'brief', 'simulation', 'study', 'conduct', 'ascertain', 'parameter', 'recovery', 'SAS', 'NLMIXED', 'procedure', 'empirical', 'example', 'regard', 'student', 'interest', 'learn', 'science', 'present', 'demonstrate', 'implication', 'application', 'new', '©', '2011', 'National', 'Council']",,rating scale item widely educational psychological test item require people subjective judgment subjective judgment usually involve randomness account randomness Wang Wilson Shih propose randomeffect rating scale threshold parameter treat random effect fix effect present study Wang et al far extend incorporate slope parameter embed new framework multilevel nonlinear mixedeffect 1 effort need derive parameter estimation procedure 2 exist computer program apply directly brief simulation study conduct ascertain parameter recovery SAS NLMIXED procedure empirical example regard student interest learn science present demonstrate implication application new © 2011 National Council,,0.023201801883502286,0.02327401894150566,0.02316148111627521,0.023215607288394526,0.9071470907703223,0.04967242975501138,0.0,0.025223143655837475,0.0,0.03474465870757038
Liu J.; Dorans N.J.,Assessing the Practical Equivalence of Conversions When Measurement Conditions Change,2012,49,"At times, the same set of test questions is administered under different measurement conditions that might affect the psychometric properties of the test scores enough to warrant different score conversions for the different conditions. We propose a procedure for assessing the practical equivalence of conversions developed for the same set of test questions but administered under different measurement conditions. This procedure assesses whether the use of separate conversions for each condition has a desirable or undesirable effect. We distinguish effects due to differences in difficulty from effects due to rounding conventions. The proposed procedure provides objective empirical information that assists in deciding to report a common conversion for a set of items or a different conversion for the set of items when the set is administered under different measurement conditions. To illustrate the use of the procedure, we consider the case where a scrambled test form is used along with a base test form. If section order effects are detected between the scrambled and base forms, a decision needs to be made whether to report a single common conversion for both forms or to report separate conversions. © 2012 by the National Council on Measurement in Education.",Assessing the Practical Equivalence of Conversions When Measurement Conditions Change,"At times, the same set of test questions is administered under different measurement conditions that might affect the psychometric properties of the test scores enough to warrant different score conversions for the different conditions. We propose a procedure for assessing the practical equivalence of conversions developed for the same set of test questions but administered under different measurement conditions. This procedure assesses whether the use of separate conversions for each condition has a desirable or undesirable effect. We distinguish effects due to differences in difficulty from effects due to rounding conventions. The proposed procedure provides objective empirical information that assists in deciding to report a common conversion for a set of items or a different conversion for the set of items when the set is administered under different measurement conditions. To illustrate the use of the procedure, we consider the case where a scrambled test form is used along with a base test form. If section order effects are detected between the scrambled and base forms, a decision needs to be made whether to report a single common conversion for both forms or to report separate conversions. © 2012 by the National Council on Measurement in Education.","['time', 'set', 'test', 'question', 'administer', 'different', 'condition', 'affect', 'psychometric', 'property', 'test', 'score', 'warrant', 'different', 'score', 'conversion', 'different', 'condition', 'propose', 'procedure', 'assess', 'practical', 'equivalence', 'conversion', 'develop', 'set', 'test', 'question', 'administer', 'different', 'condition', 'procedure', 'assess', 'separate', 'conversion', 'condition', 'desirable', 'undesirable', 'effect', 'distinguish', 'effect', 'difference', 'difficulty', 'effect', 'round', 'convention', 'propose', 'procedure', 'provide', 'objective', 'empirical', 'information', 'assist', 'decide', 'report', 'common', 'conversion', 'set', 'item', 'different', 'conversion', 'set', 'item', 'set', 'administer', 'different', 'condition', 'illustrate', 'procedure', 'consider', 'case', 'scrambled', 'test', 'form', 'base', 'test', 'form', 'section', 'order', 'effect', 'detect', 'scrambled', 'base', 'form', 'decision', 'need', 'report', 'single', 'common', 'conversion', 'form', 'report', 'separate', 'conversion', '©', '2012', 'National', 'Council']","['assess', 'Practical', 'Equivalence', 'Conversions', 'Conditions', 'Change']",time set test question administer different condition affect psychometric property test score warrant different score conversion different condition propose procedure assess practical equivalence conversion develop set test question administer different condition procedure assess separate conversion condition desirable undesirable effect distinguish effect difference difficulty effect round convention propose procedure provide objective empirical information assist decide report common conversion set item different conversion set item set administer different condition illustrate procedure consider case scrambled test form base test form section order effect detect scrambled base form decision need report single common conversion form report separate conversion © 2012 National Council,assess Practical Equivalence Conversions Conditions Change,0.8734518886226712,0.03164458940282527,0.03159164527362672,0.03164702498110246,0.03166485171977437,0.046186852387269654,0.027840557164426783,0.009444311108988475,0.005033239872128661,0.007546753014637852
