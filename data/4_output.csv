Authors,Title,Year,Volume,Abstract,Title_spell,Abstract_spell,A_tokens,T_tokens,A_tokens_join,T_tokens_join,lda_0,lda_1,lda_2,lda_3,lda_4,nmf_0,nmf_1,nmf_2,nmf_3,nmf_4
Fitzpatrick A.R.; Yen W.M.,The Psychometric Characteristics of Choice Items,1995,32,"This study investigated the psychometric characteristics of constructed‐response (CR) items referring to choice and non‐choice passages administered to students in Grades 3, 5, and 8. The items were scaled using item response theory (IRT) methodology. The results indicated no consistent differences in the difficulty and discrimination of the items referring to the two types of passages. On the average, students' scale scores on the choice and non‐choice passages were comparable. Finally, the choice passages differed in terms of overall popularity and in their attractiveness to different gender and ethnic groups Copyright © 1995, Wiley Blackwell. All rights reserved",,"This study investigated the psychometric characteristics of constructed‐response (CR) items referring to choice and non‐choice passages administered to students in Grades 3, 5, and 8. The items were scaled using item response theory (IRT) methodology. The results indicated no consistent differences in the difficulty and discrimination of the items referring to the two types of passages. On the average, students' scale scores on the choice and non‐choice passages were comparable. Finally, the choice passages differed in terms of overall popularity and in their attractiveness to different gender and ethnic groups Copyright © 1995, Wiley Blackwell. All rights reserved","['study', 'investigate', 'psychometric', 'characteristic', 'constructed‐response', 'CR', 'item', 'refer', 'choice', 'non‐choice', 'passage', 'administer', 'student', 'Grades', '3', '5', '8', 'item', 'scale', 'item', 'response', 'theory', 'IRT', 'methodology', 'result', 'indicate', 'consistent', 'difference', 'difficulty', 'discrimination', 'item', 'refer', 'type', 'passage', 'average', 'student', 'scale', 'score', 'choice', 'non‐choice', 'passage', 'comparable', 'finally', 'choice', 'passage', 'differ', 'term', 'overall', 'popularity', 'attractiveness', 'different', 'gender', 'ethnic', 'group', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']",,study investigate psychometric characteristic constructed‐response CR item refer choice non‐choice passage administer student Grades 3 5 8 item scale item response theory IRT methodology result indicate consistent difference difficulty discrimination item refer type passage average student scale score choice non‐choice passage comparable finally choice passage differ term overall popularity attractiveness different gender ethnic group Copyright © 1995 Wiley Blackwell right reserve,,0.030742479590067224,0.03128105562155901,0.0307178467815256,0.030443680767062505,0.8768149372397857,0.0018442427402614767,0.07347741020846085,0.005916564202610995,0.025496179617354335,0.015756647045232625
Gessaroli M.E.; De Champlain A.F.,Using an approximate chi-square statistic to test the number of dimensions underlying the responses to a set of items,1996,33,"An approximate x;2 statistic based on McDonald's (1967) nonlinear factor analytic representation of item response theory was proposed and investigated wah simulated data. The results were compared with Stout's T statistic (Nandakumar & Stout, 1993; Stout, 1987). Unidimensional and tWo-dimensional item response data were simulated under varying levels of sample size, test length, test reliability, and dimension dominance. The approximate X2 statistic had good control over Type I errors when unidimensional data were generated and displayed very good power in identifying the two-dimensional data The performance of the approximate X2 was at least as good as Stout's T statistic in all conditions and was better than Stout's T statistic with smaller sample sizes and shorter tests. Further implications regarding the potential use of nonlinear factor analysis and the approximate X2 in addressing current measurement issues are discussed.",Using an approximate chi-square statistic to test the number of dimensions underlying the responses to a set of items,"An approximate x;2 statistic based on McDonald's (1967) nonlinear factor analytic representation of item response theory was proposed and investigated wah simulated data. The results were compared with Stout's T statistic (Nandakumar & Stout, 1993; Stout, 1987). Unidimensional and tWo-dimensional item response data were simulated under varying levels of sample size, test length, test reliability, and dimension dominance. The approximate X2 statistic had good control over Type I errors when unidimensional data were generated and displayed very good power in identifying the two-dimensional data The performance of the approximate X2 was at least as good as Stout's T statistic in all conditions and was better than Stout's T statistic with smaller sample sizes and shorter tests. Further implications regarding the potential use of nonlinear factor analysis and the approximate X2 in addressing current measurement issues are discussed.","['approximate', 'x2', 'statistic', 'base', 'McDonalds', '1967', 'nonlinear', 'factor', 'analytic', 'representation', 'item', 'response', 'theory', 'propose', 'investigate', 'wah', 'simulate', 'datum', 'result', 'compare', 'Stouts', 'T', 'statistic', 'Nandakumar', 'Stout', '1993', 'Stout', '1987', 'unidimensional', 'tWodimensional', 'item', 'response', 'datum', 'simulate', 'vary', 'level', 'sample', 'size', 'test', 'length', 'test', 'reliability', 'dimension', 'dominance', 'approximate', 'x2', 'statistic', 'good', 'control', 'Type', 'I', 'error', 'unidimensional', 'datum', 'generate', 'display', 'good', 'power', 'identify', 'twodimensional', 'datum', 'performance', 'approximate', 'x2', 'good', 'Stouts', 'T', 'statistic', 'condition', 'Stouts', 'T', 'statistic', 'small', 'sample', 'size', 'short', 'test', 'implication', 'regard', 'potential', 'nonlinear', 'factor', 'analysis', 'approximate', 'x2', 'address', 'current', 'issue', 'discuss']","['approximate', 'chisquare', 'statistic', 'test', 'number', 'dimension', 'underlie', 'response', 'set', 'item']",approximate x2 statistic base McDonalds 1967 nonlinear factor analytic representation item response theory propose investigate wah simulate datum result compare Stouts T statistic Nandakumar Stout 1993 Stout 1987 unidimensional tWodimensional item response datum simulate vary level sample size test length test reliability dimension dominance approximate x2 statistic good control Type I error unidimensional datum generate display good power identify twodimensional datum performance approximate x2 good Stouts T statistic condition Stouts T statistic small sample size short test implication regard potential nonlinear factor analysis approximate x2 address current issue discuss,approximate chisquare statistic test number dimension underlie response set item,0.02964574449026809,0.02956494796582224,0.02967879459203456,0.029652454986525852,0.8814580579653493,0.04494302790865727,0.02075615427925026,0.007617316011762688,0.0,0.0012904361951935795
Tate R.L.,Robustness of the School‐Level IRT Model,1995,32,"The robustness of the school‐level item response theoretic (IRT) model to violations of distributional assumptions was studied in a computer simulation. Estimated precision of “expected a posteriori” (EAP) estimates of the mean school ability from BILOG 3 was compared with actual precision, varying school size, intraclass correlation, school ability, number of forms comprising the test, and item parameters. Under conditions where the school‐level precision might be possibly acceptable for real school comparisons, the EAP estimates of school ability were robust over a wide range of violations and conditions, with the estimated precision being either consistent with the actual precision or somewhat conservative. Some lack of robustness was found, however, under conditions where the precision was inherently poor and the test would presumably not be used for serious school comparisons. Copyright © 1995, Wiley Blackwell. All rights reserved",,"The robustness of the school‐level item response theoretic (IRT) model to violations of distributional assumptions was studied in a computer simulation. Estimated precision of “expected a posteriori” (EAP) estimates of the mean school ability from BILOG 3 was compared with actual precision, varying school size, intraclass correlation, school ability, number of forms comprising the test, and item parameters. Under conditions where the school‐level precision might be possibly acceptable for real school comparisons, the EAP estimates of school ability were robust over a wide range of violations and conditions, with the estimated precision being either consistent with the actual precision or somewhat conservative. Some lack of robustness was found, however, under conditions where the precision was inherently poor and the test would presumably not be used for serious school comparisons. Copyright © 1995, Wiley Blackwell. All rights reserved","['robustness', 'school‐level', 'item', 'response', 'theoretic', 'IRT', 'violation', 'distributional', 'assumption', 'study', 'computer', 'simulation', 'estimate', 'precision', '""', 'expect', 'posteriori', '""', 'EAP', 'estimate', 'mean', 'school', 'ability', 'BILOG', '3', 'compare', 'actual', 'precision', 'vary', 'school', 'size', 'intraclass', 'correlation', 'school', 'ability', 'number', 'form', 'comprise', 'test', 'item', 'parameter', 'condition', 'school‐level', 'precision', 'possibly', 'acceptable', 'real', 'school', 'comparison', 'EAP', 'estimate', 'school', 'ability', 'robust', 'wide', 'range', 'violation', 'condition', 'estimate', 'precision', 'consistent', 'actual', 'precision', 'somewhat', 'conservative', 'lack', 'robustness', 'find', 'condition', 'precision', 'inherently', 'poor', 'test', 'presumably', 'school', 'comparison', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']",,"robustness school‐level item response theoretic IRT violation distributional assumption study computer simulation estimate precision "" expect posteriori "" EAP estimate mean school ability BILOG 3 compare actual precision vary school size intraclass correlation school ability number form comprise test item parameter condition school‐level precision possibly acceptable real school comparison EAP estimate school ability robust wide range violation condition estimate precision consistent actual precision somewhat conservative lack robustness find condition precision inherently poor test presumably school comparison Copyright © 1995 Wiley Blackwell right reserve",,0.030282406118420105,0.030142037708942283,0.879847632687188,0.029251629561635534,0.030476293923813957,0.08926276470084218,0.0,0.0,0.0,0.0
Muraki E.,Stepwise analysis of differential item functioning based on multiple-group partial credit model,1999,36,"Bock, Muraki, and Pfeiffenberger (1988) proposed a dichotomous item response theory (IRT) model for the detection of differential item functioning (DIF), and they estimated the IRT parameters and the means and standard deviations of the multiple latent trait distributions. This IRT DIF detection method is extended to the partial credit model (Masters, 1982; Muraki, 1993) and presented as one of the multiple-group IRT models. Uniform and non-uniform DIF items and heterogeneous latent trait distributions were used to generate polytomous responses of multiple groups. The DIF method was applied to this simulated data using a stepwise procedure. The standardized DIF measures for slope and item location parameters successfully detected the non-uniform and uniform DIF items as well as recovered the means and standard deviations of the latent trait distributions. This stepwise DIF analysis based on the multiple-group partial credit model was then applied to the National Assessment of Educational Progress (NAEP) writing trend data.",Stepwise analysis of differential item functioning based on multiple-group partial credit model,"Bock, Muraki, and Pfeiffenberger (1988) proposed a dichotomous item response theory (IRT) model for the detection of differential item functioning (DIF), and they estimated the IRT parameters and the means and standard deviations of the multiple latent trait distributions. This IRT DIF detection method is extended to the partial credit model (Masters, 1982; Muraki, 1993) and presented as one of the multiple-group IRT models. Uniform and non-uniform DIF items and heterogeneous latent trait distributions were used to generate polytomous responses of multiple groups. The DIF method was applied to this simulated data using a stepwise procedure. The standardized DIF measures for slope and item location parameters successfully detected the non-uniform and uniform DIF items as well as recovered the means and standard deviations of the latent trait distributions. This stepwise DIF analysis based on the multiple-group partial credit model was then applied to the National Assessment of Educational Progress (NAEP) writing trend data.","['Bock', 'Muraki', 'Pfeiffenberger', '1988', 'propose', 'dichotomous', 'item', 'response', 'theory', 'IRT', 'detection', 'differential', 'item', 'function', 'DIF', 'estimate', 'IRT', 'parameter', 'mean', 'standard', 'deviation', 'multiple', 'latent', 'trait', 'distribution', 'IRT', 'dif', 'detection', 'method', 'extend', 'partial', 'credit', 'Masters', '1982', 'Muraki', '1993', 'present', 'multiplegroup', 'IRT', 'Uniform', 'nonuniform', 'dif', 'item', 'heterogeneous', 'latent', 'trait', 'distribution', 'generate', 'polytomous', 'response', 'multiple', 'group', 'DIF', 'method', 'apply', 'simulated', 'datum', 'stepwise', 'procedure', 'standardized', 'dif', 'measure', 'slope', 'item', 'location', 'parameter', 'successfully', 'detect', 'nonuniform', 'uniform', 'DIF', 'item', 'recover', 'mean', 'standard', 'deviation', 'latent', 'trait', 'distribution', 'stepwise', 'DIF', 'analysis', 'base', 'multiplegroup', 'partial', 'credit', 'apply', 'National', 'Assessment', 'Educational', 'Progress', 'NAEP', 'write', 'trend', 'datum']","['stepwise', 'analysis', 'differential', 'item', 'function', 'base', 'multiplegroup', 'partial', 'credit']",Bock Muraki Pfeiffenberger 1988 propose dichotomous item response theory IRT detection differential item function DIF estimate IRT parameter mean standard deviation multiple latent trait distribution IRT dif detection method extend partial credit Masters 1982 Muraki 1993 present multiplegroup IRT Uniform nonuniform dif item heterogeneous latent trait distribution generate polytomous response multiple group DIF method apply simulated datum stepwise procedure standardized dif measure slope item location parameter successfully detect nonuniform uniform DIF item recover mean standard deviation latent trait distribution stepwise DIF analysis base multiplegroup partial credit apply National Assessment Educational Progress NAEP write trend datum,stepwise analysis differential item function base multiplegroup partial credit,0.027115867351849843,0.027088437256839633,0.026571740166216,0.026376328651927614,0.892847626573167,0.002803003956287841,0.0,0.18208397986244765,0.023972375366189812,0.0
Allalouf A.; Hambleton R.K.; Sireci S.G.,Identifying the causes of dif in translated verbal items,1999,36,"Translated tests are being used increasingly for assessing the knowledge and skills of individuals who speak different languages. There is little research exploring why translated items sometimes function differently across languages. If the sources of differential item functioning (DIF) across languages could be predicted, it could have important implications on test development, scoring and equating. This study focuses on two questions: ""Is DIF related to item type?"", ""What are the causes of DIF?"" The data were taken from the Israeli Psychometric Entrance Test in Hebrew (source) and Russian (translated). The results indicated that 34% of the items functioned differentially across languages. The analogy items were the most problematic with 65% showing DIF, mostly in favor of the Russian-speaking examinees. The sentence completion items were also a problem (45% DIF). The main reasons for DIF were changes in word difficulty, changes in item format, differences in cultural relevance, and changes in content.",Identifying the causes of dif in translated verbal items,"Translated tests are being used increasingly for assessing the knowledge and skills of individuals who speak different languages. There is little research exploring why translated items sometimes function differently across languages. If the sources of differential item functioning (DIF) across languages could be predicted, it could have important implications on test development, scoring and equating. This study focuses on two questions: ""Is DIF related to item type?"", ""What are the causes of DIF?"" The data were taken from the Israeli Psychometric Entrance Test in Hebrew (source) and Russian (translated). The results indicated that 34% of the items functioned differentially across languages. The analogy items were the most problematic with 65% showing DIF, mostly in favor of the Russian-speaking examinees. The sentence completion items were also a problem (45% DIF). The main reasons for DIF were changes in word difficulty, changes in item format, differences in cultural relevance, and changes in content.","['translate', 'test', 'increasingly', 'assess', 'knowledge', 'skill', 'individual', 'speak', 'different', 'language', 'little', 'research', 'explore', 'translate', 'item', 'function', 'differently', 'language', 'source', 'differential', 'item', 'function', 'DIF', 'language', 'predict', 'important', 'implication', 'test', 'development', 'scoring', 'equate', 'study', 'focus', 'question', 'DIF', 'relate', 'item', 'type', 'cause', 'DIF', 'datum', 'israeli', 'Psychometric', 'Entrance', 'Test', 'hebrew', 'source', 'russian', 'translate', 'result', 'indicate', '34', 'item', 'function', 'differentially', 'language', 'analogy', 'item', 'problematic', '65', 'DIF', 'favor', 'Russianspeaking', 'examine', 'sentence', 'completion', 'item', 'problem', '45', 'dif', 'main', 'reason', 'DIF', 'change', 'word', 'difficulty', 'change', 'item', 'format', 'difference', 'cultural', 'relevance', 'change', 'content']","['identify', 'cause', 'dif', 'translate', 'verbal', 'item']",translate test increasingly assess knowledge skill individual speak different language little research explore translate item function differently language source differential item function DIF language predict important implication test development scoring equate study focus question DIF relate item type cause DIF datum israeli Psychometric Entrance Test hebrew source russian translate result indicate 34 item function differentially language analogy item problematic 65 DIF favor Russianspeaking examine sentence completion item problem 45 dif main reason DIF change word difficulty change item format difference cultural relevance change content,identify cause dif translate verbal item,0.026899039247317184,0.027029927571524393,0.8916848648733208,0.02687942656568155,0.027506741742155977,0.0,0.059380438791280306,0.12877632631338576,0.0,0.0
Bennett R.E.; Morley M.; Quardt D.; Rock D.A.; Singley M.K.; Katz I.R.; Nhouyvanisvong A.,Psychometric and cognitive functioning of an under-determined computer-based response type for quantitative reasoning,1999,36,"We evaluated a computer-delivered response type for measuring quantitative skill. ""Generating Examples"" (GE) presents under-determined problems that can have many right answers. We administered two GE tests that differed in the manipulation of specific item features hypothesized to affect difficulty. Analyses related to internal consistency reliability, external relations, and features contributing to item difficulty, adverse impact, and examinee perceptions. Results showed that GE scores were reasonably reliable but only moderately related to the GRE quantitative section, suggesting the two tests might be tapping somewhat different skills. Item features that increased difficulty included asking examinees to supply more than one correct answer and to identify whether an item was solvable. Gender differences were similar to those found on the GRE quantitative and analytical test sections. Finally, examinees were divided on whether GE items were a fairer indicator of ability than multiple-choice items, but still overwhelmingly preferred to take the more conventional questions.",Psychometric and cognitive functioning of an under-determined computer-based response type for quantitative reasoning,"We evaluated a computer-delivered response type for measuring quantitative skill. ""Generating Examples"" (GE) presents under-determined problems that can have many right answers. We administered two GE tests that differed in the manipulation of specific item features hypothesized to affect difficulty. Analyses related to internal consistency reliability, external relations, and features contributing to item difficulty, adverse impact, and examinee perceptions. Results showed that GE scores were reasonably reliable but only moderately related to the GRE quantitative section, suggesting the two tests might be tapping somewhat different skills. Item features that increased difficulty included asking examinees to supply more than one correct answer and to identify whether an item was solvable. Gender differences were similar to those found on the GRE quantitative and analytical test sections. Finally, examinees were divided on whether GE items were a fairer indicator of ability than multiple-choice items, but still overwhelmingly preferred to take the more conventional questions.","['evaluate', 'computerdelivered', 'response', 'type', 'measure', 'quantitative', 'skill', 'Generating', 'Examples', 'GE', 'present', 'underdetermined', 'problem', 'right', 'answer', 'administer', 'GE', 'test', 'differ', 'manipulation', 'specific', 'item', 'feature', 'hypothesize', 'affect', 'difficulty', 'Analyses', 'relate', 'internal', 'consistency', 'reliability', 'external', 'relation', 'feature', 'contribute', 'item', 'difficulty', 'adverse', 'impact', 'examinee', 'perception', 'result', 'GE', 'score', 'reasonably', 'reliable', 'moderately', 'relate', 'GRE', 'quantitative', 'section', 'suggest', 'test', 'tap', 'somewhat', 'different', 'skill', 'Item', 'feature', 'increase', 'difficulty', 'include', 'asking', 'examine', 'supply', 'correct', 'answer', 'identify', 'item', 'solvable', 'Gender', 'difference', 'similar', 'find', 'GRE', 'quantitative', 'analytical', 'test', 'section', 'finally', 'examine', 'divide', 'GE', 'item', 'fair', 'indicator', 'ability', 'multiplechoice', 'item', 'overwhelmingly', 'prefer', 'conventional', 'question']","['psychometric', 'cognitive', 'functioning', 'underdetermined', 'computerbase', 'response', 'type', 'quantitative', 'reasoning']",evaluate computerdelivered response type measure quantitative skill Generating Examples GE present underdetermined problem right answer administer GE test differ manipulation specific item feature hypothesize affect difficulty Analyses relate internal consistency reliability external relation feature contribute item difficulty adverse impact examinee perception result GE score reasonably reliable moderately relate GRE quantitative section suggest test tap somewhat different skill Item feature increase difficulty include asking examine supply correct answer identify item solvable Gender difference similar find GRE quantitative analytical test section finally examine divide GE item fair indicator ability multiplechoice item overwhelmingly prefer conventional question,psychometric cognitive functioning underdetermined computerbase response type quantitative reasoning,0.025005887022360153,0.8995176354162422,0.025199794714270328,0.024855959009824895,0.025420723837302366,0.0,0.12713968478051216,0.0,0.0020820174522266227,0.0
Veldkamp B.P.,Multiple objective test assembly problems,1999,36,"Mathematical programming techniques for optimal test assembly are discussed. Most methods optimize a single objective: for instance, the amount of information in a test, subject to a number of constraints. However, some test assembly problems have multiple objectives. A recent example in the literature is the problem of assembling test that measure multiple traits, where the amount of information in the test about each different trait has to be maximized. The present paper proposes methods appropriate for solving multiple objective test assembly problems. An overview of multiple objective optimization methods is given. The impact of the method on the optimality of the solution is shown and the appropriateness of the methods is discussed. The methods are illustrated using an empirical example of a test assembly problem for a two-dimensional mathematics item pool.",,"Mathematical programming techniques for optimal test assembly are discussed. Most methods optimize a single objective: for instance, the amount of information in a test, subject to a number of constraints. However, some test assembly problems have multiple objectives. A recent example in the literature is the problem of assembling test that measure multiple traits, where the amount of information in the test about each different trait has to be maximized. The present paper proposes methods appropriate for solving multiple objective test assembly problems. An overview of multiple objective optimization methods is given. The impact of the method on the optimality of the solution is shown and the appropriateness of the methods is discussed. The methods are illustrated using an empirical example of a test assembly problem for a two-dimensional mathematics item pool.","['mathematical', 'programming', 'technique', 'optimal', 'test', 'assembly', 'discuss', 'Most', 'method', 'optimize', 'single', 'objective', 'instance', 'information', 'test', 'subject', 'number', 'constraint', 'test', 'assembly', 'problem', 'multiple', 'objective', 'recent', 'example', 'literature', 'problem', 'assemble', 'test', 'measure', 'multiple', 'trait', 'information', 'test', 'different', 'trait', 'maximize', 'present', 'paper', 'propose', 'method', 'appropriate', 'solve', 'multiple', 'objective', 'test', 'assembly', 'problem', 'overview', 'multiple', 'objective', 'optimization', 'method', 'impact', 'method', 'optimality', 'solution', 'appropriateness', 'method', 'discuss', 'method', 'illustrate', 'empirical', 'example', 'test', 'assembly', 'problem', 'twodimensional', 'mathematic', 'item', 'pool']",,mathematical programming technique optimal test assembly discuss Most method optimize single objective instance information test subject number constraint test assembly problem multiple objective recent example literature problem assemble test measure multiple trait information test different trait maximize present paper propose method appropriate solve multiple objective test assembly problem overview multiple objective optimization method impact method optimality solution appropriateness method discuss method illustrate empirical example test assembly problem twodimensional mathematic item pool,,0.03120814948715977,0.031160703099065584,0.8744642118896283,0.03094793199660994,0.03221900352753637,0.0011528451186912426,0.05608356894327786,0.006922414889623759,0.05652809821951178,0.005424305662164086
Van Der Linden W.J.; Adema J.J.,Simultaneous assembly of multiple test forms,1998,35,"An algorithm for the assembly of multiple test forms is proposed in which the multiple-form problem is reduced to a series of computationally less intensive two-form problems. At each step, one form is assembled to its true specifications; the other form is a dummy assembled only to maintain a balance between the quality of the current form and the remaining forms. It is shown how the method can be implemented using the technique of 0-1 linear programming. Two empirical examples using a former item pool from the LSAT are given - one in which a set of parallel forms is assembled and another in which the targets for the information functions of the forms are shifted systematically.",,"An algorithm for the assembly of multiple test forms is proposed in which the multiple-form problem is reduced to a series of computationally less intensive two-form problems. At each step, one form is assembled to its true specifications; the other form is a dummy assembled only to maintain a balance between the quality of the current form and the remaining forms. It is shown how the method can be implemented using the technique of 0-1 linear programming. Two empirical examples using a former item pool from the LSAT are given - one in which a set of parallel forms is assembled and another in which the targets for the information functions of the forms are shifted systematically.","['algorithm', 'assembly', 'multiple', 'test', 'form', 'propose', 'multipleform', 'problem', 'reduce', 'series', 'computationally', 'intensive', 'twoform', 'problem', 'step', 'form', 'assemble', 'true', 'specification', 'form', 'dummy', 'assemble', 'maintain', 'balance', 'quality', 'current', 'form', 'remain', 'form', 'method', 'implement', 'technique', '01', 'linear', 'program', 'empirical', 'example', 'item', 'pool', 'LSAT', 'set', 'parallel', 'form', 'assemble', 'target', 'information', 'function', 'form', 'shift', 'systematically']",,algorithm assembly multiple test form propose multipleform problem reduce series computationally intensive twoform problem step form assemble true specification form dummy assemble maintain balance quality current form remain form method implement technique 01 linear program empirical example item pool LSAT set parallel form assemble target information function form shift systematically,,0.03186021146038989,0.03163530897435762,0.8728193891155421,0.03149882169508391,0.03218626875462649,0.04650115197712713,0.013415763411937108,0.0,0.0021151026914275847,0.0
Feldt L.S.,Estimation of the Reliability of Differences Under Revised Reliabilities of Component Scores,1995,32,"Projecting the changes in the reliability of a difference score (d =× ‐ Y) as a consequence of changes in the reliabilities of Xand Y does not represent a straightforward application of the Spearman‐Brown formula. Formulas are developed for estimating the changes in the reliability of X‐Yunder two possible assumptions: (a) ×and Y have equal variances both before and after their reliabilities are altered, and (b) ×and Y have unequal variances before and after×and Y are modified. The second of these situations, which includes the first as a special case, is probably the more common. Copyright © 1995, Wiley Blackwell. All rights reserved",Estimation of the Reliability of Differences Under Revised Reliabilities of Component Scores,"Projecting the changes in the reliability of a difference score (d =× ‐ Y) as a consequence of changes in the reliabilities of Xand Y does not represent a straightforward application of the Spearman‐Brown formula. Formulas are developed for estimating the changes in the reliability of X‐Yunder two possible assumptions: (a) ×and Y have equal variances both before and after their reliabilities are altered, and (b) ×and Y have unequal variances before and after×and Y are modified. The second of these situations, which includes the first as a special case, is probably the more common. Copyright © 1995, Wiley Blackwell. All rights reserved","['project', 'change', 'reliability', 'difference', 'score', 'd', '×', '‐', 'y', 'consequence', 'change', 'reliability', 'Xand', 'Y', 'represent', 'straightforward', 'application', 'Spearman‐Brown', 'formula', 'Formulas', 'develop', 'estimate', 'change', 'reliability', 'x‐yunder', 'possible', 'assumption', '×and', 'Y', 'equal', 'variance', 'reliability', 'alter', 'b', '×and', 'Y', 'unequal', 'variance', 'after×and', 'Y', 'modify', 'second', 'situation', 'include', 'special', 'case', 'probably', 'common', 'copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['estimation', 'Reliability', 'Differences', 'Revised', 'Reliabilities', 'Component', 'Scores']",project change reliability difference score d × ‐ y consequence change reliability Xand Y represent straightforward application Spearman‐Brown formula Formulas develop estimate change reliability x‐yunder possible assumption ×and Y equal variance reliability alter b ×and Y unequal variance after×and Y modify second situation include special case probably common copyright © 1995 Wiley Blackwell right reserve,estimation Reliability Differences Revised Reliabilities Component Scores,0.034296625857831615,0.03391170840145847,0.8636919521710698,0.033642156439505695,0.03445755713013454,0.0,0.022105005962158794,0.0,0.05698086719841764,0.0038907585986772326
Pommerich M.; Nicewander W.A.; Hanson B.A.,Estimating average domain scores,1999,36,"A simulation study was performed to determine whether a group's average percent correct in a content domain could be accurately estimated for groups taking a single test form and not the entire domain of items. Six Item Response Theory-based domain score estimation methods were evaluated, under conditions of few items per content area perform taken, small domains, and small group sizes. The methods used item responses to a single form taken to estimate examinee or group ability; domain scores were then computed using the ability estimates and domain item characteristics. The IRT-based domain score estimates typically showed greater accuracy and greater consistency across forms taken than observed performance on the form taken. For the smallest group size and least number of items taken, the accuracy of most IRT-based estimates was questionable; however, a procedure that operates on an estimated distribution of group ability showed promise under most conditions.",,"A simulation study was performed to determine whether a group's average percent correct in a content domain could be accurately estimated for groups taking a single test form and not the entire domain of items. Six Item Response Theory-based domain score estimation methods were evaluated, under conditions of few items per content area perform taken, small domains, and small group sizes. The methods used item responses to a single form taken to estimate examinee or group ability; domain scores were then computed using the ability estimates and domain item characteristics. The IRT-based domain score estimates typically showed greater accuracy and greater consistency across forms taken than observed performance on the form taken. For the smallest group size and least number of items taken, the accuracy of most IRT-based estimates was questionable; however, a procedure that operates on an estimated distribution of group ability showed promise under most conditions.","['simulation', 'study', 'perform', 'determine', 'group', 'average', 'percent', 'correct', 'content', 'domain', 'accurately', 'estimate', 'group', 'single', 'test', 'form', 'entire', 'domain', 'item', 'Item', 'Response', 'theorybase', 'domain', 'score', 'estimation', 'method', 'evaluate', 'condition', 'item', 'content', 'area', 'perform', 'small', 'domain', 'small', 'group', 'size', 'method', 'item', 'response', 'single', 'form', 'estimate', 'examinee', 'group', 'ability', 'domain', 'score', 'compute', 'ability', 'estimate', 'domain', 'item', 'characteristic', 'irtbased', 'domain', 'score', 'estimate', 'typically', 'great', 'accuracy', 'great', 'consistency', 'form', 'observed', 'performance', 'form', 'small', 'group', 'size', 'number', 'item', 'accuracy', 'irtbased', 'estimate', 'questionable', 'procedure', 'operate', 'estimate', 'distribution', 'group', 'ability', 'promise', 'condition']",,simulation study perform determine group average percent correct content domain accurately estimate group single test form entire domain item Item Response theorybase domain score estimation method evaluate condition item content area perform small domain small group size method item response single form estimate examinee group ability domain score compute ability estimate domain item characteristic irtbased domain score estimate typically great accuracy great consistency form observed performance form small group size number item accuracy irtbased estimate questionable procedure operate estimate distribution group ability promise condition,,0.8711091650313679,0.03213303362875782,0.03216674932752297,0.03167897043557749,0.03291208157677385,0.11069653963009948,0.0,0.0057137121220834485,0.028654601844544558,0.0
Luecht R.M.; Nungester R.J.,Some practical examples of computer-adaptive sequential testing,1998,35,"Computerized testing has created new challenges for the production and administration of test forms. Many testing organizations engaged in or considering computerized testing may find themselves changing from well-established procedures for handcrafting a small number of paper-and-pencil test forms to procedures for mass producing many computerized test forms. This paper describes an integrated approach to test development and administration called computer-adaptive sequential testing, or CAST. CAST is a structured approach to test construction which incorporates both adaptive testing methods with automated test assembly to allow test developers to maintain a greater degree of control over the production, quality assurance, and administration of different types of computerized tests. CAST retains much of the efficiency of traditional computer adaptive testing (CAT) and can be modified for computer mastery testing (CMT) applications. The CAST framework is described in detail and several applications are demonstrated using a medical licensure example.",Some practical examples of computer-adaptive sequential testing,"Computerized testing has created new challenges for the production and administration of test forms. Many testing organizations engaged in or considering computerized testing may find themselves changing from well-established procedures for handcrafting a small number of paper-and-pencil test forms to procedures for mass producing many computerized test forms. This paper describes an integrated approach to test development and administration called computer-adaptive sequential testing, or CAST. CAST is a structured approach to test construction which incorporates both adaptive testing methods with automated test assembly to allow test developers to maintain a greater degree of control over the production, quality assurance, and administration of different types of computerized tests. CAST retains much of the efficiency of traditional computer adaptive testing (CAT) and can be modified for computer mastery testing (CMT) applications. The CAST framework is described in detail and several applications are demonstrated using a medical licensure example.","['computerized', 'testing', 'create', 'new', 'challenge', 'production', 'administration', 'test', 'form', 'testing', 'organization', 'engage', 'consider', 'computerized', 'testing', 'find', 'change', 'wellestablished', 'procedure', 'handcraft', 'small', 'number', 'paperandpencil', 'test', 'form', 'procedure', 'mass', 'produce', 'computerized', 'test', 'form', 'paper', 'describe', 'integrate', 'approach', 'test', 'development', 'administration', 'computeradaptive', 'sequential', 'testing', 'CAST', 'CAST', 'structured', 'approach', 'test', 'construction', 'incorporate', 'adaptive', 'testing', 'method', 'automate', 'test', 'assembly', 'allow', 'test', 'developer', 'maintain', 'great', 'degree', 'control', 'production', 'quality', 'assurance', 'administration', 'different', 'type', 'computerized', 'test', 'cast', 'retain', 'efficiency', 'traditional', 'computer', 'adaptive', 'testing', 'CAT', 'modify', 'computer', 'mastery', 'test', 'CMT', 'application', 'CAST', 'framework', 'describe', 'detail', 'application', 'demonstrate', 'medical', 'licensure', 'example']","['practical', 'example', 'computeradaptive', 'sequential', 'testing']",computerized testing create new challenge production administration test form testing organization engage consider computerized testing find change wellestablished procedure handcraft small number paperandpencil test form procedure mass produce computerized test form paper describe integrate approach test development administration computeradaptive sequential testing CAST CAST structured approach test construction incorporate adaptive testing method automate test assembly allow test developer maintain great degree control production quality assurance administration different type computerized test cast retain efficiency traditional computer adaptive testing CAT modify computer mastery test CMT application CAST framework describe detail application demonstrate medical licensure example,practical example computeradaptive sequential testing,0.02660045778430233,0.02666908763375527,0.02670219822779431,0.3634817881919228,0.5565464681622253,0.013613604839897661,0.08326603764102629,0.0,0.0081223437324428,0.0
Stocking M.L.; Jirele T.; Lewis C.; Swanson L.,Moderating possibly irrelevant multiple mean score differences on a test of mathematical reasoning,1998,35,"A pool of items from operational tests of mathematical reasoning was constructed to investigate the feasibility of using automated test assembly (ATA) methods to simultaneously moderate possibly irrelevant differences between the performance of women and men, and African American and White test takers. None of the artificial tests exhibited substantial impact moderation, although the estimated mean scaled score differences for the relevant population indicated a modest move in the intended direction: the difference between scaled score means was reduced by about 20% for women and men and about 9% for African American and White test takers. Although many issues in the implementation of this methodology remain to be solved, the consideration of impact in ATA, along with the maintenance of the detailed test plan, appears to be a potential method of moderating possibly irrelevant mean test score differences.",Moderating possibly irrelevant multiple mean score differences on a test of mathematical reasoning,"A pool of items from operational tests of mathematical reasoning was constructed to investigate the feasibility of using automated test assembly (ATA) methods to simultaneously moderate possibly irrelevant differences between the performance of women and men, and African American and White test takers. None of the artificial tests exhibited substantial impact moderation, although the estimated mean scaled score differences for the relevant population indicated a modest move in the intended direction: the difference between scaled score means was reduced by about 20% for women and men and about 9% for African American and White test takers. Although many issues in the implementation of this methodology remain to be solved, the consideration of impact in ATA, along with the maintenance of the detailed test plan, appears to be a potential method of moderating possibly irrelevant mean test score differences.","['pool', 'item', 'operational', 'test', 'mathematical', 'reasoning', 'construct', 'investigate', 'feasibility', 'automate', 'test', 'assembly', 'ATA', 'method', 'simultaneously', 'moderate', 'possibly', 'irrelevant', 'difference', 'performance', 'woman', 'man', 'african', 'american', 'white', 'test', 'taker', 'artificial', 'test', 'exhibit', 'substantial', 'impact', 'moderation', 'estimate', 'mean', 'scale', 'score', 'difference', 'relevant', 'population', 'indicate', 'modest', 'intend', 'direction', 'difference', 'scale', 'score', 'mean', 'reduce', '20', 'woman', 'man', '9', 'african', 'american', 'white', 'test', 'taker', 'issue', 'implementation', 'methodology', 'remain', 'solve', 'consideration', 'impact', 'ATA', 'maintenance', 'detailed', 'test', 'plan', 'appear', 'potential', 'method', 'moderate', 'possibly', 'irrelevant', 'mean', 'test', 'score', 'difference']","['moderate', 'possibly', 'irrelevant', 'multiple', 'mean', 'score', 'difference', 'test', 'mathematical', 'reasoning']",pool item operational test mathematical reasoning construct investigate feasibility automate test assembly ATA method simultaneously moderate possibly irrelevant difference performance woman man african american white test taker artificial test exhibit substantial impact moderation estimate mean scale score difference relevant population indicate modest intend direction difference scale score mean reduce 20 woman man 9 african american white test taker issue implementation methodology remain solve consideration impact ATA maintenance detailed test plan appear potential method moderate possibly irrelevant mean test score difference,moderate possibly irrelevant multiple mean score difference test mathematical reasoning,0.02607224386059763,0.02607125579449956,0.026104941221149657,0.02590696183198429,0.8958445972917689,0.0,0.05081223098846018,0.0,0.13063044260437096,0.0
Vispoel W.P.; Wang T.; Bleiler T.,"Computerized adaptive and fixed-item testing of music listening skill: A comparison of efficiency, precision, and concurrent validity",1997,34,"We evaluated the efficiency, precision, and concurrent validity of results obtained from adaptive and fixed-item music listening tests in three studies: (a) a computer simulation study in which each of 2,200 simulees completed a computerized adaptive tonal memory test, a computerized fixed-item tonal memory test constructed from items in the adaptive test pool, and two standardized group-administered tonal memory tests; (b) a live testing study in which each of 204 examinees took the computerized adaptive test and the standardized tests; and (c) a live testing study in which randomly equivalent groups took either the computerized adaptive test (n = 86) or the computerized fixed-item test (n = 86). The adaptive music test required 50% to 93% fewer items to match the reliability and concurrent validity of the fixed-item tests, and it yielded higher levels of reliability and concurrent validity than the fixed-item tests when test length was held constant. These findings suggest that computerized adaptive tests, which typically have been limited to visually produced items, may also be well suited for measuring skills that require aurally produced items.","Computerized adaptive and fixed-item testing of music listening skill: A comparison of efficiency, precision, and concurrent validity","We evaluated the efficiency, precision, and concurrent validity of results obtained from adaptive and fixed-item music listening tests in three studies: (a) a computer simulation study in which each of 2,200 simulees completed a computerized adaptive tonal memory test, a computerized fixed-item tonal memory test constructed from items in the adaptive test pool, and two standardized group-administered tonal memory tests; (b) a live testing study in which each of 204 examinees took the computerized adaptive test and the standardized tests; and (c) a live testing study in which randomly equivalent groups took either the computerized adaptive test (n = 86) or the computerized fixed-item test (n = 86). The adaptive music test required 50% to 93% fewer items to match the reliability and concurrent validity of the fixed-item tests, and it yielded higher levels of reliability and concurrent validity than the fixed-item tests when test length was held constant. These findings suggest that computerized adaptive tests, which typically have been limited to visually produced items, may also be well suited for measuring skills that require aurally produced items.","['evaluate', 'efficiency', 'precision', 'concurrent', 'validity', 'result', 'obtain', 'adaptive', 'fixeditem', 'music', 'listen', 'test', 'study', 'computer', 'simulation', 'study', '2200', 'simulee', 'complete', 'computerized', 'adaptive', 'tonal', 'memory', 'test', 'computerized', 'fixeditem', 'tonal', 'memory', 'test', 'construct', 'item', 'adaptive', 'test', 'pool', 'standardized', 'groupadministered', 'tonal', 'memory', 'test', 'b', 'live', 'testing', 'study', '204', 'examinee', 'computerized', 'adaptive', 'test', 'standardized', 'test', 'c', 'live', 'testing', 'study', 'randomly', 'equivalent', 'group', 'computerized', 'adaptive', 'test', 'n', '86', 'computerized', 'fixeditem', 'test', 'n', '86', 'adaptive', 'music', 'test', 'require', '50', '93', 'item', 'match', 'reliability', 'concurrent', 'validity', 'fixeditem', 'test', 'yield', 'high', 'level', 'reliability', 'concurrent', 'validity', 'fixeditem', 'test', 'test', 'length', 'hold', 'constant', 'finding', 'suggest', 'computerized', 'adaptive', 'test', 'typically', 'limit', 'visually', 'produce', 'item', 'suited', 'measure', 'skill', 'require', 'aurally', 'produce', 'item']","['computerized', 'adaptive', 'fixeditem', 'testing', 'music', 'listening', 'skill', 'comparison', 'efficiency', 'precision', 'concurrent', 'validity']",evaluate efficiency precision concurrent validity result obtain adaptive fixeditem music listen test study computer simulation study 2200 simulee complete computerized adaptive tonal memory test computerized fixeditem tonal memory test construct item adaptive test pool standardized groupadministered tonal memory test b live testing study 204 examinee computerized adaptive test standardized test c live testing study randomly equivalent group computerized adaptive test n 86 computerized fixeditem test n 86 adaptive music test require 50 93 item match reliability concurrent validity fixeditem test yield high level reliability concurrent validity fixeditem test test length hold constant finding suggest computerized adaptive test typically limit visually produce item suited measure skill require aurally produce item,computerized adaptive fixeditem testing music listening skill comparison efficiency precision concurrent validity,0.029077499111516216,0.883447127483781,0.029115851513500317,0.028779685605859813,0.029579836285342685,0.002569378360604266,0.1016182611386426,0.0,0.0062585171151467245,0.0
Bock R.D.; Thissen D.; Zimowski M.F.,IRT estimation of domain scores,1997,34,"In classical test theory, a test is regarded as a sample of items from a domain defined by generating rules or by content, process, and format specifications. If the items are a random sample of the domain, then the percent-correct score on the test estimates the domain score, that is, the expected percent correct for all items in the domain. When the domain is represented by a large set of calibrated items, as in item banking applications, item response theory (IRT) provides an alternative estimator of the domain score by transformation of the IRT scale score on the test. This estimator has the advantage of not requiring the test items to be a random sample of the domain, and of having a simple standard error. We present here resampling results in real data demonstrating for uni- and multidimensional models that the IRT estimator is also a more accurate predictor of the domain score than is the classical percent-correct score. These results have implications for reporting outcomes of educational qualification testing and assessment.",,"In classical test theory, a test is regarded as a sample of items from a domain defined by generating rules or by content, process, and format specifications. If the items are a random sample of the domain, then the percent-correct score on the test estimates the domain score, that is, the expected percent correct for all items in the domain. When the domain is represented by a large set of calibrated items, as in item banking applications, item response theory (IRT) provides an alternative estimator of the domain score by transformation of the IRT scale score on the test. This estimator has the advantage of not requiring the test items to be a random sample of the domain, and of having a simple standard error. We present here resampling results in real data demonstrating for uni- and multidimensional models that the IRT estimator is also a more accurate predictor of the domain score than is the classical percent-correct score. These results have implications for reporting outcomes of educational qualification testing and assessment.","['classical', 'test', 'theory', 'test', 'regard', 'sample', 'item', 'domain', 'define', 'generate', 'rule', 'content', 'process', 'format', 'specification', 'item', 'random', 'sample', 'domain', 'percentcorrect', 'score', 'test', 'estimate', 'domain', 'score', 'expect', 'percent', 'correct', 'item', 'domain', 'domain', 'represent', 'large', 'set', 'calibrate', 'item', 'item', 'banking', 'application', 'item', 'response', 'theory', 'IRT', 'provide', 'alternative', 'estimator', 'domain', 'score', 'transformation', 'IRT', 'scale', 'score', 'test', 'estimator', 'advantage', 'require', 'test', 'item', 'random', 'sample', 'domain', 'simple', 'standard', 'error', 'present', 'resample', 'result', 'real', 'datum', 'demonstrating', 'uni', 'multidimensional', 'IRT', 'estimator', 'accurate', 'predictor', 'domain', 'score', 'classical', 'percentcorrect', 'score', 'result', 'implication', 'report', 'outcome', 'educational', 'qualification', 'testing', 'assessment']",,classical test theory test regard sample item domain define generate rule content process format specification item random sample domain percentcorrect score test estimate domain score expect percent correct item domain domain represent large set calibrate item item banking application item response theory IRT provide alternative estimator domain score transformation IRT scale score test estimator advantage require test item random sample domain simple standard error present resample result real datum demonstrating uni multidimensional IRT estimator accurate predictor domain score classical percentcorrect score result implication report outcome educational qualification testing assessment,,0.6975505206829192,0.03022342181600723,0.030292625764862455,0.029856038610859528,0.2120773931253515,0.02985253095838603,0.05372426594804072,0.004060242387927981,0.08611603327075223,0.0
Waltman K.K.,Using performance standards to link statewide achievement results to NAEP,1997,34,"The purpose of this study was to investigate the comparability in score meaning of the performance regions on the ITBS and NAEP mathematics score scales that resulted from using performance standards to establish two separate links: socially moderated and statistically moderated. A socially moderated link was established by using the same achievement level descriptions in an ITBS standard-setting study that were used in a NAEP standard-setting study. A statistically moderated link was accomplished by using an equipercentile procedure. The primary findings were that (a) social moderation yielded cutscores on the ITBS scales that resulted in larger percentages of Iowa public fourth-grade students being classified within the basic, proficient, and advanced achievement regions than those reported by NAEP; (b) the equipercentile link yielded percentages on the ITBS scale that were similar to those reported by NAEP for ""type of community"" subgroups; and (c) for students taking both assessments, the corresponding achievement regions on the NAEP and ITBS scales produced low to moderate percents of agreement in student classification.",Using performance standards to link statewide achievement results to NAEP,"The purpose of this study was to investigate the comparability in score meaning of the performance regions on the ITBS and NAEP mathematics score scales that resulted from using performance standards to establish two separate links: socially moderated and statistically moderated. A socially moderated link was established by using the same achievement level descriptions in an ITBS standard-setting study that were used in a NAEP standard-setting study. A statistically moderated link was accomplished by using an equipercentile procedure. The primary findings were that (a) social moderation yielded cutscores on the ITBS scales that resulted in larger percentages of Iowa public fourth-grade students being classified within the basic, proficient, and advanced achievement regions than those reported by NAEP; (b) the equipercentile link yielded percentages on the ITBS scale that were similar to those reported by NAEP for ""type of community"" subgroups; and (c) for students taking both assessments, the corresponding achievement regions on the NAEP and ITBS scales produced low to moderate percents of agreement in student classification.","['purpose', 'study', 'investigate', 'comparability', 'score', 'meaning', 'performance', 'region', 'ITBS', 'naep', 'mathematic', 'score', 'scale', 'result', 'performance', 'standard', 'establish', 'separate', 'link', 'socially', 'moderated', 'statistically', 'moderated', 'socially', 'moderate', 'link', 'establish', 'achievement', 'level', 'description', 'ITBS', 'standardsetting', 'study', 'naep', 'standardsetting', 'study', 'statistically', 'moderated', 'link', 'accomplish', 'equipercentile', 'procedure', 'primary', 'finding', 'social', 'moderation', 'yield', 'cutscore', 'ITBS', 'scale', 'result', 'large', 'percentage', 'Iowa', 'public', 'fourthgrade', 'student', 'classify', 'basic', 'proficient', 'advanced', 'achievement', 'region', 'report', 'NAEP', 'b', 'equipercentile', 'link', 'yield', 'percentage', 'ITBS', 'scale', 'similar', 'report', 'naep', 'type', 'community', 'subgroup', 'c', 'student', 'assessment', 'corresponding', 'achievement', 'region', 'NAEP', 'ITBS', 'scale', 'produce', 'low', 'moderate', 'percent', 'agreement', 'student', 'classification']","['performance', 'standard', 'link', 'statewide', 'achievement', 'result', 'NAEP']",purpose study investigate comparability score meaning performance region ITBS naep mathematic score scale result performance standard establish separate link socially moderated statistically moderated socially moderate link establish achievement level description ITBS standardsetting study naep standardsetting study statistically moderated link accomplish equipercentile procedure primary finding social moderation yield cutscore ITBS scale result large percentage Iowa public fourthgrade student classify basic proficient advanced achievement region report NAEP b equipercentile link yield percentage ITBS scale similar report naep type community subgroup c student assessment corresponding achievement region NAEP ITBS scale produce low moderate percent agreement student classification,performance standard link statewide achievement result NAEP,0.028298649231068798,0.028447695256319988,0.028525937752666042,0.028166562584171533,0.8865611551757737,0.0,0.0,0.0,0.046121549941857534,0.0719407167085988
Mazor K.M.; Kanjee A.; Clauser B.E.,Using Logistic Regression and the Mantel‐Haenszel With Multiple Ability Estimates to Detect Differential Item Functioning,1995,32,"Logistic regression has recently been advanced as a viable procedure for detecting differential item functioning (DIF). One of the advantages of this procedure is the considerable flexibility it offers in the specification of the regression equation. This article describes incorporating two ability estimates into a single regression analysis, with the result that substantially fewer items exhibit DIF. A comparable analysis is conducted using the Mantel‐Haenszel with similar results. It is argued that by simultaneously conditioning on two relevant ability estimates, more accurate matching of examinees in the reference and focal groups is obtained, and thus multidimensional item impact is not mistakenly identified as DIF. Copyright © 1995, Wiley Blackwell. All rights reserved",Using Logistic Regression and the Mantel‐Haenszel With Multiple Ability Estimates to Detect Differential Item Functioning,"Logistic regression has recently been advanced as a viable procedure for detecting differential item functioning (DIF). One of the advantages of this procedure is the considerable flexibility it offers in the specification of the regression equation. This article describes incorporating two ability estimates into a single regression analysis, with the result that substantially fewer items exhibit DIF. A comparable analysis is conducted using the Mantel‐Haenszel with similar results. It is argued that by simultaneously conditioning on two relevant ability estimates, more accurate matching of examinees in the reference and focal groups is obtained, and thus multidimensional item impact is not mistakenly identified as DIF. Copyright © 1995, Wiley Blackwell. All rights reserved","['logistic', 'regression', 'recently', 'advance', 'viable', 'procedure', 'detect', 'differential', 'item', 'function', 'DIF', 'advantage', 'procedure', 'considerable', 'flexibility', 'offer', 'specification', 'regression', 'equation', 'article', 'describe', 'incorporate', 'ability', 'estimate', 'single', 'regression', 'analysis', 'result', 'substantially', 'item', 'exhibit', 'DIF', 'comparable', 'analysis', 'conduct', 'Mantel‐Haenszel', 'similar', 'result', 'argue', 'simultaneously', 'condition', 'relevant', 'ability', 'estimate', 'accurate', 'matching', 'examinee', 'reference', 'focal', 'group', 'obtain', 'multidimensional', 'item', 'impact', 'mistakenly', 'identify', 'DIF', 'copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Logistic', 'Regression', 'Mantel‐Haenszel', 'multiple', 'ability', 'estimate', 'detect', 'Differential', 'Item', 'Functioning']",logistic regression recently advance viable procedure detect differential item function DIF advantage procedure considerable flexibility offer specification regression equation article describe incorporate ability estimate single regression analysis result substantially item exhibit DIF comparable analysis conduct Mantel‐Haenszel similar result argue simultaneously condition relevant ability estimate accurate matching examinee reference focal group obtain multidimensional item impact mistakenly identify DIF copyright © 1995 Wiley Blackwell right reserve,Logistic Regression Mantel‐Haenszel multiple ability estimate detect Differential Item Functioning,0.026309618344911707,0.02728447050639383,0.026532222021760914,0.19083818396221053,0.729035505164723,0.02031898260486083,0.0,0.16634240818390242,0.005129205263135201,0.0
Camilli G.; Wang M.‐m.; Fesq J.,The Effects of Dimensionality on Equating the Law School Admission Test,1995,32,"Using factor analysis, we conducted an assessment of multidimensionality for 6 forms of the Law School Admission Test (LSAT) and found 2 subgroups of items or factors for each of the 6 forms. The main conclusion of the factor analysis component of this study was that the LSAT appears to measure 2 different reasoning abilities: inductive and deductive. The technique of N. J. Dorans & N. M. Kingston (1985) was used to examine the effect of dimensionality on equating. We began by calibrating (with item response theory [IRT] methods) all items on a form to obtain Set I of estimated IRT item parameters. Next, the test was divided into 2 homogeneous subgroups of items, each having been determined to represent a different ability (i.e., inductive or deductive reasoning). The items within these subgroups were then recalibrated separately to obtain item parameter estimates, and then combined into Set II. The estimated item parameters and true‐score equating tables for Sets I and II corresponded closely. Copyright © 1995, Wiley Blackwell. All rights reserved",The Effects of Dimensionality on Equating the Law School Admission Test,"Using factor analysis, we conducted an assessment of multidimensionality for 6 forms of the Law School Admission Test (LSAT) and found 2 subgroups of items or factors for each of the 6 forms. The main conclusion of the factor analysis component of this study was that the LSAT appears to measure 2 different reasoning abilities: inductive and deductive. The technique of N. J. Dorans & N. M. Kingston (1985) was used to examine the effect of dimensionality on equating. We began by calibrating (with item response theory [IRT] methods) all items on a form to obtain Set I of estimated IRT item parameters. Next, the test was divided into 2 homogeneous subgroups of items, each having been determined to represent a different ability (i.e., inductive or deductive reasoning). The items within these subgroups were then recalibrated separately to obtain item parameter estimates, and then combined into Set II. The estimated item parameters and true‐score equating tables for Sets I and II corresponded closely. Copyright © 1995, Wiley Blackwell. All rights reserved","['factor', 'analysis', 'conduct', 'assessment', 'multidimensionality', '6', 'form', 'Law', 'School', 'Admission', 'test', 'LSAT', 'find', '2', 'subgroup', 'item', 'factor', '6', 'form', 'main', 'conclusion', 'factor', 'analysis', 'component', 'study', 'LSAT', 'appear', 'measure', '2', 'different', 'reasoning', 'ability', 'inductive', 'deductive', 'technique', 'N', 'J', 'Dorans', 'N', 'M', 'Kingston', '1985', 'examine', 'effect', 'dimensionality', 'equate', 'begin', 'calibrate', 'item', 'response', 'theory', 'IRT', 'method', 'item', 'form', 'obtain', 'Set', 'I', 'estimate', 'IRT', 'item', 'parameter', 'test', 'divide', '2', 'homogeneous', 'subgroup', 'item', 'having', 'determine', 'represent', 'different', 'ability', 'ie', 'inductive', 'deductive', 'reasoning', 'item', 'subgroup', 'recalibrate', 'separately', 'obtain', 'item', 'parameter', 'estimate', 'combine', 'Set', 'II', 'estimate', 'item', 'parameter', 'true‐score', 'equate', 'table', 'set', 'I', 'II', 'correspond', 'closely', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Effects', 'Dimensionality', 'equate', 'Law', 'School', 'Admission', 'test']",factor analysis conduct assessment multidimensionality 6 form Law School Admission test LSAT find 2 subgroup item factor 6 form main conclusion factor analysis component study LSAT appear measure 2 different reasoning ability inductive deductive technique N J Dorans N M Kingston 1985 examine effect dimensionality equate begin calibrate item response theory IRT method item form obtain Set I estimate IRT item parameter test divide 2 homogeneous subgroup item having determine represent different ability ie inductive deductive reasoning item subgroup recalibrate separately obtain item parameter estimate combine Set II estimate item parameter true‐score equate table set I II correspond closely Copyright © 1995 Wiley Blackwell right reserve,Effects Dimensionality equate Law School Admission test,0.024437575803199968,0.024642170080598408,0.9019006461145088,0.0241697588269614,0.024849849174731545,0.07900738117937504,0.042012641753699745,0.00606962331136224,0.0,0.0
Enright M.K.; Rock D.A.; Bennett R.E.,Improving measurement for graduate admissions,1998,35,"In this study we examined alternative item types and section configurations for improving the discriminant and convergent validity of the GRE General Test. A computer-based test of reasoning items and a generating-explanations measure was administered to a sample of 388 examinees who previously had taken the General Test. Confirmatory factor analyses indicated that three dimensions of reasoning - verbal, analytical, and quantitative - and a fourth dimension of verbal fluency based on the generating-explanations task could be distinguished. Notably, generating explanations was as distinct from new variations of reasoning items as it was from verbal and quantitative reasoning. In the full sample, this differentiation was evident in relation to such external criteria as undergraduate grade point average (UGPA), self-reported accomplishments, and a measure of ideational fluency, with generating explanations relating uniquely to aesthetic and linguistic accomplishments and to ideational fluency. For the subset of participants with undergraduate majors in the humanities and social sciences, generating explanations added to the relationship with UGPA over that contributed by the General Test.",,"In this study we examined alternative item types and section configurations for improving the discriminant and convergent validity of the GRE General Test. A computer-based test of reasoning items and a generating-explanations measure was administered to a sample of 388 examinees who previously had taken the General Test. Confirmatory factor analyses indicated that three dimensions of reasoning - verbal, analytical, and quantitative - and a fourth dimension of verbal fluency based on the generating-explanations task could be distinguished. Notably, generating explanations was as distinct from new variations of reasoning items as it was from verbal and quantitative reasoning. In the full sample, this differentiation was evident in relation to such external criteria as undergraduate grade point average (UGPA), self-reported accomplishments, and a measure of ideational fluency, with generating explanations relating uniquely to aesthetic and linguistic accomplishments and to ideational fluency. For the subset of participants with undergraduate majors in the humanities and social sciences, generating explanations added to the relationship with UGPA over that contributed by the General Test.","['study', 'examine', 'alternative', 'item', 'type', 'section', 'configuration', 'improve', 'discriminant', 'convergent', 'validity', 'GRE', 'General', 'Test', 'computerbased', 'test', 'reasoning', 'item', 'generatingexplanation', 'measure', 'administer', 'sample', '388', 'examinee', 'previously', 'General', 'Test', 'Confirmatory', 'factor', 'analysis', 'indicate', 'dimension', 'reason', 'verbal', 'analytical', 'quantitative', 'fourth', 'dimension', 'verbal', 'fluency', 'base', 'generatingexplanation', 'task', 'distinguish', 'notably', 'generate', 'explanation', 'distinct', 'new', 'variation', 'reasoning', 'item', 'verbal', 'quantitative', 'reasoning', 'sample', 'differentiation', 'evident', 'relation', 'external', 'criterion', 'undergraduate', 'grade', 'point', 'average', 'UGPA', 'selfreporte', 'accomplishment', 'measure', 'ideational', 'fluency', 'generate', 'explanation', 'relate', 'uniquely', 'aesthetic', 'linguistic', 'accomplishment', 'ideational', 'fluency', 'subset', 'participant', 'undergraduate', 'major', 'humanity', 'social', 'science', 'generate', 'explanation', 'add', 'relationship', 'UGPA', 'contribute', 'General', 'Test']",,study examine alternative item type section configuration improve discriminant convergent validity GRE General Test computerbased test reasoning item generatingexplanation measure administer sample 388 examinee previously General Test Confirmatory factor analysis indicate dimension reason verbal analytical quantitative fourth dimension verbal fluency base generatingexplanation task distinguish notably generate explanation distinct new variation reasoning item verbal quantitative reasoning sample differentiation evident relation external criterion undergraduate grade point average UGPA selfreporte accomplishment measure ideational fluency generate explanation relate uniquely aesthetic linguistic accomplishment ideational fluency subset participant undergraduate major humanity social science generate explanation add relationship UGPA contribute General Test,,0.024271282589506404,0.9024827277151853,0.02427596026871228,0.024237265882660272,0.024732763543935693,0.0,0.07604629107258964,0.0,0.0,0.022048070993009313
Roussos L.A.; Stout W.F.,Simulation studies of the effects of small sample size and studied item parameters on SIBTEST and Mantel-Haenszel type I error performance,1996,33,"Two simulation studies investigated Type I error performance of two statistical procedures for detecting differential item functioning (DIF): SIBTEST and Mantel-Haenszel (MH). Because MH and SIBTEST are based on asymptotic distributions requiring ""large"" numbers of examinees, the first study examined Type I error for small sample sizes. No significant Type I error inflation occurred for either procedure. Because MH has the potential for Type I error inflation for non-Rasch models, the second study used a markedly non-Rasch test and systematically varied the shape and location of the studied item. When differences in distribution across examinee group of the measured ability were present, both procedures displayed inflated Type I error for certain items; MH displayed the greater inflation. Also, both procedures displayed statistically biased estimation of the zero DIF for certain items, though SIBTEST displayed much less than MH. When no latent distributional differences were present, both procedures performed satisfactorily under all conditions.",Simulation studies of the effects of small sample size and studied item parameters on SIBTEST and Mantel-Haenszel type I error performance,"Two simulation studies investigated Type I error performance of two statistical procedures for detecting differential item functioning (DIF): SIBTEST and Mantel-Haenszel (MH). Because MH and SIBTEST are based on asymptotic distributions requiring ""large"" numbers of examinees, the first study examined Type I error for small sample sizes. No significant Type I error inflation occurred for either procedure. Because MH has the potential for Type I error inflation for non-Rasch models, the second study used a markedly non-Rasch test and systematically varied the shape and location of the studied item. When differences in distribution across examinee group of the measured ability were present, both procedures displayed inflated Type I error for certain items; MH displayed the greater inflation. Also, both procedures displayed statistically biased estimation of the zero DIF for certain items, though SIBTEST displayed much less than MH. When no latent distributional differences were present, both procedures performed satisfactorily under all conditions.","['simulation', 'study', 'investigate', 'Type', 'I', 'error', 'performance', 'statistical', 'procedure', 'detect', 'differential', 'item', 'function', 'DIF', 'SIBTEST', 'MantelHaenszel', 'MH', 'MH', 'SIBTEST', 'base', 'asymptotic', 'distribution', 'require', 'large', 'number', 'examine', 'study', 'examine', 'Type', 'I', 'error', 'small', 'sample', 'size', 'significant', 'type', 'I', 'error', 'inflation', 'occur', 'procedure', 'MH', 'potential', 'Type', 'I', 'error', 'inflation', 'nonrasch', 'second', 'study', 'markedly', 'nonrasch', 'test', 'systematically', 'vary', 'shape', 'location', 'study', 'item', 'difference', 'distribution', 'examinee', 'group', 'measure', 'ability', 'present', 'procedure', 'display', 'inflated', 'Type', 'I', 'error', 'certain', 'item', 'MH', 'display', 'great', 'inflation', 'procedure', 'display', 'statistically', 'biased', 'estimation', 'zero', 'DIF', 'certain', 'item', 'SIBTEST', 'display', 'MH', 'latent', 'distributional', 'difference', 'present', 'procedure', 'perform', 'satisfactorily', 'condition']","['simulation', 'study', 'effect', 'small', 'sample', 'size', 'study', 'item', 'parameter', 'SIBTEST', 'MantelHaenszel', 'type', 'I', 'error', 'performance']",simulation study investigate Type I error performance statistical procedure detect differential item function DIF SIBTEST MantelHaenszel MH MH SIBTEST base asymptotic distribution require large number examine study examine Type I error small sample size significant type I error inflation occur procedure MH potential Type I error inflation nonrasch second study markedly nonrasch test systematically vary shape location study item difference distribution examinee group measure ability present procedure display inflated Type I error certain item MH display great inflation procedure display statistically biased estimation zero DIF certain item SIBTEST display MH latent distributional difference present procedure perform satisfactorily condition,simulation study effect small sample size study item parameter SIBTEST MantelHaenszel type I error performance,0.029499319649927445,0.8817555222651396,0.02920612331037193,0.028972761786604947,0.03056627298795611,0.04502961747923998,0.0,0.07356029503837566,0.018816266466511543,0.0
Embretson S.E.,Measurement Model for Linking Individual Learning to Processes and Knowledge: Application to Mathematical Reasoning,1995,32,"Contemporary instructional theories increasingly emphasize the importance of linking an individual's learning to changes in cognitive processes and knowledge structures. In this article, an extension of the multidimensional Rasch model for learning and change (MRMLC) is presented so as to permit theories of processes and knowledge structures to be incorporated into the item response model. Like the MRMLC, this extension (MRMLC+) resolves some basic problems in measuring individual change and permits adaptive testing so that precise estimates of learning may be obtained. Additionally, MRMLC+ permits individual learning to be linked to substantive changes in processing and knowledge. An application to a study on the impact of short‐term instruction on mathematical problem solving shows the potential of MRMLC + for interpretations. In this study, a theoretically plausible model of knowledge structures (Mayer, Larkin, & Kadane, 1984) provides the basis of individual learning interpretations. Copyright © 1995, Wiley Blackwell. All rights reserved",Measurement Model for Linking Individual Learning to Processes and Knowledge: Application to Mathematical Reasoning,"Contemporary instructional theories increasingly emphasize the importance of linking an individual's learning to changes in cognitive processes and knowledge structures. In this article, an extension of the multidimensional Rasch model for learning and change (MRMLC) is presented so as to permit theories of processes and knowledge structures to be incorporated into the item response model. Like the MRMLC, this extension (MRMLC+) resolves some basic problems in measuring individual change and permits adaptive testing so that precise estimates of learning may be obtained. Additionally, MRMLC+ permits individual learning to be linked to substantive changes in processing and knowledge. An application to a study on the impact of short‐term instruction on mathematical problem solving shows the potential of MRMLC + for interpretations. In this study, a theoretically plausible model of knowledge structures (Mayer, Larkin, & Kadane, 1984) provides the basis of individual learning interpretations. Copyright © 1995, Wiley Blackwell. All rights reserved","['contemporary', 'instructional', 'theory', 'increasingly', 'emphasize', 'importance', 'link', 'individual', 'learn', 'change', 'cognitive', 'process', 'knowledge', 'structure', 'article', 'extension', 'multidimensional', 'Rasch', 'learn', 'change', 'MRMLC', 'present', 'permit', 'theory', 'process', 'knowledge', 'structure', 'incorporate', 'item', 'response', 'like', 'MRMLC', 'extension', 'MRMLC', 'resolve', 'basic', 'problem', 'measure', 'individual', 'change', 'permit', 'adaptive', 'testing', 'precise', 'estimate', 'learning', 'obtain', 'Additionally', 'MRMLC', 'permit', 'individual', 'learning', 'link', 'substantive', 'change', 'processing', 'knowledge', 'application', 'study', 'impact', 'short‐term', 'instruction', 'mathematical', 'problem', 'solve', 'potential', 'MRMLC', 'interpretation', 'study', 'theoretically', 'plausible', 'knowledge', 'structure', 'Mayer', 'Larkin', 'Kadane', '1984', 'provide', 'basis', 'individual', 'learning', 'interpretation', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['link', 'Individual', 'Learning', 'Processes', 'Knowledge', 'Application', 'Mathematical', 'Reasoning']",contemporary instructional theory increasingly emphasize importance link individual learn change cognitive process knowledge structure article extension multidimensional Rasch learn change MRMLC present permit theory process knowledge structure incorporate item response like MRMLC extension MRMLC resolve basic problem measure individual change permit adaptive testing precise estimate learning obtain Additionally MRMLC permit individual learning link substantive change processing knowledge application study impact short‐term instruction mathematical problem solve potential MRMLC interpretation study theoretically plausible knowledge structure Mayer Larkin Kadane 1984 provide basis individual learning interpretation Copyright © 1995 Wiley Blackwell right reserve,link Individual Learning Processes Knowledge Application Mathematical Reasoning,0.02863677090530579,0.028659628813296836,0.02894952338571847,0.02861995508922942,0.8851341218064495,0.0,0.04944328813331187,0.0,0.0,0.02904480818379704
Powers D.E.; Leung S.W.,Answering the New SAT Reading Comprehension Questions Without the Passages,1995,32,"It has been reasonably well established that test takers can, to varying degrees, answer some reading comprehension questions without reading the passages on which the questions are based, even for carefully constructed measures like the Scholastic Aptitude Test (SAT). The aim of this study was to determine what test‐taking strategies examinees use, and which are related to test performance, when reading passages are not available. The research focused on reading comprehension questions similar to those that will be used in the revised SAT, to be introduced in 1994. The most often cited strategies involved choosing answers on the basis of consistency with other questions and reconstructing the main theme of a missing passage from all of the questions and answers in a set. These strategies were more likely to result in successful performance on individual test items than were any of many other possible (and less constructrelevant) strategies. Copyright © 1995, Wiley Blackwell. All rights reserved",Answering the New SAT Reading Comprehension Questions Without the Passages,"It has been reasonably well established that test takers can, to varying degrees, answer some reading comprehension questions without reading the passages on which the questions are based, even for carefully constructed measures like the Scholastic Aptitude Test (SAT). The aim of this study was to determine what test‐taking strategies examinees use, and which are related to test performance, when reading passages are not available. The research focused on reading comprehension questions similar to those that will be used in the revised SAT, to be introduced in 1994. The most often cited strategies involved choosing answers on the basis of consistency with other questions and reconstructing the main theme of a missing passage from all of the questions and answers in a set. These strategies were more likely to result in successful performance on individual test items than were any of many other possible (and less constructrelevant) strategies. Copyright © 1995, Wiley Blackwell. All rights reserved","['reasonably', 'establish', 'test', 'taker', 'vary', 'degree', 'answer', 'reading', 'comprehension', 'question', 'read', 'passage', 'question', 'base', 'carefully', 'construct', 'measure', 'like', 'Scholastic', 'Aptitude', 'Test', 'SAT', 'aim', 'study', 'determine', 'test‐taking', 'strategy', 'examine', 'relate', 'test', 'performance', 'read', 'passage', 'available', 'research', 'focus', 'read', 'comprehension', 'question', 'similar', 'revise', 'SAT', 'introduce', '1994', 'cite', 'strategy', 'involve', 'choose', 'answer', 'basis', 'consistency', 'question', 'reconstruct', 'main', 'theme', 'missing', 'passage', 'question', 'answer', 'set', 'strategy', 'likely', 'result', 'successful', 'performance', 'individual', 'test', 'item', 'possible', 'constructrelevant', 'strategy', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['answer', 'New', 'SAT', 'Reading', 'Comprehension', 'Questions', 'passage']",reasonably establish test taker vary degree answer reading comprehension question read passage question base carefully construct measure like Scholastic Aptitude Test SAT aim study determine test‐taking strategy examine relate test performance read passage available research focus read comprehension question similar revise SAT introduce 1994 cite strategy involve choose answer basis consistency question reconstruct main theme missing passage question answer set strategy likely result successful performance individual test item possible constructrelevant strategy Copyright © 1995 Wiley Blackwell right reserve,answer New SAT Reading Comprehension Questions passage,0.028043840385081106,0.02861317757228335,0.8867731629957193,0.027764329696334977,0.028805489350581266,0.0,0.11316205743899893,0.0,0.0,9.653948691787698e-05
Luecht R.M.,"A reaction to ""moderating possibly irrelevant multiple mean score differences on a test of mathematical reasoning",1998,35,"A pool of items from operational tests of mathematical reasoning was constructed
to investigate the feasibility of using automated test assembly (ATA) methods to
simultaneously moderate possibly irrelevant differences between the performance
of women and men, and African American and White test takers. None of the
artificial tests exhibited substantial impact moderation, although the estimated
mean scaled score differences for the relevant population indicated a modest move in the intended direction: the difference between scaled score means was reduced by about 20% for women and men and about 9% for African American and White test takers. Although many issues in the implementation of this methodology remain to be solved, the consideration of impact in ATA, along with the maintenance of the detailed test plan, appears to be a potential method of moderating possibly irrelevant mean test score differences.","A reaction to ""moderating possibly irrelevant multiple mean score differences on a test of mathematical reasoning","A pool of items from operational tests of mathematical reasoning was constructed
to investigate the feasibility of using automated test assembly (ATA) methods to
simultaneously moderate possibly irrelevant differences between the performance
of women and men, and African American and White test takers. None of the
artificial tests exhibited substantial impact moderation, although the estimated
mean scaled score differences for the relevant population indicated a modest move in the intended direction: the difference between scaled score means was reduced by about 20% for women and men and about 9% for African American and White test takers. Although many issues in the implementation of this methodology remain to be solved, the consideration of impact in ATA, along with the maintenance of the detailed test plan, appears to be a potential method of moderating possibly irrelevant mean test score differences.","['pool', 'item', 'operational', 'test', 'mathematical', 'reasoning', 'construct', 'investigate', 'feasibility', 'automate', 'test', 'assembly', 'ATA', 'method', 'simultaneously', 'moderate', 'possibly', 'irrelevant', 'difference', 'performance', 'woman', 'man', 'african', 'american', 'white', 'test', 'taker', 'artificial', 'test', 'exhibit', 'substantial', 'impact', 'moderation', 'estimate', 'mean', 'scale', 'score', 'difference', 'relevant', 'population', 'indicate', 'modest', 'intend', 'direction', 'difference', 'scale', 'score', 'mean', 'reduce', '20', 'woman', 'man', '9', 'african', 'american', 'white', 'test', 'taker', 'issue', 'implementation', 'methodology', 'remain', 'solve', 'consideration', 'impact', 'ATA', 'maintenance', 'detailed', 'test', 'plan', 'appear', 'potential', 'method', 'moderate', 'possibly', 'irrelevant', 'mean', 'test', 'score', 'difference']","['reaction', 'moderate', 'possibly', 'irrelevant', 'multiple', 'mean', 'score', 'difference', 'test', 'mathematical', 'reasoning']",pool item operational test mathematical reasoning construct investigate feasibility automate test assembly ATA method simultaneously moderate possibly irrelevant difference performance woman man african american white test taker artificial test exhibit substantial impact moderation estimate mean scale score difference relevant population indicate modest intend direction difference scale score mean reduce 20 woman man 9 african american white test taker issue implementation methodology remain solve consideration impact ATA maintenance detailed test plan appear potential method moderate possibly irrelevant mean test score difference,reaction moderate possibly irrelevant multiple mean score difference test mathematical reasoning,0.02607224386059763,0.02607125579449956,0.026104941221149657,0.02590696183198429,0.8958445972917689,0.0,0.05081223098846018,0.0,0.13063044260437096,0.0
Camilli G.; Penfield D.A.,Variance estimation for differential test functioning based on Mantel-Haenszel statistics,1997,34,"This article concerns the simultaneous assessment of DIF for a collection of test items. Rather than an average or sum in which positive and negative DIF may cancel, we propose an index that measures the variance of DIF on a test as an indicator of the degree to which different items show DIF in different directions. It is computed from standard Mantel-Haenszel statistics (the log-odds ratio and its variance error) and may be conceptually classified as a variance component or variance effect size. Evaluated by simulation under three item response models (1PL, 2PL, and 3PL), the index is shown to be an accurate estimate of the DTF generating parameter in the case of the IPL and 2PL models with groups of equal ability. For groups of unequal ability, the index is accurate under the 1PL but not the 2PL condition; however, a weighted version of the index provides improved estimates. For the 3PL condition, the DTF generating parameter is underestimated. This latter result is due in part to a mismatch in the scales of the log-odds ratio and IRT difficulty.",Variance estimation for differential test functioning based on Mantel-Haenszel statistics,"This article concerns the simultaneous assessment of DIF for a collection of test items. Rather than an average or sum in which positive and negative DIF may cancel, we propose an index that measures the variance of DIF on a test as an indicator of the degree to which different items show DIF in different directions. It is computed from standard Mantel-Haenszel statistics (the log-odds ratio and its variance error) and may be conceptually classified as a variance component or variance effect size. Evaluated by simulation under three item response models (1PL, 2PL, and 3PL), the index is shown to be an accurate estimate of the DTF generating parameter in the case of the IPL and 2PL models with groups of equal ability. For groups of unequal ability, the index is accurate under the 1PL but not the 2PL condition; however, a weighted version of the index provides improved estimates. For the 3PL condition, the DTF generating parameter is underestimated. This latter result is due in part to a mismatch in the scales of the log-odds ratio and IRT difficulty.","['article', 'concern', 'simultaneous', 'assessment', 'dif', 'collection', 'test', 'item', 'average', 'sum', 'positive', 'negative', 'DIF', 'cancel', 'propose', 'index', 'measure', 'variance', 'DIF', 'test', 'indicator', 'degree', 'different', 'item', 'DIF', 'different', 'direction', 'compute', 'standard', 'MantelHaenszel', 'statistic', 'logodds', 'ratio', 'variance', 'error', 'conceptually', 'classify', 'variance', 'component', 'variance', 'effect', 'size', 'evaluate', 'simulation', 'item', 'response', '1PL', '2PL', '3pl', 'index', 'accurate', 'estimate', 'DTF', 'generating', 'parameter', 'case', 'IPL', '2PL', 'group', 'equal', 'ability', 'group', 'unequal', 'ability', 'index', 'accurate', '1pl', '2PL', 'condition', 'weighted', 'version', 'index', 'provide', 'improve', 'estimate', '3pl', 'condition', 'DTF', 'generating', 'parameter', 'underestimate', 'result', 'mismatch', 'scale', 'logodds', 'ratio', 'IRT', 'difficulty']","['Variance', 'estimation', 'differential', 'test', 'function', 'base', 'MantelHaenszel', 'statistic']",article concern simultaneous assessment dif collection test item average sum positive negative DIF cancel propose index measure variance DIF test indicator degree different item DIF different direction compute standard MantelHaenszel statistic logodds ratio variance error conceptually classify variance component variance effect size evaluate simulation item response 1PL 2PL 3pl index accurate estimate DTF generating parameter case IPL 2PL group equal ability group unequal ability index accurate 1pl 2PL condition weighted version index provide improve estimate 3pl condition DTF generating parameter underestimate result mismatch scale logodds ratio IRT difficulty,Variance estimation differential test function base MantelHaenszel statistic,0.8926622739695328,0.026806737342865632,0.02680577372840341,0.02640346186667144,0.027321753092526692,0.0261288413175635,0.0,0.09105963141320218,0.04087757514329126,0.0
Roussos L.A.; Stout W.F.; Marden J.I.,Using new proximity measures with hierarchical cluster analysis to detect multidimensionality,1998,35,"A new approach for partitioning test items into dimensionally distinct item clusters is introduced. The core of the approach is a new item-pair conditional-covariance-based proximity measure that can be used with hierarchical cluster analysis. An extensive simulation study designed to test the limits of the approach indicates that when approximate simple structure holds, the procedure can correctly partition the test into dimensionally homogeneous item clusters even for very high correlations between the latent dimensions. In particular, the procedure can correctly classify (on average) over 90% of the items for correlations as high as .9. The cooperative role that the procedure can play when used in conjunction with other dimensionality assessment procedures is discussed.",Using new proximity measures with hierarchical cluster analysis to detect multidimensionality,"A new approach for partitioning test items into dimensionally distinct item clusters is introduced. The core of the approach is a new item-pair conditional-covariance-based proximity measure that can be used with hierarchical cluster analysis. An extensive simulation study designed to test the limits of the approach indicates that when approximate simple structure holds, the procedure can correctly partition the test into dimensionally homogeneous item clusters even for very high correlations between the latent dimensions. In particular, the procedure can correctly classify (on average) over 90% of the items for correlations as high as .9. The cooperative role that the procedure can play when used in conjunction with other dimensionality assessment procedures is discussed.","['new', 'approach', 'partition', 'test', 'item', 'dimensionally', 'distinct', 'item', 'cluster', 'introduce', 'core', 'approach', 'new', 'itempair', 'conditionalcovariancebase', 'proximity', 'measure', 'hierarchical', 'cluster', 'analysis', 'extensive', 'simulation', 'study', 'design', 'test', 'limit', 'approach', 'indicate', 'approximate', 'simple', 'structure', 'hold', 'procedure', 'correctly', 'partition', 'test', 'dimensionally', 'homogeneous', 'item', 'cluster', 'high', 'correlation', 'latent', 'dimension', 'particular', 'procedure', 'correctly', 'classify', 'average', '90', 'item', 'correlation', 'high', '9', 'cooperative', 'role', 'procedure', 'play', 'conjunction', 'dimensionality', 'assessment', 'procedure', 'discuss']","['new', 'proximity', 'measure', 'hierarchical', 'cluster', 'analysis', 'detect', 'multidimensionality']",new approach partition test item dimensionally distinct item cluster introduce core approach new itempair conditionalcovariancebase proximity measure hierarchical cluster analysis extensive simulation study design test limit approach indicate approximate simple structure hold procedure correctly partition test dimensionally homogeneous item cluster high correlation latent dimension particular procedure correctly classify average 90 item correlation high 9 cooperative role procedure play conjunction dimensionality assessment procedure discuss,new proximity measure hierarchical cluster analysis detect multidimensionality,0.028519694982880508,0.8857037549705924,0.028510566614905513,0.028364479172545715,0.02890150425907576,0.004204155858122205,0.050621321526715196,0.0229066173929514,0.0016413630014951548,0.026306595655531163
Embretson S.E.,Cognitive design principles and the successful performer: A study on spatial ability,1996,33,"An important trend in educational measurement is the use of principles of cognitive psychology to design achievement and ability test items. Many studies show that manipulating the stimulus features of items influences the processes, strategies, and knowledge structures that are involved in solution. However, little is known about how cognitive design influences individual differences. That is, does applying cognitive design principles change the background skills and abilities that are associated with successful performance? This study compared the correlates of two spatial ability tests that used the same item type but different test design principles (cognitive design versus psychometric design). The results indicated differences in factorial complexity in the two tests; specifically, the impact of verbal abilities was substantially reduced by applying the cognitive design principles.",Cognitive design principles and the successful performer: A study on spatial ability,"An important trend in educational measurement is the use of principles of cognitive psychology to design achievement and ability test items. Many studies show that manipulating the stimulus features of items influences the processes, strategies, and knowledge structures that are involved in solution. However, little is known about how cognitive design influences individual differences. That is, does applying cognitive design principles change the background skills and abilities that are associated with successful performance? This study compared the correlates of two spatial ability tests that used the same item type but different test design principles (cognitive design versus psychometric design). The results indicated differences in factorial complexity in the two tests; specifically, the impact of verbal abilities was substantially reduced by applying the cognitive design principles.","['important', 'trend', 'educational', 'principle', 'cognitive', 'psychology', 'design', 'achievement', 'ability', 'test', 'item', 'study', 'manipulate', 'stimulus', 'feature', 'item', 'influence', 'process', 'strategy', 'knowledge', 'structure', 'involve', 'solution', 'little', 'know', 'cognitive', 'design', 'influence', 'individual', 'difference', 'apply', 'cognitive', 'design', 'principle', 'change', 'background', 'skill', 'ability', 'associate', 'successful', 'performance', 'study', 'compare', 'correlate', 'spatial', 'ability', 'test', 'item', 'type', 'different', 'test', 'design', 'principle', 'cognitive', 'design', 'versus', 'psychometric', 'design', 'result', 'indicate', 'difference', 'factorial', 'complexity', 'test', 'specifically', 'impact', 'verbal', 'ability', 'substantially', 'reduce', 'apply', 'cognitive', 'design', 'principle']","['cognitive', 'design', 'principle', 'successful', 'performer', 'study', 'spatial', 'ability']",important trend educational principle cognitive psychology design achievement ability test item study manipulate stimulus feature item influence process strategy knowledge structure involve solution little know cognitive design influence individual difference apply cognitive design principle change background skill ability associate successful performance study compare correlate spatial ability test item type different test design principle cognitive design versus psychometric design result indicate difference factorial complexity test specifically impact verbal ability substantially reduce apply cognitive design principle,cognitive design principle successful performer study spatial ability,0.030462331077413814,0.030493483515267254,0.030609751990460284,0.030380999205034138,0.8780534342118246,0.018845412888190624,0.06652184006485679,0.0,0.0,0.03638507408862396
Oshima T.C.; Raju N.S.; Flowers C.P.,Development and demonstration of multidimensional IRT-based internal measures of differential functioning of items and tests,1997,34,"This article defines and demonstrates a framework for studying differential item functioning (DIF) and differential test functioning (DTF) for tests that are intended to be multidimensional. The procedure introduced here is an extension of unidimensional differential functioning of items and tests (DFIT) recently developed by Raju, van der Linden, & Fleer (1995). To demonstrate the usefulness of these new indexes in a multidimensional IRT setting, two-dimensional data were simulated with known item parameters and known DIF and DTF. The DIF and DTF indexes were recovered reasonably well under various distributional differences of θs after multidimensional linking was applied to put the two sets of item parameters on a common scale. Further studies are suggested in the area of DIF/DTF for intentionally multidimensional tests.",Development and demonstration of multidimensional IRT-based internal measures of differential functioning of items and tests,"This article defines and demonstrates a framework for studying differential item functioning (DIF) and differential test functioning (DTF) for tests that are intended to be multidimensional. The procedure introduced here is an extension of unidimensional differential functioning of items and tests (DFIT) recently developed by Raju, van der Linden, & Fleer (1995). To demonstrate the usefulness of these new indexes in a multidimensional IRT setting, two-dimensional data were simulated with known item parameters and known DIF and DTF. The DIF and DTF indexes were recovered reasonably well under various distributional differences of θs after multidimensional linking was applied to put the two sets of item parameters on a common scale. Further studies are suggested in the area of DIF/DTF for intentionally multidimensional tests.","['article', 'define', 'demonstrate', 'framework', 'study', 'differential', 'item', 'function', 'DIF', 'differential', 'test', 'function', 'DTF', 'test', 'intend', 'multidimensional', 'procedure', 'introduce', 'extension', 'unidimensional', 'differential', 'functioning', 'item', 'test', 'DFIT', 'recently', 'develop', 'Raju', 'van', 'der', 'Linden', 'Fleer', '1995', 'demonstrate', 'usefulness', 'new', 'index', 'multidimensional', 'IRT', 'set', 'twodimensional', 'datum', 'simulate', 'know', 'item', 'parameter', 'know', 'DIF', 'DTF', 'DIF', 'DTF', 'index', 'recover', 'reasonably', 'distributional', 'difference', 'θs', 'multidimensional', 'linking', 'apply', 'set', 'item', 'parameter', 'common', 'scale', 'study', 'suggest', 'area', 'DIFDTF', 'intentionally', 'multidimensional', 'test']","['development', 'demonstration', 'multidimensional', 'irtbase', 'internal', 'measure', 'differential', 'functioning', 'item', 'test']",article define demonstrate framework study differential item function DIF differential test function DTF test intend multidimensional procedure introduce extension unidimensional differential functioning item test DFIT recently develop Raju van der Linden Fleer 1995 demonstrate usefulness new index multidimensional IRT set twodimensional datum simulate know item parameter know DIF DTF DIF DTF index recover reasonably distributional difference θs multidimensional linking apply set item parameter common scale study suggest area DIFDTF intentionally multidimensional test,development demonstration multidimensional irtbase internal measure differential functioning item test,0.028161834594207395,0.028175036158520475,0.028104935360620045,0.02790137969567771,0.8876568141909744,0.0008493525385658056,0.025813342289067454,0.12397657500466285,0.005798278632117972,0.0
Bridgeman B.; Morgan R.; Wang M.,Choice among essay topics: Impact on performance and validity,1997,34,"This study assessed the ability of history students to choose the essay topic on which they can get the highest score. A second, equally important question was whether the score on the chosen topic was more highly related to other indicators of proficiency in history than the score on the unchosen topic. Overall, for both U.S. and European history, scores were about one third of a standard deviation higher for the preferred topic than for the other topic. For U.S. history, about 32% of the students made the wrong choice; that is, 32% got a higher score on the other topic than on the preferred topic. In European history, 29% made the wrong choice In the U.S. history sample, the preferred essay correlated .40 with an external criterion score, compared to .34 for the other essay; in the European history sample, the preferred essay correlated .52 with the external criterion, compared to .44 for the other topic.",Choice among essay topics: Impact on performance and validity,"This study assessed the ability of history students to choose the essay topic on which they can get the highest score. A second, equally important question was whether the score on the chosen topic was more highly related to other indicators of proficiency in history than the score on the unchosen topic. Overall, for both U.S. and European history, scores were about one third of a standard deviation higher for the preferred topic than for the other topic. For U.S. history, about 32% of the students made the wrong choice; that is, 32% got a higher score on the other topic than on the preferred topic. In European history, 29% made the wrong choice In the U.S. history sample, the preferred essay correlated .40 with an external criterion score, compared to .34 for the other essay; in the European history sample, the preferred essay correlated .52 with the external criterion, compared to .44 for the other topic.","['study', 'assess', 'ability', 'history', 'student', 'choose', 'essay', 'topic', 'high', 'score', 'second', 'equally', 'important', 'question', 'score', 'choose', 'topic', 'highly', 'related', 'indicator', 'proficiency', 'history', 'score', 'unchosen', 'topic', 'overall', 'US', 'european', 'history', 'score', 'standard', 'deviation', 'high', 'preferred', 'topic', 'topic', 'US', 'history', '32', 'student', 'wrong', 'choice', '32', 'high', 'score', 'topic', 'preferred', 'topic', 'european', 'history', '29', 'wrong', 'choice', 'US', 'history', 'sample', 'preferred', 'essay', 'correlate', '40', 'external', 'criterion', 'score', 'compare', '34', 'essay', 'european', 'history', 'sample', 'preferred', 'essay', 'correlate', '52', 'external', 'criterion', 'compare', '44', 'topic']","['choice', 'essay', 'topic', 'Impact', 'performance', 'validity']",study assess ability history student choose essay topic high score second equally important question score choose topic highly related indicator proficiency history score unchosen topic overall US european history score standard deviation high preferred topic topic US history 32 student wrong choice 32 high score topic preferred topic european history 29 wrong choice US history sample preferred essay correlate 40 external criterion score compare 34 essay european history sample preferred essay correlate 52 external criterion compare 44 topic,choice essay topic Impact performance validity,0.03835912384232137,0.8468561504378012,0.03818785527810751,0.03790439133227732,0.03869247910949254,0.0,0.00014893573119893866,0.0,0.06389451230264376,0.020503523857944444
Gustafsson J.-E.,Measurement characteristics of the IEA reading literacy scales for 9- and 10-year-olds at country and individual levels,1997,34,"This article presents an approach for studying measurement characteristics of instruments designed for comparative studies of educational achievement. It presents a reanalysis of data from 22 countries and 73,818 9- and 10-year-old students from the IEA reading literacy study. Using 2-level confirmatory factor analysis procedures derived by Muthén (1990, 1994), it is demonstrated how the overall variance in performance on items from 3 domains - Documents, Narrative Prose, and Expository Text - may be decomposed into multiple latent sources of variance at country and individual levels. For the Documents items, a 1-factor model was fitted at both levels. For Narrative Prose and Expository Text items, 2 latent variables (a general dimension of reading performance and a factor capturing high versus low performance on passages towards the end of the tests) were fitted at both country and individual levels. For the Documents dimension, estimates of country scores were close to those of the original analysis; however, for performance in the Narrative Prose and Expository Text domains, differences between the original analysis and the reanalysis were noted for many countries.",Measurement characteristics of the IEA reading literacy scales for 9- and 10-year-olds at country and individual levels,"This article presents an approach for studying measurement characteristics of instruments designed for comparative studies of educational achievement. It presents a reanalysis of data from 22 countries and 73,818 9- and 10-year-old students from the IEA reading literacy study. Using 2-level confirmatory factor analysis procedures derived by Muthén (1990, 1994), it is demonstrated how the overall variance in performance on items from 3 domains - Documents, Narrative Prose, and Expository Text - may be decomposed into multiple latent sources of variance at country and individual levels. For the Documents items, a 1-factor model was fitted at both levels. For Narrative Prose and Expository Text items, 2 latent variables (a general dimension of reading performance and a factor capturing high versus low performance on passages towards the end of the tests) were fitted at both country and individual levels. For the Documents dimension, estimates of country scores were close to those of the original analysis; however, for performance in the Narrative Prose and Expository Text domains, differences between the original analysis and the reanalysis were noted for many countries.","['article', 'present', 'approach', 'study', 'characteristic', 'instrument', 'design', 'comparative', 'study', 'educational', 'achievement', 'present', 'reanalysis', 'datum', '22', 'country', '73818', '9', '10yearold', 'student', 'IEA', 'read', 'literacy', 'study', '2level', 'confirmatory', 'factor', 'analysis', 'procedure', 'derive', 'Muthén', '1990', '1994', 'demonstrate', 'overall', 'variance', 'performance', 'item', '3', 'domain', 'Documents', 'Narrative', 'Prose', 'Expository', 'Text', 'decompose', 'multiple', 'latent', 'source', 'variance', 'country', 'individual', 'level', 'Documents', 'item', '1factor', 'fit', 'level', 'Narrative', 'Prose', 'Expository', 'Text', 'item', '2', 'latent', 'variable', 'general', 'dimension', 'read', 'performance', 'factor', 'capture', 'high', 'versus', 'low', 'performance', 'passage', 'end', 'test', 'fit', 'country', 'individual', 'level', 'Documents', 'dimension', 'estimate', 'country', 'score', 'close', 'original', 'analysis', 'performance', 'Narrative', 'Prose', 'Expository', 'Text', 'domain', 'difference', 'original', 'analysis', 'reanalysis', 'note', 'country']","['characteristic', 'IEA', 'read', 'literacy', 'scale', '9', '10yearolds', 'country', 'individual', 'level']",article present approach study characteristic instrument design comparative study educational achievement present reanalysis datum 22 country 73818 9 10yearold student IEA read literacy study 2level confirmatory factor analysis procedure derive Muthén 1990 1994 demonstrate overall variance performance item 3 domain Documents Narrative Prose Expository Text decompose multiple latent source variance country individual level Documents item 1factor fit level Narrative Prose Expository Text item 2 latent variable general dimension read performance factor capture high versus low performance passage end test fit country individual level Documents dimension estimate country score close original analysis performance Narrative Prose Expository Text domain difference original analysis reanalysis note country,characteristic IEA read literacy scale 9 10yearolds country individual level,0.02692757438449992,0.026853595541617502,0.027155044038490946,0.02675154222936776,0.8923122438060239,0.002067063609307725,0.03106576919822712,0.007563233941272507,0.01659381350180839,0.04763501353335531
Livingston S.A.; Lewis C.,Estimating the Consistency and Accuracy of Classifications Based on Test Scores,1995,32,"This article presents a method for estimating the accuracy and consistency of classifications based on test scores. The scores can be produced by any scoring method, including a weighted composite. The estimates use data from a single form. The reliability of the score is used to estimate effective test length in terms of discrete items. The true‐score distribution is estimated by fitting a 4‐parameter beta model. The conditional distribution of scores on an alternate form, given the true score, is estimated from a binomial distribution based on the estimated effective test length. Agreement between classifications on alternate forms is estimated by assuming conditional independence, given the true score. Evaluation of the method showed estimates to be within 1 percentage point of the actual values in most cases. Estimates of decision accuracy and decision consistency statistics were only slightly affected by changes in specified minimum and maximum possible scores. Copyright © 1995, Wiley Blackwell. All rights reserved",Estimating the Consistency and Accuracy of Classifications Based on Test Scores,"This article presents a method for estimating the accuracy and consistency of classifications based on test scores. The scores can be produced by any scoring method, including a weighted composite. The estimates use data from a single form. The reliability of the score is used to estimate effective test length in terms of discrete items. The true‐score distribution is estimated by fitting a 4‐parameter beta model. The conditional distribution of scores on an alternate form, given the true score, is estimated from a binomial distribution based on the estimated effective test length. Agreement between classifications on alternate forms is estimated by assuming conditional independence, given the true score. Evaluation of the method showed estimates to be within 1 percentage point of the actual values in most cases. Estimates of decision accuracy and decision consistency statistics were only slightly affected by changes in specified minimum and maximum possible scores. Copyright © 1995, Wiley Blackwell. All rights reserved","['article', 'present', 'method', 'estimate', 'accuracy', 'consistency', 'classification', 'base', 'test', 'score', 'score', 'produce', 'scoring', 'method', 'include', 'weighted', 'composite', 'estimate', 'datum', 'single', 'form', 'reliability', 'score', 'estimate', 'effective', 'test', 'length', 'term', 'discrete', 'item', 'true‐score', 'distribution', 'estimate', 'fit', '4‐parameter', 'beta', 'conditional', 'distribution', 'score', 'alternate', 'form', 'true', 'score', 'estimate', 'binomial', 'distribution', 'base', 'estimate', 'effective', 'test', 'length', 'agreement', 'classification', 'alternate', 'form', 'estimate', 'assume', 'conditional', 'independence', 'true', 'score', 'Evaluation', 'method', 'estimate', '1', 'percentage', 'point', 'actual', 'value', 'case', 'estimate', 'decision', 'accuracy', 'decision', 'consistency', 'statistic', 'slightly', 'affect', 'change', 'specify', 'minimum', 'maximum', 'possible', 'score', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['estimate', 'Consistency', 'Accuracy', 'Classifications', 'base', 'Test', 'Scores']",article present method estimate accuracy consistency classification base test score score produce scoring method include weighted composite estimate datum single form reliability score estimate effective test length term discrete item true‐score distribution estimate fit 4‐parameter beta conditional distribution score alternate form true score estimate binomial distribution base estimate effective test length agreement classification alternate form estimate assume conditional independence true score Evaluation method estimate 1 percentage point actual value case estimate decision accuracy decision consistency statistic slightly affect change specify minimum maximum possible score Copyright © 1995 Wiley Blackwell right reserve,estimate Consistency Accuracy Classifications base Test Scores,0.027826311619284474,0.02737321805468182,0.027366642995367573,0.026951320478016204,0.89048250685265,0.05320172329161262,0.0,0.0,0.18528286397217725,0.0
Schnipke D.L.; Scrams D.J.,Modeling item response times with a two-state mixture model: A new method of measuring speededness,1997,34,"Speededness refers to the extent to which time limits affect examinees' test performance, and it is often measured by calculating the proportion of examinees who do not reach a certain percentage of test items. However, when tests are number-right scored (i.e., no points are subtracted for incorrect responses), examinees are likely to rapidly guess on items rather than leave them blank. Therefore, this traditional measure of speededness probably underestimates the true amount of speededness on such tests. A more accurate assessment of speededness should also reflect the tendency of examinees to rapidly guess on items as time expires. This rapid-guessing component of speededness can be estimated by modeling response times with a two-state mixture model, as demonstrated with data from a computer-administered reasoning tets. Taking into account the combined effect of unreached items and rapid guessing provides a more complete measure of speededness than has previously been available.",Modeling item response times with a two-state mixture model: A new method of measuring speededness,"Speededness refers to the extent to which time limits affect examinees' test performance, and it is often measured by calculating the proportion of examinees who do not reach a certain percentage of test items. However, when tests are number-right scored (i.e., no points are subtracted for incorrect responses), examinees are likely to rapidly guess on items rather than leave them blank. Therefore, this traditional measure of speededness probably underestimates the true amount of speededness on such tests. A more accurate assessment of speededness should also reflect the tendency of examinees to rapidly guess on items as time expires. This rapid-guessing component of speededness can be estimated by modeling response times with a two-state mixture model, as demonstrated with data from a computer-administered reasoning tets. Taking into account the combined effect of unreached items and rapid guessing provides a more complete measure of speededness than has previously been available.","['speededness', 'refer', 'extent', 'time', 'limit', 'affect', 'examine', 'test', 'performance', 'measure', 'calculate', 'proportion', 'examinee', 'reach', 'certain', 'percentage', 'test', 'item', 'test', 'numberright', 'score', 'ie', 'point', 'subtract', 'incorrect', 'response', 'examinee', 'likely', 'rapidly', 'guess', 'item', 'leave', 'blank', 'traditional', 'measure', 'speededness', 'probably', 'underestimate', 'true', 'speededness', 'test', 'accurate', 'assessment', 'speededness', 'reflect', 'tendency', 'examine', 'rapidly', 'guess', 'item', 'time', 'expire', 'rapidguesse', 'component', 'speededness', 'estimate', 'response', 'time', 'twostate', 'mixture', 'demonstrate', 'datum', 'computeradministere', 'reasoning', 'tet', 'account', 'combine', 'effect', 'unreached', 'item', 'rapid', 'guessing', 'provide', 'complete', 'measure', 'speededness', 'previously', 'available']","['item', 'response', 'time', 'twostate', 'mixture', 'new', 'method', 'measure', 'speededness']",speededness refer extent time limit affect examine test performance measure calculate proportion examinee reach certain percentage test item test numberright score ie point subtract incorrect response examinee likely rapidly guess item leave blank traditional measure speededness probably underestimate true speededness test accurate assessment speededness reflect tendency examine rapidly guess item time expire rapidguesse component speededness estimate response time twostate mixture demonstrate datum computeradministere reasoning tet account combine effect unreached item rapid guessing provide complete measure speededness previously available,item response time twostate mixture new method measure speededness,0.8887108481034666,0.02805628863716055,0.027804477558368058,0.027450266411045726,0.027978119289959115,0.010592691190574105,0.07789704486695422,0.0,0.006456102666972558,0.0024389893278038725
Zwick R.; Thayer D.T.; Lewis C.,An empirical Bayes approach to Mantel-Haenszel DIF analysis,1999,36,"We developed an empirical Bayes (EB) enhancement to Mantel-Haenszel (MH) DIF analysis in which we assume that the MH statistics are normally distributed and that the prior distribution of underlying DIF parameters is also normal. We use the posterior distribution of DIF parameters to make inferences about the item's true DIF status and the posterior predictive distribution to predict the item's future observed status. DIF status is expressed in terms of the probabilities associated with each of the five DIF levels defined by the ETS classification system: C-, B-, A, B+, and C+. The EB methods yield more stable DIF estimates than do conventional methods, especially in small samples, which is advantageous in computer-adaptive testing. The EB approach may also convey information about DIF stability in a more useful way by representing the state of knowledge about an item's DIF status as probabilistic.",An empirical Bayes approach to Mantel-Haenszel DIF analysis,"We developed an empirical Bayes (EB) enhancement to Mantel-Haenszel (MH) DIF analysis in which we assume that the MH statistics are normally distributed and that the prior distribution of underlying DIF parameters is also normal. We use the posterior distribution of DIF parameters to make inferences about the item's true DIF status and the posterior predictive distribution to predict the item's future observed status. DIF status is expressed in terms of the probabilities associated with each of the five DIF levels defined by the ETS classification system: C-, B-, A, B+, and C+. The EB methods yield more stable DIF estimates than do conventional methods, especially in small samples, which is advantageous in computer-adaptive testing. The EB approach may also convey information about DIF stability in a more useful way by representing the state of knowledge about an item's DIF status as probabilistic.","['develop', 'empirical', 'Bayes', 'eb', 'enhancement', 'MantelHaenszel', 'MH', 'DIF', 'analysis', 'assume', 'MH', 'statistic', 'normally', 'distribute', 'prior', 'distribution', 'underlie', 'dif', 'parameter', 'normal', 'posterior', 'distribution', 'dif', 'parameter', 'inference', 'item', 'true', 'DIF', 'status', 'posterior', 'predictive', 'distribution', 'predict', 'item', 'future', 'observed', 'status', 'DIF', 'status', 'express', 'term', 'probability', 'associate', 'dif', 'level', 'define', 'ETS', 'classification', 'system', 'c', 'B', 'A', 'b', 'c', 'EB', 'method', 'yield', 'stable', 'dif', 'estimate', 'conventional', 'method', 'especially', 'small', 'sample', 'advantageous', 'computeradaptive', 'testing', 'eb', 'approach', 'convey', 'information', 'dif', 'stability', 'useful', 'way', 'represent', 'state', 'knowledge', 'item', 'DIF', 'status', 'probabilistic']","['empirical', 'Bayes', 'approach', 'MantelHaenszel', 'DIF', 'analysis']",develop empirical Bayes eb enhancement MantelHaenszel MH DIF analysis assume MH statistic normally distribute prior distribution underlie dif parameter normal posterior distribution dif parameter inference item true DIF status posterior predictive distribution predict item future observed status DIF status express term probability associate dif level define ETS classification system c B A b c EB method yield stable dif estimate conventional method especially small sample advantageous computeradaptive testing eb approach convey information dif stability useful way represent state knowledge item DIF status probabilistic,empirical Bayes approach MantelHaenszel DIF analysis,0.028495713275016456,0.028550936561681554,0.028719650179339515,0.02827967236404158,0.8859540276199209,0.0020739970724954118,0.0,0.1505968047834252,0.008840382949152407,0.0
Clauser B.E.; Nungester R.J.; Mazor K.; Ripkey D.,A comparison of alternative matching strategies for DIF detection in tests that are multidimensional,1996,33,"Most currently accepted approaches for identifying differentially functioning test items compare performance across groups after first matching examinees on the ability of interest. The typical basis for this matching is the total test score. Previous research indicates that when the test is not approximately unidimensional, matching using the total test score may result in an inflated Type I error rate. This study compares the results of differential item functioning (DIF) analysis with matching based on the total test score, matching based on subtest scores, or multivariate matching using multiple subtest scores. Analysis of both actual and simulated data indicate that for the dimensionally complex test examined in this study, using the total test score as the matching criterion is inappropriate. The results suggest that matching on multiple subtest scores simultaneously may be superior to using either the total test score or individual relevant subtest scores.",A comparison of alternative matching strategies for DIF detection in tests that are multidimensional,"Most currently accepted approaches for identifying differentially functioning test items compare performance across groups after first matching examinees on the ability of interest. The typical basis for this matching is the total test score. Previous research indicates that when the test is not approximately unidimensional, matching using the total test score may result in an inflated Type I error rate. This study compares the results of differential item functioning (DIF) analysis with matching based on the total test score, matching based on subtest scores, or multivariate matching using multiple subtest scores. Analysis of both actual and simulated data indicate that for the dimensionally complex test examined in this study, using the total test score as the matching criterion is inappropriate. The results suggest that matching on multiple subtest scores simultaneously may be superior to using either the total test score or individual relevant subtest scores.","['currently', 'accept', 'approach', 'identify', 'differentially', 'function', 'test', 'item', 'compare', 'performance', 'group', 'matching', 'examinee', 'ability', 'interest', 'typical', 'basis', 'matching', 'total', 'test', 'score', 'previous', 'research', 'indicate', 'test', 'approximately', 'unidimensional', 'matching', 'total', 'test', 'score', 'result', 'inflated', 'Type', 'I', 'error', 'rate', 'study', 'compare', 'result', 'differential', 'item', 'function', 'DIF', 'analysis', 'matching', 'base', 'total', 'test', 'score', 'match', 'base', 'subtest', 'score', 'multivariate', 'matching', 'multiple', 'subt', 'score', 'analysis', 'actual', 'simulated', 'datum', 'indicate', 'dimensionally', 'complex', 'test', 'examine', 'study', 'total', 'test', 'score', 'matching', 'criterion', 'inappropriate', 'result', 'suggest', 'match', 'multiple', 'subt', 'score', 'simultaneously', 'superior', 'total', 'test', 'score', 'individual', 'relevant', 'subt', 'score']","['comparison', 'alternative', 'matching', 'strategy', 'dif', 'detection', 'test', 'multidimensional']",currently accept approach identify differentially function test item compare performance group matching examinee ability interest typical basis matching total test score previous research indicate test approximately unidimensional matching total test score result inflated Type I error rate study compare result differential item function DIF analysis matching base total test score match base subtest score multivariate matching multiple subt score analysis actual simulated datum indicate dimensionally complex test examine study total test score matching criterion inappropriate result suggest match multiple subt score simultaneously superior total test score individual relevant subt score,comparison alternative matching strategy dif detection test multidimensional,0.8723106111798038,0.0316424141525548,0.03226265206164488,0.03137337873856086,0.03241094386743559,0.0,0.04802991888675462,0.0464864004968088,0.12104915094192709,0.0
Bennett R.E.; Rock D.A.,"Generalizability, Validity, and Examinee Perceptions of a Computer‐Delivered Formulating‐Hypotheses Test",1995,32,"The Formulating‐Hypotheses (F‐H) item presents a situation and asks examinees to generate as many explanations for it as possible. This study examined the generalizability, validity, and examinee perceptions of a computer‐delivered version of the task. Eight F‐H questions were administered to 192 graduate students. Half of the items restricted examinees to 7 words per explanation, and half allowed up to 15 words. Generalizability results showed high interrater agreement, with tests of between 2 and 4 items scored by one judge achieving coefficients in the .80s. Construct validity analyses found that F‐H was only marginally related to the GRE General Test, and more strongly related than the General Test to a measure of ideational fluency. Different response limits tapped somewhat different abilities, with the 15‐word constraint appearing more useful for graduate assessment. These items added significantly to conventional measures in explaining school performance and creative expression. Copyright © 1995, Wiley Blackwell. All rights reserved","Generalizability, Validity, and Examinee Perceptions of a Computer‐Delivered Formulating‐Hypotheses Test","The Formulating‐Hypotheses (F‐H) item presents a situation and asks examinees to generate as many explanations for it as possible. This study examined the generalizability, validity, and examinee perceptions of a computer‐delivered version of the task. Eight F‐H questions were administered to 192 graduate students. Half of the items restricted examinees to 7 words per explanation, and half allowed up to 15 words. Generalizability results showed high interrater agreement, with tests of between 2 and 4 items scored by one judge achieving coefficients in the .80s. Construct validity analyses found that F‐H was only marginally related to the GRE General Test, and more strongly related than the General Test to a measure of ideational fluency. Different response limits tapped somewhat different abilities, with the 15‐word constraint appearing more useful for graduate assessment. These items added significantly to conventional measures in explaining school performance and creative expression. Copyright © 1995, Wiley Blackwell. All rights reserved","['Formulating‐Hypotheses', 'f‐h', 'item', 'present', 'situation', 'ask', 'examine', 'generate', 'explanation', 'possible', 'study', 'examine', 'generalizability', 'validity', 'examinee', 'perception', 'computer‐delivered', 'version', 'task', 'f‐h', 'question', 'administer', '192', 'graduate', 'student', 'half', 'item', 'restrict', 'examine', '7', 'word', 'explanation', 'half', 'allow', '15', 'word', 'Generalizability', 'result', 'high', 'interrater', 'agreement', 'test', '2', '4', 'item', 'score', 'judge', 'achieve', 'coefficient', '80', 'construct', 'validity', 'analysis', 'find', 'f‐h', 'marginally', 'related', 'GRE', 'General', 'Test', 'strongly', 'related', 'General', 'Test', 'measure', 'ideational', 'fluency', 'different', 'response', 'limit', 'tap', 'somewhat', 'different', 'ability', '15‐word', 'constraint', 'appear', 'useful', 'graduate', 'assessment', 'item', 'add', 'significantly', 'conventional', 'measure', 'explain', 'school', 'performance', 'creative', 'expression', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Generalizability', 'Validity', 'Examinee', 'Perceptions', 'Computer‐Delivered', 'Formulating‐Hypotheses', 'Test']",Formulating‐Hypotheses f‐h item present situation ask examine generate explanation possible study examine generalizability validity examinee perception computer‐delivered version task f‐h question administer 192 graduate student half item restrict examine 7 word explanation half allow 15 word Generalizability result high interrater agreement test 2 4 item score judge achieve coefficient 80 construct validity analysis find f‐h marginally related GRE General Test strongly related General Test measure ideational fluency different response limit tap somewhat different ability 15‐word constraint appear useful graduate assessment item add significantly conventional measure explain school performance creative expression Copyright © 1995 Wiley Blackwell right reserve,Generalizability Validity Examinee Perceptions Computer‐Delivered Formulating‐Hypotheses Test,0.02331684149520065,0.023452870458644533,0.023102922654175177,0.022860796266013537,0.907266569125966,0.00121376646040087,0.08420423823562623,0.0,0.0,0.054010729858948364
Bridgeman B.; Harvey A.; Braswell J.,Effects of Calculator Use on Scores on a Test of Mathematical Reasoning,1995,32,"A sample of college‐bound juniors from 275 high schools took a test consisting of 70 math questions from the SAT. A random half of the sample was allowed to use calculators on the test. Both genders and three ethnic groups (White, African American, and Asian American) benefitted about equally from being allowed to use calculators; Latinos benefitted slightly more than the other groups. Students who routinely used calculators on classroom mathematics tests were relatively advantaged on the calculator test. Test speededness was about the same whether or not students used calculators. Calculator effects on individual items ranged from positive through neutral to negative and could either increase or decrease the validity of an item as a measure of mathematical reasoning skills. Calculator effects could be either present or absent in both difficult and easy items Copyright © 1995, Wiley Blackwell. All rights reserved",Effects of Calculator Use on Scores on a Test of Mathematical Reasoning,"A sample of college‐bound juniors from 275 high schools took a test consisting of 70 math questions from the SAT. A random half of the sample was allowed to use calculators on the test. Both genders and three ethnic groups (White, African American, and Asian American) benefitted about equally from being allowed to use calculators; Latinos benefitted slightly more than the other groups. Students who routinely used calculators on classroom mathematics tests were relatively advantaged on the calculator test. Test speededness was about the same whether or not students used calculators. Calculator effects on individual items ranged from positive through neutral to negative and could either increase or decrease the validity of an item as a measure of mathematical reasoning skills. Calculator effects could be either present or absent in both difficult and easy items Copyright © 1995, Wiley Blackwell. All rights reserved","['sample', 'college‐bound', 'junior', '275', 'high', 'school', 'test', 'consist', '70', 'math', 'question', 'SAT', 'A', 'random', 'half', 'sample', 'allow', 'calculator', 'test', 'gender', 'ethnic', 'group', 'white', 'african', 'american', 'asian', 'American', 'benefit', 'equally', 'allow', 'calculator', 'Latinos', 'benefit', 'slightly', 'group', 'student', 'routinely', 'calculator', 'classroom', 'mathematic', 'test', 'relatively', 'advantaged', 'calculator', 'test', 'Test', 'speededness', 'student', 'calculator', 'Calculator', 'effect', 'individual', 'item', 'range', 'positive', 'neutral', 'negative', 'increase', 'decrease', 'validity', 'item', 'measure', 'mathematical', 'reasoning', 'skill', 'calculator', 'effect', 'present', 'absent', 'difficult', 'easy', 'item', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['effect', 'Calculator', 'Use', 'score', 'test', 'Mathematical', 'Reasoning']",sample college‐bound junior 275 high school test consist 70 math question SAT A random half sample allow calculator test gender ethnic group white african american asian American benefit equally allow calculator Latinos benefit slightly group student routinely calculator classroom mathematic test relatively advantaged calculator test Test speededness student calculator Calculator effect individual item range positive neutral negative increase decrease validity item measure mathematical reasoning skill calculator effect present absent difficult easy item Copyright © 1995 Wiley Blackwell right reserve,effect Calculator Use score test Mathematical Reasoning,0.031337662193769045,0.031538995976093234,0.03130397579839181,0.030984002274343406,0.8748353637574025,0.014404416452747452,0.04795917076125924,0.0,0.0,0.02061758504045492
Williams V.S.L.; Pommerich M.; Thissen D.,A comparison of developmental scales based on thurstone methods and item response theory,1998,35,"A developmental scale for the North Carolina End-of-Grade Mathematics Tests was created using a subset of identical test forms administered to adjacent grade levels. Thurstone scaling and item response theory (IRT) techniques were employed to analyze the changes in grade distributions across these linked forms. Three variations of Thurstone scaling were examined, one based on Thurstone's 1925 procedure and two based on Thurstone's 1938 procedure. The IRT scaling was implemented using both BIMAIN and MULTILOG. All methods indicated that average mathematics performance improved from Grade 3 to Grade 8, with similar results for the two IRT analyses and one version of Thurstone's 1938 method. The standard deviations of the IRT scales did not show a consistent pattern across grades, whereas those produced by Thurstone's 1925 procedure generally decreased; one version of the 1938 method exhibited slightly increasing variation with increasing grade level, while the other version displayed inconsistent trends. This research was supported in part by the North Carolina Department of Public Instruction and a grant from the National Science Foundation (DMS-9208758). Correspondence concerning this paper should be addressed to the first author.",A comparison of developmental scales based on thurstone methods and item response theory,"A developmental scale for the North Carolina End-of-Grade Mathematics Tests was created using a subset of identical test forms administered to adjacent grade levels. Thurstone scaling and item response theory (IRT) techniques were employed to analyze the changes in grade distributions across these linked forms. Three variations of Thurstone scaling were examined, one based on Thurstone's 1925 procedure and two based on Thurstone's 1938 procedure. The IRT scaling was implemented using both BIMAIN and MULTILOG. All methods indicated that average mathematics performance improved from Grade 3 to Grade 8, with similar results for the two IRT analyses and one version of Thurstone's 1938 method. The standard deviations of the IRT scales did not show a consistent pattern across grades, whereas those produced by Thurstone's 1925 procedure generally decreased; one version of the 1938 method exhibited slightly increasing variation with increasing grade level, while the other version displayed inconsistent trends. This research was supported in part by the North Carolina Department of Public Instruction and a grant from the National Science Foundation (DMS-9208758). Correspondence concerning this paper should be addressed to the first author.","['developmental', 'scale', 'North', 'Carolina', 'EndofGrade', 'Mathematics', 'Tests', 'create', 'subset', 'identical', 'test', 'form', 'administer', 'adjacent', 'grade', 'level', 'Thurstone', 'scaling', 'item', 'response', 'theory', 'IRT', 'technique', 'employ', 'analyze', 'change', 'grade', 'distribution', 'link', 'form', 'variation', 'Thurstone', 'scaling', 'examine', 'base', 'Thurstones', '1925', 'procedure', 'base', 'Thurstones', '1938', 'procedure', 'IRT', 'scaling', 'implement', 'BIMAIN', 'multilog', 'method', 'indicate', 'average', 'mathematic', 'performance', 'improve', 'Grade', '3', 'Grade', '8', 'similar', 'result', 'IRT', 'analysis', 'version', 'Thurstones', '1938', 'method', 'standard', 'deviation', 'IRT', 'scale', 'consistent', 'pattern', 'grade', 'produce', 'Thurstones', '1925', 'procedure', 'generally', 'decrease', 'version', '1938', 'method', 'exhibit', 'slightly', 'increase', 'variation', 'increase', 'grade', 'level', 'version', 'display', 'inconsistent', 'trend', 'research', 'support', 'North', 'Carolina', 'Department', 'Public', 'Instruction', 'grant', 'National', 'Science', 'Foundation', 'DMS9208758', 'Correspondence', 'concern', 'paper', 'address', 'author']","['comparison', 'developmental', 'scale', 'base', 'thurstone', 'method', 'item', 'response', 'theory']",developmental scale North Carolina EndofGrade Mathematics Tests create subset identical test form administer adjacent grade level Thurstone scaling item response theory IRT technique employ analyze change grade distribution link form variation Thurstone scaling examine base Thurstones 1925 procedure base Thurstones 1938 procedure IRT scaling implement BIMAIN multilog method indicate average mathematic performance improve Grade 3 Grade 8 similar result IRT analysis version Thurstones 1938 method standard deviation IRT scale consistent pattern grade produce Thurstones 1925 procedure generally decrease version 1938 method exhibit slightly increase variation increase grade level version display inconsistent trend research support North Carolina Department Public Instruction grant National Science Foundation DMS9208758 Correspondence concern paper address author,comparison developmental scale base thurstone method item response theory,0.026097366535316153,0.026874407302115048,0.026236656335562044,0.2915833285193412,0.6292082413076655,0.009330330733468659,0.0012501755102917748,0.0014974123122173367,0.07004605671220943,0.0634269236432477
Parshall C.G.; Miller T.R.,Exact Versus Asymptotic Mantel‐Haenszel DIF Statistics: A Comparison of Performance Under Small‐Sample Conditions,1995,32,"This study evaluated exact testing (Agresti, 1992) as a method for conducting Mantel‐Haenszel DIF analyses (Holland & Thayer, 1988) with relatively small samples. Sample‐size restrictions limit the standard asymptotic Mantel‐Haenszel for many practical applications; however, new developments in computing technology have made exact testing procedures feasible. The highly discrete distributions that are likely to occur in small‐sample DIF analyses could yield very different results for asymptotic versus exact methods. It is therefore important to determine under controlled conditions the extent to which the exact approach is effective in correctly identifying DIF. A series of computer simulations were conducted in which 3 levels of induced bias (IRT b‐parameter differences between groups of .25, .50, and .75) and 4 sample sizes (reference group= 500, focal group = 25, 50, 100, and 200) were investigated. Power comparisons at .01 and .05 alpha levels were carried out between the exact testing procedure and the conventional Mantel‐Haenszel Copyright © 1995, Wiley Blackwell. All rights reserved",Exact Versus Asymptotic Mantel‐Haenszel DIF Statistics: A Comparison of Performance Under Small‐Sample Conditions,"This study evaluated exact testing (Agresti, 1992) as a method for conducting Mantel‐Haenszel DIF analyses (Holland & Thayer, 1988) with relatively small samples. Sample‐size restrictions limit the standard asymptotic Mantel‐Haenszel for many practical applications; however, new developments in computing technology have made exact testing procedures feasible. The highly discrete distributions that are likely to occur in small‐sample DIF analyses could yield very different results for asymptotic versus exact methods. It is therefore important to determine under controlled conditions the extent to which the exact approach is effective in correctly identifying DIF. A series of computer simulations were conducted in which 3 levels of induced bias (IRT b‐parameter differences between groups of .25, .50, and .75) and 4 sample sizes (reference group= 500, focal group = 25, 50, 100, and 200) were investigated. Power comparisons at .01 and .05 alpha levels were carried out between the exact testing procedure and the conventional Mantel‐Haenszel Copyright © 1995, Wiley Blackwell. All rights reserved","['study', 'evaluate', 'exact', 'testing', 'Agresti', '1992', 'method', 'conduct', 'Mantel‐Haenszel', 'DIF', 'analyse', 'Holland', 'Thayer', '1988', 'relatively', 'small', 'sample', 'sample‐size', 'restriction', 'limit', 'standard', 'asymptotic', 'Mantel‐Haenszel', 'practical', 'application', 'new', 'development', 'compute', 'technology', 'exact', 'testing', 'procedure', 'feasible', 'highly', 'discrete', 'distribution', 'likely', 'occur', 'small‐sample', 'DIF', 'analysis', 'yield', 'different', 'result', 'asymptotic', 'versus', 'exact', 'method', 'important', 'determine', 'control', 'condition', 'extent', 'exact', 'approach', 'effective', 'correctly', 'identify', 'DIF', 'series', 'computer', 'simulation', 'conduct', '3', 'level', 'induce', 'bias', 'IRT', 'b‐parameter', 'difference', 'group', '25', '50', '75', '4', 'sample', 'size', 'reference', 'group', '500', 'focal', 'group', '25', '50', '100', '200', 'investigate', 'power', 'comparison', '01', '05', 'alpha', 'level', 'carry', 'exact', 'testing', 'procedure', 'conventional', 'Mantel‐Haenszel', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['exact', 'Versus', 'Asymptotic', 'Mantel‐Haenszel', 'DIF', 'Statistics', 'Comparison', 'Performance', 'Small‐Sample', 'condition']",study evaluate exact testing Agresti 1992 method conduct Mantel‐Haenszel DIF analyse Holland Thayer 1988 relatively small sample sample‐size restriction limit standard asymptotic Mantel‐Haenszel practical application new development compute technology exact testing procedure feasible highly discrete distribution likely occur small‐sample DIF analysis yield different result asymptotic versus exact method important determine control condition extent exact approach effective correctly identify DIF series computer simulation conduct 3 level induce bias IRT b‐parameter difference group 25 50 75 4 sample size reference group 500 focal group 25 50 100 200 investigate power comparison 01 05 alpha level carry exact testing procedure conventional Mantel‐Haenszel Copyright © 1995 Wiley Blackwell right reserve,exact Versus Asymptotic Mantel‐Haenszel DIF Statistics Comparison Performance Small‐Sample condition,0.02373837770577098,0.9055255655732924,0.023499298240449135,0.023065625989119747,0.024171132491367736,0.05270974969755323,0.0,0.06469033015042751,0.0,0.0023719226869386148
Wainer H.; Hambleton R.K.; Meara K.,Alternative displays for communicating naep results: A redesign and validity study,1999,36,"Five displays, chosen from the NAEP 1994 Reading: A First Look, were redesigned. The redesign was informed by the principles developed and enunciated in Wainer's 1997 popular text Visual Revelations. After the redesign was completed a survey of educational policymakers was done in which substantive questions were asked about the content of the various displays. Each redesign was paired with the original and were assigned randomly to one of two survey forms. We found that, on average, the redesigns yielded both more accurate and faster answers to the questions asked. The more difficult the question the greater the disparity between the original format and the redesigned one.",Alternative displays for communicating naep results: A redesign and validity study,"Five displays, chosen from the NAEP 1994 Reading: A First Look, were redesigned. The redesign was informed by the principles developed and enunciated in Wainer's 1997 popular text Visual Revelations. After the redesign was completed a survey of educational policymakers was done in which substantive questions were asked about the content of the various displays. Each redesign was paired with the original and were assigned randomly to one of two survey forms. We found that, on average, the redesigns yielded both more accurate and faster answers to the questions asked. The more difficult the question the greater the disparity between the original format and the redesigned one.","['display', 'choose', 'NAEP', '1994', 'read', 'First', 'Look', 'redesign', 'redesign', 'inform', 'principle', 'develop', 'enunciate', 'Wainers', '1997', 'popular', 'text', 'Visual', 'revelation', 'redesign', 'complete', 'survey', 'educational', 'policymaker', 'substantive', 'question', 'ask', 'content', 'display', 'redesign', 'pair', 'original', 'assign', 'randomly', 'survey', 'form', 'find', 'average', 'redesign', 'yield', 'accurate', 'fast', 'answer', 'question', 'ask', 'difficult', 'question', 'great', 'disparity', 'original', 'format', 'redesign']","['alternative', 'display', 'communicate', 'naep', 'result', 'redesign', 'validity', 'study']",display choose NAEP 1994 read First Look redesign redesign inform principle develop enunciate Wainers 1997 popular text Visual revelation redesign complete survey educational policymaker substantive question ask content display redesign pair original assign randomly survey form find average redesign yield accurate fast answer question ask difficult question great disparity original format redesign,alternative display communicate naep result redesign validity study,0.0351737434547042,0.03583930020479584,0.03527527243225656,0.03493983702287899,0.8587718468853645,0.0,0.03283756280015577,0.0,0.0,0.0038798917567856605
Tate R.L.,A cautionary note on irt-based linking of tests with polytomous items,1999,36,Published discussion of the year-to-year linking of tests comprised of polytomous items appear to suggest that the linking logic traditionally used for multiple-choice item is also appropriate for polytomous items. It is argued and illustrated that a modification of the traditional linking is necessary when tests consist of constructed-response items judged by raters and there is a possibility of year-to-year variation in the rating discrimination and severity.,A cautionary note on irt-based linking of tests with polytomous items,Published discussion of the year-to-year linking of tests comprised of polytomous items appear to suggest that the linking logic traditionally used for multiple-choice item is also appropriate for polytomous items. It is argued and illustrated that a modification of the traditional linking is necessary when tests consist of constructed-response items judged by raters and there is a possibility of year-to-year variation in the rating discrimination and severity.,"['publish', 'discussion', 'yeartoyear', 'link', 'test', 'comprise', 'polytomous', 'item', 'appear', 'suggest', 'link', 'logic', 'traditionally', 'multiplechoice', 'item', 'appropriate', 'polytomous', 'item', 'argue', 'illustrate', 'modification', 'traditional', 'linking', 'necessary', 'test', 'consist', 'constructedresponse', 'item', 'judge', 'rater', 'possibility', 'yeartoyear', 'variation', 'rating', 'discrimination', 'severity']","['cautionary', 'note', 'irtbased', 'linking', 'test', 'polytomous', 'item']",publish discussion yeartoyear link test comprise polytomous item appear suggest link logic traditionally multiplechoice item appropriate polytomous item argue illustrate modification traditional linking necessary test consist constructedresponse item judge rater possibility yeartoyear variation rating discrimination severity,cautionary note irtbased linking test polytomous item,0.035003076321596514,0.8594876073843329,0.03478215778520363,0.034372558702675415,0.03635459980619147,0.002045431340862628,0.05256009979773973,0.013080617950798668,0.0,0.01038117507166859
Sawyer R.,Decision theory models for validating course placement tests,1996,33,"Most American postsecondary institutions have course placement systems for their first-year students. Placement systems typically consist of an assessment component (to estimate students' probability of success in standard first-year courses) and an instructional component (in which academically underprepared students are taught the skills and knowledge they need to succeed in the standard courses). Validity issues related to these functions are discussed in the context of decision theory, and methods are proposed for determining appropriate cutoff scores on placement tests.",Decision theory models for validating course placement tests,"Most American postsecondary institutions have course placement systems for their first-year students. Placement systems typically consist of an assessment component (to estimate students' probability of success in standard first-year courses) and an instructional component (in which academically underprepared students are taught the skills and knowledge they need to succeed in the standard courses). Validity issues related to these functions are discussed in the context of decision theory, and methods are proposed for determining appropriate cutoff scores on placement tests.","['Most', 'american', 'postsecondary', 'institution', 'course', 'placement', 'system', 'firstyear', 'student', 'Placement', 'system', 'typically', 'consist', 'assessment', 'component', 'estimate', 'student', 'probability', 'success', 'standard', 'firstyear', 'course', 'instructional', 'component', 'academically', 'underprepared', 'student', 'teach', 'skill', 'knowledge', 'need', 'succeed', 'standard', 'course', 'Validity', 'issue', 'relate', 'function', 'discuss', 'context', 'decision', 'theory', 'method', 'propose', 'determine', 'appropriate', 'cutoff', 'score', 'placement', 'test']","['decision', 'theory', 'validate', 'course', 'placement', 'test']",Most american postsecondary institution course placement system firstyear student Placement system typically consist assessment component estimate student probability success standard firstyear course instructional component academically underprepared student teach skill knowledge need succeed standard course Validity issue relate function discuss context decision theory method propose determine appropriate cutoff score placement test,decision theory validate course placement test,0.03127076151447493,0.031285542943036974,0.0313954810072781,0.4030598805366799,0.5029883339985302,0.0,0.008719286308634047,0.0,0.054261222878454134,0.055026745005969165
Feldt L.S.; Qualls A.L.,Estimation of measurement error variance at specific score levels,1996,33,"An improved method is derived for estimating consitional measurement error variances, that is, error variances specific to individual examinees or specific to each point on the raw score scale of the test. The method involves partitioning the test into short parallel parts, computing for each examinne the unbiased iased estimate of the variance of part-test scores, and multiplying this variance by a constant dictated by classical test theory. Empirical data are used to corroborate the principal theoretical deductions.",Estimation of measurement error variance at specific score levels,"An improved method is derived for estimating consitional measurement error variances, that is, error variances specific to individual examinees or specific to each point on the raw score scale of the test. The method involves partitioning the test into short parallel parts, computing for each examinne the unbiased iased estimate of the variance of part-test scores, and multiplying this variance by a constant dictated by classical test theory. Empirical data are used to corroborate the principal theoretical deductions.","['improved', 'method', 'derive', 'estimate', 'consitional', 'error', 'variance', 'error', 'variance', 'specific', 'individual', 'examinee', 'specific', 'point', 'raw', 'score', 'scale', 'test', 'method', 'involve', 'partition', 'test', 'short', 'parallel', 'compute', 'examinne', 'unbiased', 'iased', 'estimate', 'variance', 'parttest', 'score', 'multiply', 'variance', 'constant', 'dictate', 'classical', 'test', 'theory', 'empirical', 'datum', 'corroborate', 'principal', 'theoretical', 'deduction']","['estimation', 'error', 'variance', 'specific', 'score', 'level']",improved method derive estimate consitional error variance error variance specific individual examinee specific point raw score scale test method involve partition test short parallel compute examinne unbiased iased estimate variance parttest score multiply variance constant dictate classical test theory empirical datum corroborate principal theoretical deduction,estimation error variance specific score level,0.8738895914438597,0.03138058700196003,0.03154250090925257,0.03113361109330477,0.03205370955162301,0.0,0.007713113466028312,0.0,0.1746639262616546,0.0
Hoskens M.; Boeck P.D.,Componential IRT Models for Polytomous Items,1995,32,"Componential IRT models for polytomous items are of particular interest in two contexts: Componential research and test development. We assume that there are basic components, such as processes and knowledge structures, involved in solving cognitive tasks. In Componential research, the subtask paradigm may be used to isolate such components in subtasks. In test development, items may be composed such that their response alternatives correspond with specific combinations of such components. In both cases the data may be modeled as polytomous items. With Bock's (1972) nominal model as a general framework, transformation matrices can be used to constrain the parameters of the response categories so as to reflect the Componential design of the response categories. In this way, both main effects and interaction effects of components can be studied. An application to a spelling task demonstrates this approach Copyright © 1995, Wiley Blackwell. All rights reserved",,"Componential IRT models for polytomous items are of particular interest in two contexts: Componential research and test development. We assume that there are basic components, such as processes and knowledge structures, involved in solving cognitive tasks. In Componential research, the subtask paradigm may be used to isolate such components in subtasks. In test development, items may be composed such that their response alternatives correspond with specific combinations of such components. In both cases the data may be modeled as polytomous items. With Bock's (1972) nominal model as a general framework, transformation matrices can be used to constrain the parameters of the response categories so as to reflect the Componential design of the response categories. In this way, both main effects and interaction effects of components can be studied. An application to a spelling task demonstrates this approach Copyright © 1995, Wiley Blackwell. All rights reserved","['Componential', 'IRT', 'polytomous', 'item', 'particular', 'interest', 'context', 'componential', 'research', 'test', 'development', 'assume', 'basic', 'component', 'process', 'knowledge', 'structure', 'involve', 'solve', 'cognitive', 'task', 'componential', 'research', 'subtask', 'paradigm', 'isolate', 'component', 'subtask', 'test', 'development', 'item', 'compose', 'response', 'alternative', 'correspond', 'specific', 'combination', 'component', 'case', 'datum', 'polytomous', 'item', 'Bocks', '1972', 'nominal', 'general', 'framework', 'transformation', 'matrix', 'constrain', 'parameter', 'response', 'category', 'reflect', 'componential', 'design', 'response', 'category', 'way', 'main', 'effect', 'interaction', 'effect', 'component', 'study', 'application', 'spelling', 'task', 'demonstrate', 'approach', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']",,Componential IRT polytomous item particular interest context componential research test development assume basic component process knowledge structure involve solve cognitive task componential research subtask paradigm isolate component subtask test development item compose response alternative correspond specific combination component case datum polytomous item Bocks 1972 nominal general framework transformation matrix constrain parameter response category reflect componential design response category way main effect interaction effect component study application spelling task demonstrate approach Copyright © 1995 Wiley Blackwell right reserve,,0.8864844035271346,0.0284246054021134,0.02863366497599764,0.027735284763015466,0.028722041331738883,0.0055874845079958825,0.06672636136434176,0.00436800601530399,0.0,0.027753615504010815
Kolen M.J.; Zeng L.; Hanson B.A.,Conditional standard errors of measurement for scale scores using IRT,1996,33,"An IRT method for estimating conditional standard errors of measurement of scale scores is presented, where scale scores are nonlinear transformations of number-correct scores. The standard errors account for measurement error that is introduced due to rounding scale scores to integers. Procedures for estimating the average conditional standard error of measurement for scale scores and reliability of scale scores are also described. An illustration of the use of the methodology is presented, and the results from the IRT method are compared to the results from a previously developed method that is based on strong true-score theory.",Conditional standard errors of measurement for scale scores using IRT,"An IRT method for estimating conditional standard errors of measurement of scale scores is presented, where scale scores are nonlinear transformations of number-correct scores. The standard errors account for measurement error that is introduced due to rounding scale scores to integers. Procedures for estimating the average conditional standard error of measurement for scale scores and reliability of scale scores are also described. An illustration of the use of the methodology is presented, and the results from the IRT method are compared to the results from a previously developed method that is based on strong true-score theory.","['IRT', 'method', 'estimate', 'conditional', 'standard', 'error', 'scale', 'score', 'present', 'scale', 'score', 'nonlinear', 'transformation', 'numbercorrect', 'score', 'standard', 'error', 'account', 'error', 'introduce', 'round', 'scale', 'score', 'integer', 'procedure', 'estimate', 'average', 'conditional', 'standard', 'error', 'scale', 'score', 'reliability', 'scale', 'score', 'describe', 'illustration', 'methodology', 'present', 'result', 'IRT', 'method', 'compare', 'result', 'previously', 'develop', 'method', 'base', 'strong', 'truescore', 'theory']","['conditional', 'standard', 'error', 'scale', 'score', 'IRT']",IRT method estimate conditional standard error scale score present scale score nonlinear transformation numbercorrect score standard error account error introduce round scale score integer procedure estimate average conditional standard error scale score reliability scale score describe illustration methodology present result IRT method compare result previously develop method base strong truescore theory,conditional standard error scale score IRT,0.03469962092290069,0.0345936564969001,0.03468200782920653,0.03433773769663168,0.861686977054361,0.0,0.0,0.0,0.3298676067011856,0.0
De Champlain A.F.,The effect of multidimensionality on IRT true-score equating for subgroups of examinees,1996,33,"The purpose of this study was to assess the dimensionality of two forms of a large-scale standardized test separately for 3 ethnic groups of examinees and to investigate whether differences in their latent trait composites have any impact on unidimensional item response theory true-score equating functions. Specifically, separate equating functions for African American and Hispanic examinees were compared to those of a Caucasian group as well as the total test taker population. On both forms, a 2-dimensional model adequately accounted for the item responses of Caucasian and African American examinees, whereas a more complex model was required for the Hispanic subgroup. The differences between equating functions for the 3 ethnic groups and the total test taker population were small and tended to be located at the low end of the score scale.",The effect of multidimensionality on IRT true-score equating for subgroups of examinees,"The purpose of this study was to assess the dimensionality of two forms of a large-scale standardized test separately for 3 ethnic groups of examinees and to investigate whether differences in their latent trait composites have any impact on unidimensional item response theory true-score equating functions. Specifically, separate equating functions for African American and Hispanic examinees were compared to those of a Caucasian group as well as the total test taker population. On both forms, a 2-dimensional model adequately accounted for the item responses of Caucasian and African American examinees, whereas a more complex model was required for the Hispanic subgroup. The differences between equating functions for the 3 ethnic groups and the total test taker population were small and tended to be located at the low end of the score scale.","['purpose', 'study', 'assess', 'dimensionality', 'form', 'largescale', 'standardized', 'test', 'separately', '3', 'ethnic', 'group', 'examinee', 'investigate', 'difference', 'latent', 'trait', 'composite', 'impact', 'unidimensional', 'item', 'response', 'theory', 'truescore', 'equate', 'function', 'specifically', 'separate', 'equate', 'function', 'african', 'American', 'hispanic', 'examinee', 'compare', 'caucasian', 'group', 'total', 'test', 'taker', 'population', 'form', '2dimensional', 'adequately', 'account', 'item', 'response', 'caucasian', 'african', 'American', 'examine', 'complex', 'require', 'hispanic', 'subgroup', 'difference', 'equate', 'function', '3', 'ethnic', 'group', 'total', 'test', 'taker', 'population', 'small', 'tend', 'locate', 'low', 'end', 'score', 'scale']","['effect', 'multidimensionality', 'IRT', 'truescore', 'equate', 'subgroup', 'examinee']",purpose study assess dimensionality form largescale standardized test separately 3 ethnic group examinee investigate difference latent trait composite impact unidimensional item response theory truescore equate function specifically separate equate function african American hispanic examinee compare caucasian group total test taker population form 2dimensional adequately account item response caucasian african American examine complex require hispanic subgroup difference equate function 3 ethnic group total test taker population small tend locate low end score scale,effect multidimensionality IRT truescore equate subgroup examinee,0.02744952629786345,0.027404857902044562,0.6832102662569597,0.02712072508808518,0.23481462445504703,0.048898103129363504,0.030482622981444957,0.022866030266085205,0.018507230557541603,0.0
Allen N.L.; Donoghue J.R.,Applying the Mantel-Haenszel procedure to complex samples of items,1996,33,"This Monte Carlo study examined the effect of complex sampling of items on the measurement of differential item functioning (DIF) using the Mantel-Haenszel procedure. Data were generated using a 3-parameter logistic item response theory model according to the balanced incomplete block (BIB) design used in the National Assessment of Educational Progress (NAEP). The length of each block of items and the number of DIF items in the matching variable were varied, as was the difficulty, discrimination, and presence of DIF in the studied item. Block, booklet, pooled booklet, and extra-information analyses were compared to a complete data analysis using the transformed log-odds on the delta scale. The pooled booklet approach is recommended for use when items are selected for examinees according to a BIB design. This study has implications for DIF analyses of other complex samples of items, such as computer administered testing or another complex assessment design.",Applying the Mantel-Haenszel procedure to complex samples of items,"This Monte Carlo study examined the effect of complex sampling of items on the measurement of differential item functioning (DIF) using the Mantel-Haenszel procedure. Data were generated using a 3-parameter logistic item response theory model according to the balanced incomplete block (BIB) design used in the National Assessment of Educational Progress (NAEP). The length of each block of items and the number of DIF items in the matching variable were varied, as was the difficulty, discrimination, and presence of DIF in the studied item. Block, booklet, pooled booklet, and extra-information analyses were compared to a complete data analysis using the transformed log-odds on the delta scale. The pooled booklet approach is recommended for use when items are selected for examinees according to a BIB design. This study has implications for DIF analyses of other complex samples of items, such as computer administered testing or another complex assessment design.","['Monte', 'Carlo', 'study', 'examine', 'effect', 'complex', 'sampling', 'item', 'differential', 'item', 'function', 'DIF', 'MantelHaenszel', 'procedure', 'Data', 'generate', '3parameter', 'logistic', 'item', 'response', 'theory', 'accord', 'balanced', 'incomplete', 'block', 'bib', 'design', 'National', 'Assessment', 'Educational', 'Progress', 'naep', 'length', 'block', 'item', 'number', 'dif', 'item', 'matching', 'variable', 'varied', 'difficulty', 'discrimination', 'presence', 'DIF', 'studied', 'item', 'Block', 'booklet', 'pool', 'booklet', 'extrainformation', 'analysis', 'compare', 'complete', 'data', 'analysis', 'transform', 'logodds', 'delta', 'scale', 'pool', 'booklet', 'approach', 'recommend', 'item', 'select', 'examinee', 'accord', 'BIB', 'design', 'study', 'implication', 'dif', 'analysis', 'complex', 'sample', 'item', 'computer', 'administer', 'testing', 'complex', 'assessment', 'design']","['apply', 'MantelHaenszel', 'procedure', 'complex', 'sample', 'item']",Monte Carlo study examine effect complex sampling item differential item function DIF MantelHaenszel procedure Data generate 3parameter logistic item response theory accord balanced incomplete block bib design National Assessment Educational Progress naep length block item number dif item matching variable varied difficulty discrimination presence DIF studied item Block booklet pool booklet extrainformation analysis compare complete data analysis transform logodds delta scale pool booklet approach recommend item select examinee accord BIB design study implication dif analysis complex sample item computer administer testing complex assessment design,apply MantelHaenszel procedure complex sample item,0.027792045502631466,0.8877846779531635,0.02783840463152635,0.027408839363774473,0.02917603254890415,0.006441628550502016,0.03358248560446546,0.11399163198374761,0.0,0.03374801515081058
Oshima T.C.,The Effect of Speededness on Parameter Estimation in Item Response Theory,1994,31,"There is a paucity of research in item response theory (IRT) examining the consequences of violating the implicit assumption of nonspeededness. In this study, test data were simulated systematically under various speeded conditions. The three factors considered in relation to speededness were proportion of test not reached (5%, 10%, and 15%), response to not reached (blank vs. random response), and item ordering (random vs. easy to hard). The effects of these factors on parameter estimation were then examined by comparing the item and ability parameter estimates with the known true parameters. Results indicated that the ability estimation was least affected by speededness in terms of the correlation between true and estimated ability parameters. On the other hand, substantial effects of speededness were observed among item parameter estimates. Recommendations for minimizing the effects of speededness are discussed Copyright © 1994, Wiley Blackwell. All rights reserved",The Effect of Speededness on Parameter Estimation in Item Response Theory,"There is a paucity of research in item response theory (IRT) examining the consequences of violating the implicit assumption of nonspeededness. In this study, test data were simulated systematically under various speeded conditions. The three factors considered in relation to speededness were proportion of test not reached (5%, 10%, and 15%), response to not reached (blank vs. random response), and item ordering (random vs. easy to hard). The effects of these factors on parameter estimation were then examined by comparing the item and ability parameter estimates with the known true parameters. Results indicated that the ability estimation was least affected by speededness in terms of the correlation between true and estimated ability parameters. On the other hand, substantial effects of speededness were observed among item parameter estimates. Recommendations for minimizing the effects of speededness are discussed Copyright © 1994, Wiley Blackwell. All rights reserved","['paucity', 'research', 'item', 'response', 'theory', 'IRT', 'examine', 'consequence', 'violate', 'implicit', 'assumption', 'nonspeededness', 'study', 'test', 'datum', 'simulate', 'systematically', 'speed', 'condition', 'factor', 'consider', 'relation', 'speededness', 'proportion', 'test', 'reach', '5', '10', '15', 'response', 'reach', 'blank', 'vs', 'random', 'response', 'item', 'order', 'random', 'vs', 'easy', 'hard', 'effect', 'factor', 'parameter', 'estimation', 'examine', 'compare', 'item', 'ability', 'parameter', 'estimate', 'know', 'true', 'parameter', 'result', 'indicate', 'ability', 'estimation', 'affect', 'speededness', 'term', 'correlation', 'true', 'estimated', 'ability', 'parameter', 'hand', 'substantial', 'effect', 'speededness', 'observe', 'item', 'parameter', 'estimate', 'Recommendations', 'minimize', 'effect', 'speededness', 'discuss', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Effect', 'Speededness', 'Parameter', 'Estimation', 'Item', 'Response', 'Theory']",paucity research item response theory IRT examine consequence violate implicit assumption nonspeededness study test datum simulate systematically speed condition factor consider relation speededness proportion test reach 5 10 15 response reach blank vs random response item order random vs easy hard effect factor parameter estimation examine compare item ability parameter estimate know true parameter result indicate ability estimation affect speededness term correlation true estimated ability parameter hand substantial effect speededness observe item parameter estimate Recommendations minimize effect speededness discuss Copyright © 1994 Wiley Blackwell right reserve,Effect Speededness Parameter Estimation Item Response Theory,0.02714637126593396,0.8918367816200243,0.027029629876525168,0.02666304772674457,0.027324169510772028,0.0776975742070373,0.03402840784528219,0.004368784955460635,0.0,0.0
Shavelson R.J.; Ruiz-Primo M.A.; Wiley E.W.,Note on sources of sampling variability in science performance assessments,1999,36,"In 1993, we reported in Journal of Educational Measurement that task-sampling variability was the Achilles' heel of science performance assessment. To reduce measurement error, tasks needed to be stratified before sampling, sampled in large number, or possibly both. However, Cronbach, Linn, Brennan, & Haertel (1997) pointed out that a task-sampling interpretation of a large person × task variance component might be incorrect. Task and occasion sampling are confounded because tasks are typically given on only a single occasion. The person × task source of measurement error is then confounded with the pt × occasion source. If pto variability accounts for a substantial part of the commonly observed pt interaction, stratifying tasks into homogenous subsets - a cost-effective way of addressing task sampling variability - might not increase accuracy. Stratification would not address the pto source of error. Another conclusion reported in JEM was that only direct observation (DO) and notebook (NB) methods of collecting performance assessment data were exchangeable; computer simulation, short-answer, and multiple-choice methods were not. However, if Cronbach et al. were right, our exchangeability conclusion might be incorrect. After re-examining and re-analyzing data, we found support for Conbach et al. We concluded that large task-sampling variability was due to both the person × task interaction and person × task × occasion interaction. Moreover, we found that direct observation, notebook and computer simulation methods were equally exchangeable, but their exchangeability was limited by the volatility of student performances across tasks and occasions.",Note on sources of sampling variability in science performance assessments,"In 1993, we reported in Journal of Educational Measurement that task-sampling variability was the Achilles' heel of science performance assessment. To reduce measurement error, tasks needed to be stratified before sampling, sampled in large number, or possibly both. However, Cronbach, Linn, Brennan, & Haertel (1997) pointed out that a task-sampling interpretation of a large person × task variance component might be incorrect. Task and occasion sampling are confounded because tasks are typically given on only a single occasion. The person × task source of measurement error is then confounded with the pt × occasion source. If pto variability accounts for a substantial part of the commonly observed pt interaction, stratifying tasks into homogenous subsets - a cost-effective way of addressing task sampling variability - might not increase accuracy. Stratification would not address the pto source of error. Another conclusion reported in JEM was that only direct observation (DO) and notebook (NB) methods of collecting performance assessment data were exchangeable; computer simulation, short-answer, and multiple-choice methods were not. However, if Cronbach et al. were right, our exchangeability conclusion might be incorrect. After re-examining and re-analyzing data, we found support for Conbach et al. We concluded that large task-sampling variability was due to both the person × task interaction and person × task × occasion interaction. Moreover, we found that direct observation, notebook and computer simulation methods were equally exchangeable, but their exchangeability was limited by the volatility of student performances across tasks and occasions.","['1993', 'report', 'Journal', 'Educational', 'tasksample', 'variability', 'Achilles', 'heel', 'science', 'performance', 'assessment', 'reduce', 'error', 'task', 'need', 'stratify', 'sample', 'sample', 'large', 'number', 'possibly', 'Cronbach', 'Linn', 'Brennan', 'Haertel', '1997', 'point', 'tasksampling', 'interpretation', 'large', 'person', '×', 'task', 'variance', 'component', 'incorrect', 'Task', 'occasion', 'sampling', 'confound', 'task', 'typically', 'single', 'occasion', 'person', '×', 'task', 'source', 'error', 'confound', 'pt', '×', 'occasion', 'source', 'pto', 'variability', 'account', 'substantial', 'commonly', 'observe', 'pt', 'interaction', 'stratify', 'task', 'homogenous', 'subset', 'costeffective', 'way', 'address', 'task', 'sample', 'variability', 'increase', 'accuracy', 'Stratification', 'address', 'pto', 'source', 'error', 'conclusion', 'report', 'JEM', 'direct', 'observation', 'notebook', 'NB', 'method', 'collect', 'performance', 'assessment', 'datum', 'exchangeable', 'computer', 'simulation', 'shortanswer', 'multiplechoice', 'method', 'Cronbach', 'et', 'al', 'right', 'exchangeability', 'conclusion', 'incorrect', 'reexamine', 'reanalyze', 'datum', 'find', 'support', 'Conbach', 'et', 'al', 'conclude', 'large', 'tasksampling', 'variability', 'person', '×', 'task', 'interaction', 'person', '×', 'task', '×', 'occasion', 'interaction', 'find', 'direct', 'observation', 'notebook', 'computer', 'simulation', 'method', 'equally', 'exchangeable', 'exchangeability', 'limit', 'volatility', 'student', 'performance', 'task', 'occasion']","['note', 'source', 'sample', 'variability', 'science', 'performance', 'assessment']",1993 report Journal Educational tasksample variability Achilles heel science performance assessment reduce error task need stratify sample sample large number possibly Cronbach Linn Brennan Haertel 1997 point tasksampling interpretation large person × task variance component incorrect Task occasion sampling confound task typically single occasion person × task source error confound pt × occasion source pto variability account substantial commonly observe pt interaction stratify task homogenous subset costeffective way address task sample variability increase accuracy Stratification address pto source error conclusion report JEM direct observation notebook NB method collect performance assessment datum exchangeable computer simulation shortanswer multiplechoice method Cronbach et al right exchangeability conclusion incorrect reexamine reanalyze datum find support Conbach et al conclude large tasksampling variability person × task interaction person × task × occasion interaction find direct observation notebook computer simulation method equally exchangeable exchangeability limit volatility student performance task occasion,note source sample variability science performance assessment,0.024617747551677376,0.024779609373672826,0.024855599219662128,0.024449289411437307,0.9012977544435503,0.002339404439210206,0.0,0.0,0.009033587365213463,0.09738372386472358
Ferrara S.; Huynh H.; Michaels H.,Contextual explanations of local dependence in item clusters in a large scale hands-on science performance assessment,1999,36,"This study provides hypothesized explanations for local item dependence (LID) in a large scale hands-on science performance assessment. Items within multi-step item clusters were classified as low or high in LID using contextual analysis procedures described in this and other studies. LID was identified statistically using the average within cluster (AWC) correlation procedure described in previous studies. Levels of LID identified in contextual analyses were compared to leves of LID identified in correlation analyses. Consistent with other studies, items that appear to elicit locally dependent responses require examinees to answer and explain their answer or to use given or generated information to respond.",Contextual explanations of local dependence in item clusters in a large scale hands-on science performance assessment,"This study provides hypothesized explanations for local item dependence (LID) in a large scale hands-on science performance assessment. Items within multi-step item clusters were classified as low or high in LID using contextual analysis procedures described in this and other studies. LID was identified statistically using the average within cluster (AWC) correlation procedure described in previous studies. Levels of LID identified in contextual analyses were compared to leves of LID identified in correlation analyses. Consistent with other studies, items that appear to elicit locally dependent responses require examinees to answer and explain their answer or to use given or generated information to respond.","['study', 'provide', 'hypothesized', 'explanation', 'local', 'item', 'dependence', 'LID', 'large', 'scale', 'handson', 'science', 'performance', 'assessment', 'item', 'multistep', 'item', 'cluster', 'classify', 'low', 'high', 'LID', 'contextual', 'analysis', 'procedure', 'describe', 'study', 'LID', 'identify', 'statistically', 'average', 'cluster', 'AWC', 'correlation', 'procedure', 'describe', 'previous', 'study', 'Levels', 'LID', 'identify', 'contextual', 'analysis', 'compare', 'leve', 'LID', 'identify', 'correlation', 'analyse', 'Consistent', 'study', 'item', 'appear', 'elicit', 'locally', 'dependent', 'response', 'require', 'examine', 'answer', 'explain', 'answer', 'generate', 'information', 'respond']","['contextual', 'explanation', 'local', 'dependence', 'item', 'cluster', 'large', 'scale', 'handson', 'science', 'performance', 'assessment']",study provide hypothesized explanation local item dependence LID large scale handson science performance assessment item multistep item cluster classify low high LID contextual analysis procedure describe study LID identify statistically average cluster AWC correlation procedure describe previous study Levels LID identify contextual analysis compare leve LID identify correlation analyse Consistent study item appear elicit locally dependent response require examine answer explain answer generate information respond,contextual explanation local dependence item cluster large scale handson science performance assessment,0.032033932426008514,0.36448034215969966,0.03152000702968498,0.539603499813441,0.03236221857116578,0.002604314047639209,0.04543608155929929,0.013833864568156764,0.0,0.043997948279539956
Powers D.E.; Rock D.A.,Effects of coaching on SAT I: Reasoning test scores,1999,36,"A College Board-sponsored survey of a nationally representative sample of 1995-96 SAT takers yielded a data base for more than 4,000 examinees, about 500 of whom had attended formal coaching programs outside their schools. Several alternative analytical methods were used to estimate the effects of coaching on SAT I: Reasoning Test scores. The various analyses produced slightly different estimates. All of the estimates, however, suggested that the effects of coaching are far less than is claimed by major commercial test preparation companies. The revised SAT does not appear to be any more coachable than its predecessor.",Effects of coaching on SAT I: Reasoning test scores,"A College Board-sponsored survey of a nationally representative sample of 1995-96 SAT takers yielded a data base for more than 4,000 examinees, about 500 of whom had attended formal coaching programs outside their schools. Several alternative analytical methods were used to estimate the effects of coaching on SAT I: Reasoning Test scores. The various analyses produced slightly different estimates. All of the estimates, however, suggested that the effects of coaching are far less than is claimed by major commercial test preparation companies. The revised SAT does not appear to be any more coachable than its predecessor.","['College', 'Boardsponsored', 'survey', 'nationally', 'representative', 'sample', '199596', 'SAT', 'taker', 'yield', 'data', 'base', '4000', 'examine', '500', 'attend', 'formal', 'coaching', 'program', 'outside', 'school', 'alternative', 'analytical', 'method', 'estimate', 'effect', 'coach', 'SAT', 'I', 'Reasoning', 'Test', 'score', 'analysis', 'produce', 'slightly', 'different', 'estimate', 'estimate', 'suggest', 'effect', 'coaching', 'far', 'claim', 'major', 'commercial', 'test', 'preparation', 'company', 'revise', 'SAT', 'appear', 'coachable', 'predecessor']","['effect', 'coach', 'SAT', 'I', 'reason', 'test', 'score']",College Boardsponsored survey nationally representative sample 199596 SAT taker yield data base 4000 examine 500 attend formal coaching program outside school alternative analytical method estimate effect coach SAT I Reasoning Test score analysis produce slightly different estimate estimate suggest effect coaching far claim major commercial test preparation company revise SAT appear coachable predecessor,effect coach SAT I reason test score,0.02790745051370349,0.02788186368802163,0.02780072361795233,0.027675224504907846,0.8887347376754147,0.04056435072973235,0.008321410319864312,0.0,0.04170019166358742,0.001571524516353942
Clauser B.E.; Clyman S.G.; Swanson D.B.,Components of rater error in a complex performance assessment,1999,36,"Numerous studies have examined performance assessment data using generalizability theory. Typically, these studies have treated raters as randomly sampled from a population, with each rater judging a given performance on a single occasion. This paper presents two studies that focus on aspects of the rating process that are not explicitly accounted for in this typical design. The first study makes explicit the ""committee"" facet, acknowledging that raters often work within groups. The second study makes explicit the ""rating-occasion"" facet by having each rater judge each performance on two separate occasions. The results of the first study highlight the importance of clearly specifying the relevant facets of the universe of interest. Failing to include the committee facet led to an overly optimistic estimate of the precision of the measurement procedure. By contrast, failing to include the rating-occasion facet, in the second study, had minimal impact on the estimated error variance.",Components of rater error in a complex performance assessment,"Numerous studies have examined performance assessment data using generalizability theory. Typically, these studies have treated raters as randomly sampled from a population, with each rater judging a given performance on a single occasion. This paper presents two studies that focus on aspects of the rating process that are not explicitly accounted for in this typical design. The first study makes explicit the ""committee"" facet, acknowledging that raters often work within groups. The second study makes explicit the ""rating-occasion"" facet by having each rater judge each performance on two separate occasions. The results of the first study highlight the importance of clearly specifying the relevant facets of the universe of interest. Failing to include the committee facet led to an overly optimistic estimate of the precision of the measurement procedure. By contrast, failing to include the rating-occasion facet, in the second study, had minimal impact on the estimated error variance.","['numerous', 'study', 'examine', 'performance', 'assessment', 'datum', 'generalizability', 'theory', 'typically', 'study', 'treat', 'rater', 'randomly', 'sample', 'population', 'rater', 'judge', 'performance', 'single', 'occasion', 'paper', 'present', 'study', 'focus', 'aspect', 'rating', 'process', 'explicitly', 'account', 'typical', 'design', 'study', 'explicit', 'committee', 'facet', 'acknowledge', 'rater', 'work', 'group', 'second', 'study', 'explicit', 'ratingoccasion', 'facet', 'rater', 'judge', 'performance', 'separate', 'occasion', 'result', 'study', 'highlight', 'importance', 'clearly', 'specify', 'relevant', 'facet', 'universe', 'interest', 'fail', 'include', 'committee', 'facet', 'lead', 'overly', 'optimistic', 'estimate', 'precision', 'procedure', 'contrast', 'fail', 'include', 'ratingoccasion', 'facet', 'second', 'study', 'minimal', 'impact', 'estimate', 'error', 'variance']","['component', 'rater', 'error', 'complex', 'performance', 'assessment']",numerous study examine performance assessment datum generalizability theory typically study treat rater randomly sample population rater judge performance single occasion paper present study focus aspect rating process explicitly account typical design study explicit committee facet acknowledge rater work group second study explicit ratingoccasion facet rater judge performance separate occasion result study highlight importance clearly specify relevant facet universe interest fail include committee facet lead overly optimistic estimate precision procedure contrast fail include ratingoccasion facet second study minimal impact estimate error variance,component rater error complex performance assessment,0.8881686997986462,0.02767259997716124,0.028058652395415388,0.027479538299505342,0.028620509529271837,0.016532931997009,0.0,0.0,0.0,0.10361681423422966
Parshall C.G.; Houghton P.D.B.; Kromrey J.D.,Equating Error and Statistical Bias in Small Sample Linear Equating,1995,32,"A resampling study was conducted to compare the statistical bias and standard errors of nonequivalent‐groups linear test equating in small samples of examinees. Sample sizes of 15, 25, 50, and 100 were examined. One thousand samples of each size were drawn with replacement from each of 5 archival data files from teacher subject area tests. For each test, data files from 2 parallel forms were used. Results suggest trivial levels of equating bias even with small samples, but substantial increases in standard errors as sample size decreases. Results were interpreted in terms of applications to testing situations in which small numbers of examinees are available. Copyright © 1995, Wiley Blackwell. All rights reserved",Equating Error and Statistical Bias in Small Sample Linear Equating,"A resampling study was conducted to compare the statistical bias and standard errors of nonequivalent‐groups linear test equating in small samples of examinees. Sample sizes of 15, 25, 50, and 100 were examined. One thousand samples of each size were drawn with replacement from each of 5 archival data files from teacher subject area tests. For each test, data files from 2 parallel forms were used. Results suggest trivial levels of equating bias even with small samples, but substantial increases in standard errors as sample size decreases. Results were interpreted in terms of applications to testing situations in which small numbers of examinees are available. Copyright © 1995, Wiley Blackwell. All rights reserved","['resampling', 'study', 'conduct', 'compare', 'statistical', 'bias', 'standard', 'error', 'nonequivalent‐group', 'linear', 'test', 'equating', 'small', 'sample', 'examinee', 'Sample', 'size', '15', '25', '50', '100', 'examine', 'thousand', 'sample', 'size', 'draw', 'replacement', '5', 'archival', 'datum', 'file', 'teacher', 'subject', 'area', 'test', 'test', 'datum', 'file', '2', 'parallel', 'form', 'result', 'suggest', 'trivial', 'level', 'equate', 'bias', 'small', 'sample', 'substantial', 'increase', 'standard', 'error', 'sample', 'size', 'decrease', 'result', 'interpret', 'term', 'application', 'testing', 'situation', 'small', 'number', 'examinee', 'available', 'copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Equating', 'Error', 'Statistical', 'Bias', 'Small', 'Sample', 'Linear', 'Equating']",resampling study conduct compare statistical bias standard error nonequivalent‐group linear test equating small sample examinee Sample size 15 25 50 100 examine thousand sample size draw replacement 5 archival datum file teacher subject area test test datum file 2 parallel form result suggest trivial level equate bias small sample substantial increase standard error sample size decrease result interpret term application testing situation small number examinee available copyright © 1995 Wiley Blackwell right reserve,Equating Error Statistical Bias Small Sample Linear Equating,0.8926669198471213,0.026875935481021226,0.026792327688882775,0.026380737128743836,0.0272840798542308,0.10991596403117013,0.0,0.0,0.0015121260251355919,0.006141026522119905
Bennett R.E.; Sebrechts M.M.,A computer-based task for measuring the representational component of quantitative proficiency,1997,34,"In this study, we created a computer-delivered problem-solving task based on the cognitive research literature and investigated its validity for graduate admissions assessment. The task asked examinees to sort mathematical Word problem stems according to prototypes. Data analyses focused on the meaning of sorting scores and examinee perceptions of the task. Results showed that those who sorted well tended to have higher GRE General Test scores and college grades than did examinees who sorted less proficiently. Examinees generally preferred this task to multiple-choice items like those found on the General Test's Quantitative section and felt the task was a fairer measure of their ability to succeed in graduate school. Adaptations of the task might be used in admissions tests, as well as for instructional assessments to help lower-scoring examinees localize and remediate problem-solving difficulties.",A computer-based task for measuring the representational component of quantitative proficiency,"In this study, we created a computer-delivered problem-solving task based on the cognitive research literature and investigated its validity for graduate admissions assessment. The task asked examinees to sort mathematical Word problem stems according to prototypes. Data analyses focused on the meaning of sorting scores and examinee perceptions of the task. Results showed that those who sorted well tended to have higher GRE General Test scores and college grades than did examinees who sorted less proficiently. Examinees generally preferred this task to multiple-choice items like those found on the General Test's Quantitative section and felt the task was a fairer measure of their ability to succeed in graduate school. Adaptations of the task might be used in admissions tests, as well as for instructional assessments to help lower-scoring examinees localize and remediate problem-solving difficulties.","['study', 'create', 'computerdelivered', 'problemsolving', 'task', 'base', 'cognitive', 'research', 'literature', 'investigate', 'validity', 'graduate', 'admission', 'assessment', 'task', 'ask', 'examine', 'sort', 'mathematical', 'Word', 'problem', 'stem', 'accord', 'prototype', 'Data', 'analysis', 'focus', 'meaning', 'sort', 'score', 'examinee', 'perception', 'task', 'result', 'sort', 'tend', 'high', 'GRE', 'General', 'Test', 'score', 'college', 'grade', 'examinee', 'sort', 'proficiently', 'examinee', 'generally', 'prefer', 'task', 'multiplechoice', 'item', 'like', 'find', 'General', 'Tests', 'Quantitative', 'section', 'feel', 'task', 'fair', 'measure', 'ability', 'succeed', 'graduate', 'school', 'Adaptations', 'task', 'admission', 'test', 'instructional', 'assessment', 'help', 'lowerscore', 'examine', 'localize', 'remediate', 'problemsolving', 'difficulty']","['computerbased', 'task', 'measure', 'representational', 'component', 'quantitative', 'proficiency']",study create computerdelivered problemsolving task base cognitive research literature investigate validity graduate admission assessment task ask examine sort mathematical Word problem stem accord prototype Data analysis focus meaning sort score examinee perception task result sort tend high GRE General Test score college grade examinee sort proficiently examinee generally prefer task multiplechoice item like find General Tests Quantitative section feel task fair measure ability succeed graduate school Adaptations task admission test instructional assessment help lowerscore examine localize remediate problemsolving difficulty,computerbased task measure representational component quantitative proficiency,0.027461997880431094,0.8893953995652509,0.0280335209545404,0.027413468620044117,0.027695612979733518,0.0,0.05032974568237062,0.0,0.004491970815993127,0.07036333816039769
Ercikan K.; Schwarz R.D.; Julian M.W.; Burket G.R.; Weber M.M.; Link V.,Calibration and scoring of tests with multiple-choice and constructed-response item types,1998,35,"This article discusses and demonstrates combining scores from multiple-choice (MC) and constructed-response (CR) items to create a common scale using item response theory methodology. Two specific issues addressed are (a) whether MC and CR items can be calibrated together and (b) whether simultaneous calibration of the two item types leads to loss of information. Procedures are discussed and empirical results are provided using a set of tests in the areas of reading, language, mathematics, and science in three grades.",Calibration and scoring of tests with multiple-choice and constructed-response item types,"This article discusses and demonstrates combining scores from multiple-choice (MC) and constructed-response (CR) items to create a common scale using item response theory methodology. Two specific issues addressed are (a) whether MC and CR items can be calibrated together and (b) whether simultaneous calibration of the two item types leads to loss of information. Procedures are discussed and empirical results are provided using a set of tests in the areas of reading, language, mathematics, and science in three grades.","['article', 'discuss', 'demonstrate', 'combine', 'score', 'multiplechoice', 'MC', 'constructedresponse', 'CR', 'item', 'create', 'common', 'scale', 'item', 'response', 'theory', 'methodology', 'specific', 'issue', 'address', 'MC', 'CR', 'item', 'calibrate', 'b', 'simultaneous', 'calibration', 'item', 'type', 'lead', 'loss', 'information', 'procedure', 'discuss', 'empirical', 'result', 'provide', 'set', 'test', 'area', 'read', 'language', 'mathematic', 'science', 'grade']","['calibration', 'scoring', 'test', 'multiplechoice', 'constructedresponse', 'item', 'type']",article discuss demonstrate combine score multiplechoice MC constructedresponse CR item create common scale item response theory methodology specific issue address MC CR item calibrate b simultaneous calibration item type lead loss information procedure discuss empirical result provide set test area read language mathematic science grade,calibration scoring test multiplechoice constructedresponse item type,0.03057624529402669,0.03084420222035699,0.0303590400301024,0.030144121126069644,0.8780763913294443,0.0,0.08191028654473571,0.009886173073164117,0.03439478151981551,0.011832336015089056
De Gruijter D.N.M.,On information of percentile ranks,1997,34,Recently May and Nicewander concluded that percentile ranks are inferior to raw scores as indicators of latent ability. Here it is argued that their arguments are faulty.,,Recently May and Nicewander concluded that percentile ranks are inferior to raw scores as indicators of latent ability. Here it is argued that their arguments are faulty.,"['recently', 'May', 'Nicewander', 'conclude', 'percentile', 'rank', 'inferior', 'raw', 'score', 'indicator', 'latent', 'ability', 'argue', 'argument', 'faulty']",,recently May Nicewander conclude percentile rank inferior raw score indicator latent ability argue argument faulty,,0.04361872587319921,0.044168981152919486,0.04323650829084279,0.04313858837734593,0.8258371963056926,0.0,0.0,0.009369190258140965,0.051189241297072835,0.0
Vispoel W.P.,Psychometric characteristics of computer-adaptive and self-adaptive vocabulary tests: The role of answer feedback and test anxiety,1998,35,"This study focused on the effects of administration mode (computer-adaptive test [CAT] versus self-adaptive test [SAT]), item-by-item answer feedback (present versus absent), and test anxiety on results obtained from computerized vocabulary tests. Examinees were assigned at random to four testing conditions (CAT with feedback, CAT without feedback, SAT with feedback, SAT without feedback). Examinees completed the Test Anxiety Inventory (Spielberger, 1980) before taking their assigned computerized tests. Results showed that the CATs were more reliable and took less time to complete than the SATs. Administration time for both the CATs and SATs was shorter when feedback was provided than when it was not, and this difference was most pronounced for examinees at medium to high levels of test anxiety. These results replicate prior findings regarding the precision and administrative efficiency of CATs and SATs but point to new possible benefits of including answer feedback on such tests.",Psychometric characteristics of computer-adaptive and self-adaptive vocabulary tests: The role of answer feedback and test anxiety,"This study focused on the effects of administration mode (computer-adaptive test [CAT] versus self-adaptive test [SAT]), item-by-item answer feedback (present versus absent), and test anxiety on results obtained from computerized vocabulary tests. Examinees were assigned at random to four testing conditions (CAT with feedback, CAT without feedback, SAT with feedback, SAT without feedback). Examinees completed the Test Anxiety Inventory (Spielberger, 1980) before taking their assigned computerized tests. Results showed that the CATs were more reliable and took less time to complete than the SATs. Administration time for both the CATs and SATs was shorter when feedback was provided than when it was not, and this difference was most pronounced for examinees at medium to high levels of test anxiety. These results replicate prior findings regarding the precision and administrative efficiency of CATs and SATs but point to new possible benefits of including answer feedback on such tests.","['study', 'focus', 'effect', 'administration', 'mode', 'computeradaptive', 'test', 'CAT', 'versus', 'selfadaptive', 'test', 'SAT', 'itembyitem', 'answer', 'feedback', 'present', 'versus', 'absent', 'test', 'anxiety', 'result', 'obtain', 'computerized', 'vocabulary', 'test', 'Examinees', 'assign', 'random', 'testing', 'condition', 'CAT', 'feedback', 'CAT', 'feedback', 'SAT', 'feedback', 'SAT', 'feedback', 'examinee', 'complete', 'Test', 'Anxiety', 'Inventory', 'Spielberger', '1980', 'assign', 'computerized', 'test', 'result', 'cat', 'reliable', 'time', 'complete', 'SATs', 'Administration', 'time', 'CATs', 'SATs', 'short', 'feedback', 'provide', 'difference', 'pronounce', 'examinee', 'medium', 'high', 'level', 'test', 'anxiety', 'result', 'replicate', 'prior', 'finding', 'regard', 'precision', 'administrative', 'efficiency', 'CATs', 'sat', 'point', 'new', 'possible', 'benefit', 'include', 'answer', 'feedback', 'test']","['psychometric', 'characteristic', 'computeradaptive', 'selfadaptive', 'vocabulary', 'test', 'role', 'answer', 'feedback', 'test', 'anxiety']",study focus effect administration mode computeradaptive test CAT versus selfadaptive test SAT itembyitem answer feedback present versus absent test anxiety result obtain computerized vocabulary test Examinees assign random testing condition CAT feedback CAT feedback SAT feedback SAT feedback examinee complete Test Anxiety Inventory Spielberger 1980 assign computerized test result cat reliable time complete SATs Administration time CATs SATs short feedback provide difference pronounce examinee medium high level test anxiety result replicate prior finding regard precision administrative efficiency CATs sat point new possible benefit include answer feedback test,psychometric characteristic computeradaptive selfadaptive vocabulary test role answer feedback test anxiety,0.03203602126712698,0.03228510547859896,0.032250056455123445,0.03184305866318392,0.8715857581359666,0.008901959867497259,0.06908137655911245,0.0,0.0014068335799462196,0.0
Wang T.; Vispoel W.P.,Properties of ability estimation methods in computerized adaptive testing,1998,35,"Simulations of computerized adaptive tests (CATs) were used to evaluate results yielded by four commonly used ability estimation methods: maximum likelihood estimation (MLE) and three Bayesian approaches-Owen's method, expected a posteriori (EAP), and maximum a posteriori. In line with the theoretical nature of the ability estimates and previous empirical research, the results showed clear distinctions between MLE and the Bayesian methods, with MLE yielding lower bias, higher standard errors, higher root mean square errors, lower fidelity, and lower administrative efficiency. Standard errors for MLE based on test information underestimated actual standard errors, whereas standard errors for the Bayesian methods based on posterior distribution standard deviations accurately estimated actual standard errors. Among the Bayesian methods, Owen's provided the worst overall results, and EAP provided the best. Using a variable starting rule in which examinees were initially classified into three broad ability groups greatly reduced the bias for the Bayesian methods, but had little effect on the results for MLE. On the basis of these results, guidelines are offered for selecting appropriate CAT ability estimation methods in different decision contexts. This article is based on a doctoral dissertation (Wang, 1995) that was directed by Walter P. Vispoel. We thank Roberto de la Torre for his assistance in developing software for the computer simulation analyses and Rebecca Zwick and the anonymous reviewers for their helpful comments on the original manuscripts. Correspondence concerning this article should be addressed to the first author.",Properties of ability estimation methods in computerized adaptive testing,"Simulations of computerized adaptive tests (CATs) were used to evaluate results yielded by four commonly used ability estimation methods: maximum likelihood estimation (MLE) and three Bayesian approaches-Owen's method, expected a posteriori (EAP), and maximum a posteriori. In line with the theoretical nature of the ability estimates and previous empirical research, the results showed clear distinctions between MLE and the Bayesian methods, with MLE yielding lower bias, higher standard errors, higher root mean square errors, lower fidelity, and lower administrative efficiency. Standard errors for MLE based on test information underestimated actual standard errors, whereas standard errors for the Bayesian methods based on posterior distribution standard deviations accurately estimated actual standard errors. Among the Bayesian methods, Owen's provided the worst overall results, and EAP provided the best. Using a variable starting rule in which examinees were initially classified into three broad ability groups greatly reduced the bias for the Bayesian methods, but had little effect on the results for MLE. On the basis of these results, guidelines are offered for selecting appropriate CAT ability estimation methods in different decision contexts. This article is based on a doctoral dissertation (Wang, 1995) that was directed by Walter P. Vispoel. We thank Roberto de la Torre for his assistance in developing software for the computer simulation analyses and Rebecca Zwick and the anonymous reviewers for their helpful comments on the original manuscripts. Correspondence concerning this article should be addressed to the first author.","['simulation', 'computerized', 'adaptive', 'test', 'CATs', 'evaluate', 'result', 'yield', 'commonly', 'ability', 'estimation', 'method', 'maximum', 'likelihood', 'estimation', 'MLE', 'bayesian', 'approachesOwens', 'method', 'expect', 'posteriori', 'EAP', 'maximum', 'posteriori', 'line', 'theoretical', 'nature', 'ability', 'estimate', 'previous', 'empirical', 'research', 'result', 'clear', 'distinction', 'MLE', 'bayesian', 'method', 'MLE', 'yield', 'low', 'bias', 'high', 'standard', 'error', 'high', 'root', 'mean', 'square', 'error', 'low', 'fidelity', 'low', 'administrative', 'efficiency', 'Standard', 'error', 'MLE', 'base', 'test', 'information', 'underestimate', 'actual', 'standard', 'error', 'standard', 'error', 'bayesian', 'method', 'base', 'posterior', 'distribution', 'standard', 'deviation', 'accurately', 'estimate', 'actual', 'standard', 'error', 'bayesian', 'method', 'Owens', 'provide', 'bad', 'overall', 'result', 'EAP', 'provide', 'good', 'variable', 'starting', 'rule', 'examinee', 'initially', 'classify', 'broad', 'ability', 'group', 'greatly', 'reduce', 'bias', 'bayesian', 'method', 'little', 'effect', 'result', 'MLE', 'basis', 'result', 'guideline', 'offer', 'select', 'appropriate', 'CAT', 'ability', 'estimation', 'method', 'different', 'decision', 'context', 'article', 'base', 'doctoral', 'dissertation', 'Wang', '1995', 'direct', 'Walter', 'P', 'Vispoel', 'thank', 'Roberto', 'de', 'la', 'Torre', 'assistance', 'develop', 'software', 'computer', 'simulation', 'analysis', 'Rebecca', 'Zwick', 'anonymous', 'reviewer', 'helpful', 'comment', 'original', 'manuscript', 'correspondence', 'concern', 'article', 'address', 'author']","['property', 'ability', 'estimation', 'method', 'computerized', 'adaptive', 'testing']",simulation computerized adaptive test CATs evaluate result yield commonly ability estimation method maximum likelihood estimation MLE bayesian approachesOwens method expect posteriori EAP maximum posteriori line theoretical nature ability estimate previous empirical research result clear distinction MLE bayesian method MLE yield low bias high standard error high root mean square error low fidelity low administrative efficiency Standard error MLE base test information underestimate actual standard error standard error bayesian method base posterior distribution standard deviation accurately estimate actual standard error bayesian method Owens provide bad overall result EAP provide good variable starting rule examinee initially classify broad ability group greatly reduce bias bayesian method little effect result MLE basis result guideline offer select appropriate CAT ability estimation method different decision context article base doctoral dissertation Wang 1995 direct Walter P Vispoel thank Roberto de la Torre assistance develop software computer simulation analysis Rebecca Zwick anonymous reviewer helpful comment original manuscript correspondence concern article address author,property ability estimation method computerized adaptive testing,0.021335277950699682,0.02142623167502847,0.021520793066003872,0.02124078519590798,0.91447691211236,0.056854265110566495,0.0,0.0,0.09371001540235066,0.0
Bennett R.E.; Steffen M.; Singley M.K.; Morley M.; Jacquemin D.,"Evaluating an automatically scorable, open-ended response type for measuring mathematical reasoning in computer-adaptive tests",1997,34,"The first generation of computer-based texts depends largely on multiple-choice items and constructed-response questions that can be scored through literal matches with a key. This study evaluated scoring accuracy and item functioning for an open-ended response type where correct answers, posed as mathematical expressions, can take many different surface forms. Items were administered to 1,864 participants in field trials of a new admissions test for quantitatively oriented graduate programs. Results showed automatic scoring to approximate the accuracy of multiple-choice scanning, with all processing errors stemming from examinees improperly entering responses. In addition, the items functioned similarly in difficulty, item-total relations, and male-female performance differences to other response types being considered for the measure.","Evaluating an automatically scorable, open-ended response type for measuring mathematical reasoning in computer-adaptive tests","The first generation of computer-based texts depends largely on multiple-choice items and constructed-response questions that can be scored through literal matches with a key. This study evaluated scoring accuracy and item functioning for an open-ended response type where correct answers, posed as mathematical expressions, can take many different surface forms. Items were administered to 1,864 participants in field trials of a new admissions test for quantitatively oriented graduate programs. Results showed automatic scoring to approximate the accuracy of multiple-choice scanning, with all processing errors stemming from examinees improperly entering responses. In addition, the items functioned similarly in difficulty, item-total relations, and male-female performance differences to other response types being considered for the measure.","['generation', 'computerbase', 'text', 'depend', 'largely', 'multiplechoice', 'item', 'constructedresponse', 'question', 'score', 'literal', 'match', 'key', 'study', 'evaluate', 'scoring', 'accuracy', 'item', 'function', 'openende', 'response', 'type', 'correct', 'answer', 'pose', 'mathematical', 'expression', 'different', 'surface', 'form', 'item', 'administer', '1864', 'participant', 'field', 'trial', 'new', 'admission', 'test', 'quantitatively', 'orient', 'graduate', 'program', 'result', 'automatic', 'scoring', 'approximate', 'accuracy', 'multiplechoice', 'scanning', 'processing', 'error', 'stem', 'examinee', 'improperly', 'enter', 'response', 'addition', 'item', 'function', 'similarly', 'difficulty', 'itemtotal', 'relation', 'malefemale', 'performance', 'difference', 'response', 'type', 'consider', 'measure']","['evaluate', 'automatically', 'scorable', 'openende', 'response', 'type', 'measure', 'mathematical', 'reasoning', 'computeradaptive', 'test']",generation computerbase text depend largely multiplechoice item constructedresponse question score literal match key study evaluate scoring accuracy item function openende response type correct answer pose mathematical expression different surface form item administer 1864 participant field trial new admission test quantitatively orient graduate program result automatic scoring approximate accuracy multiplechoice scanning processing error stem examinee improperly enter response addition item function similarly difficulty itemtotal relation malefemale performance difference response type consider measure,evaluate automatically scorable openende response type measure mathematical reasoning computeradaptive test,0.024611952084813985,0.025044697366560325,0.024842404666300544,0.899836210169635,0.025664735712689984,0.0026535194492985008,0.09243724410625873,0.005563959816664664,0.015623745574873827,0.0
Williamson D.M.; Bejar I.I.; Hone A.S.,'Mental model' comparison of automated and human scoring,1999,36,"'Mental models' used by automated scoring for the simulation divisions of the computerized Architect Registration Examination are contrasted with those used by experienced human graders. Candidate solutions (N = 3613) received both automated and human holistic scores. Quantitative analyses suggest high correspondence between automated and human scores; thereby suggesting similar mental models are implemented. Solutions with discrepancies between automated and human scores were selected for qualitative analysis. The human graders were reconvened to review the human scores and to investigate the source of score discrepancies in light of rationales provided by the automated scoring process. After review, slightly more than half of the score discrepancies were reduced or eliminated. Six sources of discrepancy between original human scores and automated scores were identified: subjective criteria; objective criteria; tolerances/ weighting; details; examinee task interpretation; and unjustified. The tendency of the human graders to be compelled by automated score rationales varied by the nature of original score discrepancy. We determine that, while the automated scores are based on a mental model consistent with that of expert graders, there remain some important differences, both intentional and incidental, which distinguish between human and automated scoring. We conclude that automated scoring has the potential to enhance the validity evidence of scores in addition to improving efficiency.",'Mental model' comparison of automated and human scoring,"'Mental models' used by automated scoring for the simulation divisions of the computerized Architect Registration Examination are contrasted with those used by experienced human graders. Candidate solutions (N = 3613) received both automated and human holistic scores. Quantitative analyses suggest high correspondence between automated and human scores; thereby suggesting similar mental models are implemented. Solutions with discrepancies between automated and human scores were selected for qualitative analysis. The human graders were reconvened to review the human scores and to investigate the source of score discrepancies in light of rationales provided by the automated scoring process. After review, slightly more than half of the score discrepancies were reduced or eliminated. Six sources of discrepancy between original human scores and automated scores were identified: subjective criteria; objective criteria; tolerances/ weighting; details; examinee task interpretation; and unjustified. The tendency of the human graders to be compelled by automated score rationales varied by the nature of original score discrepancy. We determine that, while the automated scores are based on a mental model consistent with that of expert graders, there remain some important differences, both intentional and incidental, which distinguish between human and automated scoring. We conclude that automated scoring has the potential to enhance the validity evidence of scores in addition to improving efficiency.","['mental', 'automate', 'scoring', 'simulation', 'division', 'computerized', 'Architect', 'Registration', 'Examination', 'contrast', 'experienced', 'human', 'grader', 'candidate', 'solution', 'N', '3613', 'receive', 'automate', 'human', 'holistic', 'score', 'Quantitative', 'analysis', 'suggest', 'high', 'correspondence', 'automate', 'human', 'score', 'suggest', 'similar', 'mental', 'implement', 'solution', 'discrepancy', 'automated', 'human', 'score', 'select', 'qualitative', 'analysis', 'human', 'grader', 'reconvene', 'review', 'human', 'score', 'investigate', 'source', 'score', 'discrepancy', 'light', 'rationale', 'provide', 'automate', 'scoring', 'process', 'review', 'slightly', 'half', 'score', 'discrepancy', 'reduce', 'eliminate', 'source', 'discrepancy', 'original', 'human', 'score', 'automate', 'score', 'identify', 'subjective', 'criterion', 'objective', 'criterion', 'tolerance', 'weight', 'detail', 'examinee', 'task', 'interpretation', 'unjustified', 'tendency', 'human', 'grader', 'compel', 'automate', 'score', 'rationale', 'vary', 'nature', 'original', 'score', 'discrepancy', 'determine', 'automate', 'score', 'base', 'mental', 'consistent', 'expert', 'grader', 'remain', 'important', 'difference', 'intentional', 'incidental', 'distinguish', 'human', 'automate', 'scoring', 'conclude', 'automate', 'scoring', 'potential', 'enhance', 'validity', 'evidence', 'score', 'addition', 'improve', 'efficiency']","['mental', 'comparison', 'automate', 'human', 'scoring']",mental automate scoring simulation division computerized Architect Registration Examination contrast experienced human grader candidate solution N 3613 receive automate human holistic score Quantitative analysis suggest high correspondence automate human score suggest similar mental implement solution discrepancy automated human score select qualitative analysis human grader reconvene review human score investigate source score discrepancy light rationale provide automate scoring process review slightly half score discrepancy reduce eliminate source discrepancy original human score automate score identify subjective criterion objective criterion tolerance weight detail examinee task interpretation unjustified tendency human grader compel automate score rationale vary nature original score discrepancy determine automate score base mental consistent expert grader remain important difference intentional incidental distinguish human automate scoring conclude automate scoring potential enhance validity evidence score addition improve efficiency,mental comparison automate human scoring,0.8765946968766408,0.030714091334030517,0.03131324817475793,0.030071368928652725,0.0313065946859179,0.0,0.00702606905904409,0.0,0.09316930591503692,0.007724525013434585
Béland A.; Mislevy R.J.,Probability-based inference in a domain of proportional reasoning tasks,1996,33,"Educators and psychologists are increasingly interested in modeling the processes and knowledge structures by which people learn and solve problems. Meaningful progress has been made in developing cognitive models in several domains and in devising observational settings that provide clues about subjects' cognition from this perspective. Less attention has been paid to drawing inferences or making decisions with such data, even though observations provide only imperfect information about cognition, and complex interrelationships can exist among observations and theoretical variables. This article discusses the use of probability-based reasoning to explicate hypothesized and empirical relationships and to structure inference in this context. Ideas are illustrated with an example concerning proportional reasoning.",Probability-based inference in a domain of proportional reasoning tasks,"Educators and psychologists are increasingly interested in modeling the processes and knowledge structures by which people learn and solve problems. Meaningful progress has been made in developing cognitive models in several domains and in devising observational settings that provide clues about subjects' cognition from this perspective. Less attention has been paid to drawing inferences or making decisions with such data, even though observations provide only imperfect information about cognition, and complex interrelationships can exist among observations and theoretical variables. This article discusses the use of probability-based reasoning to explicate hypothesized and empirical relationships and to structure inference in this context. Ideas are illustrated with an example concerning proportional reasoning.","['educator', 'psychologist', 'increasingly', 'interested', 'process', 'knowledge', 'structure', 'people', 'learn', 'solve', 'problem', 'meaningful', 'progress', 'develop', 'cognitive', 'domain', 'devise', 'observational', 'setting', 'provide', 'clue', 'subject', 'cognition', 'perspective', 'Less', 'attention', 'pay', 'draw', 'inference', 'decision', 'datum', 'observation', 'provide', 'imperfect', 'information', 'cognition', 'complex', 'interrelationship', 'exist', 'observation', 'theoretical', 'variable', 'article', 'discuss', 'probabilitybase', 'reasoning', 'explicate', 'hypothesized', 'empirical', 'relationship', 'structure', 'inference', 'context', 'idea', 'illustrate', 'example', 'concern', 'proportional', 'reasoning']","['probabilitybase', 'inference', 'domain', 'proportional', 'reasoning', 'task']",educator psychologist increasingly interested process knowledge structure people learn solve problem meaningful progress develop cognitive domain devise observational setting provide clue subject cognition perspective Less attention pay draw inference decision datum observation provide imperfect information cognition complex interrelationship exist observation theoretical variable article discuss probabilitybase reasoning explicate hypothesized empirical relationship structure inference context idea illustrate example concern proportional reasoning,probabilitybase inference domain proportional reasoning task,0.026106263924551697,0.026096358000602185,0.026226681667787383,0.02587974719450442,0.8956909492125543,0.0,0.02728096178874952,0.0,0.0,0.034440212211870304
Lane S.; Liu M.; Ankenmann R.D.; Stone C.A.,Generalizability and validity of a mathematics performance assessment,1996,33,"The QUASAR Cognitive Assessment Instrument (QCAI) is designed to measure program outcomes and growth in mathematics. It consists of a relatively large set of open-ended tasks that assess mathematical problem solving, reasoning, and communication at the middle-school grade levels. This study provides some evidence for the generalizability and validity of the assessment. The results from the generalizability studies indicate that the error due to raters is minimal, whereas there is considerable differential student performance across tasks. The dependability of grade level scores for absolute decision making is encouraging; when the number of students is equal to 350, the coefficients are between .80 and .97 depending on the form and grade level. As expected, there tended to be a higher relationship between the QCAI scores and both the problem solving and conceptual subtest scores from a mathematics achievement multiple-choice test than between the QCAI scores and the mathematics computation subtest scores.",Generalizability and validity of a mathematics performance assessment,"The QUASAR Cognitive Assessment Instrument (QCAI) is designed to measure program outcomes and growth in mathematics. It consists of a relatively large set of open-ended tasks that assess mathematical problem solving, reasoning, and communication at the middle-school grade levels. This study provides some evidence for the generalizability and validity of the assessment. The results from the generalizability studies indicate that the error due to raters is minimal, whereas there is considerable differential student performance across tasks. The dependability of grade level scores for absolute decision making is encouraging; when the number of students is equal to 350, the coefficients are between .80 and .97 depending on the form and grade level. As expected, there tended to be a higher relationship between the QCAI scores and both the problem solving and conceptual subtest scores from a mathematics achievement multiple-choice test than between the QCAI scores and the mathematics computation subtest scores.","['QUASAR', 'Cognitive', 'Assessment', 'Instrument', 'QCAI', 'design', 'measure', 'program', 'outcome', 'growth', 'mathematic', 'consist', 'relatively', 'large', 'set', 'openende', 'task', 'assess', 'mathematical', 'problem', 'solve', 'reasoning', 'communication', 'middleschool', 'grade', 'level', 'study', 'provide', 'evidence', 'generalizability', 'validity', 'assessment', 'result', 'generalizability', 'study', 'indicate', 'error', 'rater', 'minimal', 'considerable', 'differential', 'student', 'performance', 'task', 'dependability', 'grade', 'level', 'score', 'absolute', 'decision', 'making', 'encourage', 'number', 'student', 'equal', '350', 'coefficient', '80', '97', 'depend', 'form', 'grade', 'level', 'expect', 'tend', 'high', 'relationship', 'QCAI', 'score', 'problem', 'solve', 'conceptual', 'subtest', 'score', 'mathematics', 'achievement', 'multiplechoice', 'test', 'QCAI', 'score', 'mathematics', 'computation', 'subt', 'score']","['generalizability', 'validity', 'mathematic', 'performance', 'assessment']",QUASAR Cognitive Assessment Instrument QCAI design measure program outcome growth mathematic consist relatively large set openende task assess mathematical problem solve reasoning communication middleschool grade level study provide evidence generalizability validity assessment result generalizability study indicate error rater minimal considerable differential student performance task dependability grade level score absolute decision making encourage number student equal 350 coefficient 80 97 depend form grade level expect tend high relationship QCAI score problem solve conceptual subtest score mathematics achievement multiplechoice test QCAI score mathematics computation subt score,generalizability validity mathematic performance assessment,0.8989486327272286,0.02536705742349736,0.025322611050581714,0.024997723547757273,0.02536397525093522,0.0,0.0,0.0,0.045770445840433834,0.14065480223133187
Tatsuoka K.K.; Tatsuoka M.M.,Computerized cognitive diagnostic adaptive testing: Effect on remedial instruction as empirical validation,1997,34,"The purpose of this study is to show the usefulness of cognitive diagnoses for remedial instruction. Cognitive diagnoses were done by an adaptive testing system using the rule-space methodology, which was developed by K. K. Tatsuoka and her associates (K. K. Tatsuoka, 1983, 1990; K. K. Tatsuoka & M. M. Tatsuoka, 1987; M. M. Tatsuoka & K. K. Tatsuoka, 1989). The results of the study strongly indicate that knowing students' knowledge states prior to remediation is very effective and that the rule-space method can effectively diagnose students' knowledge states and can point out ways for remediating their errors quickly with minimum effort. It is also found that the design of instructional units for remediation can be effectively guided by the rule-space model, because the determination of all possible knowledge states in a domain of interest, given an incidence matrix, is based on a partially ordered tree structure of knowledge states, which is equivalent to item-score patterns determined logically from the incidence matrix.",Computerized cognitive diagnostic adaptive testing: Effect on remedial instruction as empirical validation,"The purpose of this study is to show the usefulness of cognitive diagnoses for remedial instruction. Cognitive diagnoses were done by an adaptive testing system using the rule-space methodology, which was developed by K. K. Tatsuoka and her associates (K. K. Tatsuoka, 1983, 1990; K. K. Tatsuoka & M. M. Tatsuoka, 1987; M. M. Tatsuoka & K. K. Tatsuoka, 1989). The results of the study strongly indicate that knowing students' knowledge states prior to remediation is very effective and that the rule-space method can effectively diagnose students' knowledge states and can point out ways for remediating their errors quickly with minimum effort. It is also found that the design of instructional units for remediation can be effectively guided by the rule-space model, because the determination of all possible knowledge states in a domain of interest, given an incidence matrix, is based on a partially ordered tree structure of knowledge states, which is equivalent to item-score patterns determined logically from the incidence matrix.","['purpose', 'study', 'usefulness', 'cognitive', 'diagnosis', 'remedial', 'instruction', 'cognitive', 'diagnosis', 'adaptive', 'testing', 'system', 'rulespace', 'methodology', 'develop', 'K', 'K', 'Tatsuoka', 'associate', 'K', 'K', 'Tatsuoka', '1983', '1990', 'K', 'K', 'Tatsuoka', 'M', 'M', 'Tatsuoka', '1987', 'M', 'M', 'Tatsuoka', 'K', 'K', 'Tatsuoka', '1989', 'result', 'study', 'strongly', 'indicate', 'know', 'student', 'knowledge', 'state', 'prior', 'remediation', 'effective', 'rulespace', 'method', 'effectively', 'diagnose', 'student', 'knowledge', 'state', 'point', 'way', 'remediate', 'error', 'quickly', 'minimum', 'effort', 'find', 'design', 'instructional', 'unit', 'remediation', 'effectively', 'guide', 'rulespace', 'determination', 'possible', 'knowledge', 'state', 'domain', 'interest', 'incidence', 'matrix', 'base', 'partially', 'order', 'tree', 'structure', 'knowledge', 'state', 'equivalent', 'itemscore', 'pattern', 'determine', 'logically', 'incidence', 'matrix']","['computerized', 'cognitive', 'diagnostic', 'adaptive', 'testing', 'Effect', 'remedial', 'instruction', 'empirical', 'validation']",purpose study usefulness cognitive diagnosis remedial instruction cognitive diagnosis adaptive testing system rulespace methodology develop K K Tatsuoka associate K K Tatsuoka 1983 1990 K K Tatsuoka M M Tatsuoka 1987 M M Tatsuoka K K Tatsuoka 1989 result study strongly indicate know student knowledge state prior remediation effective rulespace method effectively diagnose student knowledge state point way remediate error quickly minimum effort find design instructional unit remediation effectively guide rulespace determination possible knowledge state domain interest incidence matrix base partially order tree structure knowledge state equivalent itemscore pattern determine logically incidence matrix,computerized cognitive diagnostic adaptive testing Effect remedial instruction empirical validation,0.03162606500467838,0.03155728666072909,0.031689759883864216,0.6835364418979902,0.22159044655273805,0.0015197653578827351,0.01320286396125491,0.0,0.005438755624649559,0.027389355941088333
Brennan R.L.,The Conventional Wisdom About Group Mean Scores,1995,32,"Not infrequently, investigators assume that reliability for groups is greater than reliability for persons, and/or that error variance for groups is less than error variance for persons. Using generalizability theory, it is shown that this conventional wisdom is not necessarily true. Examples are provided from the course evaluation literature and the performance testing literature Copyright © 1995, Wiley Blackwell. All rights reserved",,"Not infrequently, investigators assume that reliability for groups is greater than reliability for persons, and/or that error variance for groups is less than error variance for persons. Using generalizability theory, it is shown that this conventional wisdom is not necessarily true. Examples are provided from the course evaluation literature and the performance testing literature Copyright © 1995, Wiley Blackwell. All rights reserved","['infrequently', 'investigator', 'assume', 'reliability', 'group', 'great', 'reliability', 'person', 'andor', 'error', 'variance', 'group', 'error', 'variance', 'person', 'generalizability', 'theory', 'conventional', 'wisdom', 'necessarily', 'true', 'example', 'provide', 'course', 'evaluation', 'literature', 'performance', 'testing', 'literature', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']",,infrequently investigator assume reliability group great reliability person andor error variance group error variance person generalizability theory conventional wisdom necessarily true example provide course evaluation literature performance testing literature Copyright © 1995 Wiley Blackwell right reserve,,0.03472536532486184,0.03472040816671162,0.8609849055619102,0.03464593664155459,0.03492338430496179,0.0052837150724615095,0.002595655630013773,0.008423014562869827,0.07623065987216116,0.030089996682594915
Powers D.E.; Fowles M.E.; Farnum M.; Ramsey P.,They Think Less of My Handwritten Essay If Others Word Process Theirs? Effects on Essay Scores of Intermingling Handwritten and Word‐Processed Essays,1994,31,"A study was undertaken to determine the effects on essay scores of intermingling handwritten and word‐processed versions of student essays. A sample of examinees, each of whom had produced both a handwritten and a word‐processed essay, was drawn from a larger sample of students who had participated in a pilot study of a new academic skills assessment battery. Students’original handwritten essays were converted to word‐processed versions, and their original word‐processed essays were converted to handwritten versions. Analyses revealed higher average scores for essays scored in the handwritten mode than for essays scored as word processed, regardless of the mode in which essays were originally produced. Several hypotheses were advanced to explain the discrepancies between scores on handwritten and word‐processed essays. The training of essay readers was subsequently modified on the basis of these hypotheses, and the experiment was repeated using the modified training with a new set of readers. Copyright © 1994, Wiley Blackwell. All rights reserved",They Think Less of My Handwritten Essay If Others Word Process Theirs? Effects on Essay Scores of Intermingling Handwritten and Word‐Processed Essays,"A study was undertaken to determine the effects on essay scores of intermingling handwritten and word‐processed versions of student essays. A sample of examinees, each of whom had produced both a handwritten and a word‐processed essay, was drawn from a larger sample of students who had participated in a pilot study of a new academic skills assessment battery. Students’original handwritten essays were converted to word‐processed versions, and their original word‐processed essays were converted to handwritten versions. Analyses revealed higher average scores for essays scored in the handwritten mode than for essays scored as word processed, regardless of the mode in which essays were originally produced. Several hypotheses were advanced to explain the discrepancies between scores on handwritten and word‐processed essays. The training of essay readers was subsequently modified on the basis of these hypotheses, and the experiment was repeated using the modified training with a new set of readers. Copyright © 1994, Wiley Blackwell. All rights reserved","['study', 'undertake', 'determine', 'effect', 'essay', 'score', 'intermingle', 'handwritten', 'word‐processe', 'version', 'student', 'essay', 'sample', 'examinee', 'produce', 'handwritten', 'word‐processed', 'essay', 'draw', 'large', 'sample', 'student', 'participate', 'pilot', 'study', 'new', 'academic', 'skill', 'assessment', 'battery', 'students’original', 'handwritten', 'essay', 'convert', 'word‐processe', 'version', 'original', 'word‐processe', 'essay', 'convert', 'handwritten', 'version', 'Analyses', 'reveal', 'high', 'average', 'score', 'essay', 'score', 'handwritten', 'mode', 'essay', 'score', 'word', 'process', 'regardless', 'mode', 'essay', 'originally', 'produce', 'hypothesis', 'advanced', 'explain', 'discrepancy', 'score', 'handwritten', 'word‐processe', 'essay', 'training', 'essay', 'reader', 'subsequently', 'modify', 'basis', 'hypothesis', 'experiment', 'repeat', 'modify', 'training', 'new', 'set', 'reader', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['think', 'Less', 'Handwritten', 'Essay', 'Others', 'Word', 'Process', 'Theirs', 'effect', 'Essay', 'Scores', 'intermingle', 'Handwritten', 'word‐processe', 'essay']",study undertake determine effect essay score intermingle handwritten word‐processe version student essay sample examinee produce handwritten word‐processed essay draw large sample student participate pilot study new academic skill assessment battery students’original handwritten essay convert word‐processe version original word‐processe essay convert handwritten version Analyses reveal high average score essay score handwritten mode essay score word process regardless mode essay originally produce hypothesis advanced explain discrepancy score handwritten word‐processe essay training essay reader subsequently modify basis hypothesis experiment repeat modify training new set reader Copyright © 1994 Wiley Blackwell right reserve,think Less Handwritten Essay Others Word Process Theirs effect Essay Scores intermingle Handwritten word‐processe essay,0.03523717469946256,0.035343178877949785,0.859824078208167,0.03447825689711645,0.03511731131730429,0.0,0.004019042387336159,0.0,0.04478132729221885,0.03468843777223824
Otter M.E.; Mellenbergh G.J.; Glopper K.d.,The Relation Between Information‐Processing Variables and Test‐Retest Stability for Questionnaire Items,1995,32,"Recently developed cognitive theories explain why some questionnaire items can be answered in a reliable and valid manner and others cannot. Those theories distinguish two components: (a) the interpretation or understanding of a question and (b) the role of memory. The present study investigates the ability of these two components to forecast the test‐retest association coefficients of 207 pilot questionnaire items used in an international study of reading literacy in which 2 populations were involved: Grade 5 and Grade 2 of primary and secondary education, respectively. The analysis of the data showed that for both populations, both components forecast the relative sizes of the test‐retest correlation coefficients. The results strongly suggest that if one wishes to use questionnaire items in research about relationships, then the items should be as unambiguous as possible. Moreover, the information needed to formulate an answer must be easily accessible in the respondent's memory. Copyright © 1995, Wiley Blackwell. All rights reserved",The Relation Between Information‐Processing Variables and Test‐Retest Stability for Questionnaire Items,"Recently developed cognitive theories explain why some questionnaire items can be answered in a reliable and valid manner and others cannot. Those theories distinguish two components: (a) the interpretation or understanding of a question and (b) the role of memory. The present study investigates the ability of these two components to forecast the test‐retest association coefficients of 207 pilot questionnaire items used in an international study of reading literacy in which 2 populations were involved: Grade 5 and Grade 2 of primary and secondary education, respectively. The analysis of the data showed that for both populations, both components forecast the relative sizes of the test‐retest correlation coefficients. The results strongly suggest that if one wishes to use questionnaire items in research about relationships, then the items should be as unambiguous as possible. Moreover, the information needed to formulate an answer must be easily accessible in the respondent's memory. Copyright © 1995, Wiley Blackwell. All rights reserved","['recently', 'develop', 'cognitive', 'theory', 'explain', 'questionnaire', 'item', 'answer', 'reliable', 'valid', 'manner', 'theory', 'distinguish', 'component', 'interpretation', 'understanding', 'question', 'b', 'role', 'memory', 'present', 'study', 'investigate', 'ability', 'component', 'forecast', 'test‐ret', 'association', 'coefficient', '207', 'pilot', 'questionnaire', 'item', 'international', 'study', 'read', 'literacy', '2', 'population', 'involve', 'Grade', '5', 'Grade', '2', 'primary', 'secondary', 'respectively', 'analysis', 'datum', 'population', 'component', 'forecast', 'relative', 'size', 'test‐ret', 'correlation', 'coefficient', 'result', 'strongly', 'suggest', 'wish', 'questionnaire', 'item', 'research', 'relationship', 'item', 'unambiguous', 'possible', 'information', 'need', 'formulate', 'answer', 'easily', 'accessible', 'respondent', 'memory', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Relation', 'Information‐Processing', 'Variables', 'test‐ret', 'Stability', 'Questionnaire', 'Items']",recently develop cognitive theory explain questionnaire item answer reliable valid manner theory distinguish component interpretation understanding question b role memory present study investigate ability component forecast test‐ret association coefficient 207 pilot questionnaire item international study read literacy 2 population involve Grade 5 Grade 2 primary secondary respectively analysis datum population component forecast relative size test‐ret correlation coefficient result strongly suggest wish questionnaire item research relationship item unambiguous possible information need formulate answer easily accessible respondent memory Copyright © 1995 Wiley Blackwell right reserve,Relation Information‐Processing Variables test‐ret Stability Questionnaire Items,0.8983135918171575,0.025599775803070605,0.02538719278273669,0.024972648131271966,0.0257267914657633,0.007470031037623903,0.06314704256811171,0.003011860397561558,0.0,0.026082596371727853
Engelhard Jr. G.,"Clarification to ""examining rater errors in the assessment of written composition with a many-faceted rasch model""",1996,33,"In the article ""Examining Rater Errors in the Assessment of Written Composition With a Many-Faceted Rasch Model"" (JEM, Volume 31, Number 2, Summer 1994), the data presented in Figure 3 may be misleading. The ""four clear spikes"" (p. 106) that appear in Figure 3 were highlighted by the automatic scaling procedure used by the computer program that generated this histogram; as is well known, the use of different scaling units would yield histograms with different shapes (Moore & McCabe, 1993). For example, when the same data
are presented as a bar chart (see Figure 1 below) rather than as a histogram, the four spikes are not evident. As graphical procedures become more readily available to measurement researchers, additional research and discussion are needed regarding standards for evaluating data displays that do not simply reproduce the actual data values.","Clarification to ""examining rater errors in the assessment of written composition with a many-faceted rasch model""","In the article ""Examining Rater Errors in the Assessment of Written Composition With a Many-Faceted Rasch Model"" (JEM, Volume 31, Number 2, Summer 1994), the data presented in Figure 3 may be misleading. The ""four clear spikes"" (p. 106) that appear in Figure 3 were highlighted by the automatic scaling procedure used by the computer program that generated this histogram; as is well known, the use of different scaling units would yield histograms with different shapes (Moore & McCabe, 1993). For example, when the same data
are presented as a bar chart (see Figure 1 below) rather than as a histogram, the four spikes are not evident. As graphical procedures become more readily available to measurement researchers, additional research and discussion are needed regarding standards for evaluating data displays that do not simply reproduce the actual data values.","['article', 'examine', 'Rater', 'error', 'Assessment', 'Written', 'Composition', 'ManyFaceted', 'Rasch', 'JEM', 'Volume', '31', 'number', '2', 'summer', '1994', 'datum', 'present', 'figure', '3', 'mislead', 'clear', 'spike', 'p', '106', 'appear', 'figure', '3', 'highlight', 'automatic', 'scaling', 'procedure', 'computer', 'program', 'generate', 'histogram', 'know', 'different', 'scaling', 'unit', 'yield', 'histogram', 'different', 'shape', 'Moore', 'McCabe', '1993', 'example', 'datum', 'present', 'bar', 'chart', 'Figure', '1', 'histogram', 'spike', 'evident', 'graphical', 'procedure', 'readily', 'available', 'researcher', 'additional', 'research', 'discussion', 'need', 'regard', 'standard', 'evaluate', 'data', 'display', 'simply', 'reproduce', 'actual', 'data', 'value']","['clarification', 'examine', 'rater', 'error', 'assessment', 'write', 'composition', 'manyfaceted', 'rasch']",article examine Rater error Assessment Written Composition ManyFaceted Rasch JEM Volume 31 number 2 summer 1994 datum present figure 3 mislead clear spike p 106 appear figure 3 highlight automatic scaling procedure computer program generate histogram know different scaling unit yield histogram different shape Moore McCabe 1993 example datum present bar chart Figure 1 histogram spike evident graphical procedure readily available researcher additional research discussion need regard standard evaluate data display simply reproduce actual data value,clarification examine rater error assessment write composition manyfaceted rasch,0.02551949153819098,0.02545176866869179,0.025555125236115875,0.6602879016790587,0.2631857128779427,0.00993024044557928,0.0,0.010558658192262425,0.014077403296445547,0.04060009552630019
Janssen R.; De Boeck P.,The contribution of a response-production component to a free-response synonym task,1996,33,"A cognitive approach to the study of format differences is illustrated using synonym tasks. By means of a multiple regression analysis with latent variables, it is shown that both a response-production component and an evaluation component are involved in answering a free-response synonym task. Given the results of Janssen, De Boeck, and Vander Steene (1996), the format differences between the multiple-choice evaluation task and the free-response synonym task can be explained in terms of the kinds of verbal abilities measured. The evaluation task is a pure measure of verbal comprehension, while the free-response synonym task is affected by verbal comprehension and verbal fluency, as well. The design used to study format differences controls both for content effects and for the effects of repeating item stems across formats.",The contribution of a response-production component to a free-response synonym task,"A cognitive approach to the study of format differences is illustrated using synonym tasks. By means of a multiple regression analysis with latent variables, it is shown that both a response-production component and an evaluation component are involved in answering a free-response synonym task. Given the results of Janssen, De Boeck, and Vander Steene (1996), the format differences between the multiple-choice evaluation task and the free-response synonym task can be explained in terms of the kinds of verbal abilities measured. The evaluation task is a pure measure of verbal comprehension, while the free-response synonym task is affected by verbal comprehension and verbal fluency, as well. The design used to study format differences controls both for content effects and for the effects of repeating item stems across formats.","['cognitive', 'approach', 'study', 'format', 'difference', 'illustrate', 'synonym', 'task', 'mean', 'multiple', 'regression', 'analysis', 'latent', 'variable', 'responseproduction', 'component', 'evaluation', 'component', 'involve', 'answer', 'freeresponse', 'synonym', 'task', 'result', 'Janssen', 'De', 'Boeck', 'Vander', 'Steene', '1996', 'format', 'difference', 'multiplechoice', 'evaluation', 'task', 'freeresponse', 'synonym', 'task', 'explain', 'term', 'kind', 'verbal', 'ability', 'measure', 'evaluation', 'task', 'pure', 'measure', 'verbal', 'comprehension', 'freeresponse', 'synonym', 'task', 'affect', 'verbal', 'comprehension', 'verbal', 'fluency', 'design', 'study', 'format', 'difference', 'control', 'content', 'effect', 'effect', 'repeat', 'item', 'stem', 'format']","['contribution', 'responseproduction', 'component', 'freeresponse', 'synonym', 'task']",cognitive approach study format difference illustrate synonym task mean multiple regression analysis latent variable responseproduction component evaluation component involve answer freeresponse synonym task result Janssen De Boeck Vander Steene 1996 format difference multiplechoice evaluation task freeresponse synonym task explain term kind verbal ability measure evaluation task pure measure verbal comprehension freeresponse synonym task affect verbal comprehension verbal fluency design study format difference control content effect effect repeat item stem format,contribution responseproduction component freeresponse synonym task,0.03255130264719135,0.8700546254754062,0.032554708160721146,0.03202212578617314,0.032817237930508114,0.0,0.05034059391013104,0.0,0.0,0.036648674486518366
Feldt L.S.,Confidence intervals for the proportion of mastery in criterion-referenced measurement,1996,33,"A relatively simple method is developed to obtain confidence intervals for a student's proportion of domain mastery in criterion-referenced or mastery measurement situations. The method uses the binomial distribution as a model for the student's scores under hypothetically repeated assessments. Though the use of this model is not a new idea, the method of implementation has not been proposed previously. The technique makes use of widely available F tables and hence does not require elaborate computer equipment or proprietary computer programs.",Confidence intervals for the proportion of mastery in criterion-referenced measurement,"A relatively simple method is developed to obtain confidence intervals for a student's proportion of domain mastery in criterion-referenced or mastery measurement situations. The method uses the binomial distribution as a model for the student's scores under hypothetically repeated assessments. Though the use of this model is not a new idea, the method of implementation has not been proposed previously. The technique makes use of widely available F tables and hence does not require elaborate computer equipment or proprietary computer programs.","['relatively', 'simple', 'method', 'develop', 'obtain', 'confidence', 'interval', 'student', 'proportion', 'domain', 'mastery', 'criterionreference', 'mastery', 'situation', 'method', 'binomial', 'distribution', 'student', 'score', 'hypothetically', 'repeat', 'assessment', 'new', 'idea', 'method', 'implementation', 'propose', 'previously', 'technique', 'widely', 'available', 'F', 'table', 'require', 'elaborate', 'computer', 'equipment', 'proprietary', 'computer', 'program']","['confidence', 'interval', 'proportion', 'mastery', 'criterionreference']",relatively simple method develop obtain confidence interval student proportion domain mastery criterionreference mastery situation method binomial distribution student score hypothetically repeat assessment new idea method implementation propose previously technique widely available F table require elaborate computer equipment proprietary computer program,confidence interval proportion mastery criterionreference,0.8775813092845146,0.030474923331917276,0.030620544343120434,0.030313028072623777,0.031010194967823767,0.012633759565037697,0.0,0.0008503130179380841,0.05876124274722073,0.02924190387975753
Impara J.C.; Plake B.S.,Standard setting: An alternative approach,1997,34,"Since 1971 there have been a number of studies in which a cut score has been set using a method proposed by Angoff (1971). In this method, each member of a panel of judges estimates for each test question the proportion correct for a specific target group of examinees. Prior and contemporary research suggests that this is a difficult task for judges. Angoff also proposed that judges simply indicate whether or not an examinee from the target group will be able to answer each question correctly (the yes/no method). We report on the results of two studies that compare a yes/no estimation with a proportion correct estimation. The two studies demonstrate that both methods produce essentially equal cut scores and that judges find the yes/no method more comfortable to use than the estimated proportion correct method.",,"Since 1971 there have been a number of studies in which a cut score has been set using a method proposed by Angoff (1971). In this method, each member of a panel of judges estimates for each test question the proportion correct for a specific target group of examinees. Prior and contemporary research suggests that this is a difficult task for judges. Angoff also proposed that judges simply indicate whether or not an examinee from the target group will be able to answer each question correctly (the yes/no method). We report on the results of two studies that compare a yes/no estimation with a proportion correct estimation. The two studies demonstrate that both methods produce essentially equal cut scores and that judges find the yes/no method more comfortable to use than the estimated proportion correct method.","['1971', 'number', 'study', 'cut', 'score', 'set', 'method', 'propose', 'Angoff', '1971', 'method', 'member', 'panel', 'judge', 'estimate', 'test', 'question', 'proportion', 'correct', 'specific', 'target', 'group', 'examinee', 'prior', 'contemporary', 'research', 'suggest', 'difficult', 'task', 'judge', 'Angoff', 'propose', 'judge', 'simply', 'indicate', 'examinee', 'target', 'group', 'able', 'answer', 'question', 'correctly', 'yesno', 'method', 'report', 'result', 'study', 'compare', 'yesno', 'estimation', 'proportion', 'correct', 'estimation', 'study', 'demonstrate', 'method', 'produce', 'essentially', 'equal', 'cut', 'score', 'judge', 'find', 'yesno', 'method', 'comfortable', 'estimate', 'proportion', 'correct', 'method']",,1971 number study cut score set method propose Angoff 1971 method member panel judge estimate test question proportion correct specific target group examinee prior contemporary research suggest difficult task judge Angoff propose judge simply indicate examinee target group able answer question correctly yesno method report result study compare yesno estimation proportion correct estimation study demonstrate method produce essentially equal cut score judge find yesno method comfortable estimate proportion correct method,,0.03118535053257437,0.03141217769210809,0.8748579162032688,0.030802957554626403,0.031741598017422365,0.05275702115447793,0.00158703014379931,0.0,0.0668481448135623,0.0005270610275965729
Schulz E.M.; Nicewander W.A.,Grade equivalent and IRT representations of growth,1997,34,"It has long been a part of psychometric lore that the variance of children's scores on cognitive tests increases with age. This increasing-variance phenomenon was first observed on Binet's intelligence measures in the early 1900s. An important detail in this matter is the fact that developmental scales based on age or grade have served as the medium for demonstrating the increasing-variance phenomenon. Recently, developmental scales based on item response theory (IRT) have shown constant or decreasing variance of measures of achievement with increasing age. This discrepancy is of practical and theoretical importance. Conclusions about the effects of variables on growth in achievement will depend on the metric chosen. In this study, growth in the mean of a latent educational achievement variable is assumed to be a negatively accelerated function of grade; within-grade variance is assumed to be constant across grade, and observed test scores are assumed to follow an IRT model. Under these assumptions, the variance of grade equivalent scores increases markedly. Perspective on this phenomenon is gained by examining longitudinal trends in centimeter and age equivalent measures of height.",Grade equivalent and IRT representations of growth,"It has long been a part of psychometric lore that the variance of children's scores on cognitive tests increases with age. This increasing-variance phenomenon was first observed on Binet's intelligence measures in the early 1900s. An important detail in this matter is the fact that developmental scales based on age or grade have served as the medium for demonstrating the increasing-variance phenomenon. Recently, developmental scales based on item response theory (IRT) have shown constant or decreasing variance of measures of achievement with increasing age. This discrepancy is of practical and theoretical importance. Conclusions about the effects of variables on growth in achievement will depend on the metric chosen. In this study, growth in the mean of a latent educational achievement variable is assumed to be a negatively accelerated function of grade; within-grade variance is assumed to be constant across grade, and observed test scores are assumed to follow an IRT model. Under these assumptions, the variance of grade equivalent scores increases markedly. Perspective on this phenomenon is gained by examining longitudinal trends in centimeter and age equivalent measures of height.","['long', 'psychometric', 'lore', 'variance', 'children', 'score', 'cognitive', 'test', 'increase', 'age', 'increasingvariance', 'phenomenon', 'observe', 'binet', 'intelligence', 'measure', 'early', '1900', 'important', 'detail', 'matter', 'fact', 'developmental', 'scale', 'base', 'age', 'grade', 'serve', 'medium', 'demonstrate', 'increasingvariance', 'phenomenon', 'recently', 'developmental', 'scale', 'base', 'item', 'response', 'theory', 'IRT', 'constant', 'decrease', 'variance', 'measure', 'achievement', 'increase', 'age', 'discrepancy', 'practical', 'theoretical', 'importance', 'Conclusions', 'effect', 'variable', 'growth', 'achievement', 'depend', 'metric', 'choose', 'study', 'growth', 'mean', 'latent', 'educational', 'achievement', 'variable', 'assume', 'negatively', 'accelerate', 'function', 'grade', 'withingrade', 'variance', 'assume', 'constant', 'grade', 'observe', 'test', 'score', 'assume', 'follow', 'IRT', 'assumption', 'variance', 'grade', 'equivalent', 'score', 'increase', 'markedly', 'Perspective', 'phenomenon', 'gain', 'examine', 'longitudinal', 'trend', 'centimeter', 'age', 'equivalent', 'measure', 'height']","['grade', 'equivalent', 'IRT', 'representation', 'growth']",long psychometric lore variance children score cognitive test increase age increasingvariance phenomenon observe binet intelligence measure early 1900 important detail matter fact developmental scale base age grade serve medium demonstrate increasingvariance phenomenon recently developmental scale base item response theory IRT constant decrease variance measure achievement increase age discrepancy practical theoretical importance Conclusions effect variable growth achievement depend metric choose study growth mean latent educational achievement variable assume negatively accelerate function grade withingrade variance assume constant grade observe test score assume follow IRT assumption variance grade equivalent score increase markedly Perspective phenomenon gain examine longitudinal trend centimeter age equivalent measure height,grade equivalent IRT representation growth,0.025255423248291733,0.8987845629929022,0.025349409565635354,0.024785613029642717,0.025824991163527995,0.0,0.00904103408873098,0.002182395687854796,0.08364220707940828,0.0552361935182818
Fitzpatrick A.R.; Link V.B.; Yen W.M.; Burket G.R.; Ito K.; Sykes R.C.,Scaling performance assessments: A comparison of one-parameter and two-parameter partial credit models,1996,33,"In one study, parameters were estimated for constructed-response (CR) items in 8 tests from 4 operational testing programs using the 1-parameter and 2-parameter partial credit (1PPC and 2PPC) models. Where multiple-choice (MC) items were present, these models were combined with the 1-parameter and 3-parameter logistic (1PL and 3PL) models, respectively. We found that item fit was better when the 2PPC model was used alone or with the 3PL model. Also, the slopes of the CR and MC items were found to differ substantially. In a second study, item parameter estimates produced using the 1PL-1PPC and 3PL-2PPC model combinations were evaluated for fit to simulated data generated using true parameters known to fit one model combination or the other. The results suggested that the more flexible 3PL-2PPC model combination would produce better item fit than the 1PL-1PPC combination.",Scaling performance assessments: A comparison of one-parameter and two-parameter partial credit models,"In one study, parameters were estimated for constructed-response (CR) items in 8 tests from 4 operational testing programs using the 1-parameter and 2-parameter partial credit (1PPC and 2PPC) models. Where multiple-choice (MC) items were present, these models were combined with the 1-parameter and 3-parameter logistic (1PL and 3PL) models, respectively. We found that item fit was better when the 2PPC model was used alone or with the 3PL model. Also, the slopes of the CR and MC items were found to differ substantially. In a second study, item parameter estimates produced using the 1PL-1PPC and 3PL-2PPC model combinations were evaluated for fit to simulated data generated using true parameters known to fit one model combination or the other. The results suggested that the more flexible 3PL-2PPC model combination would produce better item fit than the 1PL-1PPC combination.","['study', 'parameter', 'estimate', 'constructedresponse', 'CR', 'item', '8', 'test', '4', 'operational', 'testing', 'program', '1parameter', '2parameter', 'partial', 'credit', '1PPC', '2ppc', 'multiplechoice', 'MC', 'item', 'present', 'combine', '1parameter', '3parameter', 'logistic', '1pl', '3pl', 'respectively', 'find', 'item', 'fit', '2PPC', '3pl', 'slope', 'CR', 'MC', 'item', 'find', 'differ', 'substantially', 'second', 'study', 'item', 'parameter', 'estimate', 'produce', '1pl1ppc', '3pl2ppc', 'combination', 'evaluate', 'fit', 'simulated', 'datum', 'generate', 'true', 'parameter', 'know', 'fit', 'combination', 'result', 'suggest', 'flexible', '3pl2ppc', 'combination', 'produce', 'item', 'fit', '1pl1ppc', 'combination']","['scale', 'performance', 'assessment', 'comparison', 'oneparameter', 'twoparameter', 'partial', 'credit']",study parameter estimate constructedresponse CR item 8 test 4 operational testing program 1parameter 2parameter partial credit 1PPC 2ppc multiplechoice MC item present combine 1parameter 3parameter logistic 1pl 3pl respectively find item fit 2PPC 3pl slope CR MC item find differ substantially second study item parameter estimate produce 1pl1ppc 3pl2ppc combination evaluate fit simulated datum generate true parameter know fit combination result suggest flexible 3pl2ppc combination produce item fit 1pl1ppc combination,scale performance assessment comparison oneparameter twoparameter partial credit,0.03018453120131921,0.8795914645408258,0.029967030420748865,0.0297340824861726,0.03052289135093343,0.02168683063406361,0.04064748275392826,0.02903556888888569,0.0012652797216643418,0.0
Douglas J.A.; Roussos L.A.; Stout W.F.,Item-bundle DIF hypothesis testing: Identifying suspect bundles and assessing their differential functioning,1996,33,"This article proposes two multidimensional IRT model-based methods of selecting item bundles (clusters of not necessarily adjacent items chosen according to some organizational principle) suspected of displaying DIF amplfication. The approach embodied in these two methods is inspired by Shealy and Stout's (1993a, 1993b) multidimensional model for DIF. Each bundle selected by these methods constitutes a DIF amplification hypothesis. When SIBTEST (Shealy & Stout, 1993b) confirms DIF amplification in selected bundles, differential bundle functioning (DBF) is said to occur. Three real data examples illustrate the two methods for suspect bundle selection. The effectiveness of the methods is argued on statistical grounds. A distinction between benign and adverse DIF is made. The decision whether flagged DIF items or DBF bundles display benign or adverse DIP/DBF must depend in part on nonstatistical construct validity arguments. Conducting DBF analyses using these methods should help in the identification of the causes of DIF/DBF.",Item-bundle DIF hypothesis testing: Identifying suspect bundles and assessing their differential functioning,"This article proposes two multidimensional IRT model-based methods of selecting item bundles (clusters of not necessarily adjacent items chosen according to some organizational principle) suspected of displaying DIF amplfication. The approach embodied in these two methods is inspired by Shealy and Stout's (1993a, 1993b) multidimensional model for DIF. Each bundle selected by these methods constitutes a DIF amplification hypothesis. When SIBTEST (Shealy & Stout, 1993b) confirms DIF amplification in selected bundles, differential bundle functioning (DBF) is said to occur. Three real data examples illustrate the two methods for suspect bundle selection. The effectiveness of the methods is argued on statistical grounds. A distinction between benign and adverse DIF is made. The decision whether flagged DIF items or DBF bundles display benign or adverse DIP/DBF must depend in part on nonstatistical construct validity arguments. Conducting DBF analyses using these methods should help in the identification of the causes of DIF/DBF.","['article', 'propose', 'multidimensional', 'IRT', 'modelbase', 'method', 'select', 'item', 'bundle', 'cluster', 'necessarily', 'adjacent', 'item', 'choose', 'accord', 'organizational', 'principle', 'suspect', 'display', 'DIF', 'amplfication', 'approach', 'embody', 'method', 'inspire', 'Shealy', 'Stouts', '1993a', '1993b', 'multidimensional', 'DIF', 'bundle', 'select', 'method', 'constitute', 'DIF', 'amplification', 'hypothesis', 'SIBTEST', 'Shealy', 'Stout', '1993b', 'confirm', 'DIF', 'amplification', 'select', 'bundle', 'differential', 'bundle', 'function', 'DBF', 'occur', 'real', 'datum', 'example', 'illustrate', 'method', 'suspect', 'bundle', 'selection', 'effectiveness', 'method', 'argue', 'statistical', 'ground', 'distinction', 'benign', 'adverse', 'DIF', 'decision', 'flagged', 'dif', 'item', 'DBF', 'bundle', 'display', 'benign', 'adverse', 'DIPDBF', 'depend', 'nonstatistical', 'construct', 'validity', 'argument', 'conduct', 'DBF', 'analysis', 'method', 'help', 'identification', 'cause', 'DIFDBF']","['itembundle', 'DIF', 'hypothesis', 'testing', 'identify', 'suspect', 'bundle', 'assess', 'differential', 'functioning']",article propose multidimensional IRT modelbase method select item bundle cluster necessarily adjacent item choose accord organizational principle suspect display DIF amplfication approach embody method inspire Shealy Stouts 1993a 1993b multidimensional DIF bundle select method constitute DIF amplification hypothesis SIBTEST Shealy Stout 1993b confirm DIF amplification select bundle differential bundle function DBF occur real datum example illustrate method suspect bundle selection effectiveness method argue statistical ground distinction benign adverse DIF decision flagged dif item DBF bundle display benign adverse DIPDBF depend nonstatistical construct validity argument conduct DBF analysis method help identification cause DIFDBF,itembundle DIF hypothesis testing identify suspect bundle assess differential functioning,0.028072061370746616,0.028162429679863897,0.027761760034767612,0.02757043985303942,0.8884333090615825,0.0,0.0,0.11412597907864948,0.017974723799573267,0.0
Bridgeman B.; Lewis C.,Gender differences in college mathematics grades and SAT-M scores: A reanalysis of Wainer and Steinberg,1996,33,"Wainer and Steinberg (1992) showed that within broad categories of first-year college mathematics courses (e.g., calculus) men had substantially higher average scores on the mathematics section of the SAT (SAT-M) than women who earned the same letter grade. However, three aspects of their analyses may lead to unwarranted conclusions. First, they focused primarily on differences in SAT-M scores given course grades when the more important question for admissions officers is the difference in course grades given scores on the predictor. Second, they failed to account for differences among calculus courses (e.g., calculus for engineers versus calculus for liberal arts students). Most importantly, Wainer and Steinberg focused on the use of SAT-M as a single predictor. A reanalysis presented here indicated that a more appropriate composite indicator made up of both SAT-M and high school grade point average demonstrated minuscule gender differences for both calculus and precalculus courses.",Gender differences in college mathematics grades and SAT-M scores: A reanalysis of Wainer and Steinberg,"Wainer and Steinberg (1992) showed that within broad categories of first-year college mathematics courses (e.g., calculus) men had substantially higher average scores on the mathematics section of the SAT (SAT-M) than women who earned the same letter grade. However, three aspects of their analyses may lead to unwarranted conclusions. First, they focused primarily on differences in SAT-M scores given course grades when the more important question for admissions officers is the difference in course grades given scores on the predictor. Second, they failed to account for differences among calculus courses (e.g., calculus for engineers versus calculus for liberal arts students). Most importantly, Wainer and Steinberg focused on the use of SAT-M as a single predictor. A reanalysis presented here indicated that a more appropriate composite indicator made up of both SAT-M and high school grade point average demonstrated minuscule gender differences for both calculus and precalculus courses.","['Wainer', 'Steinberg', '1992', 'broad', 'category', 'firstyear', 'college', 'mathematic', 'course', 'eg', 'calculus', 'man', 'substantially', 'high', 'average', 'score', 'mathematic', 'section', 'SAT', 'SATM', 'woman', 'earn', 'letter', 'grade', 'aspect', 'analysis', 'lead', 'unwarranted', 'conclusion', 'focus', 'primarily', 'difference', 'SATM', 'score', 'course', 'grade', 'important', 'question', 'admission', 'officer', 'difference', 'course', 'grade', 'score', 'predictor', 'Second', 'fail', 'account', 'difference', 'calculus', 'course', 'eg', 'calculus', 'engineer', 'versus', 'calculus', 'liberal', 'art', 'student', 'importantly', 'Wainer', 'Steinberg', 'focus', 'SATM', 'single', 'predictor', 'reanalysis', 'present', 'indicate', 'appropriate', 'composite', 'indicator', 'SATM', 'high', 'school', 'grade', 'point', 'average', 'demonstrate', 'minuscule', 'gender', 'difference', 'calculus', 'precalculus', 'course']","['gender', 'difference', 'college', 'mathematic', 'grade', 'SATM', 'score', 'reanalysis', 'Wainer', 'Steinberg']",Wainer Steinberg 1992 broad category firstyear college mathematic course eg calculus man substantially high average score mathematic section SAT SATM woman earn letter grade aspect analysis lead unwarranted conclusion focus primarily difference SATM score course grade important question admission officer difference course grade score predictor Second fail account difference calculus course eg calculus engineer versus calculus liberal art student importantly Wainer Steinberg focus SATM single predictor reanalysis present indicate appropriate composite indicator SATM high school grade point average demonstrate minuscule gender difference calculus precalculus course,gender difference college mathematic grade SATM score reanalysis Wainer Steinberg,0.029108651561440494,0.29074996388784813,0.029296858347825624,0.6207615926455194,0.030082933557366354,0.0,0.0012385788178985737,0.0,0.03311050019591343,0.060293216343354016
Vispoel W.P.; Rocklin T.R.; Wang T.; Bleiler T.,Can examinees use a review option to obtain positively biased ability estimates on a computerized adaptive test?,1999,36,"Part of the controversy about allowing examinees to review and change answers to previous items on computerized adaptive tests (CATs) centers on a strategy for obtaining positively biased ability estimates attributed to Wainer (1993) in which examinees intentionally answer items incorrectly before review and to the best of their abilities upon review. Our results, based on both simulated and live testing data, showed that there were instances in which the Wainer strategy yielded inflated ability estimates as well as instances in which it yielded deflated ability estimates. The success of the strategy in inflating ability estimates depended on the ability estimation method used (maximum likelihood versus Bayesian), the examinee's true ability level, the standard error of the ability estimate, the examinee's ability to implement the strategy, and the type of decision made from the ability estimate. We discuss approaches to dealing with the Wainer strategy in operational CAT settings.",Can examinees use a review option to obtain positively biased ability estimates on a computerized adaptive test?,"Part of the controversy about allowing examinees to review and change answers to previous items on computerized adaptive tests (CATs) centers on a strategy for obtaining positively biased ability estimates attributed to Wainer (1993) in which examinees intentionally answer items incorrectly before review and to the best of their abilities upon review. Our results, based on both simulated and live testing data, showed that there were instances in which the Wainer strategy yielded inflated ability estimates as well as instances in which it yielded deflated ability estimates. The success of the strategy in inflating ability estimates depended on the ability estimation method used (maximum likelihood versus Bayesian), the examinee's true ability level, the standard error of the ability estimate, the examinee's ability to implement the strategy, and the type of decision made from the ability estimate. We discuss approaches to dealing with the Wainer strategy in operational CAT settings.","['controversy', 'allow', 'examinee', 'review', 'change', 'answer', 'previous', 'item', 'computerized', 'adaptive', 'test', 'cats', 'center', 'strategy', 'obtain', 'positively', 'bias', 'ability', 'estimate', 'attribute', 'Wainer', '1993', 'examine', 'intentionally', 'answer', 'item', 'incorrectly', 'review', 'good', 'ability', 'review', 'result', 'base', 'simulated', 'live', 'testing', 'datum', 'instance', 'Wainer', 'strategy', 'yield', 'inflated', 'ability', 'estimate', 'instance', 'yield', 'deflate', 'ability', 'estimate', 'success', 'strategy', 'inflate', 'ability', 'estimate', 'depend', 'ability', 'estimation', 'method', 'maximum', 'likelihood', 'versus', 'Bayesian', 'examine', 'true', 'ability', 'level', 'standard', 'error', 'ability', 'estimate', 'examinee', 'ability', 'implement', 'strategy', 'type', 'decision', 'ability', 'estimate', 'discuss', 'approach', 'deal', 'Wainer', 'strategy', 'operational', 'CAT', 'setting']","['examinees', 'review', 'option', 'obtain', 'positively', 'bias', 'ability', 'estimate', 'computerized', 'adaptive', 'test']",controversy allow examinee review change answer previous item computerized adaptive test cats center strategy obtain positively bias ability estimate attribute Wainer 1993 examine intentionally answer item incorrectly review good ability review result base simulated live testing datum instance Wainer strategy yield inflated ability estimate instance yield deflate ability estimate success strategy inflate ability estimate depend ability estimation method maximum likelihood versus Bayesian examine true ability level standard error ability estimate examinee ability implement strategy type decision ability estimate discuss approach deal Wainer strategy operational CAT setting,examinees review option obtain positively bias ability estimate computerized adaptive test,0.02917568830480499,0.029223800044397973,0.02933033864383207,0.029045852440917466,0.8832243205660475,0.0679392374170111,0.03453096756124109,0.0,0.021572593955537834,0.0
Hosenfeld B.; Van Den Boom D.C.; Resing W.C.M.,Constructing geometric analogies for the longitudinal testing of elementary school children,1997,34,"This article reports the development of a new geometric analogies test for elementary school children and the examination of its utility for future longitudinal investigations. Analogies are intelligence test problems of the form A : B :: C : ?  with the constraint that the relation between A and B be equivalent to the relation between C and the answer term. An example of a simple geometric analogy is ""small circle : large circle :: small square : ?,"" where the correct answer is ""large square."" Geometric analogies can be assumed to measure analogical reasoning more purely than the often-used verbal analogies, because little vocabulary or specific domain knowledge is needed for their solution. In order to create a large item pool we used a facet-design matrix including six elements---circles, squares, triangles,  entagons, hexagons, and ellipses--and five transformations adding an element, changing size, halving, doubling, and changing position. By combining one or two of the six elements with one, two, or three of the five transformations, we constructed a theoretical item pool containing
12,150 items. From this item pool we selected items of several classes of difficulty so that every element and every transformation were represented with approximately the same frequency in the final test. In order to define the theoretical difficulty level of each item we used the following equation: Difficulty = 0.5 x Elements + 1 x Transformations, a simplified version of the equation obtained by Mulholland, Pellegrino, and Glaser (1980). Items with two or more equally correct solutions as described by Bethell-Fox, Lohman, and Snow (1984) were excluded.",Constructing geometric analogies for the longitudinal testing of elementary school children,"This article reports the development of a new geometric analogies test for elementary school children and the examination of its utility for future longitudinal investigations. Analogies are intelligence test problems of the form A : B :: C : ?  with the constraint that the relation between A and B be equivalent to the relation between C and the answer term. An example of a simple geometric analogy is ""small circle : large circle :: small square : ?,"" where the correct answer is ""large square."" Geometric analogies can be assumed to measure analogical reasoning more purely than the often-used verbal analogies, because little vocabulary or specific domain knowledge is needed for their solution. In order to create a large item pool we used a facet-design matrix including six elements---circles, squares, triangles,  entagons, hexagons, and ellipses--and five transformations adding an element, changing size, halving, doubling, and changing position. By combining one or two of the six elements with one, two, or three of the five transformations, we constructed a theoretical item pool containing
12,150 items. From this item pool we selected items of several classes of difficulty so that every element and every transformation were represented with approximately the same frequency in the final test. In order to define the theoretical difficulty level of each item we used the following equation: Difficulty = 0.5 x Elements + 1 x Transformations, a simplified version of the equation obtained by Mulholland, Pellegrino, and Glaser (1980). Items with two or more equally correct solutions as described by Bethell-Fox, Lohman, and Snow (1984) were excluded.","['article', 'report', 'development', 'new', 'geometric', 'analogy', 'test', 'elementary', 'school', 'child', 'examination', 'utility', 'future', 'longitudinal', 'investigation', 'analogy', 'intelligence', 'test', 'problem', 'form', 'b', 'c', 'constraint', 'relation', 'A', 'b', 'equivalent', 'relation', 'C', 'answer', 'term', 'example', 'simple', 'geometric', 'analogy', 'small', 'circle', 'large', 'circle', 'small', 'square', 'correct', 'answer', 'large', 'square', 'geometric', 'analogy', 'assume', 'measure', 'analogical', 'reasoning', 'purely', 'oftenused', 'verbal', 'analogy', 'little', 'vocabulary', 'specific', 'domain', 'knowledge', 'need', 'solution', 'order', 'create', 'large', 'item', 'pool', 'facetdesign', 'matrix', 'include', 'elementscircle', 'square', 'triangle', 'entagon', 'hexagon', 'ellipsesand', 'transformation', 'add', 'element', 'change', 'size', 'halve', 'double', 'change', 'position', 'combine', 'element', 'transformation', 'construct', 'theoretical', 'item', 'pool', 'contain', '12150', 'item', 'item', 'pool', 'select', 'item', 'class', 'difficulty', 'element', 'transformation', 'represent', 'approximately', 'frequency', 'final', 'test', 'order', 'define', 'theoretical', 'difficulty', 'level', 'item', 'follow', 'equation', 'Difficulty', '05', 'x', 'element', '1', 'x', 'transformation', 'simplified', 'version', 'equation', 'obtain', 'Mulholland', 'Pellegrino', 'Glaser', '1980', 'Items', 'equally', 'correct', 'solution', 'describe', 'BethellFox', 'Lohman', 'Snow', '1984', 'exclude']","['construct', 'geometric', 'analogy', 'longitudinal', 'testing', 'elementary', 'school', 'child']",article report development new geometric analogy test elementary school child examination utility future longitudinal investigation analogy intelligence test problem form b c constraint relation A b equivalent relation C answer term example simple geometric analogy small circle large circle small square correct answer large square geometric analogy assume measure analogical reasoning purely oftenused verbal analogy little vocabulary specific domain knowledge need solution order create large item pool facetdesign matrix include elementscircle square triangle entagon hexagon ellipsesand transformation add element change size halve double change position combine element transformation construct theoretical item pool contain 12150 item item pool select item class difficulty element transformation represent approximately frequency final test order define theoretical difficulty level item follow equation Difficulty 05 x element 1 x transformation simplified version equation obtain Mulholland Pellegrino Glaser 1980 Items equally correct solution describe BethellFox Lohman Snow 1984 exclude,construct geometric analogy longitudinal testing elementary school child,0.021446042801925046,0.021473356939166355,0.02150068768990593,0.4206210079220771,0.5149589046469256,0.0006692872996797948,0.0865750467421958,0.0012735328878578243,0.007300826454422578,7.22970120765896e-06
Yen W.M.; Burket G.R.,Comparison of item response theory and Thurstone methods of vertical scaling,1997,34,"Vertical achievement scales, which range from the lower elementary grades to high school, are used pervasively in educational assessment. Using simulated data modeled after real tests, the present article examines two procedures available for vertical scaling: a Thurstone method and three-parameter item response theory. Neither procedure produced artifactual scale shrinkage; both procedures produced modest scale expansion for one simulated condition.",Comparison of item response theory and Thurstone methods of vertical scaling,"Vertical achievement scales, which range from the lower elementary grades to high school, are used pervasively in educational assessment. Using simulated data modeled after real tests, the present article examines two procedures available for vertical scaling: a Thurstone method and three-parameter item response theory. Neither procedure produced artifactual scale shrinkage; both procedures produced modest scale expansion for one simulated condition.","['vertical', 'achievement', 'scale', 'range', 'low', 'elementary', 'grade', 'high', 'school', 'pervasively', 'educational', 'assessment', 'simulate', 'datum', 'real', 'test', 'present', 'article', 'examine', 'procedure', 'available', 'vertical', 'scale', 'Thurstone', 'method', 'threeparameter', 'item', 'response', 'theory', 'procedure', 'produce', 'artifactual', 'scale', 'shrinkage', 'procedure', 'produce', 'modest', 'scale', 'expansion', 'simulate', 'condition']","['comparison', 'item', 'response', 'theory', 'Thurstone', 'method', 'vertical', 'scaling']",vertical achievement scale range low elementary grade high school pervasively educational assessment simulate datum real test present article examine procedure available vertical scale Thurstone method threeparameter item response theory procedure produce artifactual scale shrinkage procedure produce modest scale expansion simulate condition,comparison item response theory Thurstone method vertical scaling,0.033477013342718145,0.8652961023202658,0.03358673591818908,0.033082205950473724,0.03455794246835344,0.008458328849533596,0.0,0.020693498789781196,0.0756035424301397,0.05754344623054505
Chang H.-H.; Mazzeo J.; Roussos L.,Detecting DIF for polytomously scored items: An adaptation of the SIBTEST procedure,1996,33,"Shealy and Stout (1993) proposed a DIF detection procedure called SIBTEST and demonstrated its utility with both simulated and real data sets. Current versions of SIBTEST can be used only for dichotomous items. In this article, an extension to handle polytomous items is developed. Two simulation studies are presented which compare the modified SIBTESTprocedure with the Mantel and standardized mean difference (SMD) procedures. The first study compares the procedures under conditions in which the Mantel and SMD procedures have been shown to perform well (Zwick, Donoghue, & Grima. 1993). Results of Study 1 suggest that SIBTEST performed reasonably well, but that the Mantel and SMD procedures performed slightly better. The second study uses data simulated under conditions in which observed-score DIF methods for dichotomous items have not performed well. The results of Study 2 indicate that under these conditions the modified SIBTEST procedure provides better control of impact-induced Type I error inflation than the other procedures.",Detecting DIF for polytomously scored items: An adaptation of the SIBTEST procedure,"Shealy and Stout (1993) proposed a DIF detection procedure called SIBTEST and demonstrated its utility with both simulated and real data sets. Current versions of SIBTEST can be used only for dichotomous items. In this article, an extension to handle polytomous items is developed. Two simulation studies are presented which compare the modified SIBTESTprocedure with the Mantel and standardized mean difference (SMD) procedures. The first study compares the procedures under conditions in which the Mantel and SMD procedures have been shown to perform well (Zwick, Donoghue, & Grima. 1993). Results of Study 1 suggest that SIBTEST performed reasonably well, but that the Mantel and SMD procedures performed slightly better. The second study uses data simulated under conditions in which observed-score DIF methods for dichotomous items have not performed well. The results of Study 2 indicate that under these conditions the modified SIBTEST procedure provides better control of impact-induced Type I error inflation than the other procedures.","['Shealy', 'Stout', '1993', 'propose', 'DIF', 'detection', 'procedure', 'SIBTEST', 'demonstrate', 'utility', 'simulated', 'real', 'datum', 'set', 'current', 'version', 'sibtest', 'dichotomous', 'item', 'article', 'extension', 'handle', 'polytomous', 'item', 'develop', 'simulation', 'study', 'present', 'compare', 'modify', 'SIBTESTprocedure', 'Mantel', 'standardized', 'mean', 'difference', 'smd', 'procedure', 'study', 'compare', 'procedure', 'condition', 'Mantel', 'SMD', 'procedure', 'perform', 'Zwick', 'Donoghue', 'Grima', '1993', 'result', 'Study', '1', 'suggest', 'SIBTEST', 'perform', 'reasonably', 'Mantel', 'SMD', 'procedure', 'perform', 'slightly', 'second', 'study', 'datum', 'simulate', 'condition', 'observedscore', 'dif', 'method', 'dichotomous', 'item', 'perform', 'result', 'Study', '2', 'indicate', 'condition', 'modify', 'SIBTEST', 'procedure', 'provide', 'control', 'impactinduced', 'Type', 'I', 'error', 'inflation', 'procedure']","['detect', 'DIF', 'polytomously', 'score', 'item', 'adaptation', 'SIBTEST', 'procedure']",Shealy Stout 1993 propose DIF detection procedure SIBTEST demonstrate utility simulated real datum set current version sibtest dichotomous item article extension handle polytomous item develop simulation study present compare modify SIBTESTprocedure Mantel standardized mean difference smd procedure study compare procedure condition Mantel SMD procedure perform Zwick Donoghue Grima 1993 result Study 1 suggest SIBTEST perform reasonably Mantel SMD procedure perform slightly second study datum simulate condition observedscore dif method dichotomous item perform result Study 2 indicate condition modify SIBTEST procedure provide control impactinduced Type I error inflation procedure,detect DIF polytomously score item adaptation SIBTEST procedure,0.8850405756957789,0.028609741919167837,0.028701156524339137,0.028235712414595018,0.029412813446119124,0.029288592883845516,0.0,0.09071153566775933,0.0059865867969631295,0.0049321551504764236
Schnipke D.L.; Green B.F.,A Comparison of Item Selection Routines in Linear and Adaptive Tests,1995,32,"Two item selection algorithms were compared in simulated linear and adaptive tests of cognitive ability. One algorithm selected items that maximally differentiated between examinees. The other used item response theory (IRT) to select items having maximum information for each examinee. Normally distributed populations of 1,000 cases were simulated, using test lengths of 4, 5, 6, and 7 items. Overall, adaptive tests based on maximum information provided the most information over the widest range of ability values and, in general, differentiated among examinees slightly better than the other tests. Although the maximum differentiation technique may be adequate in some circumstances, adaptive tests based on maximum information are clearly superior. Copyright © 1995, Wiley Blackwell. All rights reserved",A Comparison of Item Selection Routines in Linear and Adaptive Tests,"Two item selection algorithms were compared in simulated linear and adaptive tests of cognitive ability. One algorithm selected items that maximally differentiated between examinees. The other used item response theory (IRT) to select items having maximum information for each examinee. Normally distributed populations of 1,000 cases were simulated, using test lengths of 4, 5, 6, and 7 items. Overall, adaptive tests based on maximum information provided the most information over the widest range of ability values and, in general, differentiated among examinees slightly better than the other tests. Although the maximum differentiation technique may be adequate in some circumstances, adaptive tests based on maximum information are clearly superior. Copyright © 1995, Wiley Blackwell. All rights reserved","['item', 'selection', 'algorithm', 'compare', 'simulate', 'linear', 'adaptive', 'test', 'cognitive', 'ability', 'algorithm', 'select', 'item', 'maximally', 'differentiate', 'examine', 'item', 'response', 'theory', 'IRT', 'select', 'item', 'maximum', 'information', 'examinee', 'normally', 'distribute', 'population', '1000', 'case', 'simulate', 'test', 'length', '4', '5', '6', '7', 'item', 'overall', 'adaptive', 'test', 'base', 'maximum', 'information', 'provide', 'information', 'widest', 'range', 'ability', 'value', 'general', 'differentiate', 'examinee', 'slightly', 'test', 'maximum', 'differentiation', 'technique', 'adequate', 'circumstance', 'adaptive', 'test', 'base', 'maximum', 'information', 'clearly', 'superior', 'copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Item', 'Selection', 'Routines', 'Linear', 'Adaptive', 'Tests']",item selection algorithm compare simulate linear adaptive test cognitive ability algorithm select item maximally differentiate examine item response theory IRT select item maximum information examinee normally distribute population 1000 case simulate test length 4 5 6 7 item overall adaptive test base maximum information provide information widest range ability value general differentiate examinee slightly test maximum differentiation technique adequate circumstance adaptive test base maximum information clearly superior copyright © 1995 Wiley Blackwell right reserve,Comparison Item Selection Routines Linear Adaptive Tests,0.028651912561924394,0.028607801664187707,0.028684102631059923,0.028362361226397355,0.8856938219164306,0.02388396749540969,0.1038616913467837,0.0,0.0,0.0
Sheehan K.M.,A tree-based approach to proficiency scaling and diagnostic assessment,1997,34,"A new procedure for generating instructionally relevant diagnostic feedback is proposed. The approach involves first constructing a strong model of student proficiency and then testing whether individual students' observed item response vectors are consistent with that model. Diagnoses are specified in terms of the combinations of skills needed to score at increasingly higher levels on a test's reported score scale. The approach is applied to the problem of developing diagnostic feedback for the SAT 1 Verbal Reasoning test. Using a variation of Wright's (1977) person-fit statistic, it is shown that the estimated proficiency model accounts for 91% of the ""explainable"" variation in students' observed item response vectors.",A tree-based approach to proficiency scaling and diagnostic assessment,"A new procedure for generating instructionally relevant diagnostic feedback is proposed. The approach involves first constructing a strong model of student proficiency and then testing whether individual students' observed item response vectors are consistent with that model. Diagnoses are specified in terms of the combinations of skills needed to score at increasingly higher levels on a test's reported score scale. The approach is applied to the problem of developing diagnostic feedback for the SAT 1 Verbal Reasoning test. Using a variation of Wright's (1977) person-fit statistic, it is shown that the estimated proficiency model accounts for 91% of the ""explainable"" variation in students' observed item response vectors.","['new', 'procedure', 'generate', 'instructionally', 'relevant', 'diagnostic', 'feedback', 'propose', 'approach', 'involve', 'construct', 'strong', 'student', 'proficiency', 'test', 'individual', 'student', 'observe', 'item', 'response', 'vector', 'consistent', 'diagnosis', 'specify', 'term', 'combination', 'skill', 'need', 'score', 'increasingly', 'high', 'level', 'test', 'report', 'score', 'scale', 'approach', 'apply', 'problem', 'develop', 'diagnostic', 'feedback', 'SAT', '1', 'Verbal', 'reasoning', 'test', 'variation', 'Wrights', '1977', 'personfit', 'statistic', 'estimate', 'proficiency', 'account', '91', 'explainable', 'variation', 'student', 'observe', 'item', 'response', 'vector']","['treebase', 'approach', 'proficiency', 'scaling', 'diagnostic', 'assessment']",new procedure generate instructionally relevant diagnostic feedback propose approach involve construct strong student proficiency test individual student observe item response vector consistent diagnosis specify term combination skill need score increasingly high level test report score scale approach apply problem develop diagnostic feedback SAT 1 Verbal reasoning test variation Wrights 1977 personfit statistic estimate proficiency account 91 explainable variation student observe item response vector,treebase approach proficiency scaling diagnostic assessment,0.8890272140221736,0.02773218851763087,0.02746935772273799,0.027323467315522394,0.028447772421935223,0.0,0.06546143025248759,0.0007613239131321652,0.05259867235633659,0.0214192113935991
Engelhard Jr. G.,Evaluating rater accuracy in performance assessments,1996,33,"A new method for evaluating rater accuracy within the context of performance assessments is described. Accuracy is defined as the match between ratings obtained from operational raters and those obtained from an expert panel on a set of benchmark, exemplar, or anchor performances. An extended Rasch measurement model called the FACETS model is presented for examining rater accuracy. The FACETS model is illustrated with 373 benchmark papers rated by 20 operational raters and an expert panel. The data are from the 1993 field test of the High School Graduation Writing Test in Georgia. The data suggest that there are statistically significant differences in rater accuracy; the data also suggest that it is easier to be accurate on some benchmark papers than on others. A small example is presented to illustrate how the accuracy ordering of raters may not be invariant over different subsets of benchmarks used to evaluate accuracy.",Evaluating rater accuracy in performance assessments,"A new method for evaluating rater accuracy within the context of performance assessments is described. Accuracy is defined as the match between ratings obtained from operational raters and those obtained from an expert panel on a set of benchmark, exemplar, or anchor performances. An extended Rasch measurement model called the FACETS model is presented for examining rater accuracy. The FACETS model is illustrated with 373 benchmark papers rated by 20 operational raters and an expert panel. The data are from the 1993 field test of the High School Graduation Writing Test in Georgia. The data suggest that there are statistically significant differences in rater accuracy; the data also suggest that it is easier to be accurate on some benchmark papers than on others. A small example is presented to illustrate how the accuracy ordering of raters may not be invariant over different subsets of benchmarks used to evaluate accuracy.","['new', 'method', 'evaluate', 'rater', 'accuracy', 'context', 'performance', 'assessment', 'describe', 'Accuracy', 'define', 'match', 'rating', 'obtain', 'operational', 'rater', 'obtain', 'expert', 'panel', 'set', 'benchmark', 'exemplar', 'anchor', 'performance', 'extended', 'Rasch', 'facets', 'present', 'examine', 'rater', 'accuracy', 'facets', 'illustrate', '373', 'benchmark', 'paper', 'rate', '20', 'operational', 'rater', 'expert', 'panel', 'datum', '1993', 'field', 'test', 'High', 'School', 'Graduation', 'writing', 'Test', 'Georgia', 'datum', 'suggest', 'statistically', 'significant', 'difference', 'rater', 'accuracy', 'datum', 'suggest', 'easy', 'accurate', 'benchmark', 'paper', 'small', 'example', 'present', 'illustrate', 'accuracy', 'ordering', 'rater', 'invariant', 'different', 'subset', 'benchmark', 'evaluate', 'accuracy']","['evaluate', 'rater', 'accuracy', 'performance', 'assessment']",new method evaluate rater accuracy context performance assessment describe Accuracy define match rating obtain operational rater obtain expert panel set benchmark exemplar anchor performance extended Rasch facets present examine rater accuracy facets illustrate 373 benchmark paper rate 20 operational rater expert panel datum 1993 field test High School Graduation writing Test Georgia datum suggest statistically significant difference rater accuracy datum suggest easy accurate benchmark paper small example present illustrate accuracy ordering rater invariant different subset benchmark evaluate accuracy,evaluate rater accuracy performance assessment,0.0315408160599187,0.03137922136499861,0.03133383459524213,0.03106563713322598,0.8746804908466146,0.01004834563399398,0.0,0.0,0.0,0.0849036460039802
Kalohn J.C.; Spray J.A.,The effect of model misspecification on classification decisions made using a computerized test,1999,36,"Many computerized testing algorithms require the fitting of some item response theory (IRT) model to examinees' responses to facilitate item selection, the determination of test stopping rules, and classification decisions. Some IRT models are thought to be particularly useful for small volume certification programs that wish to make the transition to computerized adaptive testing (CAT). The one-parameter logistic model (1-PLM) is usually assumed to require a smaller sample size than the three-parameter logistic model (3-PLM) for item parameter calibrations. This study examined the effects of model misspecification on the precision of the decisions made using the sequential probability ratio test (SPRT). For this comparison, the 1-PLM was used to estimate item parameters, even though the items' characteristics were represented by a 3-PLM. Results demonstrated that the 1-PLM produced considerably more decision errors under simulation conditions similar to a real testing environment, compared to the true model and to a fixed-form standard reference set of items.",The effect of model misspecification on classification decisions made using a computerized test,"Many computerized testing algorithms require the fitting of some item response theory (IRT) model to examinees' responses to facilitate item selection, the determination of test stopping rules, and classification decisions. Some IRT models are thought to be particularly useful for small volume certification programs that wish to make the transition to computerized adaptive testing (CAT). The one-parameter logistic model (1-PLM) is usually assumed to require a smaller sample size than the three-parameter logistic model (3-PLM) for item parameter calibrations. This study examined the effects of model misspecification on the precision of the decisions made using the sequential probability ratio test (SPRT). For this comparison, the 1-PLM was used to estimate item parameters, even though the items' characteristics were represented by a 3-PLM. Results demonstrated that the 1-PLM produced considerably more decision errors under simulation conditions similar to a real testing environment, compared to the true model and to a fixed-form standard reference set of items.","['computerized', 'testing', 'algorithm', 'require', 'fitting', 'item', 'response', 'theory', 'IRT', 'examinees', 'response', 'facilitate', 'item', 'selection', 'determination', 'test', 'stop', 'rule', 'classification', 'decision', 'IRT', 'think', 'particularly', 'useful', 'small', 'volume', 'certification', 'program', 'wish', 'transition', 'computerized', 'adaptive', 'testing', 'CAT', 'oneparameter', 'logistic', '1PLM', 'usually', 'assume', 'require', 'small', 'sample', 'size', 'threeparameter', 'logistic', '3PLM', 'item', 'parameter', 'calibration', 'study', 'examine', 'effect', 'misspecification', 'precision', 'decision', 'sequential', 'probability', 'ratio', 'test', 'SPRT', 'comparison', '1PLM', 'estimate', 'item', 'parameter', 'item', 'characteristic', 'represent', '3PLM', 'Results', 'demonstrate', '1PLM', 'produce', 'considerably', 'decision', 'error', 'simulation', 'condition', 'similar', 'real', 'testing', 'environment', 'compare', 'true', 'fixedform', 'standard', 'reference', 'set', 'item']","['effect', 'misspecification', 'classification', 'decision', 'computerized', 'test']",computerized testing algorithm require fitting item response theory IRT examinees response facilitate item selection determination test stop rule classification decision IRT think particularly useful small volume certification program wish transition computerized adaptive testing CAT oneparameter logistic 1PLM usually assume require small sample size threeparameter logistic 3PLM item parameter calibration study examine effect misspecification precision decision sequential probability ratio test SPRT comparison 1PLM estimate item parameter item characteristic represent 3PLM Results demonstrate 1PLM produce considerably decision error simulation condition similar real testing environment compare true fixedform standard reference set item,effect misspecification classification decision computerized test,0.02393951045520724,0.023966090353000048,0.02382652419330636,0.02373422632479747,0.9045336486736889,0.062949636056319,0.04458338157058571,0.012352479765045245,0.0,0.0
Welch C.J.; Miller T.R.,Assessing Differential Item Functioning in Direct Writing Assessments: Problems and an Example,1995,32,"The recent emphasis on various types of performance assessments raises questions concerning the differential effects of such assessments on population subgroups. Procedures for detecting differential item functioning (DIF) in data from performance assessments are available but may be hindered by problems that stem from this mode of assessment. Foremost among these are problems related to finding an appropriate matching variable. These problems are discussed and results are presented for three methods for DIF detection in polytomous items using data from a direct writing assessment. The purpose of the study is to examine the effects of using different combinations of internal and external matching variables. The procedures included a generalized Mantel‐Haenszel statistic, a technique based on meta‐analysis methodology, and logistic discriminant function analysis. In general, the results did not support the use of an external matching criterion and indicated that continued problems may be expected in attempts to assess DIF in performance assessments. Copyright © 1995, Wiley Blackwell. All rights reserved",Assessing Differential Item Functioning in Direct Writing Assessments: Problems and an Example,"The recent emphasis on various types of performance assessments raises questions concerning the differential effects of such assessments on population subgroups. Procedures for detecting differential item functioning (DIF) in data from performance assessments are available but may be hindered by problems that stem from this mode of assessment. Foremost among these are problems related to finding an appropriate matching variable. These problems are discussed and results are presented for three methods for DIF detection in polytomous items using data from a direct writing assessment. The purpose of the study is to examine the effects of using different combinations of internal and external matching variables. The procedures included a generalized Mantel‐Haenszel statistic, a technique based on meta‐analysis methodology, and logistic discriminant function analysis. In general, the results did not support the use of an external matching criterion and indicated that continued problems may be expected in attempts to assess DIF in performance assessments. Copyright © 1995, Wiley Blackwell. All rights reserved","['recent', 'emphasis', 'type', 'performance', 'assessment', 'raise', 'question', 'concern', 'differential', 'effect', 'assessment', 'population', 'subgroup', 'procedure', 'detect', 'differential', 'item', 'function', 'DIF', 'datum', 'performance', 'assessment', 'available', 'hinder', 'problem', 'stem', 'mode', 'assessment', 'foremost', 'problem', 'relate', 'find', 'appropriate', 'matching', 'variable', 'problem', 'discuss', 'result', 'present', 'method', 'dif', 'detection', 'polytomous', 'item', 'datum', 'direct', 'writing', 'assessment', 'purpose', 'study', 'examine', 'effect', 'different', 'combination', 'internal', 'external', 'matching', 'variable', 'procedure', 'include', 'generalized', 'Mantel‐Haenszel', 'statistic', 'technique', 'base', 'meta‐analysis', 'methodology', 'logistic', 'discriminant', 'function', 'analysis', 'general', 'result', 'support', 'external', 'matching', 'criterion', 'indicate', 'continue', 'problem', 'expect', 'attempt', 'assess', 'DIF', 'performance', 'assessment', 'copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['assess', 'Differential', 'Item', 'Functioning', 'Direct', 'Writing', 'assessment', 'problem', 'example']",recent emphasis type performance assessment raise question concern differential effect assessment population subgroup procedure detect differential item function DIF datum performance assessment available hinder problem stem mode assessment foremost problem relate find appropriate matching variable problem discuss result present method dif detection polytomous item datum direct writing assessment purpose study examine effect different combination internal external matching variable procedure include generalized Mantel‐Haenszel statistic technique base meta‐analysis methodology logistic discriminant function analysis general result support external matching criterion indicate continue problem expect attempt assess DIF performance assessment copyright © 1995 Wiley Blackwell right reserve,assess Differential Item Functioning Direct Writing assessment problem example,0.02509473372673729,0.025633865553584084,0.8991065420432887,0.024688215051337396,0.025476643625052548,0.0,0.0,0.117850215412691,0.0,0.09988095508414013
Kim S.‐H.; Cohen A.S.; Park T.‐H.,Detection of Differential Item Functioning in Multiple Groups,1995,32,"Detection of differential item functioning (DIF) is most often done between two groups of examinees under item response theory. It is sometimes important, however, to determine whether DIF is present in more than two groups. In this article we present a method for detection of DIF in multiple groups. The method is closely related to Lard's chi‐square for comparing vectors of item parameters estimated in two groups. An example using real data is provided. Copyright © 1995, Wiley Blackwell. All rights reserved",Detection of Differential Item Functioning in Multiple Groups,"Detection of differential item functioning (DIF) is most often done between two groups of examinees under item response theory. It is sometimes important, however, to determine whether DIF is present in more than two groups. In this article we present a method for detection of DIF in multiple groups. The method is closely related to Lard's chi‐square for comparing vectors of item parameters estimated in two groups. An example using real data is provided. Copyright © 1995, Wiley Blackwell. All rights reserved","['detection', 'differential', 'item', 'function', 'DIF', 'group', 'examinee', 'item', 'response', 'theory', 'important', 'determine', 'DIF', 'present', 'group', 'article', 'present', 'method', 'detection', 'DIF', 'multiple', 'group', 'method', 'closely', 'relate', 'Lards', 'chi‐square', 'compare', 'vector', 'item', 'parameter', 'estimate', 'group', 'example', 'real', 'datum', 'provide', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['detection', 'Differential', 'Item', 'Functioning', 'Multiple', 'Groups']",detection differential item function DIF group examinee item response theory important determine DIF present group article present method detection DIF multiple group method closely relate Lards chi‐square compare vector item parameter estimate group example real datum provide Copyright © 1995 Wiley Blackwell right reserve,detection Differential Item Functioning Multiple Groups,0.03371776263885873,0.03377613723152824,0.8647388136843813,0.033291701619516126,0.03447558482571563,0.030888034395269846,0.0,0.17406246890244761,0.010989037822101784,0.0
Clauser B.E.; Nungester R.J.; Swaminathan H.,Improving the matching for DIF analysis by conditioning on both test score and an educational background variable,1996,33,"When tests are designed to measure dimensionally complex material, DIF analysis with matching based on the total test score may be inappropriate. Previous research has demonstrated that matching can be improved by using multiple internal or both internal and external measures to more completely account for the latent ability space. The present article extends this line of research by examining the potential to improve matching by conditioning simultaneously on test score and a categorical variable representing the educational background of the examinees. The responses of male and female examinees from a test of medical competence were analyzed using a logistic regression procedure. Results show a substantial reduction in the number of items identified as displaying significant DIF when conditioning is based on total test score and a variable representing educational background as opposed to total test score only.",Improving the matching for DIF analysis by conditioning on both test score and an educational background variable,"When tests are designed to measure dimensionally complex material, DIF analysis with matching based on the total test score may be inappropriate. Previous research has demonstrated that matching can be improved by using multiple internal or both internal and external measures to more completely account for the latent ability space. The present article extends this line of research by examining the potential to improve matching by conditioning simultaneously on test score and a categorical variable representing the educational background of the examinees. The responses of male and female examinees from a test of medical competence were analyzed using a logistic regression procedure. Results show a substantial reduction in the number of items identified as displaying significant DIF when conditioning is based on total test score and a variable representing educational background as opposed to total test score only.","['test', 'design', 'measure', 'dimensionally', 'complex', 'material', 'DIF', 'analysis', 'matching', 'base', 'total', 'test', 'score', 'inappropriate', 'previous', 'research', 'demonstrate', 'matching', 'improve', 'multiple', 'internal', 'internal', 'external', 'measure', 'completely', 'account', 'latent', 'ability', 'space', 'present', 'article', 'extend', 'line', 'research', 'examine', 'potential', 'improve', 'match', 'conditioning', 'simultaneously', 'test', 'score', 'categorical', 'variable', 'represent', 'educational', 'background', 'examinee', 'response', 'male', 'female', 'examinee', 'test', 'medical', 'competence', 'analyze', 'logistic', 'regression', 'procedure', 'result', 'substantial', 'reduction', 'number', 'item', 'identify', 'display', 'significant', 'dif', 'conditioning', 'base', 'total', 'test', 'score', 'variable', 'represent', 'educational', 'background', 'oppose', 'total', 'test', 'score']","['improve', 'matching', 'DIF', 'analysis', 'conditioning', 'test', 'score', 'educational', 'background', 'variable']",test design measure dimensionally complex material DIF analysis matching base total test score inappropriate previous research demonstrate matching improve multiple internal internal external measure completely account latent ability space present article extend line research examine potential improve match conditioning simultaneously test score categorical variable represent educational background examinee response male female examinee test medical competence analyze logistic regression procedure result substantial reduction number item identify display significant dif conditioning base total test score variable represent educational background oppose total test score,improve matching DIF analysis conditioning test score educational background variable,0.02591560906581314,0.0263802157622142,0.026124831393055582,0.46284342495683445,0.4587359188220827,0.0,0.046391131868977455,0.06961118763533795,0.07468697640371273,0.0
Mislevy R.J.,Test theory reconceived,1996,33,"Educational test theory consists of statistical and methodological tools to support inference about examinees' knowledge, skills, and accomplishments. Its evolution has been shaped by the nature of users' inferences, which have been framed almost exclusively in terms of trait and behavioral psychology, and focused on students' tendency to act in prespecified ways in prespecified domains of tasks. Progress in the methodology of test theory enabled users to extend the range of inference and ground interpretations more solidly within these psychological paradigms. Developments in cognitive and developmental psychology have broadened the range of inferences we wish to make about students' learning to encompass conjectures about the nature and acquisition of their knowledge. The same underlying principles of inference that led to standard test theory can support inference in this broader universe of discourse. Familiar models and methods - sometimes extended, sometimes reinterpreted, sometimes applied to problems wholly different from those for which they were first devised - can play a useful role to this end.",,"Educational test theory consists of statistical and methodological tools to support inference about examinees' knowledge, skills, and accomplishments. Its evolution has been shaped by the nature of users' inferences, which have been framed almost exclusively in terms of trait and behavioral psychology, and focused on students' tendency to act in prespecified ways in prespecified domains of tasks. Progress in the methodology of test theory enabled users to extend the range of inference and ground interpretations more solidly within these psychological paradigms. Developments in cognitive and developmental psychology have broadened the range of inferences we wish to make about students' learning to encompass conjectures about the nature and acquisition of their knowledge. The same underlying principles of inference that led to standard test theory can support inference in this broader universe of discourse. Familiar models and methods - sometimes extended, sometimes reinterpreted, sometimes applied to problems wholly different from those for which they were first devised - can play a useful role to this end.","['educational', 'test', 'theory', 'consist', 'statistical', 'methodological', 'tool', 'support', 'inference', 'examine', 'knowledge', 'skill', 'accomplishment', 'evolution', 'shape', 'nature', 'user', 'inference', 'frame', 'exclusively', 'term', 'trait', 'behavioral', 'psychology', 'focus', 'student', 'tendency', 'act', 'prespecified', 'way', 'prespecified', 'domain', 'task', 'Progress', 'methodology', 'test', 'theory', 'enable', 'user', 'extend', 'range', 'inference', 'ground', 'interpretation', 'solidly', 'psychological', 'paradigm', 'Developments', 'cognitive', 'developmental', 'psychology', 'broaden', 'range', 'inference', 'wish', 'student', 'learn', 'encompass', 'conjecture', 'nature', 'acquisition', 'knowledge', 'underlying', 'principle', 'inference', 'lead', 'standard', 'test', 'theory', 'support', 'inference', 'broad', 'universe', 'discourse', 'familiar', 'method', 'extend', 'reinterpret', 'apply', 'problem', 'wholly', 'different', 'devise', 'play', 'useful', 'role', 'end']",,educational test theory consist statistical methodological tool support inference examine knowledge skill accomplishment evolution shape nature user inference frame exclusively term trait behavioral psychology focus student tendency act prespecified way prespecified domain task Progress methodology test theory enable user extend range inference ground interpretation solidly psychological paradigm Developments cognitive developmental psychology broaden range inference wish student learn encompass conjecture nature acquisition knowledge underlying principle inference lead standard test theory support inference broad universe discourse familiar method extend reinterpret apply problem wholly different devise play useful role end,,0.025194530285488247,0.025263917873834443,0.8987504614853314,0.024971180386460692,0.0258199099688853,0.0,0.025731296933319093,0.0,0.012183977074163,0.0413748644764081
Attali Y.; Goldschmidt C.,The effects of component variables on performance in graph comprehension tests,1996,33,"Some cognitive characteristics of graph comprehension items were studied, and a model comprised of several variables was developed. 132 graph items of the Psychometric Entrance Test were included in the study. By analyzing the actual difficulty of the items, an evaluation of the impact of the cognitive variables on item difficulties could be made. Results indicate that successful prediction of item difficulty can be calculated on the basis of a wide range of item characteristics and task demands. This suggests that items can be screened for processing difficulty prior to being administered to examinees. However, the results also have implications for test validity in that the various processing variables identified involve distinct ability dimensions.",The effects of component variables on performance in graph comprehension tests,"Some cognitive characteristics of graph comprehension items were studied, and a model comprised of several variables was developed. 132 graph items of the Psychometric Entrance Test were included in the study. By analyzing the actual difficulty of the items, an evaluation of the impact of the cognitive variables on item difficulties could be made. Results indicate that successful prediction of item difficulty can be calculated on the basis of a wide range of item characteristics and task demands. This suggests that items can be screened for processing difficulty prior to being administered to examinees. However, the results also have implications for test validity in that the various processing variables identified involve distinct ability dimensions.","['cognitive', 'characteristic', 'graph', 'comprehension', 'item', 'study', 'comprise', 'variable', 'develop', '132', 'graph', 'item', 'Psychometric', 'Entrance', 'Test', 'include', 'study', 'analyze', 'actual', 'difficulty', 'item', 'evaluation', 'impact', 'cognitive', 'variable', 'item', 'difficulty', 'result', 'indicate', 'successful', 'prediction', 'item', 'difficulty', 'calculate', 'basis', 'wide', 'range', 'item', 'characteristic', 'task', 'demand', 'suggest', 'item', 'screen', 'processing', 'difficulty', 'prior', 'administer', 'examinee', 'result', 'implication', 'test', 'validity', 'processing', 'variable', 'identify', 'involve', 'distinct', 'ability', 'dimension']","['effect', 'component', 'variable', 'performance', 'graph', 'comprehension', 'test']",cognitive characteristic graph comprehension item study comprise variable develop 132 graph item Psychometric Entrance Test include study analyze actual difficulty item evaluation impact cognitive variable item difficulty result indicate successful prediction item difficulty calculate basis wide range item characteristic task demand suggest item screen processing difficulty prior administer examinee result implication test validity processing variable identify involve distinct ability dimension,effect component variable performance graph comprehension test,0.029404832504370468,0.029456791995283768,0.8823435724848142,0.02910884139375044,0.02968596162178111,0.0,0.12896536435936967,0.009020980397116409,0.0,0.0
Powers D.E.; Fowles M.E.,Effects of applying different time limits to a proposed GRE writing test,1996,33,"In order to determine the role of time limits on both test performance and test validity, we asked approximately 300 volunteers - prospective graduate students - to each write two essays - one in a 40-minute time period and the other in 60 minutes. Analyses revealed that, on average, test performance was significantly better when examinees were given 60 minutes instead of 40. However, there was no interaction between test-taking style (fast vs. slow) and time limits. That is, examinees who described themselves as slow writers/test takers did not benefit any more (or any less) from generous time limits than did their quicker counterparts. In addition, there was no detectable effect of different time limits on the meaning of essay scores, as suggested by their relationship to several nontest indicators of writing ability.",Effects of applying different time limits to a proposed GRE writing test,"In order to determine the role of time limits on both test performance and test validity, we asked approximately 300 volunteers - prospective graduate students - to each write two essays - one in a 40-minute time period and the other in 60 minutes. Analyses revealed that, on average, test performance was significantly better when examinees were given 60 minutes instead of 40. However, there was no interaction between test-taking style (fast vs. slow) and time limits. That is, examinees who described themselves as slow writers/test takers did not benefit any more (or any less) from generous time limits than did their quicker counterparts. In addition, there was no detectable effect of different time limits on the meaning of essay scores, as suggested by their relationship to several nontest indicators of writing ability.","['order', 'determine', 'role', 'time', 'limit', 'test', 'performance', 'test', 'validity', 'ask', 'approximately', '300', 'volunteer', 'prospective', 'graduate', 'student', 'write', 'essay', '40minute', 'time', 'period', '60', 'minute', 'analysis', 'reveal', 'average', 'test', 'performance', 'significantly', 'examinee', '60', 'minute', 'instead', '40', 'interaction', 'testtake', 'style', 'fast', 'vs', 'slow', 'time', 'limit', 'examinee', 'describe', 'slow', 'writerst', 'taker', 'benefit', 'generous', 'time', 'limit', 'quick', 'counterpart', 'addition', 'detectable', 'effect', 'different', 'time', 'limit', 'meaning', 'essay', 'score', 'suggest', 'relationship', 'nont', 'indicator', 'write', 'ability']","['effect', 'apply', 'different', 'time', 'limit', 'propose', 'GRE', 'write', 'test']",order determine role time limit test performance test validity ask approximately 300 volunteer prospective graduate student write essay 40minute time period 60 minute analysis reveal average test performance significantly examinee 60 minute instead 40 interaction testtake style fast vs slow time limit examinee describe slow writerst taker benefit generous time limit quick counterpart addition detectable effect different time limit meaning essay score suggest relationship nont indicator write ability,effect apply different time limit propose GRE write test,0.028147206817809495,0.8875083904038344,0.02813050955583357,0.027973577829380092,0.02824031539314235,0.0,0.047062858024367056,0.0,0.007657541390972193,0.02754910805305291
Osterlind S.J.; Friedman S.J.,"Constructing test items: Multiple-choice, constructed-response, performance, and other formats (2nd edition)",1999,36,"""The primary goal for this book is to contribute to the improvement of tests and measurement by aiding good test-item construction"" (p. 9). To accomplish this goal, the author articulates four key issues that must be addressed: 1. Description of the characteristics and functions of test items; 2. Presentation of editorial guidelines for writing test items; 3. Presentation of methods for determining the quality of test items; 4. Presentation of a compendium of important issues about test items.  Following an introduction (Chapter 1), the author devotes succeeding chapters to the purpose and characteristics of test items, the content of items from a validity perspective, practical considerations when writing items, guidelines for multiple-choice items, guidelines for constructed-response and performance items, guide-lines for other formats (i.e., true-false, matching, etc.), the use of item analysis to judge the quality of items, and ethical and legal considerations.","Constructing test items: Multiple-choice, constructed-response, performance, and other formats (2nd edition)","""The primary goal for this book is to contribute to the improvement of tests and measurement by aiding good test-item construction"" (p. 9). To accomplish this goal, the author articulates four key issues that must be addressed: 1. Description of the characteristics and functions of test items; 2. Presentation of editorial guidelines for writing test items; 3. Presentation of methods for determining the quality of test items; 4. Presentation of a compendium of important issues about test items.  Following an introduction (Chapter 1), the author devotes succeeding chapters to the purpose and characteristics of test items, the content of items from a validity perspective, practical considerations when writing items, guidelines for multiple-choice items, guidelines for constructed-response and performance items, guide-lines for other formats (i.e., true-false, matching, etc.), the use of item analysis to judge the quality of items, and ethical and legal considerations.","['primary', 'goal', 'book', 'contribute', 'improvement', 'test', 'aid', 'good', 'testitem', 'construction', 'p', '9', 'accomplish', 'goal', 'author', 'articulate', 'key', 'issue', 'address', '1', 'description', 'characteristic', 'function', 'test', 'item', '2', 'presentation', 'editorial', 'guideline', 'write', 'test', 'item', '3', 'presentation', 'method', 'determine', 'quality', 'test', 'item', '4', 'presentation', 'compendium', 'important', 'issue', 'test', 'item', 'follow', 'introduction', 'chapter', '1', 'author', 'devote', 'succeed', 'chapter', 'purpose', 'characteristic', 'test', 'item', 'content', 'item', 'validity', 'perspective', 'practical', 'consideration', 'write', 'item', 'guideline', 'multiplechoice', 'item', 'guideline', 'constructedresponse', 'performance', 'item', 'guideline', 'format', 'ie', 'truefalse', 'matching', 'etc', 'item', 'analysis', 'judge', 'quality', 'item', 'ethical', 'legal', 'consideration']","['construct', 'test', 'item', 'Multiplechoice', 'constructedresponse', 'performance', 'format', '2nd', 'edition']",primary goal book contribute improvement test aid good testitem construction p 9 accomplish goal author articulate key issue address 1 description characteristic function test item 2 presentation editorial guideline write test item 3 presentation method determine quality test item 4 presentation compendium important issue test item follow introduction chapter 1 author devote succeed chapter purpose characteristic test item content item validity perspective practical consideration write item guideline multiplechoice item guideline constructedresponse performance item guideline format ie truefalse matching etc item analysis judge quality item ethical legal consideration,construct test item Multiplechoice constructedresponse performance format 2nd edition,0.027043341712862233,0.8917099300018889,0.026957345293300763,0.02672093072936282,0.02756845226258531,0.0,0.10327108517346134,0.011037658251417332,0.0,0.0
Gibson W.M.; Weiner J.A.,Generating random parallel test forms using CTT in a computer-based environment,1998,35,"This paper describes a procedure for automated test forms assembly based on Classical Test Theory (CTT). The procedure uses stratified random content sampling and test form pre-equating to ensure both content and psychometric equivalence in generating virtually unlimited parallel forms. The procedure extends the usefulness of CTT in automated test form construction, yielding classical item statistics based on representative sample distributions and pre-equated test forms with known psychometric characteristics. A rationale for the procedure is presented followed by an example application and discussion of psychometric considerations related to its use.",Generating random parallel test forms using CTT in a computer-based environment,"This paper describes a procedure for automated test forms assembly based on Classical Test Theory (CTT). The procedure uses stratified random content sampling and test form pre-equating to ensure both content and psychometric equivalence in generating virtually unlimited parallel forms. The procedure extends the usefulness of CTT in automated test form construction, yielding classical item statistics based on representative sample distributions and pre-equated test forms with known psychometric characteristics. A rationale for the procedure is presented followed by an example application and discussion of psychometric considerations related to its use.","['paper', 'describe', 'procedure', 'automate', 'test', 'form', 'assembly', 'base', 'Classical', 'Test', 'Theory', 'CTT', 'procedure', 'stratify', 'random', 'content', 'sample', 'test', 'form', 'preequate', 'ensure', 'content', 'psychometric', 'equivalence', 'generate', 'virtually', 'unlimited', 'parallel', 'form', 'procedure', 'extend', 'usefulness', 'CTT', 'automate', 'test', 'form', 'construction', 'yield', 'classical', 'item', 'statistic', 'base', 'representative', 'sample', 'distribution', 'preequate', 'test', 'form', 'know', 'psychometric', 'characteristic', 'rationale', 'procedure', 'present', 'follow', 'example', 'application', 'discussion', 'psychometric', 'consideration', 'relate']","['generate', 'random', 'parallel', 'test', 'form', 'CTT', 'computerbased', 'environment']",paper describe procedure automate test form assembly base Classical Test Theory CTT procedure stratify random content sample test form preequate ensure content psychometric equivalence generate virtually unlimited parallel form procedure extend usefulness CTT automate test form construction yield classical item statistic base representative sample distribution preequate test form know psychometric characteristic rationale procedure present follow example application discussion psychometric consideration relate,generate random parallel test form CTT computerbased environment,0.030246291323570672,0.03013072932989829,0.030109075770172824,0.029944506995468193,0.87956939658089,0.060420700032127975,0.020868488981925323,0.0006926823547946534,0.00446167694219877,0.0
Frisbie D.A.; Cantor N.K.,The Validity of Scores From Alternative Methods of Assessing Spelling Achievement,1995,32,"The purpose of this study was to gather evidence to examine the validity of alternative methods for assessing the spelling achievements of students in Grades 2–7. Students in Grades 2, 3, 5, and 7 took both a dictation spelling test and an objective spelling test. Scores from the Iowa Tests of Basic Skills were available for nearly all students. Comparisons among item formats centered on difficulty, content coverage and efficiency, reliability, relationship with dictation scores, the influence of context, and relationship to overall achievement. Overall, no single objective format stood out above the others, but some demonstrated superiority to the dictation format on several dimensions. Copyright © 1995, Wiley Blackwell. All rights reserved",The Validity of Scores From Alternative Methods of Assessing Spelling Achievement,"The purpose of this study was to gather evidence to examine the validity of alternative methods for assessing the spelling achievements of students in Grades 2–7. Students in Grades 2, 3, 5, and 7 took both a dictation spelling test and an objective spelling test. Scores from the Iowa Tests of Basic Skills were available for nearly all students. Comparisons among item formats centered on difficulty, content coverage and efficiency, reliability, relationship with dictation scores, the influence of context, and relationship to overall achievement. Overall, no single objective format stood out above the others, but some demonstrated superiority to the dictation format on several dimensions. Copyright © 1995, Wiley Blackwell. All rights reserved","['purpose', 'study', 'gather', 'evidence', 'examine', 'validity', 'alternative', 'method', 'assess', 'spelling', 'achievement', 'student', 'Grades', '2–7', 'student', 'Grades', '2', '3', '5', '7', 'dictation', 'spelling', 'test', 'objective', 'spelling', 'test', 'score', 'Iowa', 'Tests', 'Basic', 'Skills', 'available', 'nearly', 'student', 'Comparisons', 'item', 'format', 'center', 'difficulty', 'content', 'coverage', 'efficiency', 'reliability', 'relationship', 'dictation', 'score', 'influence', 'context', 'relationship', 'overall', 'achievement', 'overall', 'single', 'objective', 'format', 'stand', 'demonstrate', 'superiority', 'dictation', 'format', 'dimension', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['Validity', 'Scores', 'alternative', 'Methods', 'assess', 'Spelling', 'Achievement']",purpose study gather evidence examine validity alternative method assess spelling achievement student Grades 2–7 student Grades 2 3 5 7 dictation spelling test objective spelling test score Iowa Tests Basic Skills available nearly student Comparisons item format center difficulty content coverage efficiency reliability relationship dictation score influence context relationship overall achievement overall single objective format stand demonstrate superiority dictation format dimension Copyright © 1995 Wiley Blackwell right reserve,Validity Scores alternative Methods assess Spelling Achievement,0.02908134662719438,0.029193404156805984,0.8834901114473922,0.028727571253303576,0.02950756651530391,0.0,0.05295458621099121,0.0,0.02971666950415818,0.047522302018011074
Nandakumar R.; Yu F.,Empirical validation of DIMTEST on nonnormal ability distributions,1996,33,"DIMTEST is a nonparametric statistical test procedure for assessing unidimensionality of binary item response data. The development of Stout's statistic, T, used in the DIMTEST procedure, does not require the assumption of a particular parametric form for the ability distributions or the item response functions. The purpose of the present study was to empirically investigate the performance of the statistic T with respect to different shapes of ability distributions. Several nonnormal distributions, both symmetric and nonsymmetric, were considered for this purpose. Other factors varied in the study were test length, sample size, and the level of correlation between abilities. The results of Type I error and power studies showed that the test statistic T exhibited consistently similar performance for all different shapes of ability distributions investigated in the study, which confirmed the nonparametric nature of the statistic T.",Empirical validation of DIMTEST on nonnormal ability distributions,"DIMTEST is a nonparametric statistical test procedure for assessing unidimensionality of binary item response data. The development of Stout's statistic, T, used in the DIMTEST procedure, does not require the assumption of a particular parametric form for the ability distributions or the item response functions. The purpose of the present study was to empirically investigate the performance of the statistic T with respect to different shapes of ability distributions. Several nonnormal distributions, both symmetric and nonsymmetric, were considered for this purpose. Other factors varied in the study were test length, sample size, and the level of correlation between abilities. The results of Type I error and power studies showed that the test statistic T exhibited consistently similar performance for all different shapes of ability distributions investigated in the study, which confirmed the nonparametric nature of the statistic T.","['DIMTEST', 'nonparametric', 'statistical', 'test', 'procedure', 'assess', 'unidimensionality', 'binary', 'item', 'response', 'datum', 'development', 'Stouts', 'statistic', 'T', 'dimtest', 'procedure', 'require', 'assumption', 'particular', 'parametric', 'form', 'ability', 'distribution', 'item', 'response', 'function', 'purpose', 'present', 'study', 'empirically', 'investigate', 'performance', 'statistic', 't', 'respect', 'different', 'shape', 'ability', 'distribution', 'nonnormal', 'distribution', 'symmetric', 'nonsymmetric', 'consider', 'purpose', 'factor', 'vary', 'study', 'test', 'length', 'sample', 'size', 'level', 'correlation', 'ability', 'result', 'Type', 'I', 'error', 'power', 'study', 'test', 'statistic', 'T', 'exhibit', 'consistently', 'similar', 'performance', 'different', 'shape', 'ability', 'distribution', 'investigate', 'study', 'confirm', 'nonparametric', 'nature', 'statistic', 't']","['empirical', 'validation', 'dimtest', 'nonnormal', 'ability', 'distribution']",DIMTEST nonparametric statistical test procedure assess unidimensionality binary item response datum development Stouts statistic T dimtest procedure require assumption particular parametric form ability distribution item response function purpose present study empirically investigate performance statistic t respect different shape ability distribution nonnormal distribution symmetric nonsymmetric consider purpose factor vary study test length sample size level correlation ability result Type I error power study test statistic T exhibit consistently similar performance different shape ability distribution investigate study confirm nonparametric nature statistic t,empirical validation dimtest nonnormal ability distribution,0.02833827414741953,0.028627391501844082,0.02868313352850688,0.3683729416806702,0.5459782591415594,0.07385807228280215,0.011992030384472229,0.01000729448898838,0.0,0.009057982723431545
Revuelta J.; Ponsoda V.,A comparison of item exposure control methods in computerized adaptive testing,1998,35,"Two new methods for item exposure control were proposed. In the Progressive method, as the test progresses, the influence of a random component on item selection is reduced and the importance of item information is increasingly more prominent. In the Restricted Maximum Information method, no item is allowed to be exposed in more than a predetermined proportion of tests. Both methods were compared with six other item-selection methods (Maximum Information, One Parameter, McBride and Martin, Randomesque, Sympson and Hetter, and Random Item Selection) with regard to test precision and item exposure variables. Results showed that the Restricted method was useful to reduce maximum exposure rates and that the Progressive method reduced the number of unused items. Both did well regarding precision. Thus, a combined Progressive-Restricted method may be useful to control item exposure without a serious decrease in test precision.",A comparison of item exposure control methods in computerized adaptive testing,"Two new methods for item exposure control were proposed. In the Progressive method, as the test progresses, the influence of a random component on item selection is reduced and the importance of item information is increasingly more prominent. In the Restricted Maximum Information method, no item is allowed to be exposed in more than a predetermined proportion of tests. Both methods were compared with six other item-selection methods (Maximum Information, One Parameter, McBride and Martin, Randomesque, Sympson and Hetter, and Random Item Selection) with regard to test precision and item exposure variables. Results showed that the Restricted method was useful to reduce maximum exposure rates and that the Progressive method reduced the number of unused items. Both did well regarding precision. Thus, a combined Progressive-Restricted method may be useful to control item exposure without a serious decrease in test precision.","['new', 'method', 'item', 'exposure', 'control', 'propose', 'progressive', 'method', 'test', 'progress', 'influence', 'random', 'component', 'item', 'selection', 'reduce', 'importance', 'item', 'information', 'increasingly', 'prominent', 'Restricted', 'Maximum', 'Information', 'method', 'item', 'allow', 'expose', 'predetermine', 'proportion', 'test', 'method', 'compare', 'itemselection', 'method', 'Maximum', 'Information', 'Parameter', 'McBride', 'Martin', 'Randomesque', 'Sympson', 'Hetter', 'Random', 'Item', 'Selection', 'regard', 'test', 'precision', 'item', 'exposure', 'variable', 'result', 'Restricted', 'method', 'useful', 'reduce', 'maximum', 'exposure', 'rate', 'Progressive', 'method', 'reduce', 'number', 'unused', 'item', 'regard', 'precision', 'combine', 'ProgressiveRestricted', 'method', 'useful', 'control', 'item', 'exposure', 'decrease', 'test', 'precision']","['comparison', 'item', 'exposure', 'control', 'method', 'computerized', 'adaptive', 'testing']",new method item exposure control propose progressive method test progress influence random component item selection reduce importance item information increasingly prominent Restricted Maximum Information method item allow expose predetermine proportion test method compare itemselection method Maximum Information Parameter McBride Martin Randomesque Sympson Hetter Random Item Selection regard test precision item exposure variable result Restricted method useful reduce maximum exposure rate Progressive method reduce number unused item regard precision combine ProgressiveRestricted method useful control item exposure decrease test precision,comparison item exposure control method computerized adaptive testing,0.030744346096008145,0.030728421939467995,0.030735821143679433,0.03044266092485764,0.8773487498959868,0.03428010293019922,0.051612253771157995,0.007893410462413857,0.029705849727092157,0.0
Impara J.C.; Plake B.S.,Teachers' ability to estimate item difficulty: A test of the assumptions in the Angoff standard setting method,1998,35,"The Angoff (1971) standard setting method requires expert panelists to (a) conceptualize candidates who possess the qualifications of interest (e.g., the minimally qualified) and (b) estimate actual item performance for these candidates. Past and current research (Bejar, 1983; Shepard, 1994) suggests that estimating item performance is difficult for panelists. If panelists cannot perform this task, the validity of the standard based on these estimates is in question. This study tested the ability of 26 classroom teachers to estimate item performance for two groups of their students on a locally developed district-wide science test. Teachers were more accurate in estimating the performance of the total group than of the ""borderline group,"" but in neither case was their accuracy level high. Implications of this finding for the validity of item performance estimates by panelists using the Angoff standard setting method are discussed.",Teachers' ability to estimate item difficulty: A test of the assumptions in the Angoff standard setting method,"The Angoff (1971) standard setting method requires expert panelists to (a) conceptualize candidates who possess the qualifications of interest (e.g., the minimally qualified) and (b) estimate actual item performance for these candidates. Past and current research (Bejar, 1983; Shepard, 1994) suggests that estimating item performance is difficult for panelists. If panelists cannot perform this task, the validity of the standard based on these estimates is in question. This study tested the ability of 26 classroom teachers to estimate item performance for two groups of their students on a locally developed district-wide science test. Teachers were more accurate in estimating the performance of the total group than of the ""borderline group,"" but in neither case was their accuracy level high. Implications of this finding for the validity of item performance estimates by panelists using the Angoff standard setting method are discussed.","['Angoff', '1971', 'standard', 'setting', 'method', 'require', 'expert', 'panelist', 'conceptualize', 'candidate', 'possess', 'qualification', 'interest', 'eg', 'minimally', 'qualified', 'b', 'estimate', 'actual', 'item', 'performance', 'candidate', 'past', 'current', 'research', 'Bejar', '1983', 'Shepard', '1994', 'suggest', 'estimate', 'item', 'performance', 'difficult', 'panelist', 'panelist', 'perform', 'task', 'validity', 'standard', 'base', 'estimate', 'question', 'study', 'test', 'ability', '26', 'classroom', 'teacher', 'estimate', 'item', 'performance', 'group', 'student', 'locally', 'develop', 'districtwide', 'science', 'test', 'Teachers', 'accurate', 'estimate', 'performance', 'total', 'group', 'borderline', 'group', 'case', 'accuracy', 'level', 'high', 'implication', 'finding', 'validity', 'item', 'performance', 'estimate', 'panelist', 'Angoff', 'standard', 'setting', 'method', 'discuss']","['teacher', 'ability', 'estimate', 'item', 'difficulty', 'test', 'assumption', 'Angoff', 'standard', 'setting', 'method']",Angoff 1971 standard setting method require expert panelist conceptualize candidate possess qualification interest eg minimally qualified b estimate actual item performance candidate past current research Bejar 1983 Shepard 1994 suggest estimate item performance difficult panelist panelist perform task validity standard base estimate question study test ability 26 classroom teacher estimate item performance group student locally develop districtwide science test Teachers accurate estimate performance total group borderline group case accuracy level high implication finding validity item performance estimate panelist Angoff standard setting method discuss,teacher ability estimate item difficulty test assumption Angoff standard setting method,0.02771056547843846,0.02750183419691404,0.8896671625224959,0.027190023446195664,0.027930414355955946,0.05524052670178492,0.0072394088871784714,0.0,0.04086806874022525,0.03711827907448419
Lu C.‐h.; Suen H.K.,Assessment Approaches and Cognitive Styles,1995,32,"The outcomes on multiple‐choice tests and performance‐based assessments for field‐independent and field‐dependent students were examined. A substantial interaction between cognitive style and assessment approach was found. Results suggested that performance‐based assessment tended to favor field‐independent subjects. Dependent on the purpose and intended use of assessment, this finding may raise concerns for validity based on either fairness or curriculum relevance. Copyright © 1995, Wiley Blackwell. All rights reserved",,"The outcomes on multiple‐choice tests and performance‐based assessments for field‐independent and field‐dependent students were examined. A substantial interaction between cognitive style and assessment approach was found. Results suggested that performance‐based assessment tended to favor field‐independent subjects. Dependent on the purpose and intended use of assessment, this finding may raise concerns for validity based on either fairness or curriculum relevance. Copyright © 1995, Wiley Blackwell. All rights reserved","['outcome', 'multiple‐choice', 'test', 'performance‐base', 'assessment', 'field‐independent', 'field‐dependent', 'student', 'examine', 'substantial', 'interaction', 'cognitive', 'style', 'assessment', 'approach', 'find', 'result', 'suggest', 'performance‐base', 'assessment', 'tend', 'favor', 'field‐independent', 'subject', 'dependent', 'purpose', 'intend', 'assessment', 'finding', 'raise', 'concern', 'validity', 'base', 'fairness', 'curriculum', 'relevance', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']",,outcome multiple‐choice test performance‐base assessment field‐independent field‐dependent student examine substantial interaction cognitive style assessment approach find result suggest performance‐base assessment tend favor field‐independent subject dependent purpose intend assessment finding raise concern validity base fairness curriculum relevance Copyright © 1995 Wiley Blackwell right reserve,,0.0327714827438955,0.03281390741046895,0.8692473531351108,0.03236494676923481,0.032802309941289805,0.0,0.007018745071654913,0.0,0.0,0.11038901413907992
Allalouf A.; Ben-Shakhar G.,The effect of coaching on the predictive validity of scholastic aptitude tests,1998,35,"The present study was designed to examine whether coaching affects the predictive validity and fairness of scholastic aptitude tests. Two randomly allocated groups, coached and uncoached, were compared, and the results revealed that although coaching enhanced scores on the Israeli Psychometric Entrance Test by about 25% of a standard deviation, it did not affect predictive validity and did not create a prediction bias. These results refute claims that coaching reduces predictive validity and creates a bias against the uncoached examinees in predicting the criterion. The results are consistent with the idea that score improvement due to coaching does not result strictly from learning specific skills that are irrelevant to the criterion.",The effect of coaching on the predictive validity of scholastic aptitude tests,"The present study was designed to examine whether coaching affects the predictive validity and fairness of scholastic aptitude tests. Two randomly allocated groups, coached and uncoached, were compared, and the results revealed that although coaching enhanced scores on the Israeli Psychometric Entrance Test by about 25% of a standard deviation, it did not affect predictive validity and did not create a prediction bias. These results refute claims that coaching reduces predictive validity and creates a bias against the uncoached examinees in predicting the criterion. The results are consistent with the idea that score improvement due to coaching does not result strictly from learning specific skills that are irrelevant to the criterion.","['present', 'study', 'design', 'examine', 'coaching', 'affect', 'predictive', 'validity', 'fairness', 'scholastic', 'aptitude', 'test', 'randomly', 'allocate', 'group', 'coach', 'uncoached', 'compare', 'result', 'reveal', 'coach', 'enhance', 'score', 'israeli', 'Psychometric', 'Entrance', 'Test', '25', 'standard', 'deviation', 'affect', 'predictive', 'validity', 'create', 'prediction', 'bias', 'result', 'refute', 'claim', 'coaching', 'reduce', 'predictive', 'validity', 'create', 'bias', 'uncoached', 'examinee', 'predict', 'criterion', 'result', 'consistent', 'idea', 'score', 'improvement', 'coaching', 'result', 'strictly', 'learn', 'specific', 'skill', 'irrelevant', 'criterion']","['effect', 'coach', 'predictive', 'validity', 'scholastic', 'aptitude', 'test']",present study design examine coaching affect predictive validity fairness scholastic aptitude test randomly allocate group coach uncoached compare result reveal coach enhance score israeli Psychometric Entrance Test 25 standard deviation affect predictive validity create prediction bias result refute claim coaching reduce predictive validity create bias uncoached examinee predict criterion result consistent idea score improvement coaching result strictly learn specific skill irrelevant criterion,effect coach predictive validity scholastic aptitude test,0.8799295339511276,0.029768307906002094,0.03028146586808528,0.029382415099193086,0.03063827717559189,0.016211439698392466,0.019583493832538178,0.0,0.04653162658607657,0.011735347947388012
Stocking M.L.; Ward W.C.; Potenza M.T.,Simulating the use of disclosed items in computerized adaptive testing,1998,35,"Regular use of questions previously made available to the public (i.e., disclosed items) may provide one way to meet the requirement for large numbers of questions in a continuous testing environment, that is, an environment in which testing is offered at test taker convenience throughout the year rather than on a few prespecified test dates. First it must be shown that such use has effects on test scores small enough to be acceptable. In this study simulations are used to explore the use of disclosed items under a worst-case scenario which assumes that disclosed items are always answered correctly. Some item pool and test designs were identified in which the use of disclosed items produces effects on test scores that may be viewed as negligible.",Simulating the use of disclosed items in computerized adaptive testing,"Regular use of questions previously made available to the public (i.e., disclosed items) may provide one way to meet the requirement for large numbers of questions in a continuous testing environment, that is, an environment in which testing is offered at test taker convenience throughout the year rather than on a few prespecified test dates. First it must be shown that such use has effects on test scores small enough to be acceptable. In this study simulations are used to explore the use of disclosed items under a worst-case scenario which assumes that disclosed items are always answered correctly. Some item pool and test designs were identified in which the use of disclosed items produces effects on test scores that may be viewed as negligible.","['regular', 'question', 'previously', 'available', 'public', 'ie', 'disclose', 'item', 'provide', 'way', 'meet', 'requirement', 'large', 'number', 'question', 'continuous', 'testing', 'environment', 'environment', 'testing', 'offer', 'test', 'taker', 'convenience', 'year', 'prespecified', 'test', 'date', 'effect', 'test', 'score', 'small', 'acceptable', 'study', 'simulation', 'explore', 'disclose', 'item', 'worstcase', 'scenario', 'assume', 'disclose', 'item', 'answer', 'correctly', 'item', 'pool', 'test', 'design', 'identify', 'disclose', 'item', 'produce', 'effect', 'test', 'score', 'view', 'negligible']","['simulate', 'disclose', 'item', 'computerized', 'adaptive', 'testing']",regular question previously available public ie disclose item provide way meet requirement large number question continuous testing environment environment testing offer test taker convenience year prespecified test date effect test score small acceptable study simulation explore disclose item worstcase scenario assume disclose item answer correctly item pool test design identify disclose item produce effect test score view negligible,simulate disclose item computerized adaptive testing,0.031150869962101865,0.8752722374019806,0.03116591659168494,0.03092897014500891,0.03148200589922363,0.001966916168115183,0.0918513464387743,3.0158638170228974e-05,0.02148132605469766,0.0
Wainer H.,Using trilinear plots for NAEP state data,1996,33,"Understanding the distribution of achievement levels of students' performance in the National Assessment of Educational Progress (NAEP) is aided through the use of the trilinear chart. In this article, this chart is described and its use illustrated with data from the 1992 state NAEP mathematics assessment. It is shown that one can see readily the trends in performance for different demographic groups for all of the 44 participating jurisdictions simultaneously. It is suggested that this graphical form may be useful in other contexts, as well.",,"Understanding the distribution of achievement levels of students' performance in the National Assessment of Educational Progress (NAEP) is aided through the use of the trilinear chart. In this article, this chart is described and its use illustrated with data from the 1992 state NAEP mathematics assessment. It is shown that one can see readily the trends in performance for different demographic groups for all of the 44 participating jurisdictions simultaneously. It is suggested that this graphical form may be useful in other contexts, as well.","['understand', 'distribution', 'achievement', 'level', 'student', 'performance', 'National', 'Assessment', 'Educational', 'Progress', 'NAEP', 'aid', 'trilinear', 'chart', 'article', 'chart', 'describe', 'illustrate', 'datum', '1992', 'state', 'naep', 'mathematic', 'assessment', 'readily', 'trend', 'performance', 'different', 'demographic', 'group', '44', 'participate', 'jurisdiction', 'simultaneously', 'suggest', 'graphical', 'form', 'useful', 'context']",,understand distribution achievement level student performance National Assessment Educational Progress NAEP aid trilinear chart article chart describe illustrate datum 1992 state naep mathematic assessment readily trend performance different demographic group 44 participate jurisdiction simultaneously suggest graphical form useful context,,0.4988093248038626,0.03126698611949524,0.031099224669571678,0.031003588270053,0.4078208761370175,0.0,0.0,0.0020660734102075284,0.0,0.1239714022258584
Zwick R.; Thayer D.T.; Wingersky M.,Effect of Rasch Calibration on Ability and DIF Estimation in Computer‐Adaptive Tests,1995,32,"In a previous simulation study of methods for assessing differential item functioning (DIF) in computer‐adaptive tests (Zwick, Thayer, & Wingersky, 1993, 1994), modified versions of the Mantel‐Haenszel and standardization methods were found to perform well. In that study, data were generated using the 3‐parameter logistic (3PL) model and this same model was assumed in obtaining item parameter estimates. In the current study, the 3PL data were used but the Rasch model was assumed in obtaining the item parameter estimates, which determined the information table used for item selection. Although the obtained DIF statistics were highly correlated with the generating DIF values, they tended to be smaller in magnitude than in the 3PL analysis, resulting in a lower probability of DIF detection. This reduced sensitivity appeared to be related to a degradation in the accuracy of matching. Expected true scores from the Rasch‐based computer‐adaptive test tended to be biased downward, particularly for lower‐ability examinees Copyright © 1995, Wiley Blackwell. All rights reserved",Effect of Rasch Calibration on Ability and DIF Estimation in Computer‐Adaptive Tests,"In a previous simulation study of methods for assessing differential item functioning (DIF) in computer‐adaptive tests (Zwick, Thayer, & Wingersky, 1993, 1994), modified versions of the Mantel‐Haenszel and standardization methods were found to perform well. In that study, data were generated using the 3‐parameter logistic (3PL) model and this same model was assumed in obtaining item parameter estimates. In the current study, the 3PL data were used but the Rasch model was assumed in obtaining the item parameter estimates, which determined the information table used for item selection. Although the obtained DIF statistics were highly correlated with the generating DIF values, they tended to be smaller in magnitude than in the 3PL analysis, resulting in a lower probability of DIF detection. This reduced sensitivity appeared to be related to a degradation in the accuracy of matching. Expected true scores from the Rasch‐based computer‐adaptive test tended to be biased downward, particularly for lower‐ability examinees Copyright © 1995, Wiley Blackwell. All rights reserved","['previous', 'simulation', 'study', 'method', 'assess', 'differential', 'item', 'function', 'DIF', 'computer‐adaptive', 'test', 'Zwick', 'Thayer', 'Wingersky', '1993', '1994', 'modify', 'version', 'Mantel‐Haenszel', 'standardization', 'method', 'find', 'perform', 'study', 'datum', 'generate', '3‐parameter', 'logistic', '3pl', 'assume', 'obtain', 'item', 'parameter', 'estimate', 'current', 'study', '3pl', 'datum', 'Rasch', 'assume', 'obtain', 'item', 'parameter', 'estimate', 'determine', 'information', 'table', 'item', 'selection', 'obtain', 'DIF', 'statistic', 'highly', 'correlate', 'generate', 'DIF', 'value', 'tend', 'small', 'magnitude', '3pl', 'analysis', 'result', 'low', 'probability', 'dif', 'detection', 'reduce', 'sensitivity', 'appear', 'relate', 'degradation', 'accuracy', 'match', 'expect', 'true', 'score', 'rasch‐base', 'computer‐adaptive', 'test', 'tend', 'bias', 'downward', 'particularly', 'lower‐ability', 'examine', 'copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['effect', 'Rasch', 'Calibration', 'ability', 'DIF', 'Estimation', 'Computer‐Adaptive', 'test']",previous simulation study method assess differential item function DIF computer‐adaptive test Zwick Thayer Wingersky 1993 1994 modify version Mantel‐Haenszel standardization method find perform study datum generate 3‐parameter logistic 3pl assume obtain item parameter estimate current study 3pl datum Rasch assume obtain item parameter estimate determine information table item selection obtain DIF statistic highly correlate generate DIF value tend small magnitude 3pl analysis result low probability dif detection reduce sensitivity appear relate degradation accuracy match expect true score rasch‐base computer‐adaptive test tend bias downward particularly lower‐ability examine copyright © 1995 Wiley Blackwell right reserve,effect Rasch Calibration ability DIF Estimation Computer‐Adaptive test,0.024213353476424518,0.024368304483869084,0.9029279866638049,0.02384641249731109,0.024643942878590362,0.02910551189697739,0.0037434459228348734,0.13547937753621322,0.026131893497903354,0.0
Davey T.; Godwin J.; Mittelholtz D.,Developing and scoring an innovative computerized writing assessment,1997,34,"We describe the development and administration of a recently introduced computer-based test of writing skills. This test asks the examinee to edit a writing passage presented on a computer screen. To do this, the examinee moves a cursor to a suspect section of the passage and chooses from a list of alternative ways of rewriting that section. Any or all parts of the passage can be changed, as often as the examinee likes. An able examinee identifies and fixes errors in grammar, organization, and style, whereas a less able examinee may leave errors untouched, replace an error with another error, or even introduce errors where none existed previously. All these response alternatives contrive to present both obvious and subtle scoring difficulties. These difficulties were attacked through the combined use of option weighting and the sequential probability ratio test, the result of which is to classify examinees into several discrete ability groups. Item calibration was enabled by augmenting sparse pretest samples through data meiosis, in which response vectors were randomly recombined to produce offspring that retained much of the character of their parents. These procedures are described, and operational examples are offered.",Developing and scoring an innovative computerized writing assessment,"We describe the development and administration of a recently introduced computer-based test of writing skills. This test asks the examinee to edit a writing passage presented on a computer screen. To do this, the examinee moves a cursor to a suspect section of the passage and chooses from a list of alternative ways of rewriting that section. Any or all parts of the passage can be changed, as often as the examinee likes. An able examinee identifies and fixes errors in grammar, organization, and style, whereas a less able examinee may leave errors untouched, replace an error with another error, or even introduce errors where none existed previously. All these response alternatives contrive to present both obvious and subtle scoring difficulties. These difficulties were attacked through the combined use of option weighting and the sequential probability ratio test, the result of which is to classify examinees into several discrete ability groups. Item calibration was enabled by augmenting sparse pretest samples through data meiosis, in which response vectors were randomly recombined to produce offspring that retained much of the character of their parents. These procedures are described, and operational examples are offered.","['describe', 'development', 'administration', 'recently', 'introduce', 'computerbase', 'test', 'write', 'skill', 'test', 'ask', 'examinee', 'edit', 'writing', 'passage', 'present', 'computer', 'screen', 'examinee', 'cursor', 'suspect', 'section', 'passage', 'choose', 'list', 'alternative', 'way', 'rewrite', 'section', 'passage', 'change', 'examinee', 'like', 'able', 'examinee', 'identifie', 'fix', 'error', 'grammar', 'organization', 'style', 'able', 'examinee', 'leave', 'error', 'untouched', 'replace', 'error', 'error', 'introduce', 'error', 'exist', 'previously', 'response', 'alternative', 'contrive', 'present', 'obvious', 'subtle', 'scoring', 'difficulty', 'difficulty', 'attack', 'combine', 'option', 'weighting', 'sequential', 'probability', 'ratio', 'test', 'result', 'classify', 'examinee', 'discrete', 'ability', 'group', 'Item', 'calibration', 'enable', 'augment', 'sparse', 'pret', 'sample', 'datum', 'meiosis', 'response', 'vector', 'randomly', 'recombine', 'produce', 'offspring', 'retain', 'character', 'parent', 'procedure', 'describe', 'operational', 'example', 'offer']","['develop', 'score', 'innovative', 'computerized', 'writing', 'assessment']",describe development administration recently introduce computerbase test write skill test ask examinee edit writing passage present computer screen examinee cursor suspect section passage choose list alternative way rewrite section passage change examinee like able examinee identifie fix error grammar organization style able examinee leave error untouched replace error error introduce error exist previously response alternative contrive present obvious subtle scoring difficulty difficulty attack combine option weighting sequential probability ratio test result classify examinee discrete ability group Item calibration enable augment sparse pret sample datum meiosis response vector randomly recombine produce offspring retain character parent procedure describe operational example offer,develop score innovative computerized writing assessment,0.02243224114215827,0.9098269875965059,0.02256099181611133,0.02220647762509972,0.022973301820124685,0.01115436749271626,0.07027497044056015,0.0,0.03686047530878488,0.0
Clauser B.E.; Subhiyah R.G.; Nungester R.J.; Ripkey D.R.; Clyman S.G.; McKinley D.,Scoring a Performance‐Based Assessment by Modeling the Judgments of Experts,1995,32,"Performance assessments typically require expert judges to individually rate each performance. This results in a limitation in the use of such assessments because the rating process may be extremely time consuming. This article describes a scoring algorithm that is based on expert judgments but requires the rating of only a sample of performances. A regression‐based policy capturing procedure was implemented to model the judgment policies of experts. The data set was a seven‐case performance assessment of physician patient management skills. The assessment used a computer‐based simulation of the patient care environment. The results showed a substantial improvement in correspondence between scores produced using the algorithm and actual ratings, when compared to raw scores. Scores based on the algorithm were also shown to be superior to raw scores and equal to expert ratings for making pass/fail decisions which agreed with those made by an independent committee of experts Copyright © 1995, Wiley Blackwell. All rights reserved",Scoring a Performance‐Based Assessment by Modeling the Judgments of Experts,"Performance assessments typically require expert judges to individually rate each performance. This results in a limitation in the use of such assessments because the rating process may be extremely time consuming. This article describes a scoring algorithm that is based on expert judgments but requires the rating of only a sample of performances. A regression‐based policy capturing procedure was implemented to model the judgment policies of experts. The data set was a seven‐case performance assessment of physician patient management skills. The assessment used a computer‐based simulation of the patient care environment. The results showed a substantial improvement in correspondence between scores produced using the algorithm and actual ratings, when compared to raw scores. Scores based on the algorithm were also shown to be superior to raw scores and equal to expert ratings for making pass/fail decisions which agreed with those made by an independent committee of experts Copyright © 1995, Wiley Blackwell. All rights reserved","['performance', 'assessment', 'typically', 'require', 'expert', 'judge', 'individually', 'rate', 'performance', 'result', 'limitation', 'assessment', 'rating', 'process', 'extremely', 'time', 'consume', 'article', 'describe', 'scoring', 'algorithm', 'base', 'expert', 'judgment', 'require', 'rating', 'sample', 'performance', 'regression‐base', 'policy', 'capturing', 'procedure', 'implement', 'judgment', 'policy', 'expert', 'datum', 'set', 'seven‐case', 'performance', 'assessment', 'physician', 'patient', 'management', 'skill', 'assessment', 'computer‐based', 'simulation', 'patient', 'care', 'environment', 'result', 'substantial', 'improvement', 'correspondence', 'score', 'produce', 'algorithm', 'actual', 'rating', 'compare', 'raw', 'score', 'Scores', 'base', 'algorithm', 'superior', 'raw', 'score', 'equal', 'expert', 'rating', 'passfail', 'decision', 'agree', 'independent', 'committee', 'expert', 'Copyright', '©', '1995', 'Wiley', 'Blackwell', 'right', 'reserve']","['score', 'performance‐based', 'Assessment', 'Judgments', 'expert']",performance assessment typically require expert judge individually rate performance result limitation assessment rating process extremely time consume article describe scoring algorithm base expert judgment require rating sample performance regression‐base policy capturing procedure implement judgment policy expert datum set seven‐case performance assessment physician patient management skill assessment computer‐based simulation patient care environment result substantial improvement correspondence score produce algorithm actual rating compare raw score Scores base algorithm superior raw score equal expert rating passfail decision agree independent committee expert Copyright © 1995 Wiley Blackwell right reserve,score performance‐based Assessment Judgments expert,0.8900515052345088,0.027463525896035745,0.027472130682067813,0.02709243400481343,0.027920404182574212,0.006711318233279142,0.0,0.0,0.03527302883838642,0.09333332734360748
Potenza M.T.; Stocking M.L.,Flawed items in computerized adaptive testing,1997,34,"A multiple-choice test item is identified as flawed if it has no single best answer. In spite of extensive quality control procedures, the administration of flawed items to test takers is inevitable. A limited set of common strategies for dealing with flawed items in conventional testing, grounded in the principle of fairness to examinees, is reexamined in the context of adaptive testing. An additional strategy, available for adaptive testing, of retesting from a pool cleansed of flawed items, is compared to the existing strategies. Retesting was found to be no practical improvement over current strategies.",,"A multiple-choice test item is identified as flawed if it has no single best answer. In spite of extensive quality control procedures, the administration of flawed items to test takers is inevitable. A limited set of common strategies for dealing with flawed items in conventional testing, grounded in the principle of fairness to examinees, is reexamined in the context of adaptive testing. An additional strategy, available for adaptive testing, of retesting from a pool cleansed of flawed items, is compared to the existing strategies. Retesting was found to be no practical improvement over current strategies.","['multiplechoice', 'test', 'item', 'identify', 'flawed', 'single', 'good', 'answer', 'spite', 'extensive', 'quality', 'control', 'procedure', 'administration', 'flawed', 'item', 'test', 'taker', 'inevitable', 'limited', 'set', 'common', 'strategy', 'deal', 'flawed', 'item', 'conventional', 'testing', 'ground', 'principle', 'fairness', 'examinees', 'reexamine', 'context', 'adaptive', 'testing', 'additional', 'strategy', 'available', 'adaptive', 'testing', 'reteste', 'pool', 'cleanse', 'flawed', 'item', 'compare', 'exist', 'strategy', 'reteste', 'find', 'practical', 'improvement', 'current', 'strategy']",,multiplechoice test item identify flawed single good answer spite extensive quality control procedure administration flawed item test taker inevitable limited set common strategy deal flawed item conventional testing ground principle fairness examinees reexamine context adaptive testing additional strategy available adaptive testing reteste pool cleanse flawed item compare exist strategy reteste find practical improvement current strategy,,0.6930781768780955,0.03357376573220866,0.0329684122498468,0.03272888467125831,0.20765076046859074,0.0,0.09268559825544674,0.0,0.0,0.0
French A.W.; Miller T.R.,Logistic regression and its use in detecting differential item functioning in polytomous items,1996,33,"A computer simulation study was conducted to determine the feasibility of using logistic regression procedures to detect differential item functioning (DIF) in polytomous items. One item in a simulated test of 25 items contained DIF; parameters for that item were varied to create three conditions of nonuniform DIF and one of uniform DIF. Item scores were generated using a generalized partial credit model, and the data were recoded into multiple dichotomies in order to use logistic regression procedures. Results indicate that logistic regression is powerful in detecting most forms of DIF; however, it required large amounts of data manipulation, and interpretation of the results was sometimes difficult. Some logistic regression procedures may be useful in the post hoc analysis of DIF for polytomous items.",Logistic regression and its use in detecting differential item functioning in polytomous items,"A computer simulation study was conducted to determine the feasibility of using logistic regression procedures to detect differential item functioning (DIF) in polytomous items. One item in a simulated test of 25 items contained DIF; parameters for that item were varied to create three conditions of nonuniform DIF and one of uniform DIF. Item scores were generated using a generalized partial credit model, and the data were recoded into multiple dichotomies in order to use logistic regression procedures. Results indicate that logistic regression is powerful in detecting most forms of DIF; however, it required large amounts of data manipulation, and interpretation of the results was sometimes difficult. Some logistic regression procedures may be useful in the post hoc analysis of DIF for polytomous items.","['computer', 'simulation', 'study', 'conduct', 'determine', 'feasibility', 'logistic', 'regression', 'procedure', 'detect', 'differential', 'item', 'function', 'DIF', 'polytomous', 'item', 'item', 'simulated', 'test', '25', 'item', 'contain', 'dif', 'parameter', 'item', 'varied', 'create', 'condition', 'nonuniform', 'DIF', 'uniform', 'DIF', 'Item', 'score', 'generate', 'generalized', 'partial', 'credit', 'datum', 'recode', 'multiple', 'dichotomy', 'order', 'logistic', 'regression', 'procedure', 'result', 'indicate', 'logistic', 'regression', 'powerful', 'detect', 'form', 'DIF', 'require', 'large', 'data', 'manipulation', 'interpretation', 'result', 'difficult', 'logistic', 'regression', 'procedure', 'useful', 'post', 'hoc', 'analysis', 'dif', 'polytomous', 'item']","['logistic', 'regression', 'detect', 'differential', 'item', 'function', 'polytomous', 'item']",computer simulation study conduct determine feasibility logistic regression procedure detect differential item function DIF polytomous item item simulated test 25 item contain dif parameter item varied create condition nonuniform DIF uniform DIF Item score generate generalized partial credit datum recode multiple dichotomy order logistic regression procedure result indicate logistic regression powerful detect form DIF require large data manipulation interpretation result difficult logistic regression procedure useful post hoc analysis dif polytomous item,logistic regression detect differential item function polytomous item,0.030061232751166858,0.880196797200334,0.02992966403736112,0.02959926392981736,0.030213042081320838,0.0,0.0,0.22573733587594877,0.0,0.0
Tate R.L.; King F.,Factors Which Influence Precision of School‐Level IRT Ability Estimates,1994,31,"The precision of the group‐level IRT model applied to school ability estimation is described, assuming use of Bayesian estimation with precision represented by the standard deviation of the posterior distribution. Similarities and differences between the school‐level model and the familiar individual‐level IRT model are considered. School size and between‐school variability, two factors not relevant at the student level, are dominant determinants of school‐level precision. Under the multiple‐matrix sampling design required for the school‐level IRT, the number of items associated with a scale does not influence the precision at the school level. Also, the effects of school ability and item quality on school‐level precision are often relatively weak. It was found that the use of Bayesian estimation could result in a systematic distortion of the true ranking of schools based on ability because of an estimation bias which is a function of school size. Copyright © 1994, Wiley Blackwell. All rights reserved",Factors Which Influence Precision of School‐Level IRT Ability Estimates,"The precision of the group‐level IRT model applied to school ability estimation is described, assuming use of Bayesian estimation with precision represented by the standard deviation of the posterior distribution. Similarities and differences between the school‐level model and the familiar individual‐level IRT model are considered. School size and between‐school variability, two factors not relevant at the student level, are dominant determinants of school‐level precision. Under the multiple‐matrix sampling design required for the school‐level IRT, the number of items associated with a scale does not influence the precision at the school level. Also, the effects of school ability and item quality on school‐level precision are often relatively weak. It was found that the use of Bayesian estimation could result in a systematic distortion of the true ranking of schools based on ability because of an estimation bias which is a function of school size. Copyright © 1994, Wiley Blackwell. All rights reserved","['precision', 'group‐level', 'IRT', 'apply', 'school', 'ability', 'estimation', 'describe', 'assume', 'bayesian', 'estimation', 'precision', 'represent', 'standard', 'deviation', 'posterior', 'distribution', 'Similarities', 'difference', 'school‐level', 'familiar', 'individual‐level', 'IRT', 'consider', 'school', 'size', 'between‐school', 'variability', 'factor', 'relevant', 'student', 'level', 'dominant', 'determinant', 'school‐level', 'precision', 'multiple‐matrix', 'sample', 'design', 'require', 'school‐level', 'IRT', 'number', 'item', 'associate', 'scale', 'influence', 'precision', 'school', 'level', 'effect', 'school', 'ability', 'item', 'quality', 'school‐level', 'precision', 'relatively', 'weak', 'find', 'bayesian', 'estimation', 'result', 'systematic', 'distortion', 'true', 'ranking', 'school', 'base', 'ability', 'estimation', 'bias', 'function', 'school', 'size', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['factor', 'Influence', 'Precision', 'School‐Level', 'IRT', 'ability', 'estimate']",precision group‐level IRT apply school ability estimation describe assume bayesian estimation precision represent standard deviation posterior distribution Similarities difference school‐level familiar individual‐level IRT consider school size between‐school variability factor relevant student level dominant determinant school‐level precision multiple‐matrix sample design require school‐level IRT number item associate scale influence precision school level effect school ability item quality school‐level precision relatively weak find bayesian estimation result systematic distortion true ranking school base ability estimation bias function school size Copyright © 1994 Wiley Blackwell right reserve,factor Influence Precision School‐Level IRT ability estimate,0.02941648444882264,0.48027891809732093,0.029597036286653076,0.029181564121048633,0.43152599704615474,0.08948364676541008,0.0,0.0,0.0,0.006995599849514338
Scheuneman J.D.; Gerritz K.,Using Differential Item Functioning Procedures to Explore Sources of Item Difficulty and Group Performance Characteristics,1990,27,"Statistics used to detect differential item functioning can also reflect differential strengths and weaknesses in the performance characteristics of population subgroups. In turn, item features associated with the differential performance patterns are likely to reflect some facet of the item task and hence its difficulty, that might previously have been overlooked. In this study, several item features were identified and coded for a large number of reading comprehension items from the two admissions testing programs. Item features included subject matter content, various properties of item structure, cognitive demand indicators, and semantic content (propositional analysis). Differential item functioning was evaluated for males and females and for White and Black examinees. Results showed a number of significant relationships between item features and indicators of differential item functioning—many of which were consistent across testing programs. Implications of the results for related areas of research are discussed. Copyright © 1990, Wiley Blackwell. All rights reserved",Using Differential Item Functioning Procedures to Explore Sources of Item Difficulty and Group Performance Characteristics,"Statistics used to detect differential item functioning can also reflect differential strengths and weaknesses in the performance characteristics of population subgroups. In turn, item features associated with the differential performance patterns are likely to reflect some facet of the item task and hence its difficulty, that might previously have been overlooked. In this study, several item features were identified and coded for a large number of reading comprehension items from the two admissions testing programs. Item features included subject matter content, various properties of item structure, cognitive demand indicators, and semantic content (propositional analysis). Differential item functioning was evaluated for males and females and for White and Black examinees. Results showed a number of significant relationships between item features and indicators of differential item functioning—many of which were consistent across testing programs. Implications of the results for related areas of research are discussed. Copyright © 1990, Wiley Blackwell. All rights reserved","['statistic', 'detect', 'differential', 'item', 'functioning', 'reflect', 'differential', 'strength', 'weakness', 'performance', 'characteristic', 'population', 'subgroup', 'turn', 'item', 'feature', 'associate', 'differential', 'performance', 'pattern', 'likely', 'reflect', 'facet', 'item', 'task', 'difficulty', 'previously', 'overlook', 'study', 'item', 'feature', 'identify', 'code', 'large', 'number', 'read', 'comprehension', 'item', 'admission', 'testing', 'program', 'Item', 'feature', 'include', 'subject', 'matter', 'content', 'property', 'item', 'structure', 'cognitive', 'demand', 'indicator', 'semantic', 'content', 'propositional', 'analysis', 'differential', 'item', 'functioning', 'evaluate', 'male', 'female', 'White', 'Black', 'examine', 'result', 'number', 'significant', 'relationship', 'item', 'feature', 'indicator', 'differential', 'item', 'function', '—', 'consistent', 'test', 'program', 'Implications', 'result', 'related', 'area', 'research', 'discuss', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Differential', 'Item', 'Functioning', 'procedure', 'explore', 'Sources', 'Item', 'Difficulty', 'Group', 'Performance', 'characteristic']",statistic detect differential item functioning reflect differential strength weakness performance characteristic population subgroup turn item feature associate differential performance pattern likely reflect facet item task difficulty previously overlook study item feature identify code large number read comprehension item admission testing program Item feature include subject matter content property item structure cognitive demand indicator semantic content propositional analysis differential item functioning evaluate male female White Black examine result number significant relationship item feature indicator differential item function — consistent test program Implications result related area research discuss Copyright © 1990 Wiley Blackwell right reserve,Differential Item Functioning procedure explore Sources Item Difficulty Group Performance characteristic,0.8964389908684962,0.025996392248620603,0.026064604771455047,0.025275085516873477,0.026224926594554627,0.0,0.1009513970839661,0.05406075420540764,0.0,0.008866447214906108
Wise S.L.; Plake B.S.; Johnson P.L.; Roos L.L.,A Comparison of Self‐Adapted and Computerized Adaptive Tests,1992,29,"According to item response theory (IRT), examinee ability estimation is independent of the particular set of test items administered from a calibrated pool. Although the most popular application of this feature of IRT is computerized adaptive (CA) testing, a recently proposed alternative is self‐adapted (SA) testing, in which examinees choose the difficulty level of each of their test items. This study compared examinee performance under SA and CA tests, finding that examinees taking the SA test (a) obtained significantly higher ability scores and (b) reported significantly lower posttest state anxiety. The results of this study suggest that SA testing is a desirable format for computer‐based testing. Copyright © 1992, Wiley Blackwell. All rights reserved",A Comparison of Self‐Adapted and Computerized Adaptive Tests,"According to item response theory (IRT), examinee ability estimation is independent of the particular set of test items administered from a calibrated pool. Although the most popular application of this feature of IRT is computerized adaptive (CA) testing, a recently proposed alternative is self‐adapted (SA) testing, in which examinees choose the difficulty level of each of their test items. This study compared examinee performance under SA and CA tests, finding that examinees taking the SA test (a) obtained significantly higher ability scores and (b) reported significantly lower posttest state anxiety. The results of this study suggest that SA testing is a desirable format for computer‐based testing. Copyright © 1992, Wiley Blackwell. All rights reserved","['accord', 'item', 'response', 'theory', 'IRT', 'examinee', 'ability', 'estimation', 'independent', 'particular', 'set', 'test', 'item', 'administer', 'calibrate', 'pool', 'popular', 'application', 'feature', 'IRT', 'computerized', 'adaptive', 'CA', 'test', 'recently', 'propose', 'alternative', 'self‐adapte', 'SA', 'testing', 'examine', 'choose', 'difficulty', 'level', 'test', 'item', 'study', 'compare', 'examinee', 'performance', 'SA', 'CA', 'test', 'find', 'examine', 'SA', 'test', 'obtain', 'significantly', 'high', 'ability', 'score', 'b', 'report', 'significantly', 'low', 'postt', 'state', 'anxiety', 'result', 'study', 'suggest', 'SA', 'testing', 'desirable', 'format', 'computer‐base', 'testing', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'self‐adapted', 'Computerized', 'Adaptive', 'test']",accord item response theory IRT examinee ability estimation independent particular set test item administer calibrate pool popular application feature IRT computerized adaptive CA test recently propose alternative self‐adapte SA testing examine choose difficulty level test item study compare examinee performance SA CA test find examine SA test obtain significantly high ability score b report significantly low postt state anxiety result study suggest SA testing desirable format computer‐base testing Copyright © 1992 Wiley Blackwell right reserve,Comparison self‐adapted Computerized Adaptive test,0.027993935410783603,0.8875416757531002,0.028148855819444013,0.027671392323247574,0.028644140693424698,0.025323559104749854,0.1137373097592259,0.0,0.006770964630229618,0.0
Ackerman T.A.,"A Didactic Explanation of Item Bias, Item Impact, and Item Validity From a Multidimensional Perspective",1992,29,"Many researchers have suggested that the main cause of item bias is the misspecification of the latent ability space, where items that measure multiple abilities are scored as though they are measuring a single ability. If two different groups of examinees have different underlying multidimensional ability distributions and the test items are capable of discriminating among levels of abilities on these multiple dimensions, then any unidimensional scoring scheme has the potential to produce item bias. It is the purpose of this article to provide the testing practitioner with insight about the difference between item bias and item impact and how they relate to item validity. These concepts will be explained from a multidimensional item response theory (MIRT) perspective. Two detection procedures, the Mantel‐Haenszel (as modified by Holland and Thayer, 1988) and Shealy and Stout's Simultaneous Item Bias (SIB; 1991) strategies, will be used to illustrate how practitioners can detect item bias. Copyright © 1992, Wiley Blackwell. All rights reserved","A Didactic Explanation of Item Bias, Item Impact, and Item Validity From a Multidimensional Perspective","Many researchers have suggested that the main cause of item bias is the misspecification of the latent ability space, where items that measure multiple abilities are scored as though they are measuring a single ability. If two different groups of examinees have different underlying multidimensional ability distributions and the test items are capable of discriminating among levels of abilities on these multiple dimensions, then any unidimensional scoring scheme has the potential to produce item bias. It is the purpose of this article to provide the testing practitioner with insight about the difference between item bias and item impact and how they relate to item validity. These concepts will be explained from a multidimensional item response theory (MIRT) perspective. Two detection procedures, the Mantel‐Haenszel (as modified by Holland and Thayer, 1988) and Shealy and Stout's Simultaneous Item Bias (SIB; 1991) strategies, will be used to illustrate how practitioners can detect item bias. Copyright © 1992, Wiley Blackwell. All rights reserved","['researcher', 'suggest', 'main', 'cause', 'item', 'bias', 'misspecification', 'latent', 'ability', 'space', 'item', 'measure', 'multiple', 'ability', 'score', 'measure', 'single', 'ability', 'different', 'group', 'examinee', 'different', 'underlie', 'multidimensional', 'ability', 'distribution', 'test', 'item', 'capable', 'discriminate', 'level', 'ability', 'multiple', 'dimension', 'unidimensional', 'scoring', 'scheme', 'potential', 'produce', 'item', 'bias', 'purpose', 'article', 'provide', 'testing', 'practitioner', 'insight', 'difference', 'item', 'bias', 'item', 'impact', 'relate', 'item', 'validity', 'concept', 'explain', 'multidimensional', 'item', 'response', 'theory', 'MIRT', 'perspective', 'detection', 'procedure', 'Mantel‐Haenszel', 'modify', 'Holland', 'Thayer', '1988', 'Shealy', 'Stouts', 'Simultaneous', 'Item', 'Bias', 'SIB', '1991', 'strategy', 'illustrate', 'practitioner', 'detect', 'item', 'bias', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Didactic', 'Explanation', 'Item', 'Bias', 'Item', 'Impact', 'Item', 'Validity', 'Multidimensional', 'Perspective']",researcher suggest main cause item bias misspecification latent ability space item measure multiple ability score measure single ability different group examinee different underlie multidimensional ability distribution test item capable discriminate level ability multiple dimension unidimensional scoring scheme potential produce item bias purpose article provide testing practitioner insight difference item bias item impact relate item validity concept explain multidimensional item response theory MIRT perspective detection procedure Mantel‐Haenszel modify Holland Thayer 1988 Shealy Stouts Simultaneous Item Bias SIB 1991 strategy illustrate practitioner detect item bias Copyright © 1992 Wiley Blackwell right reserve,Didactic Explanation Item Bias Item Impact Item Validity Multidimensional Perspective,0.025694563093469895,0.025906027207268367,0.025952938933718268,0.025655117664564634,0.8967913531009788,0.037517486240818174,0.07852796497783358,0.03954123274586062,0.0,0.0
Schwarz S.P.; McMorris R.F.; DeMers L.P.,Reasons for Changing Answers: An Evaluation Using Personal Interviews,1991,28,"Researchers investigating answer changing have consistently found the preponderance of changes on objective items to be from wrong to right, but little is understood about the mechanisms involved in this phenomenon. In this study, personal interviews were combined with instruction in answer‐changing research to investigate further the processes involved in answer changing. Students changed answers and gained from changing, with those in the upper two thirds of the classes gaining the most. Each test‐taking strategy produced a mean gain, but particular strategies were not significantly correlated with percentage of gain or percentage of change. Most students reported changing answers for thoughtful reasons such as rereading, rethinking, or remembering more information; very few changes were due to clerical errors. For each reason, most changes were wrong‐to‐right. We conclude that reconsideration of test items is probably underestimated in answer‐changing studies. The role of memory should be considered in why people change and in how successful they judge their changing to have been. Copyright © 1991, Wiley Blackwell. All rights reserved",Reasons for Changing Answers: An Evaluation Using Personal Interviews,"Researchers investigating answer changing have consistently found the preponderance of changes on objective items to be from wrong to right, but little is understood about the mechanisms involved in this phenomenon. In this study, personal interviews were combined with instruction in answer‐changing research to investigate further the processes involved in answer changing. Students changed answers and gained from changing, with those in the upper two thirds of the classes gaining the most. Each test‐taking strategy produced a mean gain, but particular strategies were not significantly correlated with percentage of gain or percentage of change. Most students reported changing answers for thoughtful reasons such as rereading, rethinking, or remembering more information; very few changes were due to clerical errors. For each reason, most changes were wrong‐to‐right. We conclude that reconsideration of test items is probably underestimated in answer‐changing studies. The role of memory should be considered in why people change and in how successful they judge their changing to have been. Copyright © 1991, Wiley Blackwell. All rights reserved","['researcher', 'investigate', 'answer', 'change', 'consistently', 'find', 'preponderance', 'change', 'objective', 'item', 'wrong', 'right', 'little', 'understand', 'mechanism', 'involve', 'phenomenon', 'study', 'personal', 'interview', 'combine', 'instruction', 'answer‐change', 'research', 'investigate', 'far', 'process', 'involve', 'answer', 'change', 'student', 'change', 'answer', 'gain', 'change', 'upper', 'class', 'gain', 'test‐taking', 'strategy', 'produce', 'mean', 'gain', 'particular', 'strategy', 'significantly', 'correlate', 'percentage', 'gain', 'percentage', 'change', 'Most', 'student', 'report', 'change', 'answer', 'thoughtful', 'reason', 'reread', 'rethinking', 'remember', 'information', 'change', 'clerical', 'error', 'reason', 'change', 'wrong‐to‐right', 'conclude', 'reconsideration', 'test', 'item', 'probably', 'underestimate', 'answer‐change', 'study', 'role', 'memory', 'consider', 'people', 'change', 'successful', 'judge', 'change', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['reason', 'change', 'Answers', 'evaluation', 'Personal', 'Interviews']",researcher investigate answer change consistently find preponderance change objective item wrong right little understand mechanism involve phenomenon study personal interview combine instruction answer‐change research investigate far process involve answer change student change answer gain change upper class gain test‐taking strategy produce mean gain particular strategy significantly correlate percentage gain percentage change Most student report change answer thoughtful reason reread rethinking remember information change clerical error reason change wrong‐to‐right conclude reconsideration test item probably underestimate answer‐change study role memory consider people change successful judge change copyright © 1991 Wiley Blackwell right reserve,reason change Answers evaluation Personal Interviews,0.02909524058917963,0.029321096059102082,0.02938637162134326,0.02886458946294379,0.8833327022674312,0.0,0.0834305609736299,0.0,0.0,0.006664619607467137
Freedle R.; Kostin I.,Item Difficulty of Four Verbal Item Types and an Index of Differential Item Functioning for Black and White Examinees,1990,27,"In this study, the authors explored the importance of item difficulty (equated delta) as a predictor of differential item functioning (DIF) of Black versus matched White examinees for four verbal item types (analogies, antonyms, sentence completions, reading comprehension) using 13 GRE‐disclosed forms (988 verbal items) and 11 SAT‐disclosed forms (935 verbal items). The average correlation across test forms for each item type (and often the correlation for each individual test form as well) revealed a significant relationship between item difficulty and DIF value for both GRE and SAT. The most important finding indicates that for hard items, Black examinees perform differentially better than matched ability White examinees for each of the four item types and for both the GRE and SAT tests! The results further suggest that the amount of verbal context is an important determinant of the magnitude of the relationship between item difficulty and differential performance of Black versus matched White examinees. Several hypotheses accounting for this result were explored. Copyright © 1990, Wiley Blackwell. All rights reserved",Item Difficulty of Four Verbal Item Types and an Index of Differential Item Functioning for Black and White Examinees,"In this study, the authors explored the importance of item difficulty (equated delta) as a predictor of differential item functioning (DIF) of Black versus matched White examinees for four verbal item types (analogies, antonyms, sentence completions, reading comprehension) using 13 GRE‐disclosed forms (988 verbal items) and 11 SAT‐disclosed forms (935 verbal items). The average correlation across test forms for each item type (and often the correlation for each individual test form as well) revealed a significant relationship between item difficulty and DIF value for both GRE and SAT. The most important finding indicates that for hard items, Black examinees perform differentially better than matched ability White examinees for each of the four item types and for both the GRE and SAT tests! The results further suggest that the amount of verbal context is an important determinant of the magnitude of the relationship between item difficulty and differential performance of Black versus matched White examinees. Several hypotheses accounting for this result were explored. Copyright © 1990, Wiley Blackwell. All rights reserved","['study', 'author', 'explore', 'importance', 'item', 'difficulty', 'equate', 'delta', 'predictor', 'differential', 'item', 'function', 'DIF', 'Black', 'versus', 'match', 'White', 'examinee', 'verbal', 'item', 'type', 'analogy', 'antonym', 'sentence', 'completion', 'read', 'comprehension', '13', 'gre‐disclose', 'form', '988', 'verbal', 'item', '11', 'sat‐disclose', 'form', '935', 'verbal', 'item', 'average', 'correlation', 'test', 'form', 'item', 'type', 'correlation', 'individual', 'test', 'form', 'reveal', 'significant', 'relationship', 'item', 'difficulty', 'dif', 'value', 'GRE', 'SAT', 'important', 'finding', 'indicate', 'hard', 'item', 'black', 'examinee', 'perform', 'differentially', 'match', 'ability', 'White', 'examinee', 'item', 'type', 'GRE', 'SAT', 'test', 'result', 'far', 'suggest', 'verbal', 'context', 'important', 'determinant', 'magnitude', 'relationship', 'item', 'difficulty', 'differential', 'performance', 'Black', 'versus', 'match', 'White', 'examine', 'hypothesis', 'account', 'result', 'explore', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Item', 'Difficulty', 'Four', 'Verbal', 'Item', 'Types', 'Index', 'Differential', 'Item', 'Functioning', 'Black', 'White', 'examinee']",study author explore importance item difficulty equate delta predictor differential item function DIF Black versus match White examinee verbal item type analogy antonym sentence completion read comprehension 13 gre‐disclose form 988 verbal item 11 sat‐disclose form 935 verbal item average correlation test form item type correlation individual test form reveal significant relationship item difficulty dif value GRE SAT important finding indicate hard item black examinee perform differentially match ability White examinee item type GRE SAT test result far suggest verbal context important determinant magnitude relationship item difficulty differential performance Black versus match White examine hypothesis account result explore copyright © 1990 Wiley Blackwell right reserve,Item Difficulty Four Verbal Item Types Index Differential Item Functioning Black White examinee,0.02553677401286859,0.02585870303405113,0.025592547134129104,0.025411541540668294,0.8976004342782828,0.013438216105037447,0.10315441430690987,0.05688356788203904,0.0,0.0
Nandakumar R.,Traditional Dimensionality Versus Essential Dimensionality,1991,28,"This article addresses testing the hypothesis of one versus more than one dominant (essential) dimension in the possible presence of minor dimensions. The method used is Stout's statistical test of essential unidimensionality, which is based on the theory of essential unidimensionality. Differences between the traditional definition of dimensionality provided by item response theory, which counts all dimensions present, and essential dimensionality, which counts only dominant dimensions, are discussed. As Monte Carlo studies demonstrate, Stout's test of essential unidimensionality tends to indicate essential unidimensionality in the presence of one dominant dimension and one or more minor dimensions that have a relatively small influence on item scores. As the influence of the minor dimensions increases, Stout's test is more likely to reject the hypothesis of essential unidimensionality. To assist in interpreting these studies, a rough index of the deviation from essential unidimensionality is proposed. Copyright © 1991, Wiley Blackwell. All rights reserved",Traditional Dimensionality Versus Essential Dimensionality,"This article addresses testing the hypothesis of one versus more than one dominant (essential) dimension in the possible presence of minor dimensions. The method used is Stout's statistical test of essential unidimensionality, which is based on the theory of essential unidimensionality. Differences between the traditional definition of dimensionality provided by item response theory, which counts all dimensions present, and essential dimensionality, which counts only dominant dimensions, are discussed. As Monte Carlo studies demonstrate, Stout's test of essential unidimensionality tends to indicate essential unidimensionality in the presence of one dominant dimension and one or more minor dimensions that have a relatively small influence on item scores. As the influence of the minor dimensions increases, Stout's test is more likely to reject the hypothesis of essential unidimensionality. To assist in interpreting these studies, a rough index of the deviation from essential unidimensionality is proposed. Copyright © 1991, Wiley Blackwell. All rights reserved","['article', 'address', 'test', 'hypothesis', 'versus', 'dominant', 'essential', 'dimension', 'possible', 'presence', 'minor', 'dimension', 'method', 'Stouts', 'statistical', 'test', 'essential', 'unidimensionality', 'base', 'theory', 'essential', 'unidimensionality', 'difference', 'traditional', 'definition', 'dimensionality', 'provide', 'item', 'response', 'theory', 'count', 'dimension', 'present', 'essential', 'dimensionality', 'count', 'dominant', 'dimension', 'discuss', 'Monte', 'Carlo', 'study', 'demonstrate', 'Stouts', 'test', 'essential', 'unidimensionality', 'tend', 'indicate', 'essential', 'unidimensionality', 'presence', 'dominant', 'dimension', 'minor', 'dimension', 'relatively', 'small', 'influence', 'item', 'score', 'influence', 'minor', 'dimension', 'increase', 'Stouts', 'test', 'likely', 'reject', 'hypothesis', 'essential', 'unidimensionality', 'assist', 'interpret', 'study', 'rough', 'index', 'deviation', 'essential', 'unidimensionality', 'propose', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['traditional', 'Dimensionality', 'Versus', 'Essential', 'Dimensionality']",article address test hypothesis versus dominant essential dimension possible presence minor dimension method Stouts statistical test essential unidimensionality base theory essential unidimensionality difference traditional definition dimensionality provide item response theory count dimension present essential dimensionality count dominant dimension discuss Monte Carlo study demonstrate Stouts test essential unidimensionality tend indicate essential unidimensionality presence dominant dimension minor dimension relatively small influence item score influence minor dimension increase Stouts test likely reject hypothesis essential unidimensionality assist interpret study rough index deviation essential unidimensionality propose Copyright © 1991 Wiley Blackwell right reserve,traditional Dimensionality Versus Essential Dimensionality,0.035675977569434775,0.03559977672939713,0.035880017063947216,0.035380026128403345,0.8574642025088176,0.008278351581471222,0.032845401742802434,0.0030419480994115424,0.00892727325220251,0.007995280788515599
Johnson E.G.,The Design of the National Assessment of Educational Progress,1992,29,"The key features of the design of the National Assessment of Educational Progress (NAEP) are discussed with particular emphasis on the design to be used for the 1992 assessment. An overview of the design and its philosophy are given with a description of the multicomponent solution to the twin requirements of reliably measuring trends in achievement while responding to changing educational priorities and advances in measurement technology. The student sample designs for the National Assessment and the Trial State Assessment are described. The focused‐balanced incomplete block (focused‐BIB) spiraling method of item sampling is discussed and compared with simpler matrix sampling designs. The impact of the NAEP design on the analysis of assessment data is discussed. Copyright © 1992, Wiley Blackwell. All rights reserved",The Design of the National Assessment of Educational Progress,"The key features of the design of the National Assessment of Educational Progress (NAEP) are discussed with particular emphasis on the design to be used for the 1992 assessment. An overview of the design and its philosophy are given with a description of the multicomponent solution to the twin requirements of reliably measuring trends in achievement while responding to changing educational priorities and advances in measurement technology. The student sample designs for the National Assessment and the Trial State Assessment are described. The focused‐balanced incomplete block (focused‐BIB) spiraling method of item sampling is discussed and compared with simpler matrix sampling designs. The impact of the NAEP design on the analysis of assessment data is discussed. Copyright © 1992, Wiley Blackwell. All rights reserved","['key', 'feature', 'design', 'National', 'Assessment', 'Educational', 'Progress', 'NAEP', 'discuss', 'particular', 'emphasis', 'design', '1992', 'assessment', 'overview', 'design', 'philosophy', 'description', 'multicomponent', 'solution', 'twin', 'requirement', 'reliably', 'measure', 'trend', 'achievement', 'respond', 'change', 'educational', 'priority', 'advance', 'technology', 'student', 'sample', 'design', 'National', 'Assessment', 'Trial', 'State', 'Assessment', 'describe', 'focused‐balance', 'incomplete', 'block', 'focused‐bib', 'spiral', 'method', 'item', 'sampling', 'discuss', 'compare', 'simple', 'matrix', 'sampling', 'design', 'impact', 'naep', 'design', 'analysis', 'assessment', 'datum', 'discuss', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Design', 'National', 'Assessment', 'Educational', 'Progress']",key feature design National Assessment Educational Progress NAEP discuss particular emphasis design 1992 assessment overview design philosophy description multicomponent solution twin requirement reliably measure trend achievement respond change educational priority advance technology student sample design National Assessment Trial State Assessment describe focused‐balance incomplete block focused‐bib spiral method item sampling discuss compare simple matrix sampling design impact naep design analysis assessment datum discuss Copyright © 1992 Wiley Blackwell right reserve,Design National Assessment Educational Progress,0.028397799908740685,0.028627603827154374,0.028306319808392006,0.028275452722744197,0.8863928237329688,0.011204319280268379,0.0,0.0,0.0,0.1300504651955751
Plake B.S.; Kane M.T.,Comparison of Methods for Combining the Minimum Passing Levels for Individual Items into a Passing Score for a Test,1991,28,"The purpose of this study was to compare several methods for determining a passing score on an examination from the individual raters' estimates of minimal pass levels for the items. The methods investigated differ in the weighting that the estimates for each item receive in the aggregation process. An IRT‐based simulation method was used to model a variety of error components of minimum pass levels. The results indicate little difference in estimated passing scores across the three methods. Less error was present when the ability level of the minimally competent candidates matched the expected difficulty level of the test. No meaningful improvement in passing score estimation was achieved for a 50‐item test as opposed to a 25‐item test; however, the RMSE values for estimates with 10 raters were smaller than those for 5 raters. The results suggest that the simplest method for aggregating minimum pass levels across the items in a test–adding them up–is the preferred method. Copyright © 1991, Wiley Blackwell. All rights reserved",Comparison of Methods for Combining the Minimum Passing Levels for Individual Items into a Passing Score for a Test,"The purpose of this study was to compare several methods for determining a passing score on an examination from the individual raters' estimates of minimal pass levels for the items. The methods investigated differ in the weighting that the estimates for each item receive in the aggregation process. An IRT‐based simulation method was used to model a variety of error components of minimum pass levels. The results indicate little difference in estimated passing scores across the three methods. Less error was present when the ability level of the minimally competent candidates matched the expected difficulty level of the test. No meaningful improvement in passing score estimation was achieved for a 50‐item test as opposed to a 25‐item test; however, the RMSE values for estimates with 10 raters were smaller than those for 5 raters. The results suggest that the simplest method for aggregating minimum pass levels across the items in a test–adding them up–is the preferred method. Copyright © 1991, Wiley Blackwell. All rights reserved","['purpose', 'study', 'compare', 'method', 'determine', 'pass', 'score', 'examination', 'individual', 'rater', 'estimate', 'minimal', 'pass', 'level', 'item', 'method', 'investigate', 'differ', 'weighting', 'estimate', 'item', 'receive', 'aggregation', 'process', 'irt‐base', 'simulation', 'method', 'variety', 'error', 'component', 'minimum', 'pass', 'level', 'result', 'indicate', 'little', 'difference', 'estimate', 'pass', 'score', 'method', 'Less', 'error', 'present', 'ability', 'level', 'minimally', 'competent', 'candidate', 'match', 'expect', 'difficulty', 'level', 'test', 'meaningful', 'improvement', 'pass', 'score', 'estimation', 'achieve', '50‐item', 'test', 'oppose', '25‐item', 'test', 'RMSE', 'value', 'estimate', '10', 'rater', 'small', '5', 'rater', 'result', 'suggest', 'simple', 'method', 'aggregate', 'minimum', 'pass', 'level', 'item', 'test', '–', 'add', '–', 'preferred', 'method', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Methods', 'combine', 'Minimum', 'Passing', 'Levels', 'Individual', 'Items', 'Passing', 'Score', 'test']",purpose study compare method determine pass score examination individual rater estimate minimal pass level item method investigate differ weighting estimate item receive aggregation process irt‐base simulation method variety error component minimum pass level result indicate little difference estimate pass score method Less error present ability level minimally competent candidate match expect difficulty level test meaningful improvement pass score estimation achieve 50‐item test oppose 25‐item test RMSE value estimate 10 rater small 5 rater result suggest simple method aggregate minimum pass level item test – add – preferred method copyright © 1991 Wiley Blackwell right reserve,Comparison Methods combine Minimum Passing Levels Individual Items Passing Score test,0.8839752344237216,0.02886647140688741,0.028987059829684653,0.028513514182402847,0.029657720157303495,0.03292070707027552,0.021842700457696543,0.0,0.10219894354034065,0.015873059155028946
Sireci S.G.; Thissen D.; Wainer H.,On the Reliability of Testlet‐Based Tests,1991,28,"If a test is constructed of testlets, one must take into account the within‐testlet structure in the calculation of test statistics. Failing to do so may yield serious biases in the estimation of such statistics as reliability. We demonstrate how to calculate the reliability of a testlet‐based test. We show that traditional reliabilities calculated on two reading comprehension tests constructed of four testlets are substantial overestimates. Copyright © 1991, Wiley Blackwell. All rights reserved",,"If a test is constructed of testlets, one must take into account the within‐testlet structure in the calculation of test statistics. Failing to do so may yield serious biases in the estimation of such statistics as reliability. We demonstrate how to calculate the reliability of a testlet‐based test. We show that traditional reliabilities calculated on two reading comprehension tests constructed of four testlets are substantial overestimates. Copyright © 1991, Wiley Blackwell. All rights reserved","['test', 'construct', 'testlet', 'account', 'within‐testlet', 'structure', 'calculation', 'test', 'statistic', 'fail', 'yield', 'bias', 'estimation', 'statistic', 'reliability', 'demonstrate', 'calculate', 'reliability', 'testlet‐based', 'test', 'traditional', 'reliability', 'calculate', 'reading', 'comprehension', 'test', 'construct', 'testlet', 'substantial', 'overestimate', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']",,test construct testlet account within‐testlet structure calculation test statistic fail yield bias estimation statistic reliability demonstrate calculate reliability testlet‐based test traditional reliability calculate reading comprehension test construct testlet substantial overestimate Copyright © 1991 Wiley Blackwell right reserve,,0.8553412069672308,0.03586661561976081,0.03651095156625521,0.03551034772575235,0.03677087812100068,0.0004253280752418897,0.07400703280845682,0.0,0.026406867297372204,0.0
Harris D.J.,A Comparison of Angoff's Design I and Design II for Vertical Equating Using Traditional and IRT Methodology,1991,28,"Practical considerations in conducting an equating study often require a trade‐off between testing time and sample size. A counterbalanced design (Angoff's Design II) is often selected because, as each examinee is administered both test forms and therefore the errors are correlated, sample sizes can be dramatically reduced over those required by a spiraling design (Angoff's Design I), where each examinee is administered only one test form. However, the counterbalanced design may be subject to fatigue, practice, or context effects. This article investigated these two data collection designs (for a given sample size) with equipercentile and IRT equating methodology in the vertical equating of two mathematics achievement tests. Both designs and both methodologies were judged to adequately meet an equivalent expected score criterion; Design II was found to exhibit more stability over different samples. Copyright © 1991, Wiley Blackwell. All rights reserved",A Comparison of Angoff's Design I and Design II for Vertical Equating Using Traditional and IRT Methodology,"Practical considerations in conducting an equating study often require a trade‐off between testing time and sample size. A counterbalanced design (Angoff's Design II) is often selected because, as each examinee is administered both test forms and therefore the errors are correlated, sample sizes can be dramatically reduced over those required by a spiraling design (Angoff's Design I), where each examinee is administered only one test form. However, the counterbalanced design may be subject to fatigue, practice, or context effects. This article investigated these two data collection designs (for a given sample size) with equipercentile and IRT equating methodology in the vertical equating of two mathematics achievement tests. Both designs and both methodologies were judged to adequately meet an equivalent expected score criterion; Design II was found to exhibit more stability over different samples. Copyright © 1991, Wiley Blackwell. All rights reserved","['practical', 'consideration', 'conduct', 'equate', 'study', 'require', 'trade‐off', 'testing', 'time', 'sample', 'size', 'counterbalance', 'design', 'Angoffs', 'Design', 'II', 'select', 'examinee', 'administer', 'test', 'form', 'error', 'correlate', 'sample', 'size', 'dramatically', 'reduce', 'require', 'spiraling', 'design', 'Angoffs', 'Design', 'I', 'examinee', 'administer', 'test', 'form', 'counterbalance', 'design', 'subject', 'fatigue', 'practice', 'context', 'effect', 'article', 'investigate', 'datum', 'collection', 'design', 'sample', 'size', 'equipercentile', 'IRT', 'equate', 'methodology', 'vertical', 'equating', 'mathematic', 'achievement', 'test', 'design', 'methodology', 'judge', 'adequately', 'meet', 'equivalent', 'expect', 'score', 'criterion', 'Design', 'II', 'find', 'exhibit', 'stability', 'different', 'sample', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Angoffs', 'Design', 'I', 'Design', 'II', 'Vertical', 'Equating', 'Traditional', 'IRT', 'Methodology']",practical consideration conduct equate study require trade‐off testing time sample size counterbalance design Angoffs Design II select examinee administer test form error correlate sample size dramatically reduce require spiraling design Angoffs Design I examinee administer test form counterbalance design subject fatigue practice context effect article investigate datum collection design sample size equipercentile IRT equate methodology vertical equating mathematic achievement test design methodology judge adequately meet equivalent expect score criterion Design II find exhibit stability different sample copyright © 1991 Wiley Blackwell right reserve,Comparison Angoffs Design I Design II Vertical Equating Traditional IRT Methodology,0.6111366519293352,0.02880517174349107,0.028608636294829355,0.028149738344906482,0.30329980168743786,0.0950252663858272,0.0,0.0,0.0,0.028475018871529534
Swaminathan H.; Rogers H.J.,Detecting Differential Item Functioning Using Logistic Regression Procedures,1990,27,"A logistic regression model for characterizing differential item functioning (DIF) between two groups is presented. A distinction is drawn between uniform and nonuniform DIF in terms of the parameters of the model. A statistic for testing the hypothesis of no DIF is developed. Through simulation studies, it is shown that the logistic regression procedure is more powerful than the Mantel‐Haenszel procedure for detecting nonuniform DIF and as powerful in detecting uniform DIF. Copyright © 1990, Wiley Blackwell. All rights reserved",Detecting Differential Item Functioning Using Logistic Regression Procedures,"A logistic regression model for characterizing differential item functioning (DIF) between two groups is presented. A distinction is drawn between uniform and nonuniform DIF in terms of the parameters of the model. A statistic for testing the hypothesis of no DIF is developed. Through simulation studies, it is shown that the logistic regression procedure is more powerful than the Mantel‐Haenszel procedure for detecting nonuniform DIF and as powerful in detecting uniform DIF. Copyright © 1990, Wiley Blackwell. All rights reserved","['logistic', 'regression', 'characterize', 'differential', 'item', 'function', 'DIF', 'group', 'present', 'distinction', 'draw', 'uniform', 'nonuniform', 'DIF', 'term', 'parameter', 'statistic', 'test', 'hypothesis', 'DIF', 'develop', 'Through', 'simulation', 'study', 'logistic', 'regression', 'procedure', 'powerful', 'Mantel‐Haenszel', 'procedure', 'detect', 'nonuniform', 'DIF', 'powerful', 'detect', 'uniform', 'DIF', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['detect', 'Differential', 'Item', 'Functioning', 'Logistic', 'Regression', 'procedure']",logistic regression characterize differential item function DIF group present distinction draw uniform nonuniform DIF term parameter statistic test hypothesis DIF develop Through simulation study logistic regression procedure powerful Mantel‐Haenszel procedure detect nonuniform DIF powerful detect uniform DIF Copyright © 1990 Wiley Blackwell right reserve,detect Differential Item Functioning Logistic Regression procedure,0.03684764121202464,0.8523080793836704,0.03669891665041312,0.036378412312748354,0.037766950441143525,0.0,0.0,0.22248448815346353,0.0,0.0
Donoghue J.R.,An Empirical Examination of the IRT Information of Polytomously Scored Reading Items Under the Generalized Partial Credit Model,1994,31,"Using Muraki's (1992) generalized partial credit IRT model, polytomous items (responses to which can be scored as ordered categories) from the 1991 field test of the NAEP Reading Assessment were calibrated simultaneously with multiple‐choice and short open‐ended items. Expected information of each type of item was computed. On average, four‐category polytomous items yielded 2.1 to 3.1 times as much IRT information as dichotomous items. These results provide limited support for the ad hoc rule of weighting k‐category polytomous items the same as k ‐ 1 dichotomous items for computing total scores. Polytomous items provided the most information about examinees of moderately high proficiency; the information function peaked at 1.0 to 1.5, and the population distribution mean was 0. When scored dichotomously, information in polytomous items sharply decreased, but they still provided more expected information than did the other response formats. For reference, a derivation of the information function for the generalized partial credit model is included. Copyright © 1994, Wiley Blackwell. All rights reserved",An Empirical Examination of the IRT Information of Polytomously Scored Reading Items Under the Generalized Partial Credit Model,"Using Muraki's (1992) generalized partial credit IRT model, polytomous items (responses to which can be scored as ordered categories) from the 1991 field test of the NAEP Reading Assessment were calibrated simultaneously with multiple‐choice and short open‐ended items. Expected information of each type of item was computed. On average, four‐category polytomous items yielded 2.1 to 3.1 times as much IRT information as dichotomous items. These results provide limited support for the ad hoc rule of weighting k‐category polytomous items the same as k ‐ 1 dichotomous items for computing total scores. Polytomous items provided the most information about examinees of moderately high proficiency; the information function peaked at 1.0 to 1.5, and the population distribution mean was 0. When scored dichotomously, information in polytomous items sharply decreased, but they still provided more expected information than did the other response formats. For reference, a derivation of the information function for the generalized partial credit model is included. Copyright © 1994, Wiley Blackwell. All rights reserved","['Murakis', '1992', 'generalized', 'partial', 'credit', 'IRT', 'polytomous', 'item', 'response', 'score', 'order', 'category', '1991', 'field', 'test', 'naep', 'Reading', 'Assessment', 'calibrate', 'simultaneously', 'multiple‐choice', 'short', 'open‐ende', 'item', 'expect', 'information', 'type', 'item', 'compute', 'average', 'four‐category', 'polytomous', 'item', 'yield', '21', '31', 'time', 'IRT', 'information', 'dichotomous', 'item', 'result', 'provide', 'limited', 'support', 'ad', 'hoc', 'rule', 'weight', 'k‐category', 'polytomous', 'item', 'k', '‐', '1', 'dichotomous', 'item', 'compute', 'total', 'score', 'polytomous', 'item', 'provide', 'information', 'examinee', 'moderately', 'high', 'proficiency', 'information', 'function', 'peak', '10', '15', 'population', 'distribution', 'mean', '0', 'score', 'dichotomously', 'information', 'polytomous', 'item', 'sharply', 'decrease', 'provide', 'expected', 'information', 'response', 'format', 'reference', 'derivation', 'information', 'function', 'generalized', 'partial', 'credit', 'include', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Empirical', 'Examination', 'IRT', 'Information', 'polytomously', 'score', 'reading', 'Items', 'Generalized', 'Partial', 'Credit']",Murakis 1992 generalized partial credit IRT polytomous item response score order category 1991 field test naep Reading Assessment calibrate simultaneously multiple‐choice short open‐ende item expect information type item compute average four‐category polytomous item yield 21 31 time IRT information dichotomous item result provide limited support ad hoc rule weight k‐category polytomous item k ‐ 1 dichotomous item compute total score polytomous item provide information examinee moderately high proficiency information function peak 10 15 population distribution mean 0 score dichotomously information polytomous item sharply decrease provide expected information response format reference derivation information function generalized partial credit include Copyright © 1994 Wiley Blackwell right reserve,Empirical Examination IRT Information polytomously score reading Items Generalized Partial Credit,0.8995102684735566,0.025190265774840678,0.025065506358736736,0.0248492493639345,0.025384710028931452,0.0,0.09020615937973238,0.037377984401635596,0.026972863227180535,0.0
Nandakumar R.,Assessing Dimensionality of a Set of Item Responses‐Comparison of Different Approaches,1994,31,"This study compares the performance of three methodologies for assessing unidi‐mensionality: DIMTEST, Holland and Rosenbaum's approach, and nonlinear factor analysis. Each method is examined and compared with other methods on simulated and real data sets. Seven data sets, all with 2,000 examinees, were generated: three unidimensional and four two‐dimensional data sets. Two levels of correlation between abilities were considered:ρ=3 andρ=. 7. Eight different real data sets were used: Four of them were expected to be unidimensional, and the other four were expected to be two‐dimensional. Findings suggest that all three methods correctly confirmed unidimensionality but differed in their ability to detect lack of unidimensionality. DIMTEST showed excellent power in detecting lack of unidimensionality; Holland and Rosenbaum's and nonlinear factor analysis approaches showed good power, provided the correlation between abilities was low. Copyright © 1994, Wiley Blackwell. All rights reserved",Assessing Dimensionality of a Set of Item Responses‐Comparison of Different Approaches,"This study compares the performance of three methodologies for assessing unidi‐mensionality: DIMTEST, Holland and Rosenbaum's approach, and nonlinear factor analysis. Each method is examined and compared with other methods on simulated and real data sets. Seven data sets, all with 2,000 examinees, were generated: three unidimensional and four two‐dimensional data sets. Two levels of correlation between abilities were considered:ρ=3 andρ=. 7. Eight different real data sets were used: Four of them were expected to be unidimensional, and the other four were expected to be two‐dimensional. Findings suggest that all three methods correctly confirmed unidimensionality but differed in their ability to detect lack of unidimensionality. DIMTEST showed excellent power in detecting lack of unidimensionality; Holland and Rosenbaum's and nonlinear factor analysis approaches showed good power, provided the correlation between abilities was low. Copyright © 1994, Wiley Blackwell. All rights reserved","['study', 'compare', 'performance', 'methodology', 'assess', 'unidi‐mensionality', 'DIMTEST', 'Holland', 'Rosenbaums', 'approach', 'nonlinear', 'factor', 'analysis', 'method', 'examine', 'compare', 'method', 'simulated', 'real', 'datum', 'set', 'Seven', 'datum', 'set', '2000', 'examinee', 'generate', 'unidimensional', 'two‐dimensional', 'datum', 'set', 'level', 'correlation', 'ability', 'consideredρ3', 'andρ', '7', 'different', 'real', 'data', 'set', 'expect', 'unidimensional', 'expect', 'two‐dimensional', 'finding', 'suggest', 'method', 'correctly', 'confirm', 'unidimensionality', 'differ', 'ability', 'detect', 'lack', 'unidimensionality', 'DIMTEST', 'excellent', 'power', 'detect', 'lack', 'unidimensionality', 'Holland', 'Rosenbaums', 'nonlinear', 'factor', 'analysis', 'approach', 'good', 'power', 'provide', 'correlation', 'ability', 'low', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['assess', 'Dimensionality', 'Set', 'Item', 'Responses‐Comparison', 'Different', 'Approaches']",study compare performance methodology assess unidi‐mensionality DIMTEST Holland Rosenbaums approach nonlinear factor analysis method examine compare method simulated real datum set Seven datum set 2000 examinee generate unidimensional two‐dimensional datum set level correlation ability consideredρ3 andρ 7 different real data set expect unidimensional expect two‐dimensional finding suggest method correctly confirm unidimensionality differ ability detect lack unidimensionality DIMTEST excellent power detect lack unidimensionality Holland Rosenbaums nonlinear factor analysis approach good power provide correlation ability low Copyright © 1994 Wiley Blackwell right reserve,assess Dimensionality Set Item Responses‐Comparison Different Approaches,0.028009157270864973,0.028244450298252274,0.028211046748537666,0.4395472472822324,0.4759880984001127,0.0420145985115765,0.005463571220798603,0.01688900305884519,0.0031990402903141007,0.024471985011575298
Lane S.,Use of Restricted Item Response Models for Examining Item Difficulty Ordering and Slope Uniformity,1991,28,"This article demonstrates the utility of restricted item response models for examining item difficulty ordering and slope uniformity for an item set that reflects varying cognitive processes. Twelve sets of paired algebra word problems were developed to systematically reflect various types of cognitive processes required for successful performance. This resulted in a total of 24 items. They reflected distance‐rate–time (DRT), interest, and area problems. Hypotheses concerning difficulty ordering and slope uniformity for the items were tested by constraining item difficulty and discrimination parameters in hierarchical item response models. The first set of model comparisons tested the equality of the discrimination and difficulty parameters for each set of paired items. The second set of model comparisons examined slope uniformity within the complex DRT problems. The third set of model comparisons examined whether the familiarity of the story context affected item difficulty for two types of complex DRT problems. The last set of model comparisons tested the hypothesized difficulty ordering of the items. Copyright © 1991, Wiley Blackwell. All rights reserved",Use of Restricted Item Response Models for Examining Item Difficulty Ordering and Slope Uniformity,"This article demonstrates the utility of restricted item response models for examining item difficulty ordering and slope uniformity for an item set that reflects varying cognitive processes. Twelve sets of paired algebra word problems were developed to systematically reflect various types of cognitive processes required for successful performance. This resulted in a total of 24 items. They reflected distance‐rate–time (DRT), interest, and area problems. Hypotheses concerning difficulty ordering and slope uniformity for the items were tested by constraining item difficulty and discrimination parameters in hierarchical item response models. The first set of model comparisons tested the equality of the discrimination and difficulty parameters for each set of paired items. The second set of model comparisons examined slope uniformity within the complex DRT problems. The third set of model comparisons examined whether the familiarity of the story context affected item difficulty for two types of complex DRT problems. The last set of model comparisons tested the hypothesized difficulty ordering of the items. Copyright © 1991, Wiley Blackwell. All rights reserved","['article', 'demonstrate', 'utility', 'restricted', 'item', 'response', 'examine', 'item', 'difficulty', 'order', 'slope', 'uniformity', 'item', 'set', 'reflect', 'vary', 'cognitive', 'process', 'set', 'pair', 'algebra', 'word', 'problem', 'develop', 'systematically', 'reflect', 'type', 'cognitive', 'process', 'require', 'successful', 'performance', 'result', 'total', '24', 'item', 'reflect', 'distance‐rate', '–', 'time', 'DRT', 'interest', 'area', 'problem', 'Hypotheses', 'concern', 'difficulty', 'order', 'slope', 'uniformity', 'item', 'test', 'constrain', 'item', 'difficulty', 'discrimination', 'parameter', 'hierarchical', 'item', 'response', 'set', 'comparison', 'test', 'equality', 'discrimination', 'difficulty', 'parameter', 'set', 'pair', 'item', 'second', 'set', 'comparison', 'examine', 'slope', 'uniformity', 'complex', 'DRT', 'problem', 'set', 'comparison', 'examine', 'familiarity', 'story', 'context', 'affect', 'item', 'difficulty', 'type', 'complex', 'DRT', 'problem', 'set', 'comparison', 'test', 'hypothesized', 'difficulty', 'order', 'item', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Use', 'Restricted', 'Item', 'Response', 'Models', 'examine', 'Item', 'Difficulty', 'Ordering', 'Slope', 'Uniformity']",article demonstrate utility restricted item response examine item difficulty order slope uniformity item set reflect vary cognitive process set pair algebra word problem develop systematically reflect type cognitive process require successful performance result total 24 item reflect distance‐rate – time DRT interest area problem Hypotheses concern difficulty order slope uniformity item test constrain item difficulty discrimination parameter hierarchical item response set comparison test equality discrimination difficulty parameter set pair item second set comparison examine slope uniformity complex DRT problem set comparison examine familiarity story context affect item difficulty type complex DRT problem set comparison test hypothesized difficulty order item Copyright © 1991 Wiley Blackwell right reserve,Use Restricted Item Response Models examine Item Difficulty Ordering Slope Uniformity,0.8856146118364381,0.028574527140716368,0.028904124428260482,0.028263144513898213,0.028643592080686727,0.008390700617937985,0.11102570577321963,0.005975490538275189,0.0,0.0
Sehmitt A.P.; Dorans N.J.,Differential Item Functioning for Minority Examinees on the SAT,1990,27,"The standardization approach to assessing differential item functioning (DIF), including standardized distractor analysis, is described. The results of studies conducted on Asian Americans, Hispanics (Mexican Americans and Puerto Ricans), and Blacks on the Scholastic Aptitude Test (SAT) are described and then synthesized across studies. Where the groups were limited to include only examinees who spoke English as their best language, very few items across forms and ethnic groups exhibited large DIF. Major findings include evidence of differential speededness (where minority examinees did not complete SAT‐Verbal sections at the same rate as White students with comparable SAT‐Verbal scores) for Blacks and Hispanics and, when the item content is of special interest, advantages for the relevant ethnic group. In addition, homographs tend to disadvantage all three ethnic groups, but the effect of vertical relationships in analogy items are not as consistent. Although these findings are important in understanding DIF, they do not seem to account for all differences. Other variables related to DIF still need to be identified. Furthermore, these findings are seen as tentative until corroborated by studies using controlled data collection designs. Copyright © 1990, Wiley Blackwell. All rights reserved",Differential Item Functioning for Minority Examinees on the SAT,"The standardization approach to assessing differential item functioning (DIF), including standardized distractor analysis, is described. The results of studies conducted on Asian Americans, Hispanics (Mexican Americans and Puerto Ricans), and Blacks on the Scholastic Aptitude Test (SAT) are described and then synthesized across studies. Where the groups were limited to include only examinees who spoke English as their best language, very few items across forms and ethnic groups exhibited large DIF. Major findings include evidence of differential speededness (where minority examinees did not complete SAT‐Verbal sections at the same rate as White students with comparable SAT‐Verbal scores) for Blacks and Hispanics and, when the item content is of special interest, advantages for the relevant ethnic group. In addition, homographs tend to disadvantage all three ethnic groups, but the effect of vertical relationships in analogy items are not as consistent. Although these findings are important in understanding DIF, they do not seem to account for all differences. Other variables related to DIF still need to be identified. Furthermore, these findings are seen as tentative until corroborated by studies using controlled data collection designs. Copyright © 1990, Wiley Blackwell. All rights reserved","['standardization', 'approach', 'assess', 'differential', 'item', 'function', 'DIF', 'include', 'standardized', 'distractor', 'analysis', 'describe', 'result', 'study', 'conduct', 'Asian', 'Americans', 'Hispanics', 'Mexican', 'Americans', 'Puerto', 'Ricans', 'Blacks', 'Scholastic', 'Aptitude', 'Test', 'SAT', 'describe', 'synthesize', 'study', 'group', 'limit', 'include', 'examinee', 'speak', 'English', 'good', 'language', 'item', 'form', 'ethnic', 'group', 'exhibit', 'large', 'dif', 'major', 'finding', 'include', 'evidence', 'differential', 'speededness', 'minority', 'examinee', 'complete', 'sat‐verbal', 'section', 'rate', 'white', 'student', 'comparable', 'sat‐verbal', 'score', 'Blacks', 'Hispanics', 'item', 'content', 'special', 'interest', 'advantage', 'relevant', 'ethnic', 'group', 'addition', 'homograph', 'tend', 'disadvantage', 'ethnic', 'group', 'effect', 'vertical', 'relationship', 'analogy', 'item', 'consistent', 'finding', 'important', 'understand', 'DIF', 'account', 'difference', 'variable', 'relate', 'DIF', 'need', 'identify', 'furthermore', 'finding', 'tentative', 'corroborate', 'study', 'control', 'datum', 'collection', 'design', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Differential', 'Item', 'Functioning', 'Minority', 'Examinees', 'SAT']",standardization approach assess differential item function DIF include standardized distractor analysis describe result study conduct Asian Americans Hispanics Mexican Americans Puerto Ricans Blacks Scholastic Aptitude Test SAT describe synthesize study group limit include examinee speak English good language item form ethnic group exhibit large dif major finding include evidence differential speededness minority examinee complete sat‐verbal section rate white student comparable sat‐verbal score Blacks Hispanics item content special interest advantage relevant ethnic group addition homograph tend disadvantage ethnic group effect vertical relationship analogy item consistent finding important understand DIF account difference variable relate DIF need identify furthermore finding tentative corroborate study control datum collection design copyright © 1990 Wiley Blackwell right reserve,Differential Item Functioning Minority Examinees SAT,0.022116159990735094,0.022471100937226034,0.02246830155640219,0.25350176624348153,0.6794426712721551,0.0050976067565807045,0.03147107338138953,0.11719218613860015,0.0,0.019447223760007105
Kelderman H.; Macready G.B.,The Use of Loglinear Models for Assessing Differential Item Functioning Across Manifest and Latent Examinee Groups,1990,27,"Loglinear latent class models are used to detect differential item functioning (DIF). These models are formulated in such a manner that the attribute to be assessed may be continuous, as in a Rasch model, or categorical, as in Latent Class Mastery models. Further, an item may exhibit DIF with respect to a manifest grouping variable, a latent grouping variable, or both. Likelihood‐ratio tests for assessing the presence of various types of DIF are described, and these methods are illustrated through the analysis of a “real world” data set. Copyright © 1990, Wiley Blackwell. All rights reserved",The Use of Loglinear Models for Assessing Differential Item Functioning Across Manifest and Latent Examinee Groups,"Loglinear latent class models are used to detect differential item functioning (DIF). These models are formulated in such a manner that the attribute to be assessed may be continuous, as in a Rasch model, or categorical, as in Latent Class Mastery models. Further, an item may exhibit DIF with respect to a manifest grouping variable, a latent grouping variable, or both. Likelihood‐ratio tests for assessing the presence of various types of DIF are described, and these methods are illustrated through the analysis of a “real world” data set. Copyright © 1990, Wiley Blackwell. All rights reserved","['loglinear', 'latent', 'class', 'detect', 'differential', 'item', 'function', 'DIF', 'formulate', 'manner', 'attribute', 'assess', 'continuous', 'Rasch', 'categorical', 'Latent', 'Class', 'Mastery', 'far', 'item', 'exhibit', 'DIF', 'respect', 'manifest', 'grouping', 'variable', 'latent', 'group', 'variable', 'likelihood‐ratio', 'test', 'assess', 'presence', 'type', 'DIF', 'describe', 'method', 'illustrate', 'analysis', '""', 'real', 'world', '""', 'datum', 'set', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Use', 'Loglinear', 'Models', 'assess', 'Differential', 'Item', 'Functioning', 'Manifest', 'Latent', 'Examinee', 'Groups']","loglinear latent class detect differential item function DIF formulate manner attribute assess continuous Rasch categorical Latent Class Mastery far item exhibit DIF respect manifest grouping variable latent group variable likelihood‐ratio test assess presence type DIF describe method illustrate analysis "" real world "" datum set Copyright © 1990 Wiley Blackwell right reserve",Use Loglinear Models assess Differential Item Functioning Manifest Latent Examinee Groups,0.03031640330614009,0.030297827768747832,0.030491875625342852,0.030211762821127273,0.878682130478642,0.0,0.0016042570957196893,0.1481900977710213,0.0,0.008814858035561556
Becker D.F.; Forsyth R.A.,An Empirical Investigation of Thurstone and IRT Methods of Scaling Achievement Tests,1992,29,"The purpose of this study was to investigate the nature and characteristics of the measurement scales developed using Thurstone and item response theory (IRT) methods of scaling achievement tests for the same set of data. Expanded standard score scales were created using Thurstone, one‐parameter IRT, and three‐parameter IRT models, and descriptive information on achievement growth and variability was obtained for examinees in Grades 9 through 12 in the subject areas of vocabulary, reading, and mathematics. The results indicated increasing variability in all three test areas for all three scaling methods as grade level increased. In addition, greater average growth across grades was observed at the 90th percentile as compared to the 10th percentile, even with the IRT‐based scales. Copyright © 1992, Wiley Blackwell. All rights reserved",An Empirical Investigation of Thurstone and IRT Methods of Scaling Achievement Tests,"The purpose of this study was to investigate the nature and characteristics of the measurement scales developed using Thurstone and item response theory (IRT) methods of scaling achievement tests for the same set of data. Expanded standard score scales were created using Thurstone, one‐parameter IRT, and three‐parameter IRT models, and descriptive information on achievement growth and variability was obtained for examinees in Grades 9 through 12 in the subject areas of vocabulary, reading, and mathematics. The results indicated increasing variability in all three test areas for all three scaling methods as grade level increased. In addition, greater average growth across grades was observed at the 90th percentile as compared to the 10th percentile, even with the IRT‐based scales. Copyright © 1992, Wiley Blackwell. All rights reserved","['purpose', 'study', 'investigate', 'nature', 'characteristic', 'scale', 'develop', 'Thurstone', 'item', 'response', 'theory', 'IRT', 'method', 'scale', 'achievement', 'test', 'set', 'datum', 'expand', 'standard', 'score', 'scale', 'create', 'Thurstone', 'one‐parameter', 'IRT', 'three‐parameter', 'IRT', 'descriptive', 'information', 'achievement', 'growth', 'variability', 'obtain', 'examinee', 'Grades', '9', '12', 'subject', 'area', 'vocabulary', 'reading', 'mathematic', 'result', 'indicate', 'increase', 'variability', 'test', 'area', 'scaling', 'method', 'grade', 'level', 'increase', 'addition', 'great', 'average', 'growth', 'grade', 'observe', '90th', 'percentile', 'compare', '10th', 'percentile', 'irt‐base', 'scale', 'copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Empirical', 'Investigation', 'Thurstone', 'IRT', 'Methods', 'Scaling', 'Achievement', 'test']",purpose study investigate nature characteristic scale develop Thurstone item response theory IRT method scale achievement test set datum expand standard score scale create Thurstone one‐parameter IRT three‐parameter IRT descriptive information achievement growth variability obtain examinee Grades 9 12 subject area vocabulary reading mathematic result indicate increase variability test area scaling method grade level increase addition great average growth grade observe 90th percentile compare 10th percentile irt‐base scale copyright © 1992 Wiley Blackwell right reserve,Empirical Investigation Thurstone IRT Methods Scaling Achievement test,0.02745906176751515,0.8895565305195827,0.027680496501600232,0.027158936300240737,0.0281449749110611,0.0015523800498053303,0.01322248494304383,0.0,0.10612055318828888,0.07633197394139064
Beaton A.E.; Johnson E.G.,Overview of the Scaling Methodology Used in the National Assessment,1992,29,"The National Assessment of Educational Progress (NAEP) uses item response theory (IRT)–based scaling methods to summarize the information in complex data sets. Scale scores are presented as tools for illuminating patterns in the data and for exploiting regularities across patterns of responses to tasks requiring similar skills. In this way, the dominant features of the data are captured. Discussed are the necessity of global scores or more detailed subscores, the creation of developmental scales spanning different age levels, and the use of scale anchoring as a way of interpreting the scales. Copyright © 1992, Wiley Blackwell. All rights reserved",Overview of the Scaling Methodology Used in the National Assessment,"The National Assessment of Educational Progress (NAEP) uses item response theory (IRT)–based scaling methods to summarize the information in complex data sets. Scale scores are presented as tools for illuminating patterns in the data and for exploiting regularities across patterns of responses to tasks requiring similar skills. In this way, the dominant features of the data are captured. Discussed are the necessity of global scores or more detailed subscores, the creation of developmental scales spanning different age levels, and the use of scale anchoring as a way of interpreting the scales. Copyright © 1992, Wiley Blackwell. All rights reserved","['National', 'Assessment', 'Educational', 'Progress', 'NAEP', 'item', 'response', 'theory', 'IRT', '–', 'base', 'scale', 'method', 'summarize', 'information', 'complex', 'data', 'set', 'scale', 'score', 'present', 'tool', 'illuminate', 'pattern', 'datum', 'exploit', 'regularity', 'pattern', 'response', 'task', 'require', 'similar', 'skill', 'way', 'dominant', 'feature', 'datum', 'capture', 'discuss', 'necessity', 'global', 'score', 'detailed', 'subscore', 'creation', 'developmental', 'scale', 'span', 'different', 'age', 'level', 'scale', 'anchor', 'way', 'interpret', 'scale', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['overview', 'Scaling', 'Methodology', 'National', 'Assessment']",National Assessment Educational Progress NAEP item response theory IRT – base scale method summarize information complex data set scale score present tool illuminate pattern datum exploit regularity pattern response task require similar skill way dominant feature datum capture discuss necessity global score detailed subscore creation developmental scale span different age level scale anchor way interpret scale Copyright © 1992 Wiley Blackwell right reserve,overview Scaling Methodology National Assessment,0.02728958198635227,0.02745548182430241,0.778964942191524,0.02689265182583687,0.1393973421719845,0.0,0.016606081766552917,0.0016066911287382721,0.09337843539416246,0.06341596653399421
Samsa G.P.,Resolution of a Regression Paradox in Pretest‐Posttest Designs,1992,29,"Regression to the mean (RTM) of individuals is the tendency for extreme individuals to become less extreme on remeasurement; RTM of group means describes this same tendency among group means. Under the classical test model, in pretest‐posttest designs where subjects are selected on the basis of extreme values at the pretest, RTM of group means will always occur for the attribute used to select extreme subjects. For other attributes, however, RTM of group means requires a positive correlation between that attribute's measurement error and the measurement error of the attribute used in the selection. Thus, while all attributes will evidence RTM of individuals, extreme groups do not always regress to the mean. RTM depends most fundamentally on the magnitude of the pretest measurement error. Copyright © 1992, Wiley Blackwell. All rights reserved",Resolution of a Regression Paradox in Pretest‐Posttest Designs,"Regression to the mean (RTM) of individuals is the tendency for extreme individuals to become less extreme on remeasurement; RTM of group means describes this same tendency among group means. Under the classical test model, in pretest‐posttest designs where subjects are selected on the basis of extreme values at the pretest, RTM of group means will always occur for the attribute used to select extreme subjects. For other attributes, however, RTM of group means requires a positive correlation between that attribute's measurement error and the measurement error of the attribute used in the selection. Thus, while all attributes will evidence RTM of individuals, extreme groups do not always regress to the mean. RTM depends most fundamentally on the magnitude of the pretest measurement error. Copyright © 1992, Wiley Blackwell. All rights reserved","['regression', 'mean', 'RTM', 'individual', 'tendency', 'extreme', 'individual', 'extreme', 'remeasurement', 'RTM', 'group', 'mean', 'describe', 'tendency', 'group', 'mean', 'classical', 'test', 'pretest‐postt', 'design', 'subject', 'select', 'basis', 'extreme', 'value', 'pret', 'RTM', 'group', 'mean', 'occur', 'attribute', 'select', 'extreme', 'subject', 'attribute', 'RTM', 'group', 'mean', 'require', 'positive', 'correlation', 'attribute', 'error', 'error', 'attribute', 'selection', 'attribute', 'evidence', 'RTM', 'individual', 'extreme', 'group', 'regress', 'mean', 'RTM', 'depend', 'fundamentally', 'magnitude', 'pret', 'error', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['resolution', 'Regression', 'Paradox', 'pretest‐postt', 'design']",regression mean RTM individual tendency extreme individual extreme remeasurement RTM group mean describe tendency group mean classical test pretest‐postt design subject select basis extreme value pret RTM group mean occur attribute select extreme subject attribute RTM group mean require positive correlation attribute error error attribute selection attribute evidence RTM individual extreme group regress mean RTM depend fundamentally magnitude pret error Copyright © 1992 Wiley Blackwell right reserve,resolution Regression Paradox pretest‐postt design,0.03916148303593868,0.03919457019378791,0.8425772517577569,0.03860110312695094,0.04046559188556553,0.016536419715974693,0.0009127391818994445,0.017601136224152776,0.0137902213099556,0.011571093533279276
Hambleton R.K.; Jones R.W.; Rogers H.J.,Influence of Item Parameter Estimation Errors in Test Development,1993,30,"Item response models are finding increasing use in achievement and aptitude test development. Item response theory (IRT) test development involves the selection of test items based on a consideration of their item information functions. But a problem arises because item information functions are determined by their item parameter estimates, which contain error. When the “best” items are selected on the basis of their statistical characteristics, there is a tendency to capitalize on chance due to errors in the item parameter estimates. The resulting test, therefore, falls short of the test that was desired or expected. The purposes of this article are (a) to highlight the problem of item parameter estimation errors in the test development process, (b) to demonstrate the seriousness of the problem with several simulated data sets, and (c) to offer a conservative solution for addressing the problem in IRT‐based test development. Copyright © 1993, Wiley Blackwell. All rights reserved",Influence of Item Parameter Estimation Errors in Test Development,"Item response models are finding increasing use in achievement and aptitude test development. Item response theory (IRT) test development involves the selection of test items based on a consideration of their item information functions. But a problem arises because item information functions are determined by their item parameter estimates, which contain error. When the “best” items are selected on the basis of their statistical characteristics, there is a tendency to capitalize on chance due to errors in the item parameter estimates. The resulting test, therefore, falls short of the test that was desired or expected. The purposes of this article are (a) to highlight the problem of item parameter estimation errors in the test development process, (b) to demonstrate the seriousness of the problem with several simulated data sets, and (c) to offer a conservative solution for addressing the problem in IRT‐based test development. Copyright © 1993, Wiley Blackwell. All rights reserved","['item', 'response', 'find', 'increase', 'achievement', 'aptitude', 'test', 'development', 'Item', 'response', 'theory', 'IRT', 'test', 'development', 'involve', 'selection', 'test', 'item', 'base', 'consideration', 'item', 'information', 'function', 'problem', 'arise', 'item', 'information', 'function', 'determine', 'item', 'parameter', 'estimate', 'contain', 'error', '""', 'good', '""', 'item', 'select', 'basis', 'statistical', 'characteristic', 'tendency', 'capitalize', 'chance', 'error', 'item', 'parameter', 'estimate', 'result', 'test', 'fall', 'short', 'test', 'desire', 'expect', 'purpose', 'article', 'highlight', 'problem', 'item', 'parameter', 'estimation', 'error', 'test', 'development', 'process', 'b', 'demonstrate', 'seriousness', 'problem', 'simulate', 'data', 'set', 'c', 'offer', 'conservative', 'solution', 'address', 'problem', 'irt‐base', 'test', 'development', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['influence', 'Item', 'Parameter', 'Estimation', 'Errors', 'Test', 'Development']","item response find increase achievement aptitude test development Item response theory IRT test development involve selection test item base consideration item information function problem arise item information function determine item parameter estimate contain error "" good "" item select basis statistical characteristic tendency capitalize chance error item parameter estimate result test fall short test desire expect purpose article highlight problem item parameter estimation error test development process b demonstrate seriousness problem simulate data set c offer conservative solution address problem irt‐base test development Copyright © 1993 Wiley Blackwell right reserve",influence Item Parameter Estimation Errors Test Development,0.0267539090716019,0.026753571612467678,0.8928923216619221,0.026262044163261507,0.02733815349074686,0.023849231492809184,0.1259369998284743,0.023777149322024283,0.02411510209625823,0.0
Bennett R.E.; Rock D.A.; Wang M.,Equivalence of Free‐Response and Multiple‐Choice Items,1991,28,"This study examined the relationship of multiple‐choice and free‐response items contained on the College Board's Advanced Placement Computer Science (APCS) examination. Confirmatory factor analysis was used to test the fit of a two‐factor model where each item format marked its own factor. Results showed a single‐factor solution to provide the most parsimonious fit in each of two random‐half samples. This finding might be accounted for by several mechanisms, including overlap in the specific processes assessed by the multiple‐choice and free‐response items and the limited opportunity for skill differentiation afforded by the year‐long APCS course. Copyright © 1991, Wiley Blackwell. All rights reserved",Equivalence of Free‐Response and Multiple‐Choice Items,"This study examined the relationship of multiple‐choice and free‐response items contained on the College Board's Advanced Placement Computer Science (APCS) examination. Confirmatory factor analysis was used to test the fit of a two‐factor model where each item format marked its own factor. Results showed a single‐factor solution to provide the most parsimonious fit in each of two random‐half samples. This finding might be accounted for by several mechanisms, including overlap in the specific processes assessed by the multiple‐choice and free‐response items and the limited opportunity for skill differentiation afforded by the year‐long APCS course. Copyright © 1991, Wiley Blackwell. All rights reserved","['study', 'examine', 'relationship', 'multiple‐choice', 'free‐response', 'item', 'contain', 'College', 'Boards', 'Advanced', 'Placement', 'Computer', 'Science', 'APCS', 'examination', 'Confirmatory', 'factor', 'analysis', 'test', 'fit', 'two‐factor', 'item', 'format', 'mark', 'factor', 'result', 'single‐factor', 'solution', 'provide', 'parsimonious', 'fit', 'random‐half', 'sample', 'finding', 'account', 'mechanism', 'include', 'overlap', 'specific', 'process', 'assess', 'multiple‐choice', 'free‐response', 'item', 'limited', 'opportunity', 'skill', 'differentiation', 'afford', 'year‐long', 'APCS', 'course', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['equivalence', 'Free‐Response', 'Multiple‐Choice', 'Items']",study examine relationship multiple‐choice free‐response item contain College Boards Advanced Placement Computer Science APCS examination Confirmatory factor analysis test fit two‐factor item format mark factor result single‐factor solution provide parsimonious fit random‐half sample finding account mechanism include overlap specific process assess multiple‐choice free‐response item limited opportunity skill differentiation afford year‐long APCS course Copyright © 1991 Wiley Blackwell right reserve,equivalence Free‐Response Multiple‐Choice Items,0.02797619035300385,0.02793354142163928,0.02776337855847675,0.8882220463301,0.028104843336780167,0.0012006481917479495,0.08883304778511894,0.0,0.0,0.01720336841029129
Norris S.P.,Effect of Eliciting Verbal Reports of Thinking on Critical Thinking Test Performance,1990,27,"Verbal reports of examinees' thinking on multiple‐choice critical thinking test items can provide useful validation data only if the verbal reporting does not change the course of examinees' thinking and performance. Using a completely randomized factorial design, 343 senior high school students were divided into five groups. In four of the groups, different procedures were used to elicit students' thinking as they worked through Part A of a critical thinking test of observation appraisal (Norris & King, 1983). In the control group, students took the same test in paper‐and‐pencil format. There were no significant differences in test performance among the five groups nor in the quality of thinking among the four groups from whom verbal reports of thinking were elicited. These results are evidence that verbal reports of thinking can meet one of the necessary conditions of useful validation data—namely, that collecting the data does not alter examinees' thinking and performance. Some analyses found significant interviewer main effects and sex‐by‐interviewer and elicitation‐level‐by‐interviewer‐by‐sex‐by‐grade interaction effects. Analysis of these interactions suggested that the role of the interviewer might limit the generality of the technique. Copyright © 1990, Wiley Blackwell. All rights reserved",Effect of Eliciting Verbal Reports of Thinking on Critical Thinking Test Performance,"Verbal reports of examinees' thinking on multiple‐choice critical thinking test items can provide useful validation data only if the verbal reporting does not change the course of examinees' thinking and performance. Using a completely randomized factorial design, 343 senior high school students were divided into five groups. In four of the groups, different procedures were used to elicit students' thinking as they worked through Part A of a critical thinking test of observation appraisal (Norris & King, 1983). In the control group, students took the same test in paper‐and‐pencil format. There were no significant differences in test performance among the five groups nor in the quality of thinking among the four groups from whom verbal reports of thinking were elicited. These results are evidence that verbal reports of thinking can meet one of the necessary conditions of useful validation data—namely, that collecting the data does not alter examinees' thinking and performance. Some analyses found significant interviewer main effects and sex‐by‐interviewer and elicitation‐level‐by‐interviewer‐by‐sex‐by‐grade interaction effects. Analysis of these interactions suggested that the role of the interviewer might limit the generality of the technique. Copyright © 1990, Wiley Blackwell. All rights reserved","['verbal', 'report', 'examinee', 'think', 'multiple‐choice', 'critical', 'thinking', 'test', 'item', 'provide', 'useful', 'validation', 'datum', 'verbal', 'reporting', 'change', 'course', 'examinee', 'think', 'performance', 'completely', 'randomize', 'factorial', 'design', '343', 'senior', 'high', 'school', 'student', 'divide', 'group', 'group', 'different', 'procedure', 'elicit', 'student', 'think', 'work', 'Part', 'A', 'critical', 'thinking', 'test', 'observation', 'appraisal', 'Norris', 'King', '1983', 'control', 'group', 'student', 'test', 'paper‐and‐pencil', 'format', 'significant', 'difference', 'test', 'performance', 'group', 'quality', 'thinking', 'group', 'verbal', 'report', 'thinking', 'elicit', 'result', 'evidence', 'verbal', 'report', 'thinking', 'meet', 'necessary', 'condition', 'useful', 'validation', 'datum', '—', 'collect', 'datum', 'alter', 'examinee', 'think', 'performance', 'analysis', 'find', 'significant', 'interviewer', 'main', 'effect', 'sex‐by‐interviewer', 'elicitation‐level‐by‐interviewer‐by‐sex‐by‐grade', 'interaction', 'effect', 'Analysis', 'interaction', 'suggest', 'role', 'interviewer', 'limit', 'generality', 'technique', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['effect', 'Eliciting', 'Verbal', 'Reports', 'think', 'Critical', 'Thinking', 'Test', 'Performance']",verbal report examinee think multiple‐choice critical thinking test item provide useful validation datum verbal reporting change course examinee think performance completely randomize factorial design 343 senior high school student divide group group different procedure elicit student think work Part A critical thinking test observation appraisal Norris King 1983 control group student test paper‐and‐pencil format significant difference test performance group quality thinking group verbal report thinking elicit result evidence verbal report thinking meet necessary condition useful validation datum — collect datum alter examinee think performance analysis find significant interviewer main effect sex‐by‐interviewer elicitation‐level‐by‐interviewer‐by‐sex‐by‐grade interaction effect Analysis interaction suggest role interviewer limit generality technique Copyright © 1990 Wiley Blackwell right reserve,effect Eliciting Verbal Reports think Critical Thinking Test Performance,0.026304754983462405,0.0269451139178075,0.02656209780247962,0.7534227860365936,0.16676524725965675,0.013099303016678766,0.04449498450147591,0.006355967218743333,0.0,0.047100540135915074
Kolen M.J.; Hanson B.A.; Brennan R.L.,Conditional Standard Errors of Measurement for Scale Scores,1992,29,"Standard errors of measurement of scale scores by score level (conditional standard errors of measurement) can be valuable to users of test results. In addition, the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 1985) recommends that conditional standard errors be reported by test developers. Although a variety of procedures are available for estimating conditional standard errors of measurement for raw scores, few procedures exist for estimating conditional standard errors of measurement for scale scores from a single test administration. In this article, a procedure is described for estimating the reliability and conditional standard errors of measurement of scale scores. This method is illustrated using a strong true score model. Practical applications of this methodology are given. These applications include a procedure for constructing score scales that equalize standard errors of measurement along the score scale. Also included are examples of the effects of various nonlinear raw‐to‐scale score transformations on scale score reliability and conditional standard errors of measurement. These illustrations examine the effects on scale score reliability and conditional standard errors of measurement of (a) the different types of raw‐to‐scale score transformations (e.g., normalizing scores), (b) the number of scale score points used, and (c) the transformation used to equate alternate forms of a test. All the illustrations use data from the ACT Assessment testing program. Copyright © 1992, Wiley Blackwell. All rights reserved",Conditional Standard Errors of Measurement for Scale Scores,"Standard errors of measurement of scale scores by score level (conditional standard errors of measurement) can be valuable to users of test results. In addition, the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 1985) recommends that conditional standard errors be reported by test developers. Although a variety of procedures are available for estimating conditional standard errors of measurement for raw scores, few procedures exist for estimating conditional standard errors of measurement for scale scores from a single test administration. In this article, a procedure is described for estimating the reliability and conditional standard errors of measurement of scale scores. This method is illustrated using a strong true score model. Practical applications of this methodology are given. These applications include a procedure for constructing score scales that equalize standard errors of measurement along the score scale. Also included are examples of the effects of various nonlinear raw‐to‐scale score transformations on scale score reliability and conditional standard errors of measurement. These illustrations examine the effects on scale score reliability and conditional standard errors of measurement of (a) the different types of raw‐to‐scale score transformations (e.g., normalizing scores), (b) the number of scale score points used, and (c) the transformation used to equate alternate forms of a test. All the illustrations use data from the ACT Assessment testing program. Copyright © 1992, Wiley Blackwell. All rights reserved","['standard', 'error', 'scale', 'score', 'score', 'level', 'conditional', 'standard', 'error', 'valuable', 'user', 'test', 'result', 'addition', 'Standards', 'Educational', 'Psychological', 'Testing', 'AERA', 'APA', 'NCME', '1985', 'recommend', 'conditional', 'standard', 'error', 'report', 'test', 'developer', 'variety', 'procedure', 'available', 'estimate', 'conditional', 'standard', 'error', 'raw', 'score', 'procedure', 'exist', 'estimate', 'conditional', 'standard', 'error', 'scale', 'score', 'single', 'test', 'administration', 'article', 'procedure', 'describe', 'estimate', 'reliability', 'conditional', 'standard', 'error', 'scale', 'score', 'method', 'illustrate', 'strong', 'true', 'score', 'practical', 'application', 'methodology', 'application', 'include', 'procedure', 'construct', 'score', 'scale', 'equalize', 'standard', 'error', 'score', 'scale', 'include', 'example', 'effect', 'nonlinear', 'raw‐to‐scale', 'score', 'transformation', 'scale', 'score', 'reliability', 'conditional', 'standard', 'error', 'illustration', 'examine', 'effect', 'scale', 'score', 'reliability', 'conditional', 'standard', 'error', 'different', 'type', 'raw‐to‐scale', 'score', 'transformation', 'eg', 'normalize', 'score', 'b', 'number', 'scale', 'score', 'point', 'c', 'transformation', 'equate', 'alternate', 'form', 'test', 'illustration', 'datum', 'ACT', 'Assessment', 'testing', 'program', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Conditional', 'Standard', 'Errors', 'Scale', 'Scores']",standard error scale score score level conditional standard error valuable user test result addition Standards Educational Psychological Testing AERA APA NCME 1985 recommend conditional standard error report test developer variety procedure available estimate conditional standard error raw score procedure exist estimate conditional standard error scale score single test administration article procedure describe estimate reliability conditional standard error scale score method illustrate strong true score practical application methodology application include procedure construct score scale equalize standard error score scale include example effect nonlinear raw‐to‐scale score transformation scale score reliability conditional standard error illustration examine effect scale score reliability conditional standard error different type raw‐to‐scale score transformation eg normalize score b number scale score point c transformation equate alternate form test illustration datum ACT Assessment testing program Copyright © 1992 Wiley Blackwell right reserve,Conditional Standard Errors Scale Scores,0.029581160240472013,0.02957914013594422,0.0297337760853949,0.029394103860899967,0.881711819677289,0.0,0.0,0.0,0.3291773690926981,0.0
Reise S.P.; Yu J.,Parameter Recovery in the Graded Response Model Using MULTILOG,1990,27,"The graded response model can be used to describe test‐taking behavior when item responses are classified into ordered categories. In this study, parameter recovery in the graded response model was investigated using the MULTILOG computer program under default conditions. Based on items having five response categories, 36 simulated data sets were generated that varied on true θ distribution, true item discrimination distribution, and calibration sample size. The findings suggest, first, the correlations between the true and estimated parameters were consistently greater than 0.85 with sample sizes of at least 500. Second, the root mean square error differences between true and estimated parameters were comparable with results from binary data parameter recovery studies. Of special note was the finding that the calibration sample size had little influence on the recovery of the true ability parameter but did influence item‐parameter recovery. Therefore, it appeared that item‐parameter estimation error, due to small calibration samples, did not result in poor person‐parameter estimation. It was concluded that at least 500 examinees are needed to achieve an adequate calibration under the graded model. Copyright © 1990, Wiley Blackwell. All rights reserved",Parameter Recovery in the Graded Response Model Using MULTILOG,"The graded response model can be used to describe test‐taking behavior when item responses are classified into ordered categories. In this study, parameter recovery in the graded response model was investigated using the MULTILOG computer program under default conditions. Based on items having five response categories, 36 simulated data sets were generated that varied on true θ distribution, true item discrimination distribution, and calibration sample size. The findings suggest, first, the correlations between the true and estimated parameters were consistently greater than 0.85 with sample sizes of at least 500. Second, the root mean square error differences between true and estimated parameters were comparable with results from binary data parameter recovery studies. Of special note was the finding that the calibration sample size had little influence on the recovery of the true ability parameter but did influence item‐parameter recovery. Therefore, it appeared that item‐parameter estimation error, due to small calibration samples, did not result in poor person‐parameter estimation. It was concluded that at least 500 examinees are needed to achieve an adequate calibration under the graded model. Copyright © 1990, Wiley Blackwell. All rights reserved","['grade', 'response', 'describe', 'test‐take', 'behavior', 'item', 'response', 'classify', 'order', 'category', 'study', 'parameter', 'recovery', 'grade', 'response', 'investigate', 'MULTILOG', 'computer', 'program', 'default', 'condition', 'base', 'item', 'response', 'category', '36', 'simulated', 'data', 'set', 'generate', 'varied', 'true', 'θ', 'distribution', 'true', 'item', 'discrimination', 'distribution', 'calibration', 'sample', 'size', 'finding', 'suggest', 'correlation', 'true', 'estimate', 'parameter', 'consistently', 'great', '085', 'sample', 'size', '500', 'Second', 'root', 'mean', 'square', 'error', 'difference', 'true', 'estimate', 'parameter', 'comparable', 'result', 'binary', 'data', 'parameter', 'recovery', 'study', 'special', 'note', 'finding', 'calibration', 'sample', 'size', 'little', 'influence', 'recovery', 'true', 'ability', 'parameter', 'influence', 'item‐parameter', 'recovery', 'appear', 'item‐paramet', 'estimation', 'error', 'small', 'calibration', 'sample', 'result', 'poor', 'person‐parameter', 'estimation', 'conclude', '500', 'examinee', 'need', 'achieve', 'adequate', 'calibration', 'grade', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Parameter', 'Recovery', 'Graded', 'Response', 'Using', 'multilog']",grade response describe test‐take behavior item response classify order category study parameter recovery grade response investigate MULTILOG computer program default condition base item response category 36 simulated data set generate varied true θ distribution true item discrimination distribution calibration sample size finding suggest correlation true estimate parameter consistently great 085 sample size 500 Second root mean square error difference true estimate parameter comparable result binary data parameter recovery study special note finding calibration sample size little influence recovery true ability parameter influence item‐parameter recovery appear item‐paramet estimation error small calibration sample result poor person‐parameter estimation conclude 500 examinee need achieve adequate calibration grade Copyright © 1990 Wiley Blackwell right reserve,Parameter Recovery Graded Response Using multilog,0.8979948971997245,0.025553942666409115,0.025324129726486386,0.025041073018310583,0.026085957389069468,0.09662848417540093,0.0,0.0,0.0,0.010357816519210613
Yen W.M.,Scaling Performance Assessments: Strategies for Managing Local Item Dependence,1993,30,"Performance assessments appear on a priori grounds to be likely to produce far more local item dependence (LID) than that produced in the use of traditional multiple‐choice tests. This article (a) defines local item independence, (b) presents a compendium of causes of LID, (c) discusses some of LID's practical measurement implications, (d) details some empirical results for both performance assessments and multiple‐choice tests, and (e) suggests some strategies for managing LID in order to avoid negative measurement consequences. Copyright © 1993, Wiley Blackwell. All rights reserved",Scaling Performance Assessments: Strategies for Managing Local Item Dependence,"Performance assessments appear on a priori grounds to be likely to produce far more local item dependence (LID) than that produced in the use of traditional multiple‐choice tests. This article (a) defines local item independence, (b) presents a compendium of causes of LID, (c) discusses some of LID's practical measurement implications, (d) details some empirical results for both performance assessments and multiple‐choice tests, and (e) suggests some strategies for managing LID in order to avoid negative measurement consequences. Copyright © 1993, Wiley Blackwell. All rights reserved","['performance', 'assessment', 'appear', 'priori', 'ground', 'likely', 'produce', 'far', 'local', 'item', 'dependence', 'LID', 'produce', 'traditional', 'multiple‐choice', 'test', 'article', 'define', 'local', 'item', 'independence', 'b', 'present', 'compendium', 'cause', 'LID', 'c', 'discuss', 'LIDs', 'practical', 'implication', 'd', 'detail', 'empirical', 'result', 'performance', 'assessment', 'multiple‐choice', 'test', 'e', 'suggest', 'strategy', 'manage', 'LID', 'order', 'avoid', 'negative', 'consequence', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['scale', 'performance', 'assessment', 'strategy', 'manage', 'Local', 'Item', 'Dependence']",performance assessment appear priori ground likely produce far local item dependence LID produce traditional multiple‐choice test article define local item independence b present compendium cause LID c discuss LIDs practical implication d detail empirical result performance assessment multiple‐choice test e suggest strategy manage LID order avoid negative consequence Copyright © 1993 Wiley Blackwell right reserve,scale performance assessment strategy manage Local Item Dependence,0.8820881081565729,0.02932234211243094,0.0294988681842596,0.029180025551507942,0.029910655995228765,0.0,0.05078799983515055,0.00264029744989615,0.0,0.05477624141258867
Busch J.C.; Jaeger R.M.,"Influence of Type of Judge, Normative Information, and Discussion on Standards Recommended for the National Teacher Examinations",1990,27,"There are few empirical investigations of the consequences of using widely recommended data collection procedures in conjunction with a specific standardsetting method such as the Angoff (1971) procedure. Such recommendations include the use of several types of judges, the provision of normative information on examinees' test performance, and the opportunity to discuss and reconsider initial recommendations in an iterative standard‐setting procedure. This study of 236 expert judges investigated the effects of using these recommended procedures on (a) average recommended test standards, (b) the variability of recommended test standards, and (c) the reliability of recommended standards for seven subtests of the National Teacher Examinations Communication Skills and General Knowledge Tests. Small, but sometimes statistically significant, changes in mean recommended test standards were observed when judges were allowed to reconsider their initial recommendations following review of normative information and discussion. Means for public school judges changed more than did those for college or university judges. In addition, there was a significant reduction in the within‐group variability of standards recommended for several subtests. Methods for estimating the reliability of recommended test standards proposed by Kane and Wilson (1984) were applied, and their hypothesis of positive covariation between empirical item difficulties and mean recommended standards was confirmed. The data collection procedures examined in this study resulted in substantial increases in the reliability of recommended test standards. Copyright © 1990, Wiley Blackwell. All rights reserved","Influence of Type of Judge, Normative Information, and Discussion on Standards Recommended for the National Teacher Examinations","There are few empirical investigations of the consequences of using widely recommended data collection procedures in conjunction with a specific standardsetting method such as the Angoff (1971) procedure. Such recommendations include the use of several types of judges, the provision of normative information on examinees' test performance, and the opportunity to discuss and reconsider initial recommendations in an iterative standard‐setting procedure. This study of 236 expert judges investigated the effects of using these recommended procedures on (a) average recommended test standards, (b) the variability of recommended test standards, and (c) the reliability of recommended standards for seven subtests of the National Teacher Examinations Communication Skills and General Knowledge Tests. Small, but sometimes statistically significant, changes in mean recommended test standards were observed when judges were allowed to reconsider their initial recommendations following review of normative information and discussion. Means for public school judges changed more than did those for college or university judges. In addition, there was a significant reduction in the within‐group variability of standards recommended for several subtests. Methods for estimating the reliability of recommended test standards proposed by Kane and Wilson (1984) were applied, and their hypothesis of positive covariation between empirical item difficulties and mean recommended standards was confirmed. The data collection procedures examined in this study resulted in substantial increases in the reliability of recommended test standards. Copyright © 1990, Wiley Blackwell. All rights reserved","['empirical', 'investigation', 'consequence', 'widely', 'recommend', 'datum', 'collection', 'procedure', 'conjunction', 'specific', 'standardsetting', 'method', 'Angoff', '1971', 'procedure', 'recommendation', 'include', 'type', 'judge', 'provision', 'normative', 'information', 'examinee', 'test', 'performance', 'opportunity', 'discuss', 'reconsider', 'initial', 'recommendation', 'iterative', 'standard‐sette', 'procedure', 'study', '236', 'expert', 'judge', 'investigate', 'effect', 'recommend', 'procedure', 'average', 'recommend', 'test', 'standard', 'b', 'variability', 'recommend', 'test', 'standard', 'c', 'reliability', 'recommend', 'standard', 'seven', 'subtest', 'National', 'Teacher', 'Examinations', 'Communication', 'Skills', 'General', 'Knowledge', 'Tests', 'Small', 'statistically', 'significant', 'change', 'mean', 'recommend', 'test', 'standard', 'observe', 'judge', 'allow', 'reconsider', 'initial', 'recommendation', 'follow', 'review', 'normative', 'information', 'discussion', 'Means', 'public', 'school', 'judge', 'change', 'college', 'university', 'judge', 'addition', 'significant', 'reduction', 'within‐group', 'variability', 'standard', 'recommend', 'subtest', 'Methods', 'estimate', 'reliability', 'recommend', 'test', 'standard', 'propose', 'Kane', 'Wilson', '1984', 'apply', 'hypothesis', 'positive', 'covariation', 'empirical', 'item', 'difficulty', 'mean', 'recommend', 'standard', 'confirm', 'datum', 'collection', 'procedure', 'examine', 'study', 'result', 'substantial', 'increase', 'reliability', 'recommend', 'test', 'standard', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['influence', 'Type', 'Judge', 'Normative', 'Information', 'Discussion', 'Standards', 'Recommended', 'National', 'Teacher', 'Examinations']",empirical investigation consequence widely recommend datum collection procedure conjunction specific standardsetting method Angoff 1971 procedure recommendation include type judge provision normative information examinee test performance opportunity discuss reconsider initial recommendation iterative standard‐sette procedure study 236 expert judge investigate effect recommend procedure average recommend test standard b variability recommend test standard c reliability recommend standard seven subtest National Teacher Examinations Communication Skills General Knowledge Tests Small statistically significant change mean recommend test standard observe judge allow reconsider initial recommendation follow review normative information discussion Means public school judge change college university judge addition significant reduction within‐group variability standard recommend subtest Methods estimate reliability recommend test standard propose Kane Wilson 1984 apply hypothesis positive covariation empirical item difficulty mean recommend standard confirm datum collection procedure examine study result substantial increase reliability recommend test standard Copyright © 1990 Wiley Blackwell right reserve,influence Type Judge Normative Information Discussion Standards Recommended National Teacher Examinations,0.02580870764459838,0.025714020801126756,0.02580035052762559,0.025519567693632436,0.8971573533330168,0.02130572723098421,0.03069533566015314,0.00016688652319799747,0.07954191875865531,0.013391910419305306
Baxter G.P.; Shavelson R.J.; Goldman S.R.; Pine J.,Evaluation of Procedure‐Based Scoring for Hands‐On Science Assessment,1992,29,"This article evaluates a procedure‐based scoring system for a performance assessment (an observed paper towels investigation) and a notebook surrogate completed by fifth‐grade students varying in hands‐on science experience. Results suggested interrater reliability of scores for observed performance and notebooks was adequate (>.80) with the reliability of the former higher. In contrast, interrater agreement on procedures was higher for observed hands‐on performance (.92) than for notebooks (.66). Moreover, for the notebooks, the reliability of scores and agreement on procedures varied by student experience, but this was not so for observed performance. Both the observed‐performance and notebook measures correlated less with traditional ability than did a multiple‐choice science achievement test. The correlation between the two performance assessments and the multiple‐choice test was only moderate (mean = .46), suggesting that different aspects of science achievement have been measured. Finally, the correlation between the observed‐performance scores and the notebook scores was .83, suggesting that notebooks may provide a reasonable, albeit less reliable, surrogate for the observed hands‐on performance of students. Copyright © 1992, Wiley Blackwell. All rights reserved",Evaluation of Procedure‐Based Scoring for Hands‐On Science Assessment,"This article evaluates a procedure‐based scoring system for a performance assessment (an observed paper towels investigation) and a notebook surrogate completed by fifth‐grade students varying in hands‐on science experience. Results suggested interrater reliability of scores for observed performance and notebooks was adequate (>.80) with the reliability of the former higher. In contrast, interrater agreement on procedures was higher for observed hands‐on performance (.92) than for notebooks (.66). Moreover, for the notebooks, the reliability of scores and agreement on procedures varied by student experience, but this was not so for observed performance. Both the observed‐performance and notebook measures correlated less with traditional ability than did a multiple‐choice science achievement test. The correlation between the two performance assessments and the multiple‐choice test was only moderate (mean = .46), suggesting that different aspects of science achievement have been measured. Finally, the correlation between the observed‐performance scores and the notebook scores was .83, suggesting that notebooks may provide a reasonable, albeit less reliable, surrogate for the observed hands‐on performance of students. Copyright © 1992, Wiley Blackwell. All rights reserved","['article', 'evaluate', 'procedure‐based', 'scoring', 'system', 'performance', 'assessment', 'observed', 'paper', 'towel', 'investigation', 'notebook', 'surrogate', 'complete', 'fifth‐grade', 'student', 'vary', 'hands‐on', 'science', 'experience', 'result', 'suggest', 'interrater', 'reliability', 'score', 'observe', 'performance', 'notebook', 'adequate', '80', 'reliability', 'high', 'contrast', 'interrater', 'agreement', 'procedure', 'high', 'observed', 'hands‐on', 'performance', '92', 'notebook', '66', 'notebook', 'reliability', 'score', 'agreement', 'procedure', 'vary', 'student', 'experience', 'observed', 'performance', 'observed‐performance', 'notebook', 'measure', 'correlate', 'traditional', 'ability', 'multiple‐choice', 'science', 'achievement', 'test', 'correlation', 'performance', 'assessment', 'multiple‐choice', 'test', 'moderate', 'mean', '46', 'suggest', 'different', 'aspect', 'science', 'achievement', 'measure', 'finally', 'correlation', 'observed‐performance', 'score', 'notebook', 'score', '83', 'suggest', 'notebook', 'provide', 'reasonable', 'albeit', 'reliable', 'surrogate', 'observed', 'hands‐on', 'performance', 'student', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['evaluation', 'Procedure‐Based', 'Scoring', 'Hands‐On', 'Science', 'Assessment']",article evaluate procedure‐based scoring system performance assessment observed paper towel investigation notebook surrogate complete fifth‐grade student vary hands‐on science experience result suggest interrater reliability score observe performance notebook adequate 80 reliability high contrast interrater agreement procedure high observed hands‐on performance 92 notebook 66 notebook reliability score agreement procedure vary student experience observed performance observed‐performance notebook measure correlate traditional ability multiple‐choice science achievement test correlation performance assessment multiple‐choice test moderate mean 46 suggest different aspect science achievement measure finally correlation observed‐performance score notebook score 83 suggest notebook provide reasonable albeit reliable surrogate observed hands‐on performance student Copyright © 1992 Wiley Blackwell right reserve,evaluation Procedure‐Based Scoring Hands‐On Science Assessment,0.029177691581278708,0.882818788619568,0.02939498342482739,0.02891404694461882,0.029694489429707187,0.0,6.137824959479422e-05,0.0,0.030991388518926532,0.1095450592934938
de Gruijter D.N.M.,A Note on the Bias of UCON Item Parameter Estimation in the Rasch Model,1990,27,"Divgi (1986) demonstrated that the bias of UCON item parameter estimates is not removed by the factor (n − 1)/n. Andrich (1989) argued in this journal that the demonstration was faulty. In this note a complete proof of Divgfs conclusion is presented. Copyright © 1990, Wiley Blackwell. All rights reserved",A Note on the Bias of UCON Item Parameter Estimation in the Rasch Model,"Divgi (1986) demonstrated that the bias of UCON item parameter estimates is not removed by the factor (n − 1)/n. Andrich (1989) argued in this journal that the demonstration was faulty. In this note a complete proof of Divgfs conclusion is presented. Copyright © 1990, Wiley Blackwell. All rights reserved","['Divgi', '1986', 'demonstrate', 'bias', 'UCON', 'item', 'parameter', 'estimate', 'remove', 'factor', 'n', '−', '1n', 'Andrich', '1989', 'argue', 'journal', 'demonstration', 'faulty', 'note', 'complete', 'proof', 'Divgfs', 'conclusion', 'present', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['note', 'Bias', 'UCON', 'Item', 'Parameter', 'Estimation', 'Rasch']",Divgi 1986 demonstrate bias UCON item parameter estimate remove factor n − 1n Andrich 1989 argue journal demonstration faulty note complete proof Divgfs conclusion present Copyright © 1990 Wiley Blackwell right reserve,note Bias UCON Item Parameter Estimation Rasch,0.03235782473590987,0.032584291746533614,0.032391306667741655,0.0322620064416205,0.8704045704081944,0.025867153811941674,0.012843471959488638,0.008613972438991082,0.0001507670302611556,0.00455367388238935
Breland H.M.; Danos D.O.; Kahn H.D.; Kubota M.Y.; Bonner M.W.,Performance Versus Objective Testing and Gender: An Exploratory Study of an Advanced Placement History Examination,1994,31,"To explore a phenomenon of gender differences in Advanced Placement examinations, random samples of free‐response test booklets were taken from the 1986 examination in U.S. History. These examinations were chosen because they consistently show significant gender differences in objective scores but no gender differences in free‐response scores. A rescoring of the free responses was conducted that focused on their historical content. This rescoring was conducted by readers other than those who conducted the original scoring and involved tallies of specific historical points made, supporting evidence given, and factual errors. Ratings were also made of handwriting quality, neatness, and English composition quality of the free responses. Analyses conducted indicate that free‐response tasks of the type examined may have inherent characteristics that reward English composition abilities, and that some females may compensate for inferior historical knowledge with superior English composition abilities. Copyright © 1994, Wiley Blackwell. All rights reserved",Performance Versus Objective Testing and Gender: An Exploratory Study of an Advanced Placement History Examination,"To explore a phenomenon of gender differences in Advanced Placement examinations, random samples of free‐response test booklets were taken from the 1986 examination in U.S. History. These examinations were chosen because they consistently show significant gender differences in objective scores but no gender differences in free‐response scores. A rescoring of the free responses was conducted that focused on their historical content. This rescoring was conducted by readers other than those who conducted the original scoring and involved tallies of specific historical points made, supporting evidence given, and factual errors. Ratings were also made of handwriting quality, neatness, and English composition quality of the free responses. Analyses conducted indicate that free‐response tasks of the type examined may have inherent characteristics that reward English composition abilities, and that some females may compensate for inferior historical knowledge with superior English composition abilities. Copyright © 1994, Wiley Blackwell. All rights reserved","['explore', 'phenomenon', 'gender', 'difference', 'Advanced', 'Placement', 'examination', 'random', 'sample', 'free‐response', 'test', 'booklet', '1986', 'examination', 'US', 'history', 'examination', 'choose', 'consistently', 'significant', 'gender', 'difference', 'objective', 'score', 'gender', 'difference', 'free‐response', 'score', 'rescoring', 'free', 'response', 'conduct', 'focus', 'historical', 'content', 'rescoring', 'conduct', 'reader', 'conduct', 'original', 'scoring', 'involve', 'tally', 'specific', 'historical', 'point', 'support', 'evidence', 'factual', 'error', 'rating', 'handwriting', 'quality', 'neatness', 'english', 'composition', 'quality', 'free', 'response', 'Analyses', 'conduct', 'indicate', 'free‐response', 'task', 'type', 'examine', 'inherent', 'characteristic', 'reward', 'english', 'composition', 'ability', 'female', 'compensate', 'inferior', 'historical', 'knowledge', 'superior', 'english', 'composition', 'ability', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['performance', 'Versus', 'Objective', 'Testing', 'Gender', 'An', 'Exploratory', 'Study', 'Advanced', 'Placement', 'History', 'Examination']",explore phenomenon gender difference Advanced Placement examination random sample free‐response test booklet 1986 examination US history examination choose consistently significant gender difference objective score gender difference free‐response score rescoring free response conduct focus historical content rescoring conduct reader conduct original scoring involve tally specific historical point support evidence factual error rating handwriting quality neatness english composition quality free response Analyses conduct indicate free‐response task type examine inherent characteristic reward english composition ability female compensate inferior historical knowledge superior english composition ability Copyright © 1994 Wiley Blackwell right reserve,performance Versus Objective Testing Gender An Exploratory Study Advanced Placement History Examination,0.026685551251332706,0.8928586460133224,0.027000069216879093,0.026501899688141678,0.026953833830324077,0.002667709369501432,0.045001215615759524,0.0,0.030539062663059263,0.01862537008430312
Miller T.R.; Spray J.A.,Logistic Discriminant Function Analysis for DIF Identification of Polytomously Scored Items,1993,30,"The purpose of this article is to present logistic discriminant function analysis as a means of differential item functioning (DIF) identification of items that are polytomously scored. The procedure is presented with examples of a DIF analysis using items from a 27‐item mathematics test which includes six open‐ended response items scored polytomously. The results show that the logistic discriminant function procedure is ideally suited for DIF identification on nondichotomously scored test items. It is simpler and more practical than polytomous extensions of the logistic regression DIF procedure and appears to fee more powerful than a generalized Mantel‐Haenszelprocedure. Copyright © 1993, Wiley Blackwell. All rights reserved",Logistic Discriminant Function Analysis for DIF Identification of Polytomously Scored Items,"The purpose of this article is to present logistic discriminant function analysis as a means of differential item functioning (DIF) identification of items that are polytomously scored. The procedure is presented with examples of a DIF analysis using items from a 27‐item mathematics test which includes six open‐ended response items scored polytomously. The results show that the logistic discriminant function procedure is ideally suited for DIF identification on nondichotomously scored test items. It is simpler and more practical than polytomous extensions of the logistic regression DIF procedure and appears to fee more powerful than a generalized Mantel‐Haenszelprocedure. Copyright © 1993, Wiley Blackwell. All rights reserved","['purpose', 'article', 'present', 'logistic', 'discriminant', 'function', 'analysis', 'means', 'differential', 'item', 'function', 'dif', 'identification', 'item', 'polytomously', 'score', 'procedure', 'present', 'example', 'DIF', 'analysis', 'item', '27‐item', 'mathematic', 'test', 'include', 'open‐ende', 'response', 'item', 'score', 'polytomously', 'result', 'logistic', 'discriminant', 'function', 'procedure', 'ideally', 'suit', 'dif', 'identification', 'nondichotomously', 'score', 'test', 'item', 'simple', 'practical', 'polytomous', 'extension', 'logistic', 'regression', 'dif', 'procedure', 'appear', 'fee', 'powerful', 'generalized', 'Mantel‐Haenszelprocedure', 'copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['logistic', 'Discriminant', 'Function', 'Analysis', 'DIF', 'identification', 'Polytomously', 'Scored', 'item']",purpose article present logistic discriminant function analysis means differential item function dif identification item polytomously score procedure present example DIF analysis item 27‐item mathematic test include open‐ende response item score polytomously result logistic discriminant function procedure ideally suit dif identification nondichotomously score test item simple practical polytomous extension logistic regression dif procedure appear fee powerful generalized Mantel‐Haenszelprocedure copyright © 1993 Wiley Blackwell right reserve,logistic Discriminant Function Analysis DIF identification Polytomously Scored item,0.030436603564160728,0.8779791739487122,0.03053633457665375,0.030163548805635237,0.030884339104838315,0.0,0.007248261614200821,0.18254207635584938,0.03560790379865358,0.0
Livingston S.A.,Small‐Sample Equating With Log‐Linear Smoothing,1993,30,"This study investigated the extent to which log‐linear smoothing could improve the accuracy of common‐item equating by the chained equipercentile method in small samples of examinees. Examinee response data from a 100‐item test were used to create two overlapping forms of 58 items each, with 24 items in common. The criterion equating was a direct equipercentile equating of the two forms in the full population of 93,283 examinees. Anchor equatings were performed in samples of 25, 50, 100, and 200 examinees, with 50 pairs of samples at each size level. Four equatings were performed with each pair of samples: one based on unsmoothed distributions and three based on varying degrees of smoothing. Smoothing reduced, by at least half, the sample size required for a given degree of accuracy. Smoothing that preserved only two moments of the marginal distributions resulted in equatings that failed to capture the curvilinearity in the population equating. Copyright © 1993, Wiley Blackwell. All rights reserved",,"This study investigated the extent to which log‐linear smoothing could improve the accuracy of common‐item equating by the chained equipercentile method in small samples of examinees. Examinee response data from a 100‐item test were used to create two overlapping forms of 58 items each, with 24 items in common. The criterion equating was a direct equipercentile equating of the two forms in the full population of 93,283 examinees. Anchor equatings were performed in samples of 25, 50, 100, and 200 examinees, with 50 pairs of samples at each size level. Four equatings were performed with each pair of samples: one based on unsmoothed distributions and three based on varying degrees of smoothing. Smoothing reduced, by at least half, the sample size required for a given degree of accuracy. Smoothing that preserved only two moments of the marginal distributions resulted in equatings that failed to capture the curvilinearity in the population equating. Copyright © 1993, Wiley Blackwell. All rights reserved","['study', 'investigate', 'extent', 'log‐linear', 'smoothing', 'improve', 'accuracy', 'common‐item', 'equating', 'chain', 'equipercentile', 'method', 'small', 'sample', 'examine', 'Examinee', 'response', 'datum', '100‐item', 'test', 'create', 'overlap', 'form', '58', 'item', '24', 'item', 'common', 'criterion', 'equating', 'direct', 'equipercentile', 'equating', 'form', 'population', '93283', 'examine', 'Anchor', 'equating', 'perform', 'sample', '25', '50', '100', '200', 'examine', '50', 'pair', 'sample', 'size', 'level', 'equating', 'perform', 'pair', 'sample', 'base', 'unsmoothed', 'distribution', 'base', 'vary', 'degree', 'smooth', 'Smoothing', 'reduce', 'half', 'sample', 'size', 'require', 'degree', 'accuracy', 'Smoothing', 'preserve', 'moment', 'marginal', 'distribution', 'result', 'equating', 'fail', 'capture', 'curvilinearity', 'population', 'equate', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,study investigate extent log‐linear smoothing improve accuracy common‐item equating chain equipercentile method small sample examine Examinee response datum 100‐item test create overlap form 58 item 24 item common criterion equating direct equipercentile equating form population 93283 examine Anchor equating perform sample 25 50 100 200 examine 50 pair sample size level equating perform pair sample base unsmoothed distribution base vary degree smooth Smoothing reduce half sample size require degree accuracy Smoothing preserve moment marginal distribution result equating fail capture curvilinearity population equate Copyright © 1993 Wiley Blackwell right reserve,,0.897009675312422,0.025667968898957935,0.02571039126957117,0.025517268261253428,0.0260946962577954,0.1090043555975856,0.0,0.0,0.0,0.0
Sheehan K.; Mislevy R.J.,Integrating Cognitive and Psychometric Models to Measure Document Literacy,1990,27,"The Survey of Young Adult Literacy conducted in 1985 by the National Assessment of Educational Progress included 63 items that elicited skills in acquiring and using information from written documents. These items were analyzed using two different models: (1) a qualitative cognitive model, which characterized items in terms of the processing tasks they required, and (2) an item response theory (IRT) model, which characterized items difficulties and respondents' proficiencies simply by tendencies toward correct response. This paper demonstrates how a generalization of Fischer and Seheibleehner's Linear Logistic Test Model can be used to integrate information from the cognitive analysis into the IRT analysis, providing a foundation for subsequent item construction, test development, and diagnosis of individuals skill deficiencies. Copyright © 1990, Wiley Blackwell. All rights reserved",Integrating Cognitive and Psychometric Models to Measure Document Literacy,"The Survey of Young Adult Literacy conducted in 1985 by the National Assessment of Educational Progress included 63 items that elicited skills in acquiring and using information from written documents. These items were analyzed using two different models: (1) a qualitative cognitive model, which characterized items in terms of the processing tasks they required, and (2) an item response theory (IRT) model, which characterized items difficulties and respondents' proficiencies simply by tendencies toward correct response. This paper demonstrates how a generalization of Fischer and Seheibleehner's Linear Logistic Test Model can be used to integrate information from the cognitive analysis into the IRT analysis, providing a foundation for subsequent item construction, test development, and diagnosis of individuals skill deficiencies. Copyright © 1990, Wiley Blackwell. All rights reserved","['Survey', 'Young', 'Adult', 'Literacy', 'conduct', '1985', 'National', 'Assessment', 'Educational', 'Progress', 'include', '63', 'item', 'elicit', 'skill', 'acquire', 'information', 'write', 'document', 'item', 'analyze', 'different', '1', 'qualitative', 'cognitive', 'characterize', 'item', 'term', 'processing', 'task', 'require', '2', 'item', 'response', 'theory', 'IRT', 'characterize', 'item', 'difficulty', 'respondent', 'proficiency', 'simply', 'tendency', 'correct', 'response', 'paper', 'demonstrate', 'generalization', 'Fischer', 'Seheibleehners', 'Linear', 'Logistic', 'Test', 'integrate', 'information', 'cognitive', 'analysis', 'IRT', 'analysis', 'provide', 'foundation', 'subsequent', 'item', 'construction', 'test', 'development', 'diagnosis', 'individual', 'skill', 'deficiency', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['integrate', 'Cognitive', 'Psychometric', 'Models', 'measure', 'Document', 'Literacy']",Survey Young Adult Literacy conduct 1985 National Assessment Educational Progress include 63 item elicit skill acquire information write document item analyze different 1 qualitative cognitive characterize item term processing task require 2 item response theory IRT characterize item difficulty respondent proficiency simply tendency correct response paper demonstrate generalization Fischer Seheibleehners Linear Logistic Test integrate information cognitive analysis IRT analysis provide foundation subsequent item construction test development diagnosis individual skill deficiency Copyright © 1990 Wiley Blackwell right reserve,integrate Cognitive Psychometric Models measure Document Literacy,0.023910767104825426,0.024020768742939853,0.024335199287289343,0.023939334676140286,0.903793930188805,0.0,0.10874002403077963,0.014581261175985589,0.0,0.02262816000845941
Raymond M.R.; Viswesvaran C.,Least Squares Models to Correct for Rater Effects in Performance Assessment,1993,30,"This study describes three least squares models to control for rater effects in performance evaluation: ordinary least squares (OLS); weighted least squares (WLS); and ordinary least squares, subsequent to applying a logistic transformation to observed ratings (LOG‐OLS). The models were applied to ratings obtained from four administrations of an oral examination required for certification in a medical specialty. For any single administration, there were 40 raters and approximately 115 candidates, and each candidate was rated by four raters. The results indicated that raters exhibited significant amounts of leniency error and that application of the least squares models would change the pass‐fail status of approximately 7% to 9% of the candidates. Ratings adjusted by the models demonstrated higher reliability and correlated slightly higher than observed ratings with the scores on a written examination. Copyright © 1993, Wiley Blackwell. All rights reserved",Least Squares Models to Correct for Rater Effects in Performance Assessment,"This study describes three least squares models to control for rater effects in performance evaluation: ordinary least squares (OLS); weighted least squares (WLS); and ordinary least squares, subsequent to applying a logistic transformation to observed ratings (LOG‐OLS). The models were applied to ratings obtained from four administrations of an oral examination required for certification in a medical specialty. For any single administration, there were 40 raters and approximately 115 candidates, and each candidate was rated by four raters. The results indicated that raters exhibited significant amounts of leniency error and that application of the least squares models would change the pass‐fail status of approximately 7% to 9% of the candidates. Ratings adjusted by the models demonstrated higher reliability and correlated slightly higher than observed ratings with the scores on a written examination. Copyright © 1993, Wiley Blackwell. All rights reserved","['study', 'describe', 'square', 'control', 'rater', 'effect', 'performance', 'evaluation', 'ordinary', 'square', 'OLS', 'weight', 'square', 'wls', 'ordinary', 'square', 'subsequent', 'apply', 'logistic', 'transformation', 'observe', 'rating', 'log‐ols', 'apply', 'rating', 'obtain', 'administration', 'oral', 'examination', 'require', 'certification', 'medical', 'specialty', 'single', 'administration', '40', 'rater', 'approximately', '115', 'candidate', 'candidate', 'rate', 'rater', 'result', 'indicate', 'rater', 'exhibit', 'significant', 'leniency', 'error', 'application', 'square', 'change', 'pass‐fail', 'status', 'approximately', '7', '9', 'candidate', 'rating', 'adjust', 'demonstrate', 'high', 'reliability', 'correlate', 'slightly', 'high', 'observed', 'rating', 'score', 'write', 'examination', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['Least', 'square', 'Models', 'correct', 'Rater', 'Effects', 'Performance', 'Assessment']",study describe square control rater effect performance evaluation ordinary square OLS weight square wls ordinary square subsequent apply logistic transformation observe rating log‐ols apply rating obtain administration oral examination require certification medical specialty single administration 40 rater approximately 115 candidate candidate rate rater result indicate rater exhibit significant leniency error application square change pass‐fail status approximately 7 9 candidate rating adjust demonstrate high reliability correlate slightly high observed rating score write examination Copyright © 1993 Wiley Blackwell right reserve,Least square Models correct Rater Effects Performance Assessment,0.028088636659094467,0.02843903141870564,0.028238619740853756,0.02803114201086962,0.8872025701704765,0.0,0.004621413312015072,0.0013229140543499697,0.02145146501693302,0.06980056830236576
Skaggs G.; Lissitz R.W.,The Consistency of Detecting Item Bias Across Different Test Administrations: Implications of Another Failure,1992,29,"Several item bias detection methods were applied to the analysis of bias among males and females for items from a curriculum‐based mathematics test. The focus of this analysis was the consistency of the methods across different test administrations of the same items. The results indicated that, of the methods studied, the Mantel–Haenszel (M–H) and IRT‐based sum‐of‐squares methods were the most consistent. However, the degree of reliability and agreement for these methods was modest at best. As with most prior research, no reasonable explanation could be found for the most consistently flagged items. A likely reason for this lies in the confusion of visible genetic group characteristics with their instructional backgrounds. A multidimensional perspective of item bias is proposed for future research that will take such confounding into account. Copyright © 1992, Wiley Blackwell. All rights reserved",The Consistency of Detecting Item Bias Across Different Test Administrations: Implications of Another Failure,"Several item bias detection methods were applied to the analysis of bias among males and females for items from a curriculum‐based mathematics test. The focus of this analysis was the consistency of the methods across different test administrations of the same items. The results indicated that, of the methods studied, the Mantel–Haenszel (M–H) and IRT‐based sum‐of‐squares methods were the most consistent. However, the degree of reliability and agreement for these methods was modest at best. As with most prior research, no reasonable explanation could be found for the most consistently flagged items. A likely reason for this lies in the confusion of visible genetic group characteristics with their instructional backgrounds. A multidimensional perspective of item bias is proposed for future research that will take such confounding into account. Copyright © 1992, Wiley Blackwell. All rights reserved","['item', 'bias', 'detection', 'method', 'apply', 'analysis', 'bias', 'male', 'female', 'item', 'curriculum‐based', 'mathematic', 'test', 'focus', 'analysis', 'consistency', 'method', 'different', 'test', 'administration', 'item', 'result', 'indicate', 'method', 'study', 'Mantel', '–', 'Haenszel', 'M', '–', 'H', 'irt‐base', 'sum‐of‐square', 'method', 'consistent', 'degree', 'reliability', 'agreement', 'method', 'modest', 'prior', 'research', 'reasonable', 'explanation', 'find', 'consistently', 'flagged', 'item', 'likely', 'reason', 'lie', 'confusion', 'visible', 'genetic', 'group', 'characteristic', 'instructional', 'background', 'multidimensional', 'perspective', 'item', 'bias', 'propose', 'future', 'research', 'confound', 'account', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Consistency', 'Detecting', 'Item', 'Bias', 'Different', 'Test', 'Administrations', 'Implications', 'failure']",item bias detection method apply analysis bias male female item curriculum‐based mathematic test focus analysis consistency method different test administration item result indicate method study Mantel – Haenszel M – H irt‐base sum‐of‐square method consistent degree reliability agreement method modest prior research reasonable explanation find consistently flagged item likely reason lie confusion visible genetic group characteristic instructional background multidimensional perspective item bias propose future research confound account Copyright © 1992 Wiley Blackwell right reserve,Consistency Detecting Item Bias Different Test Administrations Implications failure,0.02515921339832711,0.02520056674055524,0.8988732237814785,0.025107991546572693,0.025659004533066392,0.025626011575879428,0.06505907030601218,0.02280474895034362,0.0359278882159286,0.0
Wainer H.; Kaplan B.; Lewis C.,A Comparison of the Performance of Simulated Hierarchical and Linear Testlets,1992,29,"A series of computer simulations were run to measure the relationship between testlet validity and the factors of item pool size and testlet length for both adaptive and linearly constructed testlets. We confirmed the generality of earlier empirical findings (Wainer, Lewis, Kaplan, & Braswell, 1991) that making a testlet adaptive yields only modest increases in aggregate validity because of the peakedness of the typical proficiency distribution. Copyright © 1992, Wiley Blackwell. All rights reserved",A Comparison of the Performance of Simulated Hierarchical and Linear Testlets,"A series of computer simulations were run to measure the relationship between testlet validity and the factors of item pool size and testlet length for both adaptive and linearly constructed testlets. We confirmed the generality of earlier empirical findings (Wainer, Lewis, Kaplan, & Braswell, 1991) that making a testlet adaptive yields only modest increases in aggregate validity because of the peakedness of the typical proficiency distribution. Copyright © 1992, Wiley Blackwell. All rights reserved","['series', 'computer', 'simulation', 'run', 'measure', 'relationship', 'testlet', 'validity', 'factor', 'item', 'pool', 'size', 'testlet', 'length', 'adaptive', 'linearly', 'construct', 'testlet', 'confirm', 'generality', 'early', 'empirical', 'finding', 'Wainer', 'Lewis', 'Kaplan', 'Braswell', '1991', 'testlet', 'adaptive', 'yield', 'modest', 'increase', 'aggregate', 'validity', 'peakedness', 'typical', 'proficiency', 'distribution', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Performance', 'Simulated', 'hierarchical', 'Linear', 'testlet']",series computer simulation run measure relationship testlet validity factor item pool size testlet length adaptive linearly construct testlet confirm generality early empirical finding Wainer Lewis Kaplan Braswell 1991 testlet adaptive yield modest increase aggregate validity peakedness typical proficiency distribution Copyright © 1992 Wiley Blackwell right reserve,Comparison Performance Simulated hierarchical Linear testlet,0.03157379555747405,0.03154197908863629,0.031695918893610286,0.031421858950559205,0.8737664475097201,0.0015907345737775423,0.06850545856686083,0.0,0.0,0.005960793527035483
Lunz M.E.; Bergstrom B.A.,An Empirical Study of Computerized Adaptive Test Administration Conditions,1994,31,"This empirical study was designed to determine the impact of computerized adap‐ tive test (CAT) administration formats on student performance. Students in medical technology programs took a paper‐and‐pencil and an individualized, computerized adaptive test. Students were randomly assigned to adaptive test administration for‐ mats to ascertain the effect on student performance of altering: (a) the difficulty of the first item, (b) the targeted level of test difficulty, (c) minimum test length, and (d) the opportunity to control the test. Computerized adaptive test data were analyzed with ANCO VA. The paper‐and.pencil test was used as a covariate to equalize abil‐ ity variance among cells. The only significant main effect was for opportunity to control the test. There were no significant interactions among test administration formats. This study provides evidence concerning adjusting traditional computer‐ ized adaptive testing to more familiar testing modalities. Copyright © 1994, Wiley Blackwell. All rights reserved",An Empirical Study of Computerized Adaptive Test Administration Conditions,"This empirical study was designed to determine the impact of computerized adap‐ tive test (CAT) administration formats on student performance. Students in medical technology programs took a paper‐and‐pencil and an individualized, computerized adaptive test. Students were randomly assigned to adaptive test administration for‐ mats to ascertain the effect on student performance of altering: (a) the difficulty of the first item, (b) the targeted level of test difficulty, (c) minimum test length, and (d) the opportunity to control the test. Computerized adaptive test data were analyzed with ANCO VA. The paper‐and.pencil test was used as a covariate to equalize abil‐ ity variance among cells. The only significant main effect was for opportunity to control the test. There were no significant interactions among test administration formats. This study provides evidence concerning adjusting traditional computer‐ ized adaptive testing to more familiar testing modalities. Copyright © 1994, Wiley Blackwell. All rights reserved","['empirical', 'study', 'design', 'determine', 'impact', 'computerized', 'adap‐', 'tive', 'test', 'CAT', 'administration', 'format', 'student', 'performance', 'Students', 'medical', 'technology', 'program', 'paper‐and‐pencil', 'individualized', 'computerized', 'adaptive', 'test', 'student', 'randomly', 'assign', 'adaptive', 'test', 'administration', 'for‐', 'mat', 'ascertain', 'effect', 'student', 'performance', 'alter', 'difficulty', 'item', 'b', 'target', 'level', 'test', 'difficulty', 'c', 'minimum', 'test', 'length', 'd', 'opportunity', 'control', 'test', 'Computerized', 'adaptive', 'test', 'datum', 'analyze', 'anco', 'VA', 'paper‐andpencil', 'test', 'covariate', 'equalize', 'abil‐', 'ity', 'variance', 'cell', 'significant', 'main', 'effect', 'opportunity', 'control', 'test', 'significant', 'interaction', 'test', 'administration', 'format', 'study', 'provide', 'evidence', 'concern', 'adjust', 'traditional', 'computer‐', 'ize', 'adaptive', 'testing', 'familiar', 'testing', 'modality', 'copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['empirical', 'study', 'Computerized', 'Adaptive', 'Test', 'Administration', 'Conditions']",empirical study design determine impact computerized adap‐ tive test CAT administration format student performance Students medical technology program paper‐and‐pencil individualized computerized adaptive test student randomly assign adaptive test administration for‐ mat ascertain effect student performance alter difficulty item b target level test difficulty c minimum test length d opportunity control test Computerized adaptive test datum analyze anco VA paper‐andpencil test covariate equalize abil‐ ity variance cell significant main effect opportunity control test significant interaction test administration format study provide evidence concern adjust traditional computer‐ ize adaptive testing familiar testing modality copyright © 1994 Wiley Blackwell right reserve,empirical study Computerized Adaptive Test Administration Conditions,0.024110793872062216,0.024193962725312775,0.0243253783697823,0.6772557849645603,0.25011408006828245,0.0,0.11347436093779123,0.0,0.0,0.021052275918268984
Brookhart S.M.,Teachers' Grading Practices: Meaning and Values,1993,30,"Classroom teachers do not always follow recommended grading practices. Why not? It is possible to conceptualize this question as a validity issue and ask whether teachers' concerns over the many uses of grades outweigh concerns about the interpretation of grades. The purpose of this study was to investigate the meaning classroom teachers associate with grades, the value judgments they make when considering grades, and whether the meaning or values associated with grades differed by whether teachers had measurement instruction. A sample of 84 teachers, 40 with and 44 without measurement instruction, responded to classroom grading scenarios in two ways–with multiple‐choice responses indicating what they would do and with written responses to the question, “Why did you make this choice?” A coding scheme based on Messick's (1989a, 1989b) progressive matrix of facets of validity was used for quantitative and qualitative analyses of written responses. The meaning of grades is closely related to the idea of student work; grades are pay students earn for activities they perform. The relationship of this notion to classroom management should be investigated. Teachers do make value judgments when assigning grades and are especially concerned about being fair. Teachers also are concerned about the consequences of grade use, especially for developing student self‐esteem and good attitudes toward future school work. Measurement instruction made very little difference, although it did reduce the amount of self‐referenced grading reported. Copyright © 1993, Wiley Blackwell. All rights reserved",,"Classroom teachers do not always follow recommended grading practices. Why not? It is possible to conceptualize this question as a validity issue and ask whether teachers' concerns over the many uses of grades outweigh concerns about the interpretation of grades. The purpose of this study was to investigate the meaning classroom teachers associate with grades, the value judgments they make when considering grades, and whether the meaning or values associated with grades differed by whether teachers had measurement instruction. A sample of 84 teachers, 40 with and 44 without measurement instruction, responded to classroom grading scenarios in two ways–with multiple‐choice responses indicating what they would do and with written responses to the question, “Why did you make this choice?” A coding scheme based on Messick's (1989a, 1989b) progressive matrix of facets of validity was used for quantitative and qualitative analyses of written responses. The meaning of grades is closely related to the idea of student work; grades are pay students earn for activities they perform. The relationship of this notion to classroom management should be investigated. Teachers do make value judgments when assigning grades and are especially concerned about being fair. Teachers also are concerned about the consequences of grade use, especially for developing student self‐esteem and good attitudes toward future school work. Measurement instruction made very little difference, although it did reduce the amount of self‐referenced grading reported. Copyright © 1993, Wiley Blackwell. All rights reserved","['classroom', 'teacher', 'follow', 'recommend', 'grade', 'practice', 'possible', 'conceptualize', 'question', 'validity', 'issue', 'ask', 'teacher', 'concern', 'grade', 'outweigh', 'concern', 'interpretation', 'grade', 'purpose', 'study', 'investigate', 'meaning', 'classroom', 'teacher', 'associate', 'grade', 'value', 'judgment', 'consider', 'grade', 'meaning', 'value', 'associate', 'grade', 'differ', 'teacher', 'instruction', 'sample', '84', 'teacher', '40', '44', 'instruction', 'respond', 'classroom', 'grade', 'scenario', 'way', '–', 'multiple‐choice', 'response', 'indicate', 'write', 'response', 'question', '""', 'choice', '""', 'code', 'scheme', 'base', 'Messicks', '1989a', '1989b', 'progressive', 'matrix', 'facet', 'validity', 'quantitative', 'qualitative', 'analysis', 'write', 'response', 'meaning', 'grade', 'closely', 'relate', 'idea', 'student', 'work', 'grade', 'pay', 'student', 'earn', 'activity', 'perform', 'relationship', 'notion', 'classroom', 'management', 'investigate', 'teacher', 'value', 'judgment', 'assign', 'grade', 'especially', 'concerned', 'fair', 'Teachers', 'concerned', 'consequence', 'grade', 'especially', 'develop', 'student', 'self‐esteem', 'good', 'attitude', 'future', 'school', 'work', 'instruction', 'little', 'difference', 'reduce', 'self‐referenced', 'grading', 'report', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,"classroom teacher follow recommend grade practice possible conceptualize question validity issue ask teacher concern grade outweigh concern interpretation grade purpose study investigate meaning classroom teacher associate grade value judgment consider grade meaning value associate grade differ teacher instruction sample 84 teacher 40 44 instruction respond classroom grade scenario way – multiple‐choice response indicate write response question "" choice "" code scheme base Messicks 1989a 1989b progressive matrix facet validity quantitative qualitative analysis write response meaning grade closely relate idea student work grade pay student earn activity perform relationship notion classroom management investigate teacher value judgment assign grade especially concerned fair Teachers concerned consequence grade especially develop student self‐esteem good attitude future school work instruction little difference reduce self‐referenced grading report Copyright © 1993 Wiley Blackwell right reserve",,0.02638873898861311,0.8941885637175527,0.026523351755174687,0.026010275445938293,0.026889070092721196,0.00038398463712004723,0.010972573463187562,0.0,0.0,0.08911329302061452
Adema J.J.,The Construction of Customized Two‐Stage Tests,1990,27,"In this paper mixed integer linear programming models for customizing two‐stage tests are given. Model constraints are imposed with respect to test composition, administration time, inter‐item dependencies, and other practical considerations. It is not difficult to modify the models to make them useful in constructing multistage tests. Copyright © 1990, Wiley Blackwell. All rights reserved",,"In this paper mixed integer linear programming models for customizing two‐stage tests are given. Model constraints are imposed with respect to test composition, administration time, inter‐item dependencies, and other practical considerations. It is not difficult to modify the models to make them useful in constructing multistage tests. Copyright © 1990, Wiley Blackwell. All rights reserved","['paper', 'mix', 'integer', 'linear', 'programming', 'customize', 'two‐stage', 'test', 'constraint', 'impose', 'respect', 'test', 'composition', 'administration', 'time', 'inter‐item', 'dependency', 'practical', 'consideration', 'difficult', 'modify', 'useful', 'construct', 'multistage', 'test', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']",,paper mix integer linear programming customize two‐stage test constraint impose respect test composition administration time inter‐item dependency practical consideration difficult modify useful construct multistage test copyright © 1990 Wiley Blackwell right reserve,,0.8679702040563315,0.03298016109680764,0.033020553409154135,0.03278278860093335,0.03324629283677319,0.0,0.058044996974450196,0.0,0.01043959478215176,0.0003587821498509191
May K.; Nicewander W.A.,Reliability and Information Functions for Percentile Ranks,1994,31,"Reliabilities and information functions for percentile ranks and number‐right scores were compared in the context of item response theory. The basic results were: (a) The percentile rank is always less informative and reliable than the number‐right score; and (b)for easy or difficult tests composed of highly discriminating items, the percentile rank often yields unacceptably low reliability and information relative to the number‐right score. These results suggest that standardized scores that are linear transformations of the number‐right score (e.g., z scores) are much more reliable and informative indicators of the relative standing of a test score than are percentile ranks. The findings reported here demonstrate that there exist situations in which the percent of items known by examinees can be accurately estimated, but that the percent of persons falling below a given score cannot. Copyright © 1994, Wiley Blackwell. All rights reserved",Reliability and Information Functions for Percentile Ranks,"Reliabilities and information functions for percentile ranks and number‐right scores were compared in the context of item response theory. The basic results were: (a) The percentile rank is always less informative and reliable than the number‐right score; and (b)for easy or difficult tests composed of highly discriminating items, the percentile rank often yields unacceptably low reliability and information relative to the number‐right score. These results suggest that standardized scores that are linear transformations of the number‐right score (e.g., z scores) are much more reliable and informative indicators of the relative standing of a test score than are percentile ranks. The findings reported here demonstrate that there exist situations in which the percent of items known by examinees can be accurately estimated, but that the percent of persons falling below a given score cannot. Copyright © 1994, Wiley Blackwell. All rights reserved","['reliability', 'information', 'function', 'percentile', 'rank', 'number‐right', 'score', 'compare', 'context', 'item', 'response', 'theory', 'basic', 'result', 'percentile', 'rank', 'informative', 'reliable', 'number‐right', 'score', 'bfor', 'easy', 'difficult', 'test', 'compose', 'highly', 'discriminate', 'item', 'percentile', 'rank', 'yield', 'unacceptably', 'low', 'reliability', 'information', 'relative', 'number‐right', 'score', 'result', 'suggest', 'standardized', 'score', 'linear', 'transformation', 'number‐right', 'score', 'eg', 'z', 'score', 'reliable', 'informative', 'indicator', 'relative', 'standing', 'test', 'score', 'percentile', 'rank', 'finding', 'report', 'demonstrate', 'exist', 'situation', 'percent', 'item', 'know', 'examinee', 'accurately', 'estimate', 'percent', 'person', 'fall', 'score', 'copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['reliability', 'Information', 'Functions', 'Percentile', 'rank']",reliability information function percentile rank number‐right score compare context item response theory basic result percentile rank informative reliable number‐right score bfor easy difficult test compose highly discriminate item percentile rank yield unacceptably low reliability information relative number‐right score result suggest standardized score linear transformation number‐right score eg z score reliable informative indicator relative standing test score percentile rank finding report demonstrate exist situation percent item know examinee accurately estimate percent person fall score copyright © 1994 Wiley Blackwell right reserve,reliability Information Functions Percentile rank,0.030891487073698064,0.8768940010962321,0.030729148938527136,0.030150843885217615,0.031334519006325216,0.0,0.035171357480046635,0.0011317685837346227,0.12077591311083244,0.0
Mislevy R.J.; Sheehan K.M.; Wingersky M.,How to Equate Tests With Little or No Data,1993,30,"Standard procedures for equating tests, including those based on item response theory (IRT), require item responses from large numbers of examinees. Such data may not be forthcoming for reasons theoretical, political, or practical. Information about items' operating characteristics may be available from other sources, however, such as content and format specifications, expert opinion, or psychological theories about the skills and strategies required to solve them. This article shows how, in the IRT framework, collateral information about items can be exploited to augment or even replace examinee responses when linking or equating new tests to established scales. The procedures are illustrated with data from the Pre‐Professional Skills Test (PPST). Copyright © 1993, Wiley Blackwell. All rights reserved",,"Standard procedures for equating tests, including those based on item response theory (IRT), require item responses from large numbers of examinees. Such data may not be forthcoming for reasons theoretical, political, or practical. Information about items' operating characteristics may be available from other sources, however, such as content and format specifications, expert opinion, or psychological theories about the skills and strategies required to solve them. This article shows how, in the IRT framework, collateral information about items can be exploited to augment or even replace examinee responses when linking or equating new tests to established scales. The procedures are illustrated with data from the Pre‐Professional Skills Test (PPST). Copyright © 1993, Wiley Blackwell. All rights reserved","['standard', 'procedure', 'equate', 'test', 'include', 'base', 'item', 'response', 'theory', 'IRT', 'require', 'item', 'response', 'large', 'number', 'examinee', 'datum', 'forthcoming', 'reason', 'theoretical', 'political', 'practical', 'Information', 'item', 'operate', 'characteristic', 'available', 'source', 'content', 'format', 'specification', 'expert', 'opinion', 'psychological', 'theory', 'skill', 'strategy', 'require', 'solve', 'article', 'IRT', 'framework', 'collateral', 'information', 'item', 'exploit', 'augment', 'replace', 'examinee', 'response', 'link', 'equate', 'new', 'test', 'establish', 'scale', 'procedure', 'illustrate', 'datum', 'pre‐professional', 'Skills', 'test', 'ppst', 'copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,standard procedure equate test include base item response theory IRT require item response large number examinee datum forthcoming reason theoretical political practical Information item operate characteristic available source content format specification expert opinion psychological theory skill strategy require solve article IRT framework collateral information item exploit augment replace examinee response link equate new test establish scale procedure illustrate datum pre‐professional Skills test ppst copyright © 1993 Wiley Blackwell right reserve,,0.02559514959746633,0.025589645684912003,0.02554689647189239,0.025297162664241684,0.8979711455814876,0.04139097389126097,0.08913633507820905,0.0040380596589176115,0.014818850935600411,0.0
Muthén B.O.; Kao C.‐F.; Burstein L.,Instructionally Sensitive Psychometrics: Application of a New IRT‐Based Detection Technique to Mathematics Achievement Test Items,1991,28,"Achievement modeling is carried out in groups of students characterized by heterogeneous instructional background. Extensions of item response theory models incorporate variables reflecting different amounts of opportunity‐to‐leam (OTL). The effects of these OTL variables are studied with respect to their influence on both the latent trait and the item performance directly. Such direct effects may reflect instructionally sensitive items. U.S. eighth‐grade mathematics data from the Second International Mathematics Study are analyzed. Here, the same test is taken by students enrolled in typical instruction and students enrolled in elementary algebra classes. It is shown that the new analysis provides a more detailed way to examine the influence of instruction on responses to test items than does conventional item response theory. Copyright © 1991, Wiley Blackwell. All rights reserved",Instructionally Sensitive Psychometrics: Application of a New IRT‐Based Detection Technique to Mathematics Achievement Test Items,"Achievement modeling is carried out in groups of students characterized by heterogeneous instructional background. Extensions of item response theory models incorporate variables reflecting different amounts of opportunity‐to‐leam (OTL). The effects of these OTL variables are studied with respect to their influence on both the latent trait and the item performance directly. Such direct effects may reflect instructionally sensitive items. U.S. eighth‐grade mathematics data from the Second International Mathematics Study are analyzed. Here, the same test is taken by students enrolled in typical instruction and students enrolled in elementary algebra classes. It is shown that the new analysis provides a more detailed way to examine the influence of instruction on responses to test items than does conventional item response theory. Copyright © 1991, Wiley Blackwell. All rights reserved","['achievement', 'modeling', 'carry', 'group', 'student', 'characterize', 'heterogeneous', 'instructional', 'background', 'Extensions', 'item', 'response', 'theory', 'incorporate', 'variable', 'reflect', 'different', 'opportunity‐to‐leam', 'OTL', 'effect', 'OTL', 'variable', 'study', 'respect', 'influence', 'latent', 'trait', 'item', 'performance', 'directly', 'direct', 'effect', 'reflect', 'instructionally', 'sensitive', 'item', 'US', 'eighth‐grade', 'mathematic', 'datum', 'Second', 'International', 'Mathematics', 'Study', 'analyze', 'test', 'student', 'enrol', 'typical', 'instruction', 'student', 'enrol', 'elementary', 'algebra', 'class', 'new', 'analysis', 'provide', 'detailed', 'way', 'examine', 'influence', 'instruction', 'response', 'test', 'item', 'conventional', 'item', 'response', 'theory', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Instructionally', 'Sensitive', 'Psychometrics', 'Application', 'New', 'IRT‐Based', 'Detection', 'Technique', 'Mathematics', 'Achievement', 'Test', 'Items']",achievement modeling carry group student characterize heterogeneous instructional background Extensions item response theory incorporate variable reflect different opportunity‐to‐leam OTL effect OTL variable study respect influence latent trait item performance directly direct effect reflect instructionally sensitive item US eighth‐grade mathematic datum Second International Mathematics Study analyze test student enrol typical instruction student enrol elementary algebra class new analysis provide detailed way examine influence instruction response test item conventional item response theory Copyright © 1991 Wiley Blackwell right reserve,Instructionally Sensitive Psychometrics Application New IRT‐Based Detection Technique Mathematics Achievement Test Items,0.026348275454670094,0.026340872953459396,0.026165774583942706,0.47324442121686333,0.4479006557910645,0.005404122690407612,0.07338946340644634,0.019175614673033296,0.0,0.04790959629811549
Oshima T.C.; Miller M.D.,Multidimensionality and IRT‐Based Item lnvariance Indexes: The Effect of Between‐Group Variation in Trait Correlation,1990,27,"Using a bidimensional two‐parameter logistic model, the authors generated data for two groups on a 40‐item test. The item parameters were the same for the two groups, but the correlation between the two traits varied between groups. The difference in the trait correlation was directly related to the number of items judged not to be invariant using traditional unidimensional IRT‐based unsigned item invariance indexes; the higher trait correlation leads to higher discrimination parameter estimates when a unidimensional IRT model is fit to the multidimensional data. In the most extreme case, when rθ1 θ2= Ofor one group and r θ1 θ2= 1.0 for the other group, 33 out of 40 items were identified as not invariant. When using signed indexes, the effect was much smaller. The authors, therefore, suggest a cautious use of IRT‐based item invariance indexes when data are potentially multidimensional and groups may vary in the strength of the correlations among traits. Copyright © 1990, Wiley Blackwell. All rights reserved",Multidimensionality and IRT‐Based Item lnvariance Indexes: The Effect of Between‐Group Variation in Trait Correlation,"Using a bidimensional two‐parameter logistic model, the authors generated data for two groups on a 40‐item test. The item parameters were the same for the two groups, but the correlation between the two traits varied between groups. The difference in the trait correlation was directly related to the number of items judged not to be invariant using traditional unidimensional IRT‐based unsigned item invariance indexes; the higher trait correlation leads to higher discrimination parameter estimates when a unidimensional IRT model is fit to the multidimensional data. In the most extreme case, when rθ1 θ2= Ofor one group and r θ1 θ2= 1.0 for the other group, 33 out of 40 items were identified as not invariant. When using signed indexes, the effect was much smaller. The authors, therefore, suggest a cautious use of IRT‐based item invariance indexes when data are potentially multidimensional and groups may vary in the strength of the correlations among traits. Copyright © 1990, Wiley Blackwell. All rights reserved","['bidimensional', 'two‐parameter', 'logistic', 'author', 'generate', 'datum', 'group', '40‐item', 'test', 'item', 'parameter', 'group', 'correlation', 'trait', 'varied', 'group', 'difference', 'trait', 'correlation', 'directly', 'relate', 'number', 'item', 'judge', 'invariant', 'traditional', 'unidimensional', 'irt‐base', 'unsigned', 'item', 'invariance', 'index', 'high', 'trait', 'correlation', 'lead', 'high', 'discrimination', 'parameter', 'estimate', 'unidimensional', 'IRT', 'fit', 'multidimensional', 'datum', 'extreme', 'case', 'rθ1', 'θ2', 'Ofor', 'group', 'r', 'θ1', 'θ2', '10', 'group', '33', '40', 'item', 'identify', 'invariant', 'sign', 'index', 'effect', 'small', 'author', 'suggest', 'cautious', 'irt‐base', 'item', 'invariance', 'index', 'datum', 'potentially', 'multidimensional', 'group', 'vary', 'strength', 'correlation', 'trait', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['multidimensionality', 'irt‐base', 'Item', 'lnvariance', 'index', 'Effect', 'Between‐Group', 'Variation', 'Trait', 'Correlation']",bidimensional two‐parameter logistic author generate datum group 40‐item test item parameter group correlation trait varied group difference trait correlation directly relate number item judge invariant traditional unidimensional irt‐base unsigned item invariance index high trait correlation lead high discrimination parameter estimate unidimensional IRT fit multidimensional datum extreme case rθ1 θ2 Ofor group r θ1 θ2 10 group 33 40 item identify invariant sign index effect small author suggest cautious irt‐base item invariance index datum potentially multidimensional group vary strength correlation trait copyright © 1990 Wiley Blackwell right reserve,multidimensionality irt‐base Item lnvariance index Effect Between‐Group Variation Trait Correlation,0.02727219037278241,0.8908786236437506,0.02732465778536983,0.02688751908437008,0.027637009113727146,0.043139400201545545,0.0129282386116946,0.04789243626487274,0.0,0.0025165156793478505
Woodruff D.,Stepping Up Test Score Conditional Variances,1991,28,"In Woodruff (1990), I derived estimates for the conditional standard error of measurement in prediction (CSEMP), the conditional standard error of estimation (CSEE), and the conditional standard error of prediction (CSEP). My original estimates assume that the conditional residual error score variances and the conditional residual true score variances, obtained from the regression of an observed score onto a parallel observed score, obey the same step‐up rules as do the marginal error score variance and the marginal true score variance. The present article derives alternative estimates for the various test score conditional variances that do not depend on these assumptions. Copyright © 1991, Wiley Blackwell. All rights reserved",,"In Woodruff (1990), I derived estimates for the conditional standard error of measurement in prediction (CSEMP), the conditional standard error of estimation (CSEE), and the conditional standard error of prediction (CSEP). My original estimates assume that the conditional residual error score variances and the conditional residual true score variances, obtained from the regression of an observed score onto a parallel observed score, obey the same step‐up rules as do the marginal error score variance and the marginal true score variance. The present article derives alternative estimates for the various test score conditional variances that do not depend on these assumptions. Copyright © 1991, Wiley Blackwell. All rights reserved","['Woodruff', '1990', 'I', 'derive', 'estimate', 'conditional', 'standard', 'error', 'prediction', 'csemp', 'conditional', 'standard', 'error', 'estimation', 'CSEE', 'conditional', 'standard', 'error', 'prediction', 'CSEP', 'original', 'estimate', 'assume', 'conditional', 'residual', 'error', 'score', 'variance', 'conditional', 'residual', 'true', 'score', 'variance', 'obtain', 'regression', 'observed', 'score', 'parallel', 'observed', 'score', 'obey', 'step‐up', 'rule', 'marginal', 'error', 'score', 'variance', 'marginal', 'true', 'score', 'variance', 'present', 'article', 'derive', 'alternative', 'estimate', 'test', 'score', 'conditional', 'variance', 'depend', 'assumption', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']",,Woodruff 1990 I derive estimate conditional standard error prediction csemp conditional standard error estimation CSEE conditional standard error prediction CSEP original estimate assume conditional residual error score variance conditional residual true score variance obtain regression observed score parallel observed score obey step‐up rule marginal error score variance marginal true score variance present article derive alternative estimate test score conditional variance depend assumption Copyright © 1991 Wiley Blackwell right reserve,,0.03452991930516823,0.03450926133025589,0.8610959279749881,0.03411475403608876,0.03575013735349894,0.0,0.0,0.0,0.2723067260795698,0.0
Hassmén P.; Hunt D.P.,Human Self‐Assessment in Multiple‐Choice Testing,1994,31,"Research indicates that the multiple‐choice format in itself often seems to favor males over females. The present study utilizes a method that enables test takers to assess the correctness of their answers. Applying this self‐assessment method may not only make the multiple‐choice tests less biased but also provide a more comprehensive measurement of usable knowledge‐that is, the kind of knowledge about which a person is sufficiently sure so that he or she will use the knowledge to make decisions and take actions. The performance of male and female undergraduates on a conventional multiple‐choice test was compared with their performance on a multiple‐choice self‐assessment test. Results show that the difference between test scores of males and those of females was reduced when subjects were allowed to make self‐assessments. This may be explained in terms of the alleged difference in cognitive style between the genders. Copyright © 1994, Wiley Blackwell. All rights reserved",,"Research indicates that the multiple‐choice format in itself often seems to favor males over females. The present study utilizes a method that enables test takers to assess the correctness of their answers. Applying this self‐assessment method may not only make the multiple‐choice tests less biased but also provide a more comprehensive measurement of usable knowledge‐that is, the kind of knowledge about which a person is sufficiently sure so that he or she will use the knowledge to make decisions and take actions. The performance of male and female undergraduates on a conventional multiple‐choice test was compared with their performance on a multiple‐choice self‐assessment test. Results show that the difference between test scores of males and those of females was reduced when subjects were allowed to make self‐assessments. This may be explained in terms of the alleged difference in cognitive style between the genders. Copyright © 1994, Wiley Blackwell. All rights reserved","['Research', 'indicate', 'multiple‐choice', 'format', 'favor', 'male', 'female', 'present', 'study', 'utilize', 'method', 'enable', 'test', 'taker', 'assess', 'correctness', 'answer', 'apply', 'self‐assessment', 'method', 'multiple‐choice', 'test', 'biased', 'provide', 'comprehensive', 'usable', 'knowledge‐that', 'kind', 'knowledge', 'person', 'sufficiently', 'sure', 'knowledge', 'decision', 'action', 'performance', 'male', 'female', 'undergraduate', 'conventional', 'multiple‐choice', 'test', 'compare', 'performance', 'multiple‐choice', 'self‐assessment', 'test', 'result', 'difference', 'test', 'score', 'male', 'female', 'reduce', 'subject', 'allow', 'self‐assessment', 'explain', 'term', 'allege', 'difference', 'cognitive', 'style', 'gender', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']",,Research indicate multiple‐choice format favor male female present study utilize method enable test taker assess correctness answer apply self‐assessment method multiple‐choice test biased provide comprehensive usable knowledge‐that kind knowledge person sufficiently sure knowledge decision action performance male female undergraduate conventional multiple‐choice test compare performance multiple‐choice self‐assessment test result difference test score male female reduce subject allow self‐assessment explain term allege difference cognitive style gender Copyright © 1994 Wiley Blackwell right reserve,,0.02857144620685663,0.8849827204743683,0.028941303543548923,0.028305689164769048,0.029198840610457102,0.0,0.06989181466579023,0.0,0.030225431050179937,0.023883813257416266
Huynh H.; Ferrara S.,A Comparison of Equal Percentile and Partial Credit Equatings for Performance‐Based Assessments Composed of Free‐Response Items,1994,31,"This study compares the equal percentile (EP) and partial credit (PC) equatings for raw scores derived from performance‐based assessments composed of free‐response (open‐ended) items clustered around long reading selections or multistep mathematics problems. Data are from the Maryland School Performance Assessment Program. The results suggest that Masters (1982; Wright & Masters, 1982) partial credit model may be useful for equating examinations composed of moderately easy (or not too difficult)items sharing a first principal component with at least 25% of the total variance. This conclusion appears to hold even in the presence of some level of response dependency for the items within each cluster. Although visible discrepancies were found between PC and EP equated scores in the skewed tail of the score distributions, the direction of these discrepancies is unpredictable. Therefore, it cannot be concluded from the study that the two methods give equivalent results when the distributions are markedly skewed. Copyright © 1994, Wiley Blackwell. All rights reserved",A Comparison of Equal Percentile and Partial Credit Equatings for Performance‐Based Assessments Composed of Free‐Response Items,"This study compares the equal percentile (EP) and partial credit (PC) equatings for raw scores derived from performance‐based assessments composed of free‐response (open‐ended) items clustered around long reading selections or multistep mathematics problems. Data are from the Maryland School Performance Assessment Program. The results suggest that Masters (1982; Wright & Masters, 1982) partial credit model may be useful for equating examinations composed of moderately easy (or not too difficult)items sharing a first principal component with at least 25% of the total variance. This conclusion appears to hold even in the presence of some level of response dependency for the items within each cluster. Although visible discrepancies were found between PC and EP equated scores in the skewed tail of the score distributions, the direction of these discrepancies is unpredictable. Therefore, it cannot be concluded from the study that the two methods give equivalent results when the distributions are markedly skewed. Copyright © 1994, Wiley Blackwell. All rights reserved","['study', 'compare', 'equal', 'percentile', 'ep', 'partial', 'credit', 'pc', 'equating', 'raw', 'score', 'derive', 'performance‐base', 'assessment', 'compose', 'free‐response', 'open‐ende', 'item', 'cluster', 'long', 'reading', 'selection', 'multistep', 'mathematics', 'problem', 'Data', 'Maryland', 'School', 'Performance', 'Assessment', 'Program', 'result', 'suggest', 'Masters', '1982', 'Wright', 'Masters', '1982', 'partial', 'credit', 'useful', 'equate', 'examination', 'compose', 'moderately', 'easy', 'difficultitem', 'share', 'principal', 'component', '25', 'total', 'variance', 'conclusion', 'appear', 'hold', 'presence', 'level', 'response', 'dependency', 'item', 'cluster', 'visible', 'discrepancy', 'find', 'pc', 'EP', 'equate', 'score', 'skewed', 'tail', 'score', 'distribution', 'direction', 'discrepancy', 'unpredictable', 'conclude', 'study', 'method', 'equivalent', 'result', 'distribution', 'markedly', 'skewed', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Equal', 'Percentile', 'Partial', 'Credit', 'Equatings', 'Performance‐Based', 'assessment', 'compose', 'free‐response', 'item']",study compare equal percentile ep partial credit pc equating raw score derive performance‐base assessment compose free‐response open‐ende item cluster long reading selection multistep mathematics problem Data Maryland School Performance Assessment Program result suggest Masters 1982 Wright Masters 1982 partial credit useful equate examination compose moderately easy difficultitem share principal component 25 total variance conclusion appear hold presence level response dependency item cluster visible discrepancy find pc EP equate score skewed tail score distribution direction discrepancy unpredictable conclude study method equivalent result distribution markedly skewed Copyright © 1994 Wiley Blackwell right reserve,Comparison Equal Percentile Partial Credit Equatings Performance‐Based assessment compose free‐response item,0.9040163271598359,0.024219754614153974,0.0240679421734623,0.023645720034785208,0.024050256017762645,0.022170882900778188,0.002950258661824408,0.011561848178130605,0.05029611545926703,0.0405713522005293
Norcini J.J.; Shea J.A.,Equivalent Estimates of Borderline Group Performance in Standard Setting,1992,29,"The purpose of this study was to determine if a linear procedure, typically applied to an entire examination when equating scores and reseating judges' standards, could be used with individual item data gathered through Angoffs standard‐setting method (1971). Specifically, experts estimates of borderline group performance on one form of a test were transformed to be on the same scale as experts' estimates of borderline group performance on another form of the test. The transformations were based on examinees' responses to the items and on judges' estimates of borderline group performance. The transformed values were compared to the actual estimates provided by a group of judges. The equated and reseated values were reasonably close to those actually assigned by the experts. Bias in the estimates was also relatively small. In general, the reseating procedure was more accurate than the equating procedure, especially when the examinee sample size for equating was small. Copyright © 1992, Wiley Blackwell. All rights reserved",Equivalent Estimates of Borderline Group Performance in Standard Setting,"The purpose of this study was to determine if a linear procedure, typically applied to an entire examination when equating scores and reseating judges' standards, could be used with individual item data gathered through Angoffs standard‐setting method (1971). Specifically, experts estimates of borderline group performance on one form of a test were transformed to be on the same scale as experts' estimates of borderline group performance on another form of the test. The transformations were based on examinees' responses to the items and on judges' estimates of borderline group performance. The transformed values were compared to the actual estimates provided by a group of judges. The equated and reseated values were reasonably close to those actually assigned by the experts. Bias in the estimates was also relatively small. In general, the reseating procedure was more accurate than the equating procedure, especially when the examinee sample size for equating was small. Copyright © 1992, Wiley Blackwell. All rights reserved","['purpose', 'study', 'determine', 'linear', 'procedure', 'typically', 'apply', 'entire', 'examination', 'equate', 'score', 'reseat', 'judge', 'standard', 'individual', 'item', 'datum', 'gather', 'Angoffs', 'standard‐setting', 'method', '1971', 'Specifically', 'expert', 'estimate', 'borderline', 'group', 'performance', 'form', 'test', 'transform', 'scale', 'expert', 'estimate', 'borderline', 'group', 'performance', 'form', 'test', 'transformation', 'base', 'examine', 'response', 'item', 'judge', 'estimate', 'borderline', 'group', 'performance', 'transform', 'value', 'compare', 'actual', 'estimate', 'provide', 'group', 'judge', 'equate', 'reseat', 'value', 'reasonably', 'close', 'actually', 'assign', 'expert', 'Bias', 'estimate', 'relatively', 'small', 'general', 'reseat', 'procedure', 'accurate', 'equate', 'procedure', 'especially', 'examinee', 'sample', 'size', 'equating', 'small', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['equivalent', 'estimate', 'Borderline', 'Group', 'Performance', 'Standard', 'Setting']",purpose study determine linear procedure typically apply entire examination equate score reseat judge standard individual item datum gather Angoffs standard‐setting method 1971 Specifically expert estimate borderline group performance form test transform scale expert estimate borderline group performance form test transformation base examine response item judge estimate borderline group performance transform value compare actual estimate provide group judge equate reseat value reasonably close actually assign expert Bias estimate relatively small general reseat procedure accurate equate procedure especially examinee sample size equating small Copyright © 1992 Wiley Blackwell right reserve,equivalent estimate Borderline Group Performance Standard Setting,0.02813288476935242,0.027992590934534094,0.028236175421656653,0.02778114623797599,0.8878572026364808,0.10927346627815979,0.0,0.0,0.01937663606334164,0.008408783955706326
Thissen D.; Wainer H.; Wang X.‐B.,Are Tests Comprising Both Multiple‐Choice and Free‐Response Items Necessarily Less Unidimensional Than Multiple‐Choice Tests?An Analysis of Two Tests,1994,31,"We consider the relationship between the multiple‐choice and free‐response sections on the Computer Science and Chemistry tests of the College Board's Advanced Placement program. Restricted factor analysis shows that the free‐response sections measure the same underlying proficiency as the multiple‐choice sections for the most part. However, there is also a significant, if relatively small, amount of local dependence among the free‐response items that produces a small degree of multidimensionauty for each test Copyright © 1994, Wiley Blackwell. All rights reserved",Are Tests Comprising Both Multiple‐Choice and Free‐Response Items Necessarily Less Unidimensional Than Multiple‐Choice Tests?An Analysis of Two Tests,"We consider the relationship between the multiple‐choice and free‐response sections on the Computer Science and Chemistry tests of the College Board's Advanced Placement program. Restricted factor analysis shows that the free‐response sections measure the same underlying proficiency as the multiple‐choice sections for the most part. However, there is also a significant, if relatively small, amount of local dependence among the free‐response items that produces a small degree of multidimensionauty for each test Copyright © 1994, Wiley Blackwell. All rights reserved","['consider', 'relationship', 'multiple‐choice', 'free‐response', 'section', 'Computer', 'Science', 'Chemistry', 'test', 'College', 'Boards', 'Advanced', 'Placement', 'program', 'Restricted', 'factor', 'analysis', 'free‐response', 'section', 'measure', 'underlying', 'proficiency', 'multiple‐choice', 'section', 'significant', 'relatively', 'small', 'local', 'dependence', 'free‐response', 'item', 'produce', 'small', 'degree', 'multidimensionauty', 'test', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['test', 'comprise', 'Multiple‐Choice', 'free‐response', 'Items', 'necessarily', 'unidimensional', 'Than', 'Multiple‐Choice', 'TestsAn', 'Analysis', 'test']",consider relationship multiple‐choice free‐response section Computer Science Chemistry test College Boards Advanced Placement program Restricted factor analysis free‐response section measure underlying proficiency multiple‐choice section significant relatively small local dependence free‐response item produce small degree multidimensionauty test Copyright © 1994 Wiley Blackwell right reserve,test comprise Multiple‐Choice free‐response Items necessarily unidimensional Than Multiple‐Choice TestsAn Analysis test,0.8649863247818066,0.034040045342550646,0.033772152100242286,0.033625960302109466,0.03357551747329093,0.0,0.08339783870285182,0.0,0.0,0.01201078950539307
Stricker L.J.,Current Validity of 1975 and 1985 SATs: Implications for Validity Trends Since the Mid‐1970s,1991,28,"The aim of this study was to appraise whether different forms of the SA T used since the mid‐1970s varied in their correlations with academic performance criteria in the same cohort of examinees. A 1975 form and a 1985 form were administered to two random samples of high school juniors, and self‐reported grade point average and high school rank were obtained. The SAT‐Verbal and SAT‐Mathematical scores generally had similar correlations with the grade criteria in the two samples. The principal conclusion is that the 1975 form of the SAT does not have greater validity than the 1985 form in assessing academic performance, at least at the high school level. This outcome offers no support for the hypothesis that the decline in the SAT's ability to predict college grades since the mid‐1970s, observed in recent research, is attributable to changes in the test. Copyright © 1991, Wiley Blackwell. All rights reserved",Current Validity of 1975 and 1985 SATs: Implications for Validity Trends Since the Mid‐1970s,"The aim of this study was to appraise whether different forms of the SA T used since the mid‐1970s varied in their correlations with academic performance criteria in the same cohort of examinees. A 1975 form and a 1985 form were administered to two random samples of high school juniors, and self‐reported grade point average and high school rank were obtained. The SAT‐Verbal and SAT‐Mathematical scores generally had similar correlations with the grade criteria in the two samples. The principal conclusion is that the 1975 form of the SAT does not have greater validity than the 1985 form in assessing academic performance, at least at the high school level. This outcome offers no support for the hypothesis that the decline in the SAT's ability to predict college grades since the mid‐1970s, observed in recent research, is attributable to changes in the test. Copyright © 1991, Wiley Blackwell. All rights reserved","['aim', 'study', 'appraise', 'different', 'form', 'SA', 'T', 'mid‐1970s', 'varied', 'correlation', 'academic', 'performance', 'criterion', 'cohort', 'examinee', '1975', 'form', '1985', 'form', 'administer', 'random', 'sample', 'high', 'school', 'junior', 'self‐reporte', 'grade', 'point', 'average', 'high', 'school', 'rank', 'obtain', 'sat‐verbal', 'sat‐mathematical', 'score', 'generally', 'similar', 'correlation', 'grade', 'criterion', 'sample', 'principal', 'conclusion', '1975', 'form', 'SAT', 'great', 'validity', '1985', 'form', 'assess', 'academic', 'performance', 'high', 'school', 'level', 'outcome', 'offer', 'support', 'hypothesis', 'decline', 'sats', 'ability', 'predict', 'college', 'grade', 'mid‐1970s', 'observe', 'recent', 'research', 'attributable', 'change', 'test', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['current', 'Validity', '1975', '1985', 'sats', 'Implications', 'Validity', 'Trends', 'Mid‐1970s']",aim study appraise different form SA T mid‐1970s varied correlation academic performance criterion cohort examinee 1975 form 1985 form administer random sample high school junior self‐reporte grade point average high school rank obtain sat‐verbal sat‐mathematical score generally similar correlation grade criterion sample principal conclusion 1975 form SAT great validity 1985 form assess academic performance high school level outcome offer support hypothesis decline sats ability predict college grade mid‐1970s observe recent research attributable change test Copyright © 1991 Wiley Blackwell right reserve,current Validity 1975 1985 sats Implications Validity Trends Mid‐1970s,0.025260519601494887,0.8978727622076664,0.025623453151111482,0.025063925260535483,0.026179339779191665,0.04840757478103339,0.0,0.0,0.0,0.07096285501511831
Homan S.; Hewitt M.; Linder J.,The Development and Validation of a Formula for Measuring Single‐Sentence Test Item Readability,1994,31,"This study describes the development and validation of the Homan‐Hewitt Readability Formula. This formula estimates the readability level of single‐sentence test items. Its initial development is based on the assumption that differences in readability level will affect item difficulty. The validation of the formula is achieved by (a) estimating the readability levels of sets of test items predicted to be written at 2nd‐ through 8th‐grade levels; (b) administering the tests to 782 students in grades 2 through 5; (3) using the class means as the unit of analyses and subjecting the data to a two‐factor repeated measures ANOVA. Significant differences were found on class mean performance scores across the levels of readability. These results indicated that a relationship exists between students’reading grade levels and their responses to test items written at higher readability levels. Copyright © 1994, Wiley Blackwell. All rights reserved",The Development and Validation of a Formula for Measuring Single‐Sentence Test Item Readability,"This study describes the development and validation of the Homan‐Hewitt Readability Formula. This formula estimates the readability level of single‐sentence test items. Its initial development is based on the assumption that differences in readability level will affect item difficulty. The validation of the formula is achieved by (a) estimating the readability levels of sets of test items predicted to be written at 2nd‐ through 8th‐grade levels; (b) administering the tests to 782 students in grades 2 through 5; (3) using the class means as the unit of analyses and subjecting the data to a two‐factor repeated measures ANOVA. Significant differences were found on class mean performance scores across the levels of readability. These results indicated that a relationship exists between students’reading grade levels and their responses to test items written at higher readability levels. Copyright © 1994, Wiley Blackwell. All rights reserved","['study', 'describe', 'development', 'validation', 'Homan‐Hewitt', 'Readability', 'Formula', 'formula', 'estimate', 'readability', 'level', 'single‐sentence', 'test', 'item', 'initial', 'development', 'base', 'assumption', 'difference', 'readability', 'level', 'affect', 'item', 'difficulty', 'validation', 'formula', 'achieve', 'estimating', 'readability', 'level', 'set', 'test', 'item', 'predict', 'write', '2nd‐', '8th‐grade', 'level', 'b', 'administer', 'test', '782', 'student', 'grade', '2', '5', '3', 'class', 'mean', 'unit', 'analysis', 'subject', 'datum', 'two‐factor', 'repeat', 'measure', 'ANOVA', 'significant', 'difference', 'find', 'class', 'mean', 'performance', 'score', 'level', 'readability', 'result', 'indicate', 'relationship', 'exist', 'students’reade', 'grade', 'level', 'response', 'test', 'item', 'write', 'high', 'readability', 'level', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Development', 'Validation', 'Formula', 'Measuring', 'single‐sentence', 'Test', 'Item', 'Readability']",study describe development validation Homan‐Hewitt Readability Formula formula estimate readability level single‐sentence test item initial development base assumption difference readability level affect item difficulty validation formula achieve estimating readability level set test item predict write 2nd‐ 8th‐grade level b administer test 782 student grade 2 5 3 class mean unit analysis subject datum two‐factor repeat measure ANOVA significant difference find class mean performance score level readability result indicate relationship exist students’reade grade level response test item write high readability level Copyright © 1994 Wiley Blackwell right reserve,Development Validation Formula Measuring single‐sentence Test Item Readability,0.031194072210811566,0.874582251974815,0.03153805352492604,0.030954603822775255,0.03173101846667214,0.006112009968270809,0.058803768703254544,0.0,0.01867964203898916,0.040274590312619665
Kolen M.J.,Smoothing Methods for Estimating Test Score Distributions,1991,28,"Frequency distributions of test scores may appear irregular and, as estimates of a population distribution, contain a substantial amount of sampling error. Techniques for smoothing score distributions are available that have the capacity to improve estimation. In this article, estimation/smoothing methods that are flexible enough to fit a wide variety of test score distributions are reviewed. The methods are a kernel method, a strong true–score model–based method, and a method that uses polynomial log–linear models. The use of these methods is then reviewed, and applications of the methods are presented that include describing and comparing test score distributions, estimating norms, and estimating equipercentile equivalents in test score equating. Suggestions for further research are also provided. Copyright © 1991, Wiley Blackwell. All rights reserved",Smoothing Methods for Estimating Test Score Distributions,"Frequency distributions of test scores may appear irregular and, as estimates of a population distribution, contain a substantial amount of sampling error. Techniques for smoothing score distributions are available that have the capacity to improve estimation. In this article, estimation/smoothing methods that are flexible enough to fit a wide variety of test score distributions are reviewed. The methods are a kernel method, a strong true–score model–based method, and a method that uses polynomial log–linear models. The use of these methods is then reviewed, and applications of the methods are presented that include describing and comparing test score distributions, estimating norms, and estimating equipercentile equivalents in test score equating. Suggestions for further research are also provided. Copyright © 1991, Wiley Blackwell. All rights reserved","['frequency', 'distribution', 'test', 'score', 'appear', 'irregular', 'estimate', 'population', 'distribution', 'contain', 'substantial', 'sample', 'error', 'technique', 'smooth', 'score', 'distribution', 'available', 'capacity', 'improve', 'estimation', 'article', 'estimationsmoothe', 'method', 'flexible', 'fit', 'wide', 'variety', 'test', 'score', 'distribution', 'review', 'method', 'kernel', 'method', 'strong', 'true', '–', 'score', '–', 'base', 'method', 'method', 'polynomial', 'log', '–', 'linear', 'method', 'review', 'application', 'method', 'present', 'include', 'describe', 'compare', 'test', 'score', 'distribution', 'estimate', 'norm', 'estimate', 'equipercentile', 'equivalent', 'test', 'score', 'equate', 'suggestion', 'research', 'provide', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['smooth', 'Methods', 'estimate', 'Test', 'Score', 'distribution']",frequency distribution test score appear irregular estimate population distribution contain substantial sample error technique smooth score distribution available capacity improve estimation article estimationsmoothe method flexible fit wide variety test score distribution review method kernel method strong true – score – base method method polynomial log – linear method review application method present include describe compare test score distribution estimate norm estimate equipercentile equivalent test score equate suggestion research provide Copyright © 1991 Wiley Blackwell right reserve,smooth Methods estimate Test Score distribution,0.028596178806104666,0.02841655536707776,0.028743432383088817,0.02830016760811204,0.8859436658356168,0.04863771925995849,0.0,0.0,0.1782579430314669,0.0
Ben‐Shakhar G.; Sinai Y.,Gender Differences in Multiple‐Choice Tests: The Role of Differential Guessing Tendencies,1991,28,"The present study focused on gender differences in the tendency to omit items and to guess in multiple‐choice tests. It was hypothesized that males would show greater guessing tendencies than females and that the use of formula scoring rather than the use of number of correct answers would result in a relative advantage for females. Two samples were examined: ninth graders and applicants to Israeli universities. The teenagers took a battery of five or six aptitude tests used to place them in various high schools, and the adults took a battery of five tests designed to select candidates to the various faculties of the Israeli universities. The results revealed a clear male advantage in most subtests of both batteries. Four measures of item‐omission tendencies were computed for each subtest, and a consistent pattern of greater omission rates among females was revealed by all measures in most subtests of the two batteries. This pattern was observed even in the few subtests that did not show male superiority and even when permissive instructions were used. Correcting the raw scores for guessing reduced the male advantage in all cases (and in the few subtests that showed female advantage the difference increased as a result of this correction), but this effect was small. It was concluded that although gender differences in guessing tendencies are robust they account for only a small fraction of the observed gender differences in multiple‐choice tests. The results were discussed, focusing on practical implications. Copyright © 1991, Wiley Blackwell. All rights reserved",Gender Differences in Multiple‐Choice Tests: The Role of Differential Guessing Tendencies,"The present study focused on gender differences in the tendency to omit items and to guess in multiple‐choice tests. It was hypothesized that males would show greater guessing tendencies than females and that the use of formula scoring rather than the use of number of correct answers would result in a relative advantage for females. Two samples were examined: ninth graders and applicants to Israeli universities. The teenagers took a battery of five or six aptitude tests used to place them in various high schools, and the adults took a battery of five tests designed to select candidates to the various faculties of the Israeli universities. The results revealed a clear male advantage in most subtests of both batteries. Four measures of item‐omission tendencies were computed for each subtest, and a consistent pattern of greater omission rates among females was revealed by all measures in most subtests of the two batteries. This pattern was observed even in the few subtests that did not show male superiority and even when permissive instructions were used. Correcting the raw scores for guessing reduced the male advantage in all cases (and in the few subtests that showed female advantage the difference increased as a result of this correction), but this effect was small. It was concluded that although gender differences in guessing tendencies are robust they account for only a small fraction of the observed gender differences in multiple‐choice tests. The results were discussed, focusing on practical implications. Copyright © 1991, Wiley Blackwell. All rights reserved","['present', 'study', 'focus', 'gender', 'difference', 'tendency', 'omit', 'item', 'guess', 'multiple‐choice', 'test', 'hypothesize', 'male', 'great', 'guessing', 'tendency', 'female', 'formula', 'scoring', 'number', 'correct', 'answer', 'result', 'relative', 'advantage', 'female', 'sample', 'examine', 'ninth', 'grader', 'applicant', 'israeli', 'university', 'teenager', 'battery', 'aptitude', 'test', 'place', 'high', 'school', 'adult', 'battery', 'test', 'design', 'select', 'candidate', 'faculty', 'israeli', 'university', 'result', 'reveal', 'clear', 'male', 'advantage', 'subtest', 'battery', 'measure', 'item‐omission', 'tendency', 'compute', 'subt', 'consistent', 'pattern', 'great', 'omission', 'rate', 'female', 'reveal', 'measure', 'subtest', 'battery', 'pattern', 'observe', 'subtest', 'male', 'superiority', 'permissive', 'instruction', 'correct', 'raw', 'score', 'guess', 'reduce', 'male', 'advantage', 'case', 'subtest', 'female', 'advantage', 'difference', 'increase', 'result', 'correction', 'effect', 'small', 'conclude', 'gender', 'difference', 'guess', 'tendency', 'robust', 'account', 'small', 'fraction', 'observed', 'gender', 'difference', 'multiple‐choice', 'test', 'result', 'discuss', 'focus', 'practical', 'implication', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Gender', 'Differences', 'Multiple‐Choice', 'test', 'Role', 'Differential', 'Guessing', 'tendency']",present study focus gender difference tendency omit item guess multiple‐choice test hypothesize male great guessing tendency female formula scoring number correct answer result relative advantage female sample examine ninth grader applicant israeli university teenager battery aptitude test place high school adult battery test design select candidate faculty israeli university result reveal clear male advantage subtest battery measure item‐omission tendency compute subt consistent pattern great omission rate female reveal measure subtest battery pattern observe subtest male superiority permissive instruction correct raw score guess reduce male advantage case subtest female advantage difference increase result correction effect small conclude gender difference guess tendency robust account small fraction observed gender difference multiple‐choice test result discuss focus practical implication copyright © 1991 Wiley Blackwell right reserve,Gender Differences Multiple‐Choice test Role Differential Guessing tendency,0.026435802717557993,0.026206743970041285,0.8958281130459211,0.0254526186619456,0.026076721604534073,0.0036581476924938227,0.0556179529080649,0.0,0.019586679089921677,0.019827055318491865
Roberts D.M.,An Empirical Study on the Nature of Trick Test Questions,1993,30,"This study attempted to better define trick questions and see if students could differentiate between trick and not–trick questions. Phase 1 elicited definitions of trick questions so as to identify essential characteristics. Seven components were found. Phase 2 obtained ratings to see which components of trick questions were considered to be most crucial. The intention of the item constructor and the fact that the questions had multiple correct answers received highest ratings from students. Phase 3 presented a collection of statistics items, some of which were labeled on an a priori basis as being trick or not–trick. The analysis indicated that examinees were able to statistically differentiate between trick and not–trick items, but the difference compared to chance was small. Not–trick items were more successfully sorted than trick items, and trick items that were classified as intentional were sorted about as well as nonintentional items. Evidence seems to suggest that the concept of trickiness is not as clear as some test construction textbook authors suggest. Copyright © 1993, Wiley Blackwell. All rights reserved",An Empirical Study on the Nature of Trick Test Questions,"This study attempted to better define trick questions and see if students could differentiate between trick and not–trick questions. Phase 1 elicited definitions of trick questions so as to identify essential characteristics. Seven components were found. Phase 2 obtained ratings to see which components of trick questions were considered to be most crucial. The intention of the item constructor and the fact that the questions had multiple correct answers received highest ratings from students. Phase 3 presented a collection of statistics items, some of which were labeled on an a priori basis as being trick or not–trick. The analysis indicated that examinees were able to statistically differentiate between trick and not–trick items, but the difference compared to chance was small. Not–trick items were more successfully sorted than trick items, and trick items that were classified as intentional were sorted about as well as nonintentional items. Evidence seems to suggest that the concept of trickiness is not as clear as some test construction textbook authors suggest. Copyright © 1993, Wiley Blackwell. All rights reserved","['study', 'attempt', 'define', 'trick', 'question', 'student', 'differentiate', 'trick', '–', 'trick', 'question', 'Phase', '1', 'elicit', 'definition', 'trick', 'question', 'identify', 'essential', 'characteristic', 'seven', 'component', 'find', 'Phase', '2', 'obtain', 'rating', 'component', 'trick', 'question', 'consider', 'crucial', 'intention', 'item', 'constructor', 'fact', 'question', 'multiple', 'correct', 'answer', 'receive', 'high', 'rating', 'student', 'Phase', '3', 'present', 'collection', 'statistic', 'item', 'label', 'priori', 'basis', 'trick', '–', 'trick', 'analysis', 'indicate', 'examinee', 'able', 'statistically', 'differentiate', 'trick', '–', 'trick', 'item', 'difference', 'compare', 'chance', 'small', '–', 'trick', 'item', 'successfully', 'sort', 'trick', 'item', 'trick', 'item', 'classify', 'intentional', 'sort', 'nonintentional', 'item', 'Evidence', 'suggest', 'concept', 'trickiness', 'clear', 'test', 'construction', 'textbook', 'author', 'suggest', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['empirical', 'Study', 'Nature', 'Trick', 'Test', 'Questions']",study attempt define trick question student differentiate trick – trick question Phase 1 elicit definition trick question identify essential characteristic seven component find Phase 2 obtain rating component trick question consider crucial intention item constructor fact question multiple correct answer receive high rating student Phase 3 present collection statistic item label priori basis trick – trick analysis indicate examinee able statistically differentiate trick – trick item difference compare chance small – trick item successfully sort trick item trick item classify intentional sort nonintentional item Evidence suggest concept trickiness clear test construction textbook author suggest Copyright © 1993 Wiley Blackwell right reserve,empirical Study Nature Trick Test Questions,0.0355874638611904,0.03574707589697407,0.8563840402734696,0.035390746798243757,0.03689067317012214,0.0,0.061550001884507204,0.0012419848042188942,0.0,0.005465723915943069
Wainer H.,Measurement Problems,1993,30,"History teaches the continuity of science; the developments of tomorrow have their genesis in the problems of today. Thus any attempt to look forward is well begun with an examination of unsettled questions. Since a clearer idea of where we are going smoothes the path into the unknown future, a periodic review of such questions is prudent. The present day, lying near the juncture of the centuries, is well suited for such a review. This article reports 16 unsolved problems in educational measurement and points toward what seem to be promising avenues of solution. Copyright © 1993, Wiley Blackwell. All rights reserved",,"History teaches the continuity of science; the developments of tomorrow have their genesis in the problems of today. Thus any attempt to look forward is well begun with an examination of unsettled questions. Since a clearer idea of where we are going smoothes the path into the unknown future, a periodic review of such questions is prudent. The present day, lying near the juncture of the centuries, is well suited for such a review. This article reports 16 unsolved problems in educational measurement and points toward what seem to be promising avenues of solution. Copyright © 1993, Wiley Blackwell. All rights reserved","['history', 'teach', 'continuity', 'science', 'development', 'tomorrow', 'genesis', 'problem', 'today', 'attempt', 'look', 'forward', 'begin', 'examination', 'unsettled', 'question', 'clear', 'idea', 'smoothe', 'path', 'unknown', 'future', 'periodic', 'review', 'question', 'prudent', 'present', 'day', 'lie', 'near', 'juncture', 'century', 'suited', 'review', 'article', 'report', '16', 'unsolved', 'problem', 'educational', 'point', 'promise', 'avenue', 'solution', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,history teach continuity science development tomorrow genesis problem today attempt look forward begin examination unsettled question clear idea smoothe path unknown future periodic review question prudent present day lie near juncture century suited review article report 16 unsolved problem educational point promise avenue solution Copyright © 1993 Wiley Blackwell right reserve,,0.02696762091255723,0.02724233248492329,0.027606518895641575,0.8908129172881727,0.027370610418705255,0.0,0.022081154745577092,0.0,0.008602049154027736,0.023380495979807524
Tan E.S.; Imbos Tj.; Does R.J.M.M.,A Distribution‐Free Approach for Comparing Growth of Knowledge,1994,31,"The longitudinal testing of student achievement requires the solution of several new problem areas. In this article, several small groups of medical students at the University of Limburg Medical School in Maastricht, The Netherlands, are compared with respect to their performances. The results indicate, that, despite the possession of more knowledge at entrance, students with a low rate of growth of knowledge in the first year demonstrate a lower level of knowledge after the second academic year and continue to do so throughout the academic program when compared to students who show a higher rate of growth of knowledge in the first year. The analysis has been carried out using a distribution‐free version of a longitudinal IRT‐model suggested by Albers, Does, Imbos, and Janssen (1989). Furthermore, growth of knowledge has been described by means of a general regression model. Statistical inferences are possible using a randomization design extended to the situation where the observations are time‐dependent proportions of correct answers. Copyright © 1994, Wiley Blackwell. All rights reserved",A Distribution‐Free Approach for Comparing Growth of Knowledge,"The longitudinal testing of student achievement requires the solution of several new problem areas. In this article, several small groups of medical students at the University of Limburg Medical School in Maastricht, The Netherlands, are compared with respect to their performances. The results indicate, that, despite the possession of more knowledge at entrance, students with a low rate of growth of knowledge in the first year demonstrate a lower level of knowledge after the second academic year and continue to do so throughout the academic program when compared to students who show a higher rate of growth of knowledge in the first year. The analysis has been carried out using a distribution‐free version of a longitudinal IRT‐model suggested by Albers, Does, Imbos, and Janssen (1989). Furthermore, growth of knowledge has been described by means of a general regression model. Statistical inferences are possible using a randomization design extended to the situation where the observations are time‐dependent proportions of correct answers. Copyright © 1994, Wiley Blackwell. All rights reserved","['longitudinal', 'testing', 'student', 'achievement', 'require', 'solution', 'new', 'problem', 'area', 'article', 'small', 'group', 'medical', 'student', 'University', 'Limburg', 'Medical', 'School', 'Maastricht', 'Netherlands', 'compare', 'respect', 'performance', 'result', 'indicate', 'despite', 'possession', 'knowledge', 'entrance', 'student', 'low', 'rate', 'growth', 'knowledge', 'year', 'demonstrate', 'low', 'level', 'knowledge', 'second', 'academic', 'year', 'continue', 'academic', 'program', 'compare', 'student', 'high', 'rate', 'growth', 'knowledge', 'year', 'analysis', 'carry', 'distribution‐free', 'version', 'longitudinal', 'irt‐model', 'suggest', 'alber', 'Imbos', 'Janssen', '1989', 'furthermore', 'growth', 'knowledge', 'describe', 'mean', 'general', 'regression', 'statistical', 'inference', 'possible', 'randomization', 'design', 'extend', 'situation', 'observation', 'time‐dependent', 'proportion', 'correct', 'answer', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['distribution‐free', 'approach', 'compare', 'Growth', 'Knowledge']",longitudinal testing student achievement require solution new problem area article small group medical student University Limburg Medical School Maastricht Netherlands compare respect performance result indicate despite possession knowledge entrance student low rate growth knowledge year demonstrate low level knowledge second academic year continue academic program compare student high rate growth knowledge year analysis carry distribution‐free version longitudinal irt‐model suggest alber Imbos Janssen 1989 furthermore growth knowledge describe mean general regression statistical inference possible randomization design extend situation observation time‐dependent proportion correct answer Copyright © 1994 Wiley Blackwell right reserve,distribution‐free approach compare Growth Knowledge,0.024915018423302588,0.8995465505784257,0.02528032529221114,0.024869958181389738,0.025388147524670696,0.002313166888518145,0.011605620303844507,0.002429411926875304,0.0,0.09024437116739385
"Engelhard G., Jr",Examining Rater Errors in the Assessment of Written Composition With a Many‐Faceted Rasch Model,1994,31,"This study describes several categories of rater errors (rater severity, halo effect, central tendency, and restriction of range). Criteria are presented for evaluating the quality of ratings based on a many‐faceted Rasch measurement (FACETS) model for analyzing judgments. A random sample of 264 compositions rated by 15 raters and a validity committee from the 1990 administration of the Eighth Grade Writing Test in Georgia is used to illustrate the model. The data suggest that there are significant differences in rater severity. Evidence of a halo effect is found for two raters who appear to be rating the compositions holistically rather than analytically. Approximately 80% of the ratings are in the two middle categories of the rating scale, indicating that the error of central tendency is present. Restriction of range is evident when the unadjusted raw score distribution is examined, although this rater error is less evident when adjusted estimates of writing competence are used Copyright © 1994, Wiley Blackwell. All rights reserved",Examining Rater Errors in the Assessment of Written Composition With a Many‐Faceted Rasch Model,"This study describes several categories of rater errors (rater severity, halo effect, central tendency, and restriction of range). Criteria are presented for evaluating the quality of ratings based on a many‐faceted Rasch measurement (FACETS) model for analyzing judgments. A random sample of 264 compositions rated by 15 raters and a validity committee from the 1990 administration of the Eighth Grade Writing Test in Georgia is used to illustrate the model. The data suggest that there are significant differences in rater severity. Evidence of a halo effect is found for two raters who appear to be rating the compositions holistically rather than analytically. Approximately 80% of the ratings are in the two middle categories of the rating scale, indicating that the error of central tendency is present. Restriction of range is evident when the unadjusted raw score distribution is examined, although this rater error is less evident when adjusted estimates of writing competence are used Copyright © 1994, Wiley Blackwell. All rights reserved","['study', 'describe', 'category', 'rater', 'error', 'rater', 'severity', 'halo', 'effect', 'central', 'tendency', 'restriction', 'range', 'criterion', 'present', 'evaluate', 'quality', 'rating', 'base', 'many‐faceted', 'Rasch', 'facets', 'analyze', 'judgment', 'random', 'sample', '264', 'composition', 'rate', '15', 'rater', 'validity', 'committee', '1990', 'administration', 'Eighth', 'Grade', 'Writing', 'Test', 'Georgia', 'illustrate', 'datum', 'suggest', 'significant', 'difference', 'rater', 'severity', 'Evidence', 'halo', 'effect', 'find', 'rater', 'appear', 'rate', 'composition', 'holistically', 'analytically', 'approximately', '80', 'rating', 'middle', 'category', 'rating', 'scale', 'indicate', 'error', 'central', 'tendency', 'present', 'restriction', 'range', 'evident', 'unadjusted', 'raw', 'score', 'distribution', 'examine', 'rater', 'error', 'evident', 'adjust', 'estimate', 'writing', 'competence', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['examine', 'Rater', 'error', 'Assessment', 'Written', 'Composition', 'many‐faceted', 'Rasch']",study describe category rater error rater severity halo effect central tendency restriction range criterion present evaluate quality rating base many‐faceted Rasch facets analyze judgment random sample 264 composition rate 15 rater validity committee 1990 administration Eighth Grade Writing Test Georgia illustrate datum suggest significant difference rater severity Evidence halo effect find rater appear rate composition holistically analytically approximately 80 rating middle category rating scale indicate error central tendency present restriction range evident unadjusted raw score distribution examine rater error evident adjust estimate writing competence Copyright © 1994 Wiley Blackwell right reserve,examine Rater error Assessment Written Composition many‐faceted Rasch,0.02712729469392318,0.026770020437393117,0.8919590226822341,0.026088104574511203,0.02805555761193831,0.004000323773746969,0.0,0.0,0.03225242628603414,0.09133512999247863
Ryan K.E.,The Performance of the Mantel‐Haenszel Procedure Across Samples and Matching Criteria,1991,28,"This study examined the reliability of the Mantel‐Haenszel indexes across different samples of test takers as well as across sample sizes and investigated whether these indexes are robust to item context effects. Mathematics data from the Second International Mathematics Study (SIMS; 1985) for U.S. eighth‐grade students were analyzed. The results suggest that the MH D‐DIF is robust to item context effects. However, larger sample sizes than those used in this investigation (N = 141‐167 for the focal group) may be necessary to obtain stable estimates from the Mantel‐Haenszel procedure. Copyright © 1991, Wiley Blackwell. All rights reserved",The Performance of the Mantel‐Haenszel Procedure Across Samples and Matching Criteria,"This study examined the reliability of the Mantel‐Haenszel indexes across different samples of test takers as well as across sample sizes and investigated whether these indexes are robust to item context effects. Mathematics data from the Second International Mathematics Study (SIMS; 1985) for U.S. eighth‐grade students were analyzed. The results suggest that the MH D‐DIF is robust to item context effects. However, larger sample sizes than those used in this investigation (N = 141‐167 for the focal group) may be necessary to obtain stable estimates from the Mantel‐Haenszel procedure. Copyright © 1991, Wiley Blackwell. All rights reserved","['study', 'examine', 'reliability', 'Mantel‐Haenszel', 'index', 'different', 'sample', 'test', 'taker', 'sample', 'size', 'investigate', 'index', 'robust', 'item', 'context', 'effect', 'mathematic', 'datum', 'Second', 'International', 'Mathematics', 'Study', 'SIMS', '1985', 'US', 'eighth‐grade', 'student', 'analyze', 'result', 'suggest', 'MH', 'D‐DIF', 'robust', 'item', 'context', 'effect', 'large', 'sample', 'size', 'investigation', 'N', '141‐167', 'focal', 'group', 'necessary', 'obtain', 'stable', 'estimate', 'Mantel‐Haenszel', 'procedure', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Performance', 'Mantel‐Haenszel', 'Procedure', 'Samples', 'Matching', 'Criteria']",study examine reliability Mantel‐Haenszel index different sample test taker sample size investigate index robust item context effect mathematic datum Second International Mathematics Study SIMS 1985 US eighth‐grade student analyze result suggest MH D‐DIF robust item context effect large sample size investigation N 141‐167 focal group necessary obtain stable estimate Mantel‐Haenszel procedure Copyright © 1991 Wiley Blackwell right reserve,Performance Mantel‐Haenszel Procedure Samples Matching Criteria,0.02877919053962289,0.8850206137064999,0.02873905231356674,0.028535135211051155,0.028926008229259437,0.07540638759336098,0.0,0.011269714255967626,0.0,0.03906782434073874
Baker F.B.; Al‐Karni A.,A Comparison of Two Procedures for Computing IRT Equating Coefficients,1991,28,"In order to equate tests under Item Response Theory (IRT), one must obtain the slope and intercept coefficients of the appropriate linear transformation. This article compares two methods for computing such equating coefficients–Loyd and Hoover (1980) and Stocking and Lord (1983). The former is based upon summary statistics of the test calibrations; the latter is based upon matching test characteristic curves by minimizing a quadratic loss function. Three types of equating situations: horizontal, vertical, and that inherent in IRT parameter recovery studies–were investigated. The results showed that the two computing procedures generally yielded similar equating coefficients in all three situations. In addition, two sets of SAT data were equated via the two procedures, and little difference in the obtained results was observed. Overall, the results suggest that the Loyd and Hoover procedure usually yields acceptable equating coefficients. The Stocking and Lord procedure improves upon the Loyd and Hoover values and appears to be less sensitive to atypical test characteristics. When the user has reason to suspect that the test calibrations may be associated with data sets that are typically troublesome to calibrate, the Stocking and Lord procedure is to be preferred. Copyright © 1991, Wiley Blackwell. All rights reserved",A Comparison of Two Procedures for Computing IRT Equating Coefficients,"In order to equate tests under Item Response Theory (IRT), one must obtain the slope and intercept coefficients of the appropriate linear transformation. This article compares two methods for computing such equating coefficients–Loyd and Hoover (1980) and Stocking and Lord (1983). The former is based upon summary statistics of the test calibrations; the latter is based upon matching test characteristic curves by minimizing a quadratic loss function. Three types of equating situations: horizontal, vertical, and that inherent in IRT parameter recovery studies–were investigated. The results showed that the two computing procedures generally yielded similar equating coefficients in all three situations. In addition, two sets of SAT data were equated via the two procedures, and little difference in the obtained results was observed. Overall, the results suggest that the Loyd and Hoover procedure usually yields acceptable equating coefficients. The Stocking and Lord procedure improves upon the Loyd and Hoover values and appears to be less sensitive to atypical test characteristics. When the user has reason to suspect that the test calibrations may be associated with data sets that are typically troublesome to calibrate, the Stocking and Lord procedure is to be preferred. Copyright © 1991, Wiley Blackwell. All rights reserved","['order', 'equate', 'test', 'Item', 'Response', 'Theory', 'IRT', 'obtain', 'slope', 'intercept', 'coefficient', 'appropriate', 'linear', 'transformation', 'article', 'compare', 'method', 'compute', 'equate', 'coefficient', '–', 'Loyd', 'Hoover', '1980', 'Stocking', 'Lord', '1983', 'base', 'summary', 'statistic', 'test', 'calibration', 'base', 'match', 'test', 'characteristic', 'curve', 'minimize', 'quadratic', 'loss', 'function', 'type', 'equate', 'situation', 'horizontal', 'vertical', 'inherent', 'IRT', 'parameter', 'recovery', 'study', '–', 'investigate', 'result', 'compute', 'procedure', 'generally', 'yield', 'similar', 'equate', 'coefficient', 'situation', 'addition', 'set', 'SAT', 'datum', 'equate', 'procedure', 'little', 'difference', 'obtain', 'result', 'observe', 'overall', 'result', 'suggest', 'Loyd', 'Hoover', 'procedure', 'usually', 'yield', 'acceptable', 'equate', 'coefficient', 'Stocking', 'Lord', 'procedure', 'improve', 'Loyd', 'Hoover', 'value', 'appear', 'sensitive', 'atypical', 'test', 'characteristic', 'user', 'reason', 'suspect', 'test', 'calibration', 'associate', 'datum', 'set', 'typically', 'troublesome', 'calibrate', 'Stocking', 'Lord', 'procedure', 'prefer', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Procedures', 'Computing', 'IRT', 'Equating', 'Coefficients']",order equate test Item Response Theory IRT obtain slope intercept coefficient appropriate linear transformation article compare method compute equate coefficient – Loyd Hoover 1980 Stocking Lord 1983 base summary statistic test calibration base match test characteristic curve minimize quadratic loss function type equate situation horizontal vertical inherent IRT parameter recovery study – investigate result compute procedure generally yield similar equate coefficient situation addition set SAT datum equate procedure little difference obtain result observe overall result suggest Loyd Hoover procedure usually yield acceptable equate coefficient Stocking Lord procedure improve Loyd Hoover value appear sensitive atypical test characteristic user reason suspect test calibration associate datum set typically troublesome calibrate Stocking Lord procedure prefer Copyright © 1991 Wiley Blackwell right reserve,Comparison Procedures Computing IRT Equating Coefficients,0.9017616419194986,0.02450575645429699,0.024502507303845053,0.024132034005635374,0.02509806031672402,0.06887686850235643,0.018256156992989783,0.004822542969709613,0.006049020109068111,0.0
Gitomer D.H.; Yamamoto K.,Performance Modeling That Integrates Latent Trait and Class Theory,1991,28,"The qualitative characterization of individual performance that is central to modem psychological theory is not adequately modeled by traditional psychometric theory that assumes, among other things, unidimensionality. In the present study, data are presented that are more adequately modeled by HYBRID, a model that incorporates both latent trait and latent class components. The latent classes were defined by a cognitive analysis of the understanding that individuals have for a circumscribed domain. In addition to providing a better statistical fit, the analysis also improves the amount of diagnostic information available for a given individual. Copyright © 1991, Wiley Blackwell. All rights reserved",Performance Modeling That Integrates Latent Trait and Class Theory,"The qualitative characterization of individual performance that is central to modem psychological theory is not adequately modeled by traditional psychometric theory that assumes, among other things, unidimensionality. In the present study, data are presented that are more adequately modeled by HYBRID, a model that incorporates both latent trait and latent class components. The latent classes were defined by a cognitive analysis of the understanding that individuals have for a circumscribed domain. In addition to providing a better statistical fit, the analysis also improves the amount of diagnostic information available for a given individual. Copyright © 1991, Wiley Blackwell. All rights reserved","['qualitative', 'characterization', 'individual', 'performance', 'central', 'modem', 'psychological', 'theory', 'adequately', 'traditional', 'psychometric', 'theory', 'assume', 'thing', 'unidimensionality', 'present', 'study', 'datum', 'present', 'adequately', 'hybrid', 'incorporate', 'latent', 'trait', 'latent', 'class', 'component', 'latent', 'class', 'define', 'cognitive', 'analysis', 'understanding', 'individual', 'circumscribed', 'domain', 'addition', 'provide', 'statistical', 'fit', 'analysis', 'improve', 'diagnostic', 'information', 'available', 'individual', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['performance', 'Integrates', 'Latent', 'Trait', 'Class', 'Theory']",qualitative characterization individual performance central modem psychological theory adequately traditional psychometric theory assume thing unidimensionality present study datum present adequately hybrid incorporate latent trait latent class component latent class define cognitive analysis understanding individual circumscribed domain addition provide statistical fit analysis improve diagnostic information available individual copyright © 1991 Wiley Blackwell right reserve,performance Integrates Latent Trait Class Theory,0.0295051974066176,0.029257716986227797,0.029345203586433763,0.029165325305673957,0.882726556715047,0.0,0.025956046738294113,0.02188341512958826,0.0019434449395365882,0.04399283720962272
Young J.W.,Gender Bias in Predicting College Academic Performance: A New Approach Using Item Response Theory,1991,28,"The possibility of differential prediction of college academic performance for men and women is, at present, an issue of concern both to measurement specialists and to the general public. In prior studies, the use of a single equation to predict grade point average (GPA) from preadmissions measures has generally led to the systematic underprediction of women's college grades. In the following study, Item Response Theory (IRT) was used to develop a form of adjusted cumulative GPA, called the IRT‐based GPA. Significant underprediction for women occurred using a version of the cumulative GPA as the outcome measure. In contrast, the use of the IRT‐based GPA indicated no significant underprediction for men or women. A single regression equation worked best in predicting both men's and women's IRT‐based GPA. In addition, the IRT‐based GPA was substantially more predictable from preadmissions measures than the cumulative GPA. Copyright © 1991, Wiley Blackwell. All rights reserved",Gender Bias in Predicting College Academic Performance: A New Approach Using Item Response Theory,"The possibility of differential prediction of college academic performance for men and women is, at present, an issue of concern both to measurement specialists and to the general public. In prior studies, the use of a single equation to predict grade point average (GPA) from preadmissions measures has generally led to the systematic underprediction of women's college grades. In the following study, Item Response Theory (IRT) was used to develop a form of adjusted cumulative GPA, called the IRT‐based GPA. Significant underprediction for women occurred using a version of the cumulative GPA as the outcome measure. In contrast, the use of the IRT‐based GPA indicated no significant underprediction for men or women. A single regression equation worked best in predicting both men's and women's IRT‐based GPA. In addition, the IRT‐based GPA was substantially more predictable from preadmissions measures than the cumulative GPA. Copyright © 1991, Wiley Blackwell. All rights reserved","['possibility', 'differential', 'prediction', 'college', 'academic', 'performance', 'man', 'woman', 'present', 'issue', 'concern', 'specialist', 'general', 'public', 'prior', 'study', 'single', 'equation', 'predict', 'grade', 'point', 'average', 'GPA', 'preadmission', 'measure', 'generally', 'lead', 'systematic', 'underprediction', 'women', 'college', 'grade', 'follow', 'study', 'Item', 'Response', 'Theory', 'IRT', 'develop', 'form', 'adjust', 'cumulative', 'GPA', 'irt‐base', 'GPA', 'significant', 'underprediction', 'woman', 'occur', 'version', 'cumulative', 'GPA', 'outcome', 'measure', 'contrast', 'irt‐base', 'GPA', 'indicate', 'significant', 'underprediction', 'man', 'woman', 'single', 'regression', 'equation', 'work', 'predict', 'men', 'women', 'irt‐base', 'GPA', 'addition', 'irt‐base', 'GPA', 'substantially', 'predictable', 'preadmission', 'measure', 'cumulative', 'GPA', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Gender', 'Bias', 'Predicting', 'College', 'Academic', 'Performance', 'New', 'Approach', 'Item', 'Response', 'Theory']",possibility differential prediction college academic performance man woman present issue concern specialist general public prior study single equation predict grade point average GPA preadmission measure generally lead systematic underprediction women college grade follow study Item Response Theory IRT develop form adjust cumulative GPA irt‐base GPA significant underprediction woman occur version cumulative GPA outcome measure contrast irt‐base GPA indicate significant underprediction man woman single regression equation work predict men women irt‐base GPA addition irt‐base GPA substantially predictable preadmission measure cumulative GPA Copyright © 1991 Wiley Blackwell right reserve,Gender Bias Predicting College Academic Performance New Approach Item Response Theory,0.03335338420088761,0.03392474063683079,0.8658846017442655,0.033113936448533145,0.03372333696948288,0.0,0.0051490042734415875,0.007538117723900534,0.005527831854939711,0.05907803623757893
Clauser B.; Mazor K.M.; Hambleton R.K.,The Effects of Score Group Width on the Mantel‐Haenszel Procedure,1994,31,"Previous research examining the effects of reducing the number of score groups used in the matching criterion of the Mantel‐Haenszel procedure, when screening for DIF, has produced ambiguous results. The goal of this study was to resolve the ambiguity by examining the problem with a simulated data set. The main results from this study call into question the preliminary recommendations of several other researchers that four or more score groups are sufficient and produce stable results. Although considerable stability and very little Type I error was noted with equal ability distribution comparisons, with unequal ability distributions, the Type I error rate was substantially inflated. These results argue against the appropriateness of implementing the procedure by collapsing score groups. The current data suggest that more than modest reductions in the number of score groups cannot be recommended when the ability distributions of the reference and focal groups differ Copyright © 1994, Wiley Blackwell. All rights reserved",The Effects of Score Group Width on the Mantel‐Haenszel Procedure,"Previous research examining the effects of reducing the number of score groups used in the matching criterion of the Mantel‐Haenszel procedure, when screening for DIF, has produced ambiguous results. The goal of this study was to resolve the ambiguity by examining the problem with a simulated data set. The main results from this study call into question the preliminary recommendations of several other researchers that four or more score groups are sufficient and produce stable results. Although considerable stability and very little Type I error was noted with equal ability distribution comparisons, with unequal ability distributions, the Type I error rate was substantially inflated. These results argue against the appropriateness of implementing the procedure by collapsing score groups. The current data suggest that more than modest reductions in the number of score groups cannot be recommended when the ability distributions of the reference and focal groups differ Copyright © 1994, Wiley Blackwell. All rights reserved","['previous', 'research', 'examine', 'effect', 'reduce', 'number', 'score', 'group', 'matching', 'criterion', 'Mantel‐Haenszel', 'procedure', 'screen', 'DIF', 'produce', 'ambiguous', 'result', 'goal', 'study', 'resolve', 'ambiguity', 'examine', 'problem', 'simulated', 'datum', 'set', 'main', 'result', 'study', 'question', 'preliminary', 'recommendation', 'researcher', 'score', 'group', 'sufficient', 'produce', 'stable', 'result', 'considerable', 'stability', 'little', 'Type', 'I', 'error', 'note', 'equal', 'ability', 'distribution', 'comparison', 'unequal', 'ability', 'distribution', 'Type', 'I', 'error', 'rate', 'substantially', 'inflate', 'result', 'argue', 'appropriateness', 'implement', 'procedure', 'collapse', 'score', 'group', 'current', 'datum', 'suggest', 'modest', 'reduction', 'number', 'score', 'group', 'recommend', 'ability', 'distribution', 'reference', 'focal', 'group', 'differ', 'copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Effects', 'Score', 'Group', 'Width', 'Mantel‐Haenszel', 'Procedure']",previous research examine effect reduce number score group matching criterion Mantel‐Haenszel procedure screen DIF produce ambiguous result goal study resolve ambiguity examine problem simulated datum set main result study question preliminary recommendation researcher score group sufficient produce stable result considerable stability little Type I error note equal ability distribution comparison unequal ability distribution Type I error rate substantially inflate result argue appropriateness implement procedure collapse score group current datum suggest modest reduction number score group recommend ability distribution reference focal group differ copyright © 1994 Wiley Blackwell right reserve,Effects Score Group Width Mantel‐Haenszel Procedure,0.024162839285201628,0.024365394105513683,0.024487854717437266,0.19074690765423824,0.7362370042376091,0.051887233266339046,0.0,0.04870553424023403,0.070698149990365,0.0057077112259773894
Colton D.A.; Kane M.T.; Kingsbury C.; Estes C.A.,A Strategy for Examining the Validity of Job Analysis Data,1991,28,"A strategy for examining the validity of job analysis inventory data is introduced. This strategy involves the testing of hypotheses regarding expected consistencies within the data, and between the data and independent sources of information. Although the purpose of the job analysis is to obtain an empirical description of work patterns, some relationships among the data can be predicted with a high degree of confidence, and these predictions can then be used to test the validity of the job analysis data. An example investigates the validity of data collected as part of a job analysis for nurses. Copyright © 1991, Wiley Blackwell. All rights reserved",A Strategy for Examining the Validity of Job Analysis Data,"A strategy for examining the validity of job analysis inventory data is introduced. This strategy involves the testing of hypotheses regarding expected consistencies within the data, and between the data and independent sources of information. Although the purpose of the job analysis is to obtain an empirical description of work patterns, some relationships among the data can be predicted with a high degree of confidence, and these predictions can then be used to test the validity of the job analysis data. An example investigates the validity of data collected as part of a job analysis for nurses. Copyright © 1991, Wiley Blackwell. All rights reserved","['strategy', 'examine', 'validity', 'job', 'analysis', 'inventory', 'datum', 'introduce', 'strategy', 'involve', 'testing', 'hypothesis', 'regard', 'expect', 'consistency', 'datum', 'datum', 'independent', 'source', 'information', 'purpose', 'job', 'analysis', 'obtain', 'empirical', 'description', 'work', 'pattern', 'relationship', 'datum', 'predict', 'high', 'degree', 'confidence', 'prediction', 'test', 'validity', 'job', 'analysis', 'datum', 'example', 'investigate', 'validity', 'datum', 'collect', 'job', 'analysis', 'nurse', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['strategy', 'examine', 'Validity', 'Job', 'Analysis', 'Data']",strategy examine validity job analysis inventory datum introduce strategy involve testing hypothesis regard expect consistency datum datum independent source information purpose job analysis obtain empirical description work pattern relationship datum predict high degree confidence prediction test validity job analysis datum example investigate validity datum collect job analysis nurse copyright © 1991 Wiley Blackwell right reserve,strategy examine Validity Job Analysis Data,0.03409919118760069,0.03406668510372889,0.8636907145450066,0.03376293909163121,0.03438047007203243,0.0011809393915204137,0.04410286056291222,0.00669420233304415,0.0,0.03823255476674486
Lukhele R.; Thissen D.; Wainer H.,"On the Relative Value of Multiple‐Choice, Constructed Response, and Examinee‐Selected Items on Two Achievement Tests",1994,31,"Using analyses based on fitting item response models to data from the College Board's Advanced Placement exams in chemistry and United States history, we found that the constructed response portion of the tests yielded little information over and above that provided by the multiple‐choice sections. These tests also allow examinees to select subsets of the constructed response items; we found that scoring on the basis of the selections themselves provided almost as much information as did scoring on the basis of the answers Copyright © 1994, Wiley Blackwell. All rights reserved","On the Relative Value of Multiple‐Choice, Constructed Response, and Examinee‐Selected Items on Two Achievement Tests","Using analyses based on fitting item response models to data from the College Board's Advanced Placement exams in chemistry and United States history, we found that the constructed response portion of the tests yielded little information over and above that provided by the multiple‐choice sections. These tests also allow examinees to select subsets of the constructed response items; we found that scoring on the basis of the selections themselves provided almost as much information as did scoring on the basis of the answers Copyright © 1994, Wiley Blackwell. All rights reserved","['analysis', 'base', 'fitting', 'item', 'response', 'datum', 'College', 'Boards', 'Advanced', 'Placement', 'exam', 'chemistry', 'United', 'States', 'history', 'find', 'construct', 'response', 'portion', 'test', 'yield', 'little', 'information', 'provide', 'multiple‐choice', 'section', 'test', 'allow', 'examinee', 'select', 'subset', 'construct', 'response', 'item', 'find', 'scoring', 'basis', 'selection', 'provide', 'information', 'scoring', 'basis', 'answer', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Relative', 'Value', 'Multiple‐Choice', 'Constructed', 'Response', 'Examinee‐Selected', 'Items', 'Achievement', 'test']",analysis base fitting item response datum College Boards Advanced Placement exam chemistry United States history find construct response portion test yield little information provide multiple‐choice section test allow examinee select subset construct response item find scoring basis selection provide information scoring basis answer Copyright © 1994 Wiley Blackwell right reserve,Relative Value Multiple‐Choice Constructed Response Examinee‐Selected Items Achievement test,0.029944888893100548,0.030545625201472113,0.879608330160387,0.029642475561065335,0.03025868018397507,0.0,0.12258346454822525,0.0,0.0,0.0
Stevenson J.C.; Evans G.T.,Conceptualization and Measurement of Cognitive Holding Power,1994,31,"The concept of cognitive holding power is synthesized from theories of settings and of cognitive structures and is conceptualized as a characteristic of a learning setting that presses students into different kinds of cognitive activity. Settings which press students into using first‐ or second‐order cognitive procedures are regarded as having first‐ or second‐order cognitive holding power. The development of an instrument to measure these two dimensions of cognitive holding power is outlined. The independence of the dimensions, their reliabilities and validity, and factor structures are examined. Each dimension was found to have high reliability across vocational education and high school settings, and each was correlated as predicted with other classroom variables. The potential contribution of this research to understanding the relationship between different approaches to the teaching of problem solving and the ability to undertake problem‐solving transfer tasks is outlined. Copyright © 1994, Wiley Blackwell. All rights reserved",Conceptualization and Measurement of Cognitive Holding Power,"The concept of cognitive holding power is synthesized from theories of settings and of cognitive structures and is conceptualized as a characteristic of a learning setting that presses students into different kinds of cognitive activity. Settings which press students into using first‐ or second‐order cognitive procedures are regarded as having first‐ or second‐order cognitive holding power. The development of an instrument to measure these two dimensions of cognitive holding power is outlined. The independence of the dimensions, their reliabilities and validity, and factor structures are examined. Each dimension was found to have high reliability across vocational education and high school settings, and each was correlated as predicted with other classroom variables. The potential contribution of this research to understanding the relationship between different approaches to the teaching of problem solving and the ability to undertake problem‐solving transfer tasks is outlined. Copyright © 1994, Wiley Blackwell. All rights reserved","['concept', 'cognitive', 'holding', 'power', 'synthesize', 'theory', 'setting', 'cognitive', 'structure', 'conceptualize', 'characteristic', 'learning', 'set', 'press', 'student', 'different', 'kind', 'cognitive', 'activity', 'Settings', 'press', 'student', 'first‐', 'second‐ord', 'cognitive', 'procedure', 'regard', 'first‐', 'second‐ord', 'cognitive', 'holding', 'power', 'development', 'instrument', 'measure', 'dimension', 'cognitive', 'holding', 'power', 'outline', 'independence', 'dimension', 'reliability', 'validity', 'factor', 'structure', 'examine', 'dimension', 'find', 'high', 'reliability', 'vocational', 'high', 'school', 'setting', 'correlate', 'predict', 'classroom', 'variable', 'potential', 'contribution', 'research', 'understand', 'relationship', 'different', 'approach', 'teaching', 'problem', 'solve', 'ability', 'undertake', 'problem‐solve', 'transfer', 'task', 'outline', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['conceptualization', 'Cognitive', 'Holding', 'Power']",concept cognitive holding power synthesize theory setting cognitive structure conceptualize characteristic learning set press student different kind cognitive activity Settings press student first‐ second‐ord cognitive procedure regard first‐ second‐ord cognitive holding power development instrument measure dimension cognitive holding power outline independence dimension reliability validity factor structure examine dimension find high reliability vocational high school setting correlate predict classroom variable potential contribution research understand relationship different approach teaching problem solve ability undertake problem‐solve transfer task outline Copyright © 1994 Wiley Blackwell right reserve,conceptualization Cognitive Holding Power,0.027294748371166588,0.027443587525744717,0.8902926209941191,0.02717627375738338,0.02779276935158613,0.0,0.03997168341185443,0.0,0.0,0.05263479337346611
Wainer H.; Lewis C.,Toward a Psychometrics for Testlets,1990,27,"In 1987 the testlet was introduced as one way of dealing with a variety of problems that might occur with the algorithmic construction of tests. In the short time since its introduction, its range of plausible usages has been broadened considerably through the work of other researchers. In this paper we examine three different applications of the testlet concept and describe the psychometric models which seem most suitable for each application. In each case, the testlet concept gracefully solves a problem that would have been awkward with other, more traditional approaches. Copyright © 1990, Wiley Blackwell. All rights reserved",,"In 1987 the testlet was introduced as one way of dealing with a variety of problems that might occur with the algorithmic construction of tests. In the short time since its introduction, its range of plausible usages has been broadened considerably through the work of other researchers. In this paper we examine three different applications of the testlet concept and describe the psychometric models which seem most suitable for each application. In each case, the testlet concept gracefully solves a problem that would have been awkward with other, more traditional approaches. Copyright © 1990, Wiley Blackwell. All rights reserved","['1987', 'testlet', 'introduce', 'way', 'deal', 'variety', 'problem', 'occur', 'algorithmic', 'construction', 'test', 'short', 'time', 'introduction', 'range', 'plausible', 'usage', 'broaden', 'considerably', 'work', 'researcher', 'paper', 'examine', 'different', 'application', 'testlet', 'concept', 'describe', 'psychometric', 'suitable', 'application', 'case', 'testlet', 'concept', 'gracefully', 'solve', 'problem', 'awkward', 'traditional', 'approach', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']",,1987 testlet introduce way deal variety problem occur algorithmic construction test short time introduction range plausible usage broaden considerably work researcher paper examine different application testlet concept describe psychometric suitable application case testlet concept gracefully solve problem awkward traditional approach copyright © 1990 Wiley Blackwell right reserve,,0.030002343128795263,0.02992150315908417,0.030114427405136604,0.02973951435993387,0.8802222119470501,0.0,0.05562892544695669,0.0,0.0005468468304614961,0.012261005645036738
Prinsell C.P.; Ramsey P.H.; Ramsey P.P.,"Score Gains, Attitudes, and Behavior Changes Due to Answer‐Changing Instruction",1994,31,"Six undergraduate and three graduate classes were given multiple‐choice tests with subsequent evaluation of answer changes. The 300 students were tested twice, once before and once after instruction on answer changing. After each test, students were asked to complete two forms. The forms evaluated attitude toward answer changing, reasons for changing, and confidence in final answers. Students showed a significant increase in favorability toward answer changing after instruction. No significant change was found in number of answers changed. Psychology students were found to change significantly more items than were business students. Mean gain score did not change significantly after instruction. It was concluded that although instruction does lead to a change in attitude in answer changing, the number of changes and overall gain due to answer changing do not change. It was also determined that students continue to make significant gains even when their confidence in the final answer is less than 50 on a 100‐point scale. Copyright © 1994, Wiley Blackwell. All rights reserved","Score Gains, Attitudes, and Behavior Changes Due to Answer‐Changing Instruction","Six undergraduate and three graduate classes were given multiple‐choice tests with subsequent evaluation of answer changes. The 300 students were tested twice, once before and once after instruction on answer changing. After each test, students were asked to complete two forms. The forms evaluated attitude toward answer changing, reasons for changing, and confidence in final answers. Students showed a significant increase in favorability toward answer changing after instruction. No significant change was found in number of answers changed. Psychology students were found to change significantly more items than were business students. Mean gain score did not change significantly after instruction. It was concluded that although instruction does lead to a change in attitude in answer changing, the number of changes and overall gain due to answer changing do not change. It was also determined that students continue to make significant gains even when their confidence in the final answer is less than 50 on a 100‐point scale. Copyright © 1994, Wiley Blackwell. All rights reserved","['undergraduate', 'graduate', 'class', 'multiple‐choice', 'test', 'subsequent', 'evaluation', 'answer', 'change', '300', 'student', 'test', 'twice', 'instruction', 'answer', 'change', 'test', 'student', 'ask', 'complete', 'form', 'form', 'evaluate', 'attitude', 'answer', 'change', 'reason', 'change', 'confidence', 'final', 'answer', 'student', 'significant', 'increase', 'favorability', 'answer', 'change', 'instruction', 'significant', 'change', 'find', 'number', 'answer', 'change', 'psychology', 'student', 'find', 'change', 'significantly', 'item', 'business', 'student', 'mean', 'gain', 'score', 'change', 'significantly', 'instruction', 'conclude', 'instruction', 'lead', 'change', 'attitude', 'answer', 'change', 'number', 'change', 'overall', 'gain', 'answer', 'change', 'change', 'determined', 'student', 'continue', 'significant', 'gain', 'confidence', 'final', 'answer', '50', '100‐point', 'scale', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['score', 'Gains', 'Attitudes', 'Behavior', 'Changes', 'Answer‐Changing', 'Instruction']",undergraduate graduate class multiple‐choice test subsequent evaluation answer change 300 student test twice instruction answer change test student ask complete form form evaluate attitude answer change reason change confidence final answer student significant increase favorability answer change instruction significant change find number answer change psychology student find change significantly item business student mean gain score change significantly instruction conclude instruction lead change attitude answer change number change overall gain answer change change determined student continue significant gain confidence final answer 50 100‐point scale Copyright © 1994 Wiley Blackwell right reserve,score Gains Attitudes Behavior Changes Answer‐Changing Instruction,0.03645625858818566,0.03760270744438879,0.03635053838436054,0.03603765801274931,0.8535528375703156,0.0,0.08176601020410638,0.0,0.0,0.016712258674821484
Dorans N.J.; Schmitt A.P.; Bleistein C.A.,The Standardization Approach to Assessing Comprehensive Differential Item Functioning,1992,29,"The standardization approach to comprehensive differential item functioning (Cdif) is described and contrasted with the log‐linear approach to differential distractor functioning explicated by Green, Crone, and Folk (1989) and with the IRT‐based approach to differential alternative functioning explicated by Thissen, Steinberg, and Wainer (1992). This descriptive approach is used routinely as an adjunct to Mantel‐Haenszel differential item functioning (DIP) detection (Dorans & Holland, 1992) in many operational testing programs at the Educational Testing Service. Data from an edition of the SAT are used to illustrate how the standardization approach to Cdif could be used to uncover differential speededness. Speculations about the sources of differential speededness for Black examinees and Hispanic examinees are offered, and some implications of the existence of differential speededness for DIP detection are mentioned. Copyright © 1992, Wiley Blackwell. All rights reserved",The Standardization Approach to Assessing Comprehensive Differential Item Functioning,"The standardization approach to comprehensive differential item functioning (Cdif) is described and contrasted with the log‐linear approach to differential distractor functioning explicated by Green, Crone, and Folk (1989) and with the IRT‐based approach to differential alternative functioning explicated by Thissen, Steinberg, and Wainer (1992). This descriptive approach is used routinely as an adjunct to Mantel‐Haenszel differential item functioning (DIP) detection (Dorans & Holland, 1992) in many operational testing programs at the Educational Testing Service. Data from an edition of the SAT are used to illustrate how the standardization approach to Cdif could be used to uncover differential speededness. Speculations about the sources of differential speededness for Black examinees and Hispanic examinees are offered, and some implications of the existence of differential speededness for DIP detection are mentioned. Copyright © 1992, Wiley Blackwell. All rights reserved","['standardization', 'approach', 'comprehensive', 'differential', 'item', 'function', 'Cdif', 'describe', 'contrast', 'log‐linear', 'approach', 'differential', 'distractor', 'function', 'explicate', 'Green', 'Crone', 'Folk', '1989', 'irt‐base', 'approach', 'differential', 'alternative', 'functioning', 'explicate', 'Thissen', 'Steinberg', 'Wainer', '1992', 'descriptive', 'approach', 'routinely', 'adjunct', 'Mantel‐Haenszel', 'differential', 'item', 'function', 'DIP', 'detection', 'Dorans', 'Holland', '1992', 'operational', 'testing', 'program', 'Educational', 'Testing', 'Service', 'Data', 'edition', 'SAT', 'illustrate', 'standardization', 'approach', 'Cdif', 'uncover', 'differential', 'speededness', 'speculation', 'source', 'differential', 'speededness', 'black', 'examinee', 'hispanic', 'examinee', 'offer', 'implication', 'existence', 'differential', 'speededness', 'DIP', 'detection', 'mention', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Standardization', 'approach', 'assess', 'Comprehensive', 'Differential', 'Item', 'Functioning']",standardization approach comprehensive differential item function Cdif describe contrast log‐linear approach differential distractor function explicate Green Crone Folk 1989 irt‐base approach differential alternative functioning explicate Thissen Steinberg Wainer 1992 descriptive approach routinely adjunct Mantel‐Haenszel differential item function DIP detection Dorans Holland 1992 operational testing program Educational Testing Service Data edition SAT illustrate standardization approach Cdif uncover differential speededness speculation source differential speededness black examinee hispanic examinee offer implication existence differential speededness DIP detection mention Copyright © 1992 Wiley Blackwell right reserve,Standardization approach assess Comprehensive Differential Item Functioning,0.02787748227444765,0.028016958982389973,0.8877379043890294,0.027790955193904007,0.02857669916022899,0.0,0.01735077797920465,0.06909754379212431,0.0,0.013117581111917012
Plake B.S.; Impara J.C.; Potenza M.T.,Content Specificity of Expert Judgments in a Standard‐Setting Study,1994,31,"This study investigated the comparability of Angoff‐based item ratings on a general education test battery made by judges from within‐content specialties and across content domains. Judges were from English, mathematics, science, and social studies specialties in teacher education programs in a midwestem state. Cutscores established from the judges’ratings of out‐of‐content items differed little from the cutscores set using the ratings made by the content specialists. Further, out‐of‐content ratings by judges were not more influenced by performance data than were the ratings provided by judges rating items within their content specialty. The degree to ‐which these results generalize to other content specialties needs to be investigated. Copyright © 1994, Wiley Blackwell. All rights reserved",Content Specificity of Expert Judgments in a Standard‐Setting Study,"This study investigated the comparability of Angoff‐based item ratings on a general education test battery made by judges from within‐content specialties and across content domains. Judges were from English, mathematics, science, and social studies specialties in teacher education programs in a midwestem state. Cutscores established from the judges’ratings of out‐of‐content items differed little from the cutscores set using the ratings made by the content specialists. Further, out‐of‐content ratings by judges were not more influenced by performance data than were the ratings provided by judges rating items within their content specialty. The degree to ‐which these results generalize to other content specialties needs to be investigated. Copyright © 1994, Wiley Blackwell. All rights reserved","['study', 'investigate', 'comparability', 'angoff‐based', 'item', 'rating', 'general', 'test', 'battery', 'judge', 'within‐content', 'specialty', 'content', 'domain', 'Judges', 'English', 'mathematic', 'science', 'social', 'study', 'specialty', 'teacher', 'program', 'midwestem', 'state', 'Cutscores', 'establish', 'judges’rating', 'out‐of‐content', 'item', 'differ', 'little', 'cutscore', 'set', 'rating', 'content', 'specialist', 'out‐of‐content', 'rating', 'judge', 'influence', 'performance', 'datum', 'rating', 'provide', 'judge', 'rating', 'item', 'content', 'specialty', 'degree', '‐which', 'result', 'generalize', 'content', 'specialty', 'need', 'investigate', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Content', 'Specificity', 'Expert', 'Judgments', 'Standard‐Setting', 'study']",study investigate comparability angoff‐based item rating general test battery judge within‐content specialty content domain Judges English mathematic science social study specialty teacher program midwestem state Cutscores establish judges’rating out‐of‐content item differ little cutscore set rating content specialist out‐of‐content rating judge influence performance datum rating provide judge rating item content specialty degree ‐which result generalize content specialty need investigate Copyright © 1994 Wiley Blackwell right reserve,Content Specificity Expert Judgments Standard‐Setting study,0.03294473725078943,0.03302312419456924,0.03300118220374216,0.0326355537000935,0.8683954026508056,0.015125934887343638,0.019684614406564868,0.0,0.0,0.046785598220419326
Cohen A.S.; Kim S.‐H.; Subkoviak M.J.,Influence of Prior Distributions on Detection of DIF,1991,28,"Detection of differential item functioning (DIF) on items intentionally constructed to favor one group over another was investigated on item parameter estimates obtained from two item response theory‐based computer programs, LOGIST and BILOG. Signed‐ and unsigned‐area measures based on joint maximum likelihood estimation, marginal maximum likelihood estimation, and two marginal maximum a posteriori estimation procedures were compared with each other to determine whether detection of DIF could be improved using prior distributions. Results indicated that item parameter estimates obtained using either prior condition were less deviant than when priors were not used. Differences in detection of DIF appeared to be related to item parameter estimation condition and to some extent to sample size. Copyright © 1991, Wiley Blackwell. All rights reserved",Influence of Prior Distributions on Detection of DIF,"Detection of differential item functioning (DIF) on items intentionally constructed to favor one group over another was investigated on item parameter estimates obtained from two item response theory‐based computer programs, LOGIST and BILOG. Signed‐ and unsigned‐area measures based on joint maximum likelihood estimation, marginal maximum likelihood estimation, and two marginal maximum a posteriori estimation procedures were compared with each other to determine whether detection of DIF could be improved using prior distributions. Results indicated that item parameter estimates obtained using either prior condition were less deviant than when priors were not used. Differences in detection of DIF appeared to be related to item parameter estimation condition and to some extent to sample size. Copyright © 1991, Wiley Blackwell. All rights reserved","['detection', 'differential', 'item', 'function', 'DIF', 'item', 'intentionally', 'construct', 'favor', 'group', 'investigate', 'item', 'parameter', 'estimate', 'obtain', 'item', 'response', 'theory‐base', 'computer', 'program', 'LOGIST', 'BILOG', 'Signed‐', 'unsigned‐area', 'measure', 'base', 'joint', 'maximum', 'likelihood', 'estimation', 'marginal', 'maximum', 'likelihood', 'estimation', 'marginal', 'maximum', 'posteriori', 'estimation', 'procedure', 'compare', 'determine', 'detection', 'DIF', 'improve', 'prior', 'distribution', 'result', 'indicate', 'item', 'parameter', 'estimate', 'obtain', 'prior', 'condition', 'deviant', 'prior', 'difference', 'detection', 'DIF', 'appear', 'relate', 'item', 'parameter', 'estimation', 'condition', 'extent', 'sample', 'size', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['influence', 'Prior', 'Distributions', 'Detection', 'DIF']",detection differential item function DIF item intentionally construct favor group investigate item parameter estimate obtain item response theory‐base computer program LOGIST BILOG Signed‐ unsigned‐area measure base joint maximum likelihood estimation marginal maximum likelihood estimation marginal maximum posteriori estimation procedure compare determine detection DIF improve prior distribution result indicate item parameter estimate obtain prior condition deviant prior difference detection DIF appear relate item parameter estimation condition extent sample size Copyright © 1991 Wiley Blackwell right reserve,influence Prior Distributions Detection DIF,0.028830825815324455,0.028792011388989458,0.028953920289010903,0.02855545178681638,0.8848677907198588,0.06758369802164108,0.0,0.09813492363084211,0.0,0.0
Spray J.A.; Welch C.J.,Estimation of Classification Consistency When the Probability of a Correct Response Varies,1990,27,"The purpose of this study was to examine the effect that large, within‐examinee item difficulty variability had on estimates of the proportion of consistent classification of examinees into mastery categories over two test administrations. The classification consistency estimate was based on a single test administration from an estimation procedure suggested by Subkoviak (1976). Analyses of simulated data revealed that the use of a single estimate for an examinee's probability of a correct response, even when that probability varied greatly within a test for an examinee, did not affect the estimation of the proportion of consistent classifications. Copyright © 1990, Wiley Blackwell. All rights reserved",Estimation of Classification Consistency When the Probability of a Correct Response Varies,"The purpose of this study was to examine the effect that large, within‐examinee item difficulty variability had on estimates of the proportion of consistent classification of examinees into mastery categories over two test administrations. The classification consistency estimate was based on a single test administration from an estimation procedure suggested by Subkoviak (1976). Analyses of simulated data revealed that the use of a single estimate for an examinee's probability of a correct response, even when that probability varied greatly within a test for an examinee, did not affect the estimation of the proportion of consistent classifications. Copyright © 1990, Wiley Blackwell. All rights reserved","['purpose', 'study', 'examine', 'effect', 'large', 'within‐examinee', 'item', 'difficulty', 'variability', 'estimate', 'proportion', 'consistent', 'classification', 'examinee', 'mastery', 'category', 'test', 'administration', 'classification', 'consistency', 'estimate', 'base', 'single', 'test', 'administration', 'estimation', 'procedure', 'suggest', 'Subkoviak', '1976', 'Analyses', 'simulated', 'datum', 'reveal', 'single', 'estimate', 'examinees', 'probability', 'correct', 'response', 'probability', 'vary', 'greatly', 'test', 'examinee', 'affect', 'estimation', 'proportion', 'consistent', 'classification', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['estimation', 'Classification', 'Consistency', 'Probability', 'Correct', 'Response', 'Varies']",purpose study examine effect large within‐examinee item difficulty variability estimate proportion consistent classification examinee mastery category test administration classification consistency estimate base single test administration estimation procedure suggest Subkoviak 1976 Analyses simulated datum reveal single estimate examinees probability correct response probability vary greatly test examinee affect estimation proportion consistent classification Copyright © 1990 Wiley Blackwell right reserve,estimation Classification Consistency Probability Correct Response Varies,0.03076976829145541,0.877272110090961,0.03067508854058973,0.03024086441053825,0.03104216866645576,0.06225475402593675,0.03743864556590799,0.0,0.009040332064356593,0.0
Wainer H.; Lewis C.; Kaplan B.; Braswell J.,Building Algebra Testlets: A Comparison of Hierarchical and Linear Structures,1991,28,"Earlier (Wainer & Lewis, 1990), we reported the initial development of a testlet‐based algebra test. In this account, we provide the details of this excursion into the use of testlets. A pretest of two 15–item algebra tests was carried out in which examinees' performance on a 4‐item subset of each test (a 4–item testlet) was used to predict performance on the entire test. Two models for constructing the testlets were considered: hierarchical (adaptive) and linear (fixed format). These models are compared with each other. It was found on cross–validation that, although an adaptive testlet is superior to a fixed format testlet, this superiority is modest, whereas the potential cost of that superiority is considerable. It was concluded that in circumstances similar to those we report a fixed format testlet that uses the best items in a pool can do almost as well as the optimal adaptive testlet of equal length from that same pool. Copyright © 1991, Wiley Blackwell. All rights reserved",Building Algebra Testlets: A Comparison of Hierarchical and Linear Structures,"Earlier (Wainer & Lewis, 1990), we reported the initial development of a testlet‐based algebra test. In this account, we provide the details of this excursion into the use of testlets. A pretest of two 15–item algebra tests was carried out in which examinees' performance on a 4‐item subset of each test (a 4–item testlet) was used to predict performance on the entire test. Two models for constructing the testlets were considered: hierarchical (adaptive) and linear (fixed format). These models are compared with each other. It was found on cross–validation that, although an adaptive testlet is superior to a fixed format testlet, this superiority is modest, whereas the potential cost of that superiority is considerable. It was concluded that in circumstances similar to those we report a fixed format testlet that uses the best items in a pool can do almost as well as the optimal adaptive testlet of equal length from that same pool. Copyright © 1991, Wiley Blackwell. All rights reserved","['early', 'Wainer', 'Lewis', '1990', 'report', 'initial', 'development', 'testlet‐based', 'algebra', 'test', 'account', 'provide', 'detail', 'excursion', 'testlet', 'pretest', '15', '–', 'item', 'algebra', 'test', 'carry', 'examine', 'performance', '4‐item', 'subset', 'test', '4', '–', 'item', 'testlet', 'predict', 'performance', 'entire', 'test', 'construct', 'testlet', 'consider', 'hierarchical', 'adaptive', 'linear', 'fix', 'format', 'compare', 'find', 'cross', '–', 'validation', 'adaptive', 'testlet', 'superior', 'fix', 'format', 'testlet', 'superiority', 'modest', 'potential', 'cost', 'superiority', 'considerable', 'conclude', 'circumstance', 'similar', 'report', 'fix', 'format', 'testlet', 'good', 'item', 'pool', 'optimal', 'adaptive', 'testlet', 'equal', 'length', 'pool', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['build', 'Algebra', 'testlet', 'Comparison', 'Hierarchical', 'Linear', 'Structures']",early Wainer Lewis 1990 report initial development testlet‐based algebra test account provide detail excursion testlet pretest 15 – item algebra test carry examine performance 4‐item subset test 4 – item testlet predict performance entire test construct testlet consider hierarchical adaptive linear fix format compare find cross – validation adaptive testlet superior fix format testlet superiority modest potential cost superiority considerable conclude circumstance similar report fix format testlet good item pool optimal adaptive testlet equal length pool Copyright © 1991 Wiley Blackwell right reserve,build Algebra testlet Comparison Hierarchical Linear Structures,0.029452176654040464,0.029589808352425043,0.8811376184358721,0.02937403195527006,0.030446364602392313,0.0,0.09834041851953979,0.0,0.0,0.0
Mullis I.V.S.,"Developing the NAEP Content‐Area Frameworks and Innovative Assessment Methods in the 1992 Assessments of Mathematics, Reading, and Writing",1992,29,"This article provides an overview of the consensus processes for the development of the frameworks underlying the NAEP assessments, with emphasis on those for the 1990 and 1992 assessments of mathematics, the 1992 assessment of reading, and the 1994 assessment of science. In addition, innovative assessment techniques included in the 1992 assessments of mathematics, reading, and writing are described, including use of mathematics tools, oral interviews, and portfolio assessment. Copyright © 1992, Wiley Blackwell. All rights reserved","Developing the NAEP Content‐Area Frameworks and Innovative Assessment Methods in the 1992 Assessments of Mathematics, Reading, and Writing","This article provides an overview of the consensus processes for the development of the frameworks underlying the NAEP assessments, with emphasis on those for the 1990 and 1992 assessments of mathematics, the 1992 assessment of reading, and the 1994 assessment of science. In addition, innovative assessment techniques included in the 1992 assessments of mathematics, reading, and writing are described, including use of mathematics tools, oral interviews, and portfolio assessment. Copyright © 1992, Wiley Blackwell. All rights reserved","['article', 'provide', 'overview', 'consensus', 'process', 'development', 'framework', 'underlie', 'naep', 'assessment', 'emphasis', '1990', '1992', 'assessment', 'mathematic', '1992', 'assessment', 'reading', '1994', 'assessment', 'science', 'addition', 'innovative', 'assessment', 'technique', 'include', '1992', 'assessment', 'mathematic', 'reading', 'writing', 'describe', 'include', 'mathematics', 'tool', 'oral', 'interview', 'portfolio', 'assessment', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['develop', 'NAEP', 'content‐area', 'Frameworks', 'Innovative', 'Assessment', 'Methods', '1992', 'assessment', 'Mathematics', 'Reading', 'write']",article provide overview consensus process development framework underlie naep assessment emphasis 1990 1992 assessment mathematic 1992 assessment reading 1994 assessment science addition innovative assessment technique include 1992 assessment mathematic reading writing describe include mathematics tool oral interview portfolio assessment Copyright © 1992 Wiley Blackwell right reserve,develop NAEP content‐area Frameworks Innovative Assessment Methods 1992 assessment Mathematics Reading write,0.03496685797750916,0.8594831202881831,0.035153511326503836,0.034501712617563116,0.03589479779024082,0.0,0.0,0.0,0.0,0.13572550895449711
Kim S.‐H.; Cohen A.S.,Effects of Linking Methods on Detection of DIF,1992,29,"Studies of differential item functioning under item response theory require that item parameter estimates be placed on the same metric before comparisons can be made. The present study compared the effects of three methods for linking metrics: a weighted mean and sigma method (WMS); the test characteristic curve method (TCC); and the minimum chi‐square method (MCS), on detection of differential item functioning. Both iterative and noniterative linking procedures were compared for each method. Results indicated that detection of differentially functioning items following linking via the test characteristic curve method gave the most accurate results when the sample size was small. When the sample size was large, results for the three linking methods were essentially the same. Iterative linking provided an improvement in detection of differentially functioning items over noniterative linking particularly with the .05 alpha level. The weighted mean and sigma method showed greater improvement with iterative linking than either the test characteristic curve or minimum chi‐square method. Copyright © 1992, Wiley Blackwell. All rights reserved",,"Studies of differential item functioning under item response theory require that item parameter estimates be placed on the same metric before comparisons can be made. The present study compared the effects of three methods for linking metrics: a weighted mean and sigma method (WMS); the test characteristic curve method (TCC); and the minimum chi‐square method (MCS), on detection of differential item functioning. Both iterative and noniterative linking procedures were compared for each method. Results indicated that detection of differentially functioning items following linking via the test characteristic curve method gave the most accurate results when the sample size was small. When the sample size was large, results for the three linking methods were essentially the same. Iterative linking provided an improvement in detection of differentially functioning items over noniterative linking particularly with the .05 alpha level. The weighted mean and sigma method showed greater improvement with iterative linking than either the test characteristic curve or minimum chi‐square method. Copyright © 1992, Wiley Blackwell. All rights reserved","['study', 'differential', 'item', 'function', 'item', 'response', 'theory', 'require', 'item', 'parameter', 'estimate', 'place', 'metric', 'comparison', 'present', 'study', 'compare', 'effect', 'method', 'link', 'metric', 'weight', 'mean', 'sigma', 'method', 'WMS', 'test', 'characteristic', 'curve', 'method', 'TCC', 'minimum', 'chi‐square', 'method', 'mcs', 'detection', 'differential', 'item', 'function', 'iterative', 'noniterative', 'link', 'procedure', 'compare', 'method', 'result', 'indicate', 'detection', 'differentially', 'function', 'item', 'follow', 'link', 'test', 'characteristic', 'curve', 'method', 'accurate', 'result', 'sample', 'size', 'small', 'sample', 'size', 'large', 'result', 'link', 'method', 'essentially', 'Iterative', 'linking', 'provide', 'improvement', 'detection', 'differentially', 'function', 'item', 'noniterative', 'link', 'particularly', '05', 'alpha', 'level', 'weight', 'mean', 'sigma', 'method', 'great', 'improvement', 'iterative', 'linking', 'test', 'characteristic', 'curve', 'minimum', 'chi‐square', 'method', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']",,study differential item function item response theory require item parameter estimate place metric comparison present study compare effect method link metric weight mean sigma method WMS test characteristic curve method TCC minimum chi‐square method mcs detection differential item function iterative noniterative link procedure compare method result indicate detection differentially function item follow link test characteristic curve method accurate result sample size small sample size large result link method essentially Iterative linking provide improvement detection differentially function item noniterative link particularly 05 alpha level weight mean sigma method great improvement iterative linking test characteristic curve minimum chi‐square method Copyright © 1992 Wiley Blackwell right reserve,,0.02913122651400142,0.029035516778153823,0.029092833002242004,0.028814036148384403,0.8839263875572184,0.04863887355493927,0.016673287095505526,0.040613353709158466,0.035922325271828565,0.0
Sykes R.C.; Fitzpatrick A.R.,The Stability of IRT b Values,1992,29,"This study investigated possible explanations for an observed change in Rasch item parameters (b values) obtained from consecutive administrations of a professional licensure examination. Considered in this investigation were variables related to item position, item type, item content, and elapsed time between administrations of the item. An analysis of covariance methodology was used to assess the relations between these variables and change in item b values, with the elapsed time index serving to control for differences that could be attributed to average or pool changes in b values over time. A series of analysis of covariance models were fitted to the data in an attempt to identify item characteristics that were significantly related to the change in b values after the time elapsed between item administrations had been controlled. The findings indicated that the change in item b values was not related either to item position or to item type. A small, positive relationship between this change and elapsed time indicated that the pool b values were increasing over time. A test of simple effects suggested the presence of greater change for one of the content categories analyzed. These findings are interpreted, and suggestions for future research are provided. Copyright © 1992, Wiley Blackwell. All rights reserved",,"This study investigated possible explanations for an observed change in Rasch item parameters (b values) obtained from consecutive administrations of a professional licensure examination. Considered in this investigation were variables related to item position, item type, item content, and elapsed time between administrations of the item. An analysis of covariance methodology was used to assess the relations between these variables and change in item b values, with the elapsed time index serving to control for differences that could be attributed to average or pool changes in b values over time. A series of analysis of covariance models were fitted to the data in an attempt to identify item characteristics that were significantly related to the change in b values after the time elapsed between item administrations had been controlled. The findings indicated that the change in item b values was not related either to item position or to item type. A small, positive relationship between this change and elapsed time indicated that the pool b values were increasing over time. A test of simple effects suggested the presence of greater change for one of the content categories analyzed. These findings are interpreted, and suggestions for future research are provided. Copyright © 1992, Wiley Blackwell. All rights reserved","['study', 'investigate', 'possible', 'explanation', 'observed', 'change', 'Rasch', 'item', 'parameter', 'b', 'value', 'obtain', 'consecutive', 'administration', 'professional', 'licensure', 'examination', 'consider', 'investigation', 'variable', 'relate', 'item', 'position', 'item', 'type', 'item', 'content', 'elapse', 'time', 'administration', 'item', 'analysis', 'covariance', 'methodology', 'assess', 'relation', 'variable', 'change', 'item', 'b', 'value', 'elapse', 'time', 'index', 'serve', 'control', 'difference', 'attribute', 'average', 'pool', 'change', 'b', 'value', 'time', 'series', 'analysis', 'covariance', 'fit', 'datum', 'attempt', 'identify', 'item', 'characteristic', 'significantly', 'relate', 'change', 'b', 'value', 'time', 'elapse', 'item', 'administration', 'control', 'finding', 'indicate', 'change', 'item', 'b', 'value', 'relate', 'item', 'position', 'item', 'type', 'small', 'positive', 'relationship', 'change', 'elapse', 'time', 'indicate', 'pool', 'b', 'value', 'increase', 'time', 'test', 'simple', 'effect', 'suggest', 'presence', 'great', 'change', 'content', 'category', 'analyze', 'finding', 'interpret', 'suggestion', 'future', 'research', 'provide', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']",,study investigate possible explanation observed change Rasch item parameter b value obtain consecutive administration professional licensure examination consider investigation variable relate item position item type item content elapse time administration item analysis covariance methodology assess relation variable change item b value elapse time index serve control difference attribute average pool change b value time series analysis covariance fit datum attempt identify item characteristic significantly relate change b value time elapse item administration control finding indicate change item b value relate item position item type small positive relationship change elapse time indicate pool b value increase time test simple effect suggest presence great change content category analyze finding interpret suggestion future research provide Copyright © 1992 Wiley Blackwell right reserve,,0.02825668787028565,0.02847859308925413,0.8865294409373575,0.027967975730043775,0.028767302373059044,0.0,0.11298465750915501,0.012062683182251614,0.0,0.0
Norcini J.J.,Equivalent Pass/Fail Decisions,1990,27,"In competency testing, it is sometimes difficult to properly equate scores of different forms of a test and thereby assure equivalent cutting scores. Under such circumstances, it is possible to set standards separately for each test form and then scale the judgments of the standard setters to achieve equivalent pass/fail decisions. Data from standard setters and examinees for a medical certifying examination were reanalyzed. Cutting score equivalents were derived by applying a linear procedure to the standard‐setting results. These were compared against criteria along with the cutting score equivalents derived from typical examination equating procedures. Results indicated that the cutting score equivalents produced by the experts were closer to the criteria than standards derived from examinee performance, especially when the number of examinees used in equating was small. The root mean square error estimate was about 1 item on a 189‐item test. Copyright © 1990, Wiley Blackwell. All rights reserved",,"In competency testing, it is sometimes difficult to properly equate scores of different forms of a test and thereby assure equivalent cutting scores. Under such circumstances, it is possible to set standards separately for each test form and then scale the judgments of the standard setters to achieve equivalent pass/fail decisions. Data from standard setters and examinees for a medical certifying examination were reanalyzed. Cutting score equivalents were derived by applying a linear procedure to the standard‐setting results. These were compared against criteria along with the cutting score equivalents derived from typical examination equating procedures. Results indicated that the cutting score equivalents produced by the experts were closer to the criteria than standards derived from examinee performance, especially when the number of examinees used in equating was small. The root mean square error estimate was about 1 item on a 189‐item test. Copyright © 1990, Wiley Blackwell. All rights reserved","['competency', 'testing', 'difficult', 'properly', 'equate', 'score', 'different', 'form', 'test', 'assure', 'equivalent', 'cut', 'score', 'circumstance', 'possible', 'set', 'standard', 'separately', 'test', 'form', 'scale', 'judgment', 'standard', 'setter', 'achieve', 'equivalent', 'passfail', 'decision', 'datum', 'standard', 'setter', 'examine', 'medical', 'certifying', 'examination', 'reanalyze', 'cut', 'score', 'equivalent', 'derive', 'apply', 'linear', 'procedure', 'standard‐sette', 'result', 'compare', 'criterion', 'cut', 'score', 'equivalent', 'derive', 'typical', 'examination', 'equate', 'procedure', 'result', 'indicate', 'cut', 'score', 'equivalent', 'produce', 'expert', 'close', 'criterion', 'standard', 'derive', 'examinee', 'performance', 'especially', 'number', 'examinee', 'equate', 'small', 'root', 'mean', 'square', 'error', 'estimate', '1', 'item', '189‐item', 'test', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']",,competency testing difficult properly equate score different form test assure equivalent cut score circumstance possible set standard separately test form scale judgment standard setter achieve equivalent passfail decision datum standard setter examine medical certifying examination reanalyze cut score equivalent derive apply linear procedure standard‐sette result compare criterion cut score equivalent derive typical examination equate procedure result indicate cut score equivalent produce expert close criterion standard derive examinee performance especially number examinee equate small root mean square error estimate 1 item 189‐item test Copyright © 1990 Wiley Blackwell right reserve,,0.028250599487072748,0.028218498501598275,0.8868457349217015,0.02778667679196962,0.02889849029765779,0.04212442922089039,0.0036822206606525252,0.0,0.12506625010721112,0.0
Mislevy R.J.; Beaton A.E.; Kaplan B.; Sheehan K.M.,Estimating Population Characteristics From Sparse Matrix Samples of Item Responses,1992,29,"The multiple‐matrix item sampling designs that provide information about population characteristics most efficiently administer too few responses to students to estimate their proficiencies individually. Marginal estimation procedures, which estimate population characteristics directly from item responses, must be employed to realize the benefits of such a sampling design. Numerical approximations of the appropriate marginal estimation procedures for a broad variety of analyses can be obtained by constructing, from the results of a comprehensive extensive marginal solution, files of plausible values of student proficiencies. This article develops the concepts behind plausible values in a simplified setting, sketches their use in the National Assessment of Educational Progress (NAEP), and illustrates the approach with data from the Scholastic Aptitude Test (SA T). Copyright © 1992, Wiley Blackwell. All rights reserved",Estimating Population Characteristics From Sparse Matrix Samples of Item Responses,"The multiple‐matrix item sampling designs that provide information about population characteristics most efficiently administer too few responses to students to estimate their proficiencies individually. Marginal estimation procedures, which estimate population characteristics directly from item responses, must be employed to realize the benefits of such a sampling design. Numerical approximations of the appropriate marginal estimation procedures for a broad variety of analyses can be obtained by constructing, from the results of a comprehensive extensive marginal solution, files of plausible values of student proficiencies. This article develops the concepts behind plausible values in a simplified setting, sketches their use in the National Assessment of Educational Progress (NAEP), and illustrates the approach with data from the Scholastic Aptitude Test (SA T). Copyright © 1992, Wiley Blackwell. All rights reserved","['multiple‐matrix', 'item', 'sampling', 'design', 'provide', 'information', 'population', 'characteristic', 'efficiently', 'administer', 'response', 'student', 'estimate', 'proficiency', 'individually', 'marginal', 'estimation', 'procedure', 'estimate', 'population', 'characteristic', 'directly', 'item', 'response', 'employ', 'realize', 'benefit', 'sample', 'design', 'numerical', 'approximation', 'appropriate', 'marginal', 'estimation', 'procedure', 'broad', 'variety', 'analysis', 'obtain', 'construct', 'result', 'comprehensive', 'extensive', 'marginal', 'solution', 'file', 'plausible', 'value', 'student', 'proficiency', 'article', 'develop', 'concept', 'plausible', 'value', 'simplified', 'setting', 'sketch', 'National', 'Assessment', 'Educational', 'Progress', 'NAEP', 'illustrate', 'approach', 'datum', 'Scholastic', 'Aptitude', 'Test', 'SA', 'T', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['estimate', 'Population', 'Characteristics', 'Sparse', 'Matrix', 'Samples', 'Item', 'response']",multiple‐matrix item sampling design provide information population characteristic efficiently administer response student estimate proficiency individually marginal estimation procedure estimate population characteristic directly item response employ realize benefit sample design numerical approximation appropriate marginal estimation procedure broad variety analysis obtain construct result comprehensive extensive marginal solution file plausible value student proficiency article develop concept plausible value simplified setting sketch National Assessment Educational Progress NAEP illustrate approach datum Scholastic Aptitude Test SA T Copyright © 1992 Wiley Blackwell right reserve,estimate Population Characteristics Sparse Matrix Samples Item response,0.02502527304896905,0.025114438019000263,0.024960435095557062,0.024800549224281064,0.9000993046121927,0.05112839394321941,0.025450425140493255,0.0,0.0,0.04423996233028156
Bridgeman B.; Rock D.A.,Relationships Among Multiple–Choice and Open–Ended Analytical Questions,1993,30,"Exploratory and confirmatory factor analyses were used to explore relationships among existing item types and three new computer–administered item types for the analytical scale of the Graduate Record Examination General Test. One new item type was an open–ended version of the current multiple–choice analytical reasoning item type. The other new item types had no counterparts on the existing test. The computer tests were administered at four sites to a sample of students who had previously taken the GRE General Test. Scores from the regular GRE and the special computer administration were matched for a sample of 349 students. Factor analyses suggested that the new item types with no counterparts in the existing GRE were reliably assessing unique constructs but the open–ended analytical reasoning items were not measuring anything beyond what is measured by the current multiple–choice version of these items. Copyright © 1993, Wiley Blackwell. All rights reserved",Relationships Among Multiple–Choice and Open–Ended Analytical Questions,"Exploratory and confirmatory factor analyses were used to explore relationships among existing item types and three new computer–administered item types for the analytical scale of the Graduate Record Examination General Test. One new item type was an open–ended version of the current multiple–choice analytical reasoning item type. The other new item types had no counterparts on the existing test. The computer tests were administered at four sites to a sample of students who had previously taken the GRE General Test. Scores from the regular GRE and the special computer administration were matched for a sample of 349 students. Factor analyses suggested that the new item types with no counterparts in the existing GRE were reliably assessing unique constructs but the open–ended analytical reasoning items were not measuring anything beyond what is measured by the current multiple–choice version of these items. Copyright © 1993, Wiley Blackwell. All rights reserved","['exploratory', 'confirmatory', 'factor', 'analysis', 'explore', 'relationship', 'exist', 'item', 'type', 'new', 'computer', '–', 'administer', 'item', 'type', 'analytical', 'scale', 'Graduate', 'Record', 'Examination', 'General', 'Test', 'new', 'item', 'type', 'open', '–', 'end', 'version', 'current', 'multiple', '–', 'choice', 'analytical', 'reasoning', 'item', 'type', 'new', 'item', 'type', 'counterpart', 'exist', 'test', 'computer', 'test', 'administer', 'site', 'sample', 'student', 'previously', 'GRE', 'General', 'Test', 'Scores', 'regular', 'GRE', 'special', 'computer', 'administration', 'match', 'sample', '349', 'student', 'Factor', 'analysis', 'suggest', 'new', 'item', 'type', 'counterpart', 'exist', 'GRE', 'reliably', 'assess', 'unique', 'construct', 'open', '–', 'end', 'analytical', 'reasoning', 'item', 'measure', 'measure', 'current', 'multiple', '–', 'choice', 'version', 'item', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['relationship', 'multiple', '–', 'Choice', 'open', '–', 'end', 'Analytical', 'Questions']",exploratory confirmatory factor analysis explore relationship exist item type new computer – administer item type analytical scale Graduate Record Examination General Test new item type open – end version current multiple – choice analytical reasoning item type new item type counterpart exist test computer test administer site sample student previously GRE General Test Scores regular GRE special computer administration match sample 349 student Factor analysis suggest new item type counterpart exist GRE reliably assess unique construct open – end analytical reasoning item measure measure current multiple – choice version item Copyright © 1993 Wiley Blackwell right reserve,relationship multiple – Choice open – end Analytical Questions,0.028887555239265936,0.029310792292094136,0.029058877034089256,0.6026997980647062,0.3100429773698445,0.0,0.11883084028116724,0.004188630045335367,0.01415988163132173,0.0
Kolen M.J.; Harris D.J.,Comparison of Item Preequating and Random Groups Equating Using IRT and Equipercentile Methods,1990,27,"An item‐preequating design and a random groups design were used to equate forms of the American College Testing (ACT) Assessment Mathematics Test. Equipercentile and 3‐parameter logistic model item‐response theory (IRT) procedures were used for both designs. Both pretest methods produced inadequate equating results, and the IRT item preequating method resulted in more equating error than had no equating been conducted. Although neither of the item preequating methods performed well, the results from the equipercentile preequating method were more consistent with those from the random groups method than were the results from the IRT item pretest method. Item context and position effects were likely responsible, at least in part, for the inadequate results for item preequating. Such effects need to be either controlled or modeled, and the design further researched before the item preequating design can be recommended for operational use. Copyright © 1990, Wiley Blackwell. All rights reserved",Comparison of Item Preequating and Random Groups Equating Using IRT and Equipercentile Methods,"An item‐preequating design and a random groups design were used to equate forms of the American College Testing (ACT) Assessment Mathematics Test. Equipercentile and 3‐parameter logistic model item‐response theory (IRT) procedures were used for both designs. Both pretest methods produced inadequate equating results, and the IRT item preequating method resulted in more equating error than had no equating been conducted. Although neither of the item preequating methods performed well, the results from the equipercentile preequating method were more consistent with those from the random groups method than were the results from the IRT item pretest method. Item context and position effects were likely responsible, at least in part, for the inadequate results for item preequating. Such effects need to be either controlled or modeled, and the design further researched before the item preequating design can be recommended for operational use. Copyright © 1990, Wiley Blackwell. All rights reserved","['item‐preequate', 'design', 'random', 'group', 'design', 'equate', 'form', 'American', 'College', 'Testing', 'ACT', 'Assessment', 'Mathematics', 'Test', 'Equipercentile', '3‐parameter', 'logistic', 'item‐response', 'theory', 'IRT', 'procedure', 'design', 'pretest', 'method', 'produce', 'inadequate', 'equate', 'result', 'IRT', 'item', 'preequate', 'method', 'result', 'equate', 'error', 'equating', 'conduct', 'item', 'preequate', 'method', 'perform', 'result', 'equipercentile', 'preequate', 'method', 'consistent', 'random', 'group', 'method', 'result', 'IRT', 'item', 'pret', 'method', 'Item', 'context', 'position', 'effect', 'likely', 'responsible', 'inadequate', 'result', 'item', 'preequate', 'effect', 'need', 'control', 'design', 'far', 'research', 'item', 'preequate', 'design', 'recommend', 'operational', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Item', 'Preequating', 'Random', 'Groups', 'Equating', 'IRT', 'Equipercentile', 'Methods']",item‐preequate design random group design equate form American College Testing ACT Assessment Mathematics Test Equipercentile 3‐parameter logistic item‐response theory IRT procedure design pretest method produce inadequate equate result IRT item preequate method result equate error equating conduct item preequate method perform result equipercentile preequate method consistent random group method result IRT item pret method Item context position effect likely responsible inadequate result item preequate effect need control design far research item preequate design recommend operational Copyright © 1990 Wiley Blackwell right reserve,Comparison Item Preequating Random Groups Equating IRT Equipercentile Methods,0.031820752549497525,0.03170194126915351,0.03172266864569112,0.03139451571972291,0.8733601218159349,0.08398013364075398,0.0031739606198774284,0.011396428562443833,0.015592500412231985,0.01040536890115054
Muthén B.O.,Multilevel Factor Analysis of Class and Student Achievement Components,1991,28,"This article analyzes mathematics achievement data from the Second International Mathematics Study (SIMS; Crosswhite, Dossey, Swafford, McKnight, & Cooney, 1985) in which U.S. students are measured at the beginning and end of eighth grade. The aim of the article is to address some substantive analysis questions in the SIMS data and show the potential of multilevel factor analysis methodology. Issues related to between‐ and within‐class decomposition of achievement variance and the change of this decomposition over the course of the eighth grade are studied. As a starting point, random effects ANOVA is considered for each achievement score. Each score contains a large amount of measurement error. The effects of unreliability on variance decomposition are shown with the help of a multilevel factor analysis model. Unreliability has severely distorting effects on this type of ANOVA while multilevel factor analysis gives results corresponding to what would be obtained with perfectly reliable scores. Copyright © 1991, Wiley Blackwell. All rights reserved",Multilevel Factor Analysis of Class and Student Achievement Components,"This article analyzes mathematics achievement data from the Second International Mathematics Study (SIMS; Crosswhite, Dossey, Swafford, McKnight, & Cooney, 1985) in which U.S. students are measured at the beginning and end of eighth grade. The aim of the article is to address some substantive analysis questions in the SIMS data and show the potential of multilevel factor analysis methodology. Issues related to between‐ and within‐class decomposition of achievement variance and the change of this decomposition over the course of the eighth grade are studied. As a starting point, random effects ANOVA is considered for each achievement score. Each score contains a large amount of measurement error. The effects of unreliability on variance decomposition are shown with the help of a multilevel factor analysis model. Unreliability has severely distorting effects on this type of ANOVA while multilevel factor analysis gives results corresponding to what would be obtained with perfectly reliable scores. Copyright © 1991, Wiley Blackwell. All rights reserved","['article', 'analyze', 'mathematics', 'achievement', 'datum', 'Second', 'International', 'Mathematics', 'Study', 'SIMS', 'Crosswhite', 'Dossey', 'Swafford', 'McKnight', 'Cooney', '1985', 'US', 'student', 'measure', 'beginning', 'end', 'eighth', 'grade', 'aim', 'article', 'address', 'substantive', 'analysis', 'question', 'SIMS', 'datum', 'potential', 'multilevel', 'factor', 'analysis', 'methodology', 'issue', 'relate', 'between‐', 'within‐class', 'decomposition', 'achievement', 'variance', 'change', 'decomposition', 'course', 'eighth', 'grade', 'study', 'starting', 'point', 'random', 'effect', 'ANOVA', 'consider', 'achievement', 'score', 'score', 'contain', 'large', 'error', 'effect', 'unreliability', 'variance', 'decomposition', 'help', 'multilevel', 'factor', 'analysis', 'Unreliability', 'severely', 'distort', 'effect', 'type', 'ANOVA', 'multilevel', 'factor', 'analysis', 'result', 'correspond', 'obtain', 'perfectly', 'reliable', 'score', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Multilevel', 'Factor', 'Analysis', 'Class', 'Student', 'Achievement', 'Components']",article analyze mathematics achievement datum Second International Mathematics Study SIMS Crosswhite Dossey Swafford McKnight Cooney 1985 US student measure beginning end eighth grade aim article address substantive analysis question SIMS datum potential multilevel factor analysis methodology issue relate between‐ within‐class decomposition achievement variance change decomposition course eighth grade study starting point random effect ANOVA consider achievement score score contain large error effect unreliability variance decomposition help multilevel factor analysis Unreliability severely distort effect type ANOVA multilevel factor analysis result correspond obtain perfectly reliable score Copyright © 1991 Wiley Blackwell right reserve,Multilevel Factor Analysis Class Student Achievement Components,0.025704328523572812,0.026017679716570128,0.8966517144027344,0.02568758338525439,0.025938693971868217,0.0,0.01201243091369186,0.0026270844070464074,0.043622324450722776,0.06395187891336898
Hanson B.A.; Brennan R.L.,An Investigation of Classification Consistency Indexes Estimated Under Alternative Strong True Score Models,1990,27,"Using several data sets, the authors examine the relative performance of the beta binomial model and two other more general strong true score models in estimating several indexes of classification consistency. It is shown that the beta binomial model can provide inadequate fits to raw score distributions compared to more general models. This lack of fit is reflected in differences in decision consistency indexes computed using the beta binomial model and the other models. It is recommended that the adequacy of a model in fitting the data be assessed before the model is used to estimate decision consistency indexes. When the beta binomial model does not fit the data, the more general models discussed here may provide an adequate fit and, in such cases, would be more appropriate for computing decision consistency indexes. Copyright © 1990, Wiley Blackwell. All rights reserved",An Investigation of Classification Consistency Indexes Estimated Under Alternative Strong True Score Models,"Using several data sets, the authors examine the relative performance of the beta binomial model and two other more general strong true score models in estimating several indexes of classification consistency. It is shown that the beta binomial model can provide inadequate fits to raw score distributions compared to more general models. This lack of fit is reflected in differences in decision consistency indexes computed using the beta binomial model and the other models. It is recommended that the adequacy of a model in fitting the data be assessed before the model is used to estimate decision consistency indexes. When the beta binomial model does not fit the data, the more general models discussed here may provide an adequate fit and, in such cases, would be more appropriate for computing decision consistency indexes. Copyright © 1990, Wiley Blackwell. All rights reserved","['datum', 'set', 'author', 'examine', 'relative', 'performance', 'beta', 'binomial', 'general', 'strong', 'true', 'score', 'estimate', 'index', 'classification', 'consistency', 'beta', 'binomial', 'provide', 'inadequate', 'fit', 'raw', 'score', 'distribution', 'compare', 'general', 'lack', 'fit', 'reflect', 'difference', 'decision', 'consistency', 'index', 'compute', 'beta', 'binomial', 'recommend', 'adequacy', 'fit', 'datum', 'assess', 'estimate', 'decision', 'consistency', 'index', 'beta', 'binomial', 'fit', 'datum', 'general', 'discuss', 'provide', 'adequate', 'fit', 'case', 'appropriate', 'compute', 'decision', 'consistency', 'index', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Investigation', 'Classification', 'Consistency', 'Indexes', 'estimate', 'alternative', 'Strong', 'True', 'Score', 'Models']",datum set author examine relative performance beta binomial general strong true score estimate index classification consistency beta binomial provide inadequate fit raw score distribution compare general lack fit reflect difference decision consistency index compute beta binomial recommend adequacy fit datum assess estimate decision consistency index beta binomial fit datum general discuss provide adequate fit case appropriate compute decision consistency index copyright © 1990 Wiley Blackwell right reserve,Investigation Classification Consistency Indexes estimate alternative Strong True Score Models,0.8567887233541005,0.035747327079897806,0.035489682897365316,0.035125731739027506,0.03684853492960896,0.02539113009592466,0.003241980061751053,0.0008241125663499373,0.04476099192900971,0.012297454231573269
Marsh H.W.,Stability of Individual Differences in Multiwave Panel Studies: Comparison of Simplex Models and One‐Factor Models,1993,30,"The purpose of this investigation is to evaluate structural equation models (SEMs) for measures of the same construct collected on multiple occasions (one‐variable, multiwave panel studies). Simplex models hypothesize that a measure at any one wave is substantially influenced by the measure at the 0immediately preceding wave; correlations between the same construct measured on different occasions are predicted to decline systematically as the number of intervening occasions increases. Alternatively, a one‐factor model posits that a person's score at any one time is a function of some underlying “true” score and a random disturbance that is idiosyncratic to the time; no temporal ordering of correlations is assumed. Both the simplex and one‐factor models can befit when there is only a single indicator of each construct at each wave (e.g., scale scores), but there are serious limitations to such models. Stronger models are possible when the same set of multiple indicators (e.g., the items that make up the scale) is measured at each wave. In Study 1, based on students' evaluations of teaching effectiveness collected over an 8‐year period, one‐factor models fit the data well, whereas simplex models did not. In Study 2, based on personality variables collected over a 4‐year period during adolescence, one‐factor models again provided an excellent fit to the data, whereas the simplex model did marginally poorer. The results challenge an overreliance on simplex models and demonstrate that a one‐factor model is a potentially useful alternative that should be considered in multiwave studies. Copyright © 1993, Wiley Blackwell. All rights reserved",Stability of Individual Differences in Multiwave Panel Studies: Comparison of Simplex Models and One‐Factor Models,"The purpose of this investigation is to evaluate structural equation models (SEMs) for measures of the same construct collected on multiple occasions (one‐variable, multiwave panel studies). Simplex models hypothesize that a measure at any one wave is substantially influenced by the measure at the 0immediately preceding wave; correlations between the same construct measured on different occasions are predicted to decline systematically as the number of intervening occasions increases. Alternatively, a one‐factor model posits that a person's score at any one time is a function of some underlying “true” score and a random disturbance that is idiosyncratic to the time; no temporal ordering of correlations is assumed. Both the simplex and one‐factor models can befit when there is only a single indicator of each construct at each wave (e.g., scale scores), but there are serious limitations to such models. Stronger models are possible when the same set of multiple indicators (e.g., the items that make up the scale) is measured at each wave. In Study 1, based on students' evaluations of teaching effectiveness collected over an 8‐year period, one‐factor models fit the data well, whereas simplex models did not. In Study 2, based on personality variables collected over a 4‐year period during adolescence, one‐factor models again provided an excellent fit to the data, whereas the simplex model did marginally poorer. The results challenge an overreliance on simplex models and demonstrate that a one‐factor model is a potentially useful alternative that should be considered in multiwave studies. Copyright © 1993, Wiley Blackwell. All rights reserved","['purpose', 'investigation', 'evaluate', 'structural', 'equation', 'sem', 'measure', 'construct', 'collect', 'multiple', 'occasion', 'one‐variable', 'multiwave', 'panel', 'study', 'Simplex', 'hypothesize', 'measure', 'wave', 'substantially', 'influence', 'measure', '0immediately', 'precede', 'wave', 'correlation', 'construct', 'measure', 'different', 'occasion', 'predict', 'decline', 'systematically', 'number', 'intervene', 'occasion', 'increase', 'alternatively', 'one‐factor', 'posit', 'person', 'score', 'time', 'function', 'underlying', '""', 'true', '""', 'score', 'random', 'disturbance', 'idiosyncratic', 'time', 'temporal', 'ordering', 'correlation', 'assume', 'simplex', 'one‐factor', 'befit', 'single', 'indicator', 'construct', 'wave', 'eg', 'scale', 'score', 'limitation', 'strong', 'possible', 'set', 'multiple', 'indicator', 'eg', 'item', 'scale', 'measure', 'wave', 'Study', '1', 'base', 'student', 'evaluation', 'teaching', 'effectiveness', 'collect', '8‐year', 'period', 'one‐factor', 'fit', 'data', 'simplex', 'Study', '2', 'base', 'personality', 'variable', 'collect', '4‐year', 'period', 'adolescence', 'one‐factor', 'provide', 'excellent', 'fit', 'datum', 'simplex', 'marginally', 'poor', 'result', 'challenge', 'overreliance', 'simplex', 'demonstrate', 'one‐factor', 'potentially', 'useful', 'alternative', 'consider', 'multiwave', 'study', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['stability', 'Individual', 'Differences', 'Multiwave', 'Panel', 'Studies', 'Comparison', 'Simplex', 'Models', 'one‐factor']","purpose investigation evaluate structural equation sem measure construct collect multiple occasion one‐variable multiwave panel study Simplex hypothesize measure wave substantially influence measure 0immediately precede wave correlation construct measure different occasion predict decline systematically number intervene occasion increase alternatively one‐factor posit person score time function underlying "" true "" score random disturbance idiosyncratic time temporal ordering correlation assume simplex one‐factor befit single indicator construct wave eg scale score limitation strong possible set multiple indicator eg item scale measure wave Study 1 base student evaluation teaching effectiveness collect 8‐year period one‐factor fit data simplex Study 2 base personality variable collect 4‐year period adolescence one‐factor provide excellent fit datum simplex marginally poor result challenge overreliance simplex demonstrate one‐factor potentially useful alternative consider multiwave study Copyright © 1993 Wiley Blackwell right reserve",stability Individual Differences Multiwave Panel Studies Comparison Simplex Models one‐factor,0.02424701049194215,0.024249851581463852,0.9030505060059311,0.024082187914967544,0.024370444005695383,0.0,0.02078986251239329,0.004131717606643325,0.03496006613442559,0.039071578669750154
Martinez M.E.,A Comparison of Multiple‐Choice and Constructed Figural Response Items,1991,28,"In contrast to multiple‐choice test questions, figural response items call for constructed responses and rely upon figural material, such as illustrations and graphs, as the response medium. Figural response questions in various science domains were created and administered to a sample of 4th‐, 8th‐, and 12th‐grade students. Item and test statistics from parallel sets of figural response and multiple‐choice questions were compared. Figural response items were generally more difficult, especially for questions that were difficult (p < .5) in their constructed‐response forms. Figural response questions were also slightly more discriminating and reliable than their multiple‐choice counterparts, but they had higher omit rates. This article addresses the relevance of guessing to figural response items and the diagnostic value of the item type. Plans for future research on figural response items are discussed. Copyright © 1991, Wiley Blackwell. All rights reserved",A Comparison of Multiple‐Choice and Constructed Figural Response Items,"In contrast to multiple‐choice test questions, figural response items call for constructed responses and rely upon figural material, such as illustrations and graphs, as the response medium. Figural response questions in various science domains were created and administered to a sample of 4th‐, 8th‐, and 12th‐grade students. Item and test statistics from parallel sets of figural response and multiple‐choice questions were compared. Figural response items were generally more difficult, especially for questions that were difficult (p < .5) in their constructed‐response forms. Figural response questions were also slightly more discriminating and reliable than their multiple‐choice counterparts, but they had higher omit rates. This article addresses the relevance of guessing to figural response items and the diagnostic value of the item type. Plans for future research on figural response items are discussed. Copyright © 1991, Wiley Blackwell. All rights reserved","['contrast', 'multiple‐choice', 'test', 'question', 'figural', 'response', 'item', 'construct', 'response', 'rely', 'figural', 'material', 'illustration', 'graph', 'response', 'medium', 'Figural', 'response', 'question', 'science', 'domain', 'create', 'administer', 'sample', '4th‐', '8th‐', '12th‐grade', 'student', 'Item', 'test', 'statistic', 'parallel', 'set', 'figural', 'response', 'multiple‐choice', 'question', 'compare', 'figural', 'response', 'item', 'generally', 'difficult', 'especially', 'question', 'difficult', 'p', '5', 'constructed‐response', 'form', 'figural', 'response', 'question', 'slightly', 'discriminating', 'reliable', 'multiple‐choice', 'counterpart', 'high', 'omit', 'rate', 'article', 'address', 'relevance', 'guess', 'figural', 'response', 'item', 'diagnostic', 'value', 'item', 'type', 'plan', 'future', 'research', 'figural', 'response', 'item', 'discuss', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Multiple‐Choice', 'Constructed', 'Figural', 'Response', 'Items']",contrast multiple‐choice test question figural response item construct response rely figural material illustration graph response medium Figural response question science domain create administer sample 4th‐ 8th‐ 12th‐grade student Item test statistic parallel set figural response multiple‐choice question compare figural response item generally difficult especially question difficult p 5 constructed‐response form figural response question slightly discriminating reliable multiple‐choice counterpart high omit rate article address relevance guess figural response item diagnostic value item type plan future research figural response item discuss Copyright © 1991 Wiley Blackwell right reserve,Comparison Multiple‐Choice Constructed Figural Response Items,0.8617823779100513,0.034657163679324476,0.03480484069708073,0.034074971437394484,0.034680646276148815,0.00942542321957523,0.0823670382924057,0.0,0.0,0.0
Wainer H.; Sireci S.G.; Thissen D.,Differential Testlet Functioning: Definitions and Detection,1991,28,"It is sometimes sensible to think of the fundamental unit of test construction as being larger than an individual item. This unit, dubbed the testlet, must pass muster in the same way that items do. One criterion of a good item is the absence of DIF–the item must function in the same way in all important subpopulations of examinees. In this article, we define what we mean by testlet DIF and provide a statistical methodology to detect it. This methodology parallels the IRT‐based likelihood ratio procedures explored previously by Thissen, Steinberg, and Wainer (1988, in press). We illustrate this methodology with analyses of data from a testlet‐based experimental version of the Scholastic Aptitude Test (SAT). Copyright © 1991, Wiley Blackwell. All rights reserved",Differential Testlet Functioning: Definitions and Detection,"It is sometimes sensible to think of the fundamental unit of test construction as being larger than an individual item. This unit, dubbed the testlet, must pass muster in the same way that items do. One criterion of a good item is the absence of DIF–the item must function in the same way in all important subpopulations of examinees. In this article, we define what we mean by testlet DIF and provide a statistical methodology to detect it. This methodology parallels the IRT‐based likelihood ratio procedures explored previously by Thissen, Steinberg, and Wainer (1988, in press). We illustrate this methodology with analyses of data from a testlet‐based experimental version of the Scholastic Aptitude Test (SAT). Copyright © 1991, Wiley Blackwell. All rights reserved","['sensible', 'think', 'fundamental', 'unit', 'test', 'construction', 'large', 'individual', 'item', 'unit', 'dub', 'testlet', 'pass', 'muster', 'way', 'item', 'criterion', 'good', 'item', 'absence', 'DIF', '–', 'item', 'function', 'way', 'important', 'subpopulation', 'examinee', 'article', 'define', 'mean', 'testlet', 'DIF', 'provide', 'statistical', 'methodology', 'detect', 'methodology', 'parallel', 'irt‐base', 'likelihood', 'ratio', 'procedure', 'explore', 'previously', 'Thissen', 'Steinberg', 'Wainer', '1988', 'press', 'illustrate', 'methodology', 'analysis', 'datum', 'testlet‐based', 'experimental', 'version', 'Scholastic', 'Aptitude', 'Test', 'SAT', 'copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Differential', 'Testlet', 'Functioning', 'Definitions', 'Detection']",sensible think fundamental unit test construction large individual item unit dub testlet pass muster way item criterion good item absence DIF – item function way important subpopulation examinee article define mean testlet DIF provide statistical methodology detect methodology parallel irt‐base likelihood ratio procedure explore previously Thissen Steinberg Wainer 1988 press illustrate methodology analysis datum testlet‐based experimental version Scholastic Aptitude Test SAT copyright © 1991 Wiley Blackwell right reserve,Differential Testlet Functioning Definitions Detection,0.025578455727516863,0.025456581692247713,0.025643646495639143,0.025381365747256794,0.8979399503373394,0.0,0.06121329152863067,0.07513696030921457,0.004887867166794742,0.0
Quails‐Payne A.L.,A Comparison of Score Level Estimates of the Standard Error of Measurement,1992,29,"Numerous methods have been proposed and investigated for estimating · the standard error of measurement (SEM) at specific score levels. Consensus on the preferred method has not been obtained, in part because there is no standard criterion. The criterion procedure in previous investigations has been a single test occasion procedure. This study compares six estimation techniques. Two criteria were calculated by using test results obtained from a test‐retest or parallel forms design. The relationship between estimated score level standard errors and the score scale was similar for the six procedures. These relationships were also congruent to findings from previous investigations. Similarity between estimates and criteria varied over methods and criteria. For test‐retest conditions, the estimation techniques are interchangeable. The user's selection could be based on personal preference. However, for parallel forms conditions, the procedures resulted in estimates that were meaningfully different. The preferred estimation technique would be Feldt's method (cited in Gupta, 1965; Feldt, 1984). Copyright © 1992, Wiley Blackwell. All rights reserved",A Comparison of Score Level Estimates of the Standard Error of Measurement,"Numerous methods have been proposed and investigated for estimating · the standard error of measurement (SEM) at specific score levels. Consensus on the preferred method has not been obtained, in part because there is no standard criterion. The criterion procedure in previous investigations has been a single test occasion procedure. This study compares six estimation techniques. Two criteria were calculated by using test results obtained from a test‐retest or parallel forms design. The relationship between estimated score level standard errors and the score scale was similar for the six procedures. These relationships were also congruent to findings from previous investigations. Similarity between estimates and criteria varied over methods and criteria. For test‐retest conditions, the estimation techniques are interchangeable. The user's selection could be based on personal preference. However, for parallel forms conditions, the procedures resulted in estimates that were meaningfully different. The preferred estimation technique would be Feldt's method (cited in Gupta, 1965; Feldt, 1984). Copyright © 1992, Wiley Blackwell. All rights reserved","['numerous', 'method', 'propose', 'investigate', 'estimate', '·', 'standard', 'error', 'SEM', 'specific', 'score', 'level', 'Consensus', 'preferred', 'method', 'obtain', 'standard', 'criterion', 'criterion', 'procedure', 'previous', 'investigation', 'single', 'test', 'occasion', 'procedure', 'study', 'compare', 'estimation', 'technique', 'criterion', 'calculate', 'test', 'result', 'obtain', 'test‐ret', 'parallel', 'form', 'design', 'relationship', 'estimate', 'score', 'level', 'standard', 'error', 'score', 'scale', 'similar', 'procedure', 'relationship', 'congruent', 'finding', 'previous', 'investigation', 'Similarity', 'estimate', 'criterion', 'vary', 'method', 'criterion', 'test‐ret', 'condition', 'estimation', 'technique', 'interchangeable', 'user', 'selection', 'base', 'personal', 'preference', 'parallel', 'form', 'condition', 'procedure', 'result', 'estimate', 'meaningfully', 'different', 'preferred', 'estimation', 'technique', 'Feldts', 'method', 'cite', 'Gupta', '1965', 'Feldt', '1984', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Score', 'Level', 'estimate', 'Standard', 'Error']",numerous method propose investigate estimate · standard error SEM specific score level Consensus preferred method obtain standard criterion criterion procedure previous investigation single test occasion procedure study compare estimation technique criterion calculate test result obtain test‐ret parallel form design relationship estimate score level standard error score scale similar procedure relationship congruent finding previous investigation Similarity estimate criterion vary method criterion test‐ret condition estimation technique interchangeable user selection base personal preference parallel form condition procedure result estimate meaningfully different preferred estimation technique Feldts method cite Gupta 1965 Feldt 1984 Copyright © 1992 Wiley Blackwell right reserve,Comparison Score Level estimate Standard Error,0.025748574268887218,0.025670585153910474,0.025675789907117507,0.025365382200729973,0.8975396684693548,0.05962576992613974,0.0,0.0,0.13320058940240068,0.006260919479247632
Farr R.; Pritchard R.; Smitten B.,A Description of What Happens When an Examinee Takes a Multiple‐Choice Reading Comprehension Test,1990,27,"This study investigated the strategies a group of college students used to complete a portion of a standardized reading comprehension test. Twenty‐six students were randomly assigned to either an introspective interview, in which the subjects explained to a researcher what they were doing and thinking as they read the test passages and answered the multiple‐choice questions, or a retrospective interview, in which the students completed the test without interruption and then recounted for the researcher how they had gone about the task. Data analysis resulted in the identification of three broad categories of processing behavior: an overall approach to the test task, reading strategies, and test‐taking strategies. In addition, difficulties encountered by the subjects were identified. Results indicate that the common element in each subject's approach to the test was a focus on getting to the questions as quickly as possible and then using the questions to direct a search of the passage to locate the best possible information to answer the questions. The implications of these results for better understanding the relationship between test‐taking behavior and reading are discussed. Copyright © 1990, Wiley Blackwell. All rights reserved",A Description of What Happens When an Examinee Takes a Multiple‐Choice Reading Comprehension Test,"This study investigated the strategies a group of college students used to complete a portion of a standardized reading comprehension test. Twenty‐six students were randomly assigned to either an introspective interview, in which the subjects explained to a researcher what they were doing and thinking as they read the test passages and answered the multiple‐choice questions, or a retrospective interview, in which the students completed the test without interruption and then recounted for the researcher how they had gone about the task. Data analysis resulted in the identification of three broad categories of processing behavior: an overall approach to the test task, reading strategies, and test‐taking strategies. In addition, difficulties encountered by the subjects were identified. Results indicate that the common element in each subject's approach to the test was a focus on getting to the questions as quickly as possible and then using the questions to direct a search of the passage to locate the best possible information to answer the questions. The implications of these results for better understanding the relationship between test‐taking behavior and reading are discussed. Copyright © 1990, Wiley Blackwell. All rights reserved","['study', 'investigate', 'strategy', 'group', 'college', 'student', 'complete', 'portion', 'standardized', 'reading', 'comprehension', 'test', 'Twenty‐six', 'student', 'randomly', 'assign', 'introspective', 'interview', 'subject', 'explain', 'researcher', 'think', 'read', 'test', 'passage', 'answer', 'multiple‐choice', 'question', 'retrospective', 'interview', 'student', 'complete', 'test', 'interruption', 'recount', 'researcher', 'task', 'Data', 'analysis', 'result', 'identification', 'broad', 'category', 'process', 'behavior', 'overall', 'approach', 'test', 'task', 'read', 'strategy', 'test‐taking', 'strategy', 'addition', 'difficulty', 'encounter', 'subject', 'identify', 'result', 'indicate', 'common', 'element', 'subject', 'approach', 'test', 'focus', 'question', 'quickly', 'possible', 'question', 'direct', 'search', 'passage', 'locate', 'good', 'possible', 'information', 'answer', 'question', 'implication', 'result', 'understand', 'relationship', 'test‐take', 'behavior', 'reading', 'discuss', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['description', 'happen', 'Examinee', 'Multiple‐Choice', 'Reading', 'Comprehension', 'Test']",study investigate strategy group college student complete portion standardized reading comprehension test Twenty‐six student randomly assign introspective interview subject explain researcher think read test passage answer multiple‐choice question retrospective interview student complete test interruption recount researcher task Data analysis result identification broad category process behavior overall approach test task read strategy test‐taking strategy addition difficulty encounter subject identify result indicate common element subject approach test focus question quickly possible question direct search passage locate good possible information answer question implication result understand relationship test‐take behavior reading discuss Copyright © 1990 Wiley Blackwell right reserve,description happen Examinee Multiple‐Choice Reading Comprehension Test,0.024801692266214568,0.9003723841253516,0.024944142867919215,0.024665062869509168,0.025216717871005268,0.0,0.09916648227547527,0.0,0.0,0.028460384085864925
Embretson S.E.,Measuring and Validating Cognitive Modifiability as an Ability: A Study in the Spatial Domain,1992,29,"Measuring cognitive modifiability from the responsiveness of an individual's performance to intervention has long been viewed (e.g., Dearborne, 1921) as an alternative to traditional (static) ability measurement. Currently, dynamic testing, in which cues or instruction are presented with ability test items, is a popular method for assessing cognitive modifiability. Despite the long‐standing interest, however, little data exists to support the validity of cognitive modifiability measures in any ability domain. Several special methodological difficulties have limited validity studies, including psychometric problems in measuring modifiability (i.e., as change), lack of appropriate validation criteria, and difficulty in linking modifiability to cognitive theory. In this article, relatively new developments for solving the validation problems are applied to measuring and validating spatial modifiability. Criterion‐related validity for predicting learning in an applied knowledge domain, as well as construct validity, is supported. Copyright © 1992, Wiley Blackwell. All rights reserved",Measuring and Validating Cognitive Modifiability as an Ability: A Study in the Spatial Domain,"Measuring cognitive modifiability from the responsiveness of an individual's performance to intervention has long been viewed (e.g., Dearborne, 1921) as an alternative to traditional (static) ability measurement. Currently, dynamic testing, in which cues or instruction are presented with ability test items, is a popular method for assessing cognitive modifiability. Despite the long‐standing interest, however, little data exists to support the validity of cognitive modifiability measures in any ability domain. Several special methodological difficulties have limited validity studies, including psychometric problems in measuring modifiability (i.e., as change), lack of appropriate validation criteria, and difficulty in linking modifiability to cognitive theory. In this article, relatively new developments for solving the validation problems are applied to measuring and validating spatial modifiability. Criterion‐related validity for predicting learning in an applied knowledge domain, as well as construct validity, is supported. Copyright © 1992, Wiley Blackwell. All rights reserved","['measure', 'cognitive', 'modifiability', 'responsiveness', 'individual', 'performance', 'intervention', 'long', 'view', 'eg', 'Dearborne', '1921', 'alternative', 'traditional', 'static', 'ability', 'currently', 'dynamic', 'testing', 'cue', 'instruction', 'present', 'ability', 'test', 'item', 'popular', 'method', 'assess', 'cognitive', 'modifiability', 'despite', 'long‐standing', 'interest', 'little', 'datum', 'exist', 'support', 'validity', 'cognitive', 'modifiability', 'measure', 'ability', 'domain', 'special', 'methodological', 'difficulty', 'limit', 'validity', 'study', 'include', 'psychometric', 'problem', 'measure', 'modifiability', 'ie', 'change', 'lack', 'appropriate', 'validation', 'criterion', 'difficulty', 'link', 'modifiability', 'cognitive', 'theory', 'article', 'relatively', 'new', 'development', 'solve', 'validation', 'problem', 'apply', 'measure', 'validate', 'spatial', 'modifiability', 'criterion‐relate', 'validity', 'predict', 'learn', 'apply', 'knowledge', 'domain', 'construct', 'validity', 'support', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['measure', 'validate', 'Cognitive', 'Modifiability', 'ability', 'A', 'Study', 'Spatial', 'Domain']",measure cognitive modifiability responsiveness individual performance intervention long view eg Dearborne 1921 alternative traditional static ability currently dynamic testing cue instruction present ability test item popular method assess cognitive modifiability despite long‐standing interest little datum exist support validity cognitive modifiability measure ability domain special methodological difficulty limit validity study include psychometric problem measure modifiability ie change lack appropriate validation criterion difficulty link modifiability cognitive theory article relatively new development solve validation problem apply measure validate spatial modifiability criterion‐relate validity predict learn apply knowledge domain construct validity support Copyright © 1992 Wiley Blackwell right reserve,measure validate Cognitive Modifiability ability A Study Spatial Domain,0.02767969063838633,0.027727758611754937,0.8890635571093719,0.02717047033456118,0.028358523305925618,0.0,0.05446219497535042,0.0,0.0,0.04606372924578954
Woodruff D.,Conditional Standard Error of Measurement in Prediction,1990,27,"Previous methods for estimating the conditional standard error of measurement (CSEM) at specific score or ability levels are critically discussed, and a brief summary of prior empirical results is given. A new method is developed that avoids theoretical problems inherent in some prior methods, is easy to implement, and estimates not only a quantity analogous to the CSEM at each score but also the conditional standard error of prediction (CSEP) at each score and the conditional true score standard deviation (CTSSD) at each score, The new method differs from previous methods in that previous methods have concentrated on attempting to estimate error variance conditional on a fixed value of true score, whereas the new method considers the variance of observed scores conditional on a fixed value of an observed parallel measurement and decomposes these conditional observed score variances into true and error parts. The new method and several older methods are applied to a variety of tests, and representative results are graphically displayed. The CSEM‐Iike estimates produced by the new method are called conditional standard error of measurement in prediction (CSEMP) estimates and are similar to those produced by older methods, but the CSEP estimates produced by the new method offer an alternative interpretation of the accuracy of a test at different scores. Finally, evidence is presented that shows that previous methods can produce dissimilar results and that the shape of the score distribution may influence the way in which the CSEM varies across the score scale. Copyright © 1990, Wiley Blackwell. All rights reserved",Conditional Standard Error of Measurement in Prediction,"Previous methods for estimating the conditional standard error of measurement (CSEM) at specific score or ability levels are critically discussed, and a brief summary of prior empirical results is given. A new method is developed that avoids theoretical problems inherent in some prior methods, is easy to implement, and estimates not only a quantity analogous to the CSEM at each score but also the conditional standard error of prediction (CSEP) at each score and the conditional true score standard deviation (CTSSD) at each score, The new method differs from previous methods in that previous methods have concentrated on attempting to estimate error variance conditional on a fixed value of true score, whereas the new method considers the variance of observed scores conditional on a fixed value of an observed parallel measurement and decomposes these conditional observed score variances into true and error parts. The new method and several older methods are applied to a variety of tests, and representative results are graphically displayed. The CSEM‐Iike estimates produced by the new method are called conditional standard error of measurement in prediction (CSEMP) estimates and are similar to those produced by older methods, but the CSEP estimates produced by the new method offer an alternative interpretation of the accuracy of a test at different scores. Finally, evidence is presented that shows that previous methods can produce dissimilar results and that the shape of the score distribution may influence the way in which the CSEM varies across the score scale. Copyright © 1990, Wiley Blackwell. All rights reserved","['previous', 'method', 'estimate', 'conditional', 'standard', 'error', 'CSEM', 'specific', 'score', 'ability', 'level', 'critically', 'discuss', 'brief', 'summary', 'prior', 'empirical', 'result', 'new', 'method', 'develop', 'avoid', 'theoretical', 'problem', 'inherent', 'prior', 'method', 'easy', 'implement', 'estimate', 'quantity', 'analogous', 'CSEM', 'score', 'conditional', 'standard', 'error', 'prediction', 'CSEP', 'score', 'conditional', 'true', 'score', 'standard', 'deviation', 'CTSSD', 'score', 'new', 'method', 'differ', 'previous', 'method', 'previous', 'method', 'concentrate', 'attempt', 'estimate', 'error', 'variance', 'conditional', 'fix', 'value', 'true', 'score', 'new', 'method', 'consider', 'variance', 'observe', 'score', 'conditional', 'fix', 'value', 'observe', 'parallel', 'decompose', 'conditional', 'observed', 'score', 'variance', 'true', 'error', 'new', 'method', 'old', 'method', 'apply', 'variety', 'test', 'representative', 'result', 'graphically', 'display', 'CSEM‐Iike', 'estimate', 'produce', 'new', 'method', 'conditional', 'standard', 'error', 'prediction', 'csemp', 'estimate', 'similar', 'produce', 'old', 'method', 'CSEP', 'estimate', 'produce', 'new', 'method', 'offer', 'alternative', 'interpretation', 'accuracy', 'test', 'different', 'score', 'finally', 'evidence', 'present', 'previous', 'method', 'produce', 'dissimilar', 'result', 'shape', 'score', 'distribution', 'influence', 'way', 'CSEM', 'vary', 'score', 'scale', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Conditional', 'Standard', 'Error', 'prediction']",previous method estimate conditional standard error CSEM specific score ability level critically discuss brief summary prior empirical result new method develop avoid theoretical problem inherent prior method easy implement estimate quantity analogous CSEM score conditional standard error prediction CSEP score conditional true score standard deviation CTSSD score new method differ previous method previous method concentrate attempt estimate error variance conditional fix value true score new method consider variance observe score conditional fix value observe parallel decompose conditional observed score variance true error new method old method apply variety test representative result graphically display CSEM‐Iike estimate produce new method conditional standard error prediction csemp estimate similar produce old method CSEP estimate produce new method offer alternative interpretation accuracy test different score finally evidence present previous method produce dissimilar result shape score distribution influence way CSEM vary score scale Copyright © 1990 Wiley Blackwell right reserve,Conditional Standard Error prediction,0.02665041271962683,0.026555268905249993,0.027002029381168833,0.026452604534329877,0.8933396844596244,0.004273818611850037,0.0,0.0,0.29623817242979583,0.0
Wainer H.; Wang X.‐B.; Thissen D.,How Well Can We Compare Scores on Test Forms That Are Constructed by Examinees Choice?,1994,31,"When an exam consists, in whole or in part, of constructed‐response items, it is a common practice to allow the examinee to choose a subset of the questions to answer. This procedure is usually adopted so that the limited number of items that can be completed in the allotted time does not unfairly affect the examinee. This results in the de facto administration of several different test forms, where the exact structure of any particular form is determined by the examinee. However, when different forms are administered, a canon of good testing practice requires that those forms be equated to adjust for differences in their difficulty. When the items are chosen by the examinee, traditional equating procedures do not strictly apply due to the nonignorable nature of the missing responses. In this article, we examine the comparability of scores on such tests within an IRT framework. We illustrate the approach with data from the College Board's Advanced Placement Test in Chemistry Copyright © 1994, Wiley Blackwell. All rights reserved",How Well Can We Compare Scores on Test Forms That Are Constructed by Examinees Choice?,"When an exam consists, in whole or in part, of constructed‐response items, it is a common practice to allow the examinee to choose a subset of the questions to answer. This procedure is usually adopted so that the limited number of items that can be completed in the allotted time does not unfairly affect the examinee. This results in the de facto administration of several different test forms, where the exact structure of any particular form is determined by the examinee. However, when different forms are administered, a canon of good testing practice requires that those forms be equated to adjust for differences in their difficulty. When the items are chosen by the examinee, traditional equating procedures do not strictly apply due to the nonignorable nature of the missing responses. In this article, we examine the comparability of scores on such tests within an IRT framework. We illustrate the approach with data from the College Board's Advanced Placement Test in Chemistry Copyright © 1994, Wiley Blackwell. All rights reserved","['exam', 'consist', 'constructed‐response', 'item', 'common', 'practice', 'allow', 'examinee', 'choose', 'subset', 'question', 'answer', 'procedure', 'usually', 'adopt', 'limited', 'number', 'item', 'complete', 'allotted', 'time', 'unfairly', 'affect', 'examinee', 'result', 'de', 'facto', 'administration', 'different', 'test', 'form', 'exact', 'structure', 'particular', 'form', 'determine', 'examinee', 'different', 'form', 'administer', 'canon', 'good', 'testing', 'practice', 'require', 'form', 'equate', 'adjust', 'difference', 'difficulty', 'item', 'choose', 'examinee', 'traditional', 'equate', 'procedure', 'strictly', 'apply', 'nonignorable', 'nature', 'miss', 'response', 'article', 'examine', 'comparability', 'score', 'test', 'IRT', 'framework', 'illustrate', 'approach', 'datum', 'College', 'Boards', 'Advanced', 'Placement', 'Test', 'Chemistry', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['compare', 'Scores', 'test', 'Forms', 'construct', 'Examinees', 'Choice']",exam consist constructed‐response item common practice allow examinee choose subset question answer procedure usually adopt limited number item complete allotted time unfairly affect examinee result de facto administration different test form exact structure particular form determine examinee different form administer canon good testing practice require form equate adjust difference difficulty item choose examinee traditional equate procedure strictly apply nonignorable nature miss response article examine comparability score test IRT framework illustrate approach datum College Boards Advanced Placement Test Chemistry Copyright © 1994 Wiley Blackwell right reserve,compare Scores test Forms construct Examinees Choice,0.023811077748492594,0.9051171458025171,0.023740769561079695,0.023339956508777404,0.023991050379133363,0.05152614902560174,0.08474378774071985,0.0,0.0,0.0
Zimmerman D.W.,A Note on Interpretation of Formulas for the Reliability of Differences,1994,31,"It is widely recognized that the reliability of a difference score depends on the reliabilities of the constituent scores and their intercorrelation. Authors often use a well‐known identity to express the reliability of a difference as a function of the reliabilities of the components, assuming that the intercorrelation remains constant. This approach is misleading, because the familiar formula is a composite function in which the correlation between components is a function of reliability. An alternative formula, containing the correlation between true scores instead of the correlation between observed scores, provides more useful information and yields values that are not quite as anomalous as the ones usually obtained Copyright © 1994, Wiley Blackwell. All rights reserved",A Note on Interpretation of Formulas for the Reliability of Differences,"It is widely recognized that the reliability of a difference score depends on the reliabilities of the constituent scores and their intercorrelation. Authors often use a well‐known identity to express the reliability of a difference as a function of the reliabilities of the components, assuming that the intercorrelation remains constant. This approach is misleading, because the familiar formula is a composite function in which the correlation between components is a function of reliability. An alternative formula, containing the correlation between true scores instead of the correlation between observed scores, provides more useful information and yields values that are not quite as anomalous as the ones usually obtained Copyright © 1994, Wiley Blackwell. All rights reserved","['widely', 'recognize', 'reliability', 'difference', 'score', 'depend', 'reliability', 'constituent', 'score', 'intercorrelation', 'author', 'well‐known', 'identity', 'express', 'reliability', 'difference', 'function', 'reliability', 'component', 'assume', 'intercorrelation', 'remain', 'constant', 'approach', 'misleading', 'familiar', 'formula', 'composite', 'function', 'correlation', 'component', 'function', 'reliability', 'alternative', 'formula', 'contain', 'correlation', 'true', 'score', 'instead', 'correlation', 'observe', 'score', 'provide', 'useful', 'information', 'yield', 'value', 'anomalous', 'usually', 'obtain', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['note', 'Interpretation', 'Formulas', 'Reliability', 'difference']",widely recognize reliability difference score depend reliability constituent score intercorrelation author well‐known identity express reliability difference function reliability component assume intercorrelation remain constant approach misleading familiar formula composite function correlation component function reliability alternative formula contain correlation true score instead correlation observe score provide useful information yield value anomalous usually obtain Copyright © 1994 Wiley Blackwell right reserve,note Interpretation Formulas Reliability difference,0.030676286678485074,0.03074466552540125,0.8774432230201841,0.030353886969338,0.03078193780659153,0.0,0.020119972311637296,0.01020949276791082,0.10341945865369896,0.0049684644724802395
Bridgeman B.,A Comparison of Quantitative Questions in Open‐Ended and Multiple‐Choice Formats,1992,29,"Open–ended counterparts to a set of items from the quantitative section of the Graduate Record Examination (GRE–Q) were developed. Examinees responded to these items by gridding a numerical answer on a machine‐readable answer sheet or by typing on a computer. The test section with the special answer sheets was administered at the end of a regular GRE administration. Test forms were spiraled so that random groups received either the grid‐in questions or the same questions in a multiple‐choice format. In a separate data collection effort, 364 paid volunteers who had recently taken the GRE used a computer keyboard to enter answers to the same set of questions. Despite substantial format differences noted for individual items, total scores for the multiple‐choice and open‐ended tests demonstrated remarkably similar correlational patterns. There were no significant interactions of test format with either gender or ethnicity. Copyright © 1992, Wiley Blackwell. All rights reserved",A Comparison of Quantitative Questions in Open‐Ended and Multiple‐Choice Formats,"Open–ended counterparts to a set of items from the quantitative section of the Graduate Record Examination (GRE–Q) were developed. Examinees responded to these items by gridding a numerical answer on a machine‐readable answer sheet or by typing on a computer. The test section with the special answer sheets was administered at the end of a regular GRE administration. Test forms were spiraled so that random groups received either the grid‐in questions or the same questions in a multiple‐choice format. In a separate data collection effort, 364 paid volunteers who had recently taken the GRE used a computer keyboard to enter answers to the same set of questions. Despite substantial format differences noted for individual items, total scores for the multiple‐choice and open‐ended tests demonstrated remarkably similar correlational patterns. There were no significant interactions of test format with either gender or ethnicity. Copyright © 1992, Wiley Blackwell. All rights reserved","['open', '–', 'end', 'counterpart', 'set', 'item', 'quantitative', 'section', 'Graduate', 'Record', 'Examination', 'GRE', '–', 'Q', 'develop', 'examinee', 'respond', 'item', 'gridde', 'numerical', 'answer', 'machine‐readable', 'answer', 'sheet', 'type', 'computer', 'test', 'section', 'special', 'answer', 'sheet', 'administer', 'end', 'regular', 'GRE', 'administration', 'test', 'form', 'spiral', 'random', 'group', 'receive', 'grid‐in', 'question', 'question', 'multiple‐choice', 'format', 'separate', 'datum', 'collection', 'effort', '364', 'pay', 'volunteer', 'recently', 'GRE', 'computer', 'keyboard', 'enter', 'answer', 'set', 'question', 'despite', 'substantial', 'format', 'difference', 'note', 'individual', 'item', 'total', 'score', 'multiple‐choice', 'open‐ende', 'test', 'demonstrate', 'remarkably', 'similar', 'correlational', 'pattern', 'significant', 'interaction', 'test', 'format', 'gender', 'ethnicity', 'Copyright', '©', '1992', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Quantitative', 'Questions', 'Open‐Ended', 'Multiple‐Choice', 'Formats']",open – end counterpart set item quantitative section Graduate Record Examination GRE – Q develop examinee respond item gridde numerical answer machine‐readable answer sheet type computer test section special answer sheet administer end regular GRE administration test form spiral random group receive grid‐in question question multiple‐choice format separate datum collection effort 364 pay volunteer recently GRE computer keyboard enter answer set question despite substantial format difference note individual item total score multiple‐choice open‐ende test demonstrate remarkably similar correlational pattern significant interaction test format gender ethnicity Copyright © 1992 Wiley Blackwell right reserve,Comparison Quantitative Questions Open‐Ended Multiple‐Choice Formats,0.023931654267295665,0.9041024111066616,0.02402381528154736,0.023788148009972886,0.024153971334522448,0.0,0.12465714784454032,0.0,0.0023550643483818447,0.0
Williamson G.L.; Appelbaum M.; Epanchin A.,Longitudinal Analyses of Academic Achievement,1991,28,"Mathematical models of individual growth are the basis for analyzing academic achievement over time. This study demonstrates that much can be learned about academic growth from the analysis of individual growth curves. In addition, we illustrate the aggregation of individual responses to provide descriptions of institutional growth. Although more efficient statistical methods are available, the simple approach taken here serves to demonstrate the logic and approach to analyzing longitudinal data. Copyright © 1991, Wiley Blackwell. All rights reserved",,"Mathematical models of individual growth are the basis for analyzing academic achievement over time. This study demonstrates that much can be learned about academic growth from the analysis of individual growth curves. In addition, we illustrate the aggregation of individual responses to provide descriptions of institutional growth. Although more efficient statistical methods are available, the simple approach taken here serves to demonstrate the logic and approach to analyzing longitudinal data. Copyright © 1991, Wiley Blackwell. All rights reserved","['mathematical', 'individual', 'growth', 'basis', 'analyze', 'academic', 'achievement', 'time', 'study', 'demonstrate', 'learn', 'academic', 'growth', 'analysis', 'individual', 'growth', 'curve', 'addition', 'illustrate', 'aggregation', 'individual', 'response', 'provide', 'description', 'institutional', 'growth', 'efficient', 'statistical', 'method', 'available', 'simple', 'approach', 'serve', 'demonstrate', 'logic', 'approach', 'analyze', 'longitudinal', 'datum', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']",,mathematical individual growth basis analyze academic achievement time study demonstrate learn academic growth analysis individual growth curve addition illustrate aggregation individual response provide description institutional growth efficient statistical method available simple approach serve demonstrate logic approach analyze longitudinal datum Copyright © 1991 Wiley Blackwell right reserve,,0.03453364426887412,0.034766723188666235,0.03477869118865726,0.6646829314872775,0.2312380098665248,0.0,0.02004841863270237,0.0022500324725005403,0.0,0.0613085909061587
Zwick R.; Donoghue J.R.; Grima A.,Assessment of Differential Item Functioning for Performance Tasks,1993,30,"Although the belief has been expressed that performance assessments are intrinsically more fair than multiple‐choice measures, some forms of performance assessment may in fact be more likely than conventional tests to tap construct‐irrelevant factors. The assessment of differential item functioning (DIF) can be helpful in investigating the effect on subpopulations of the introduction of performance tasks. In this study, two extensions of the Mantel‐Haenszel (MH; 1959) procedure that may be useful in assessing DIP in performance measures were explored. The test of conditional association proposed by Mantel (1963) seems promising as a test of DIF for pofytomous items when the primary interest is in the between‐group difference in item means, conditional on some measure of ability. The generalized statistic proposed by Mantel and Haenszel may be more useful than Mantel's test when the entire response distributions of the groups are of interest. Simulation results showed that, for both inferential procedures, the studied item should be included in the matching variable, as in the dichotomous case. Descriptive statistics that index the magnitude of DIP were also investigated. Copyright © 1993, Wiley Blackwell. All rights reserved",Assessment of Differential Item Functioning for Performance Tasks,"Although the belief has been expressed that performance assessments are intrinsically more fair than multiple‐choice measures, some forms of performance assessment may in fact be more likely than conventional tests to tap construct‐irrelevant factors. The assessment of differential item functioning (DIF) can be helpful in investigating the effect on subpopulations of the introduction of performance tasks. In this study, two extensions of the Mantel‐Haenszel (MH; 1959) procedure that may be useful in assessing DIP in performance measures were explored. The test of conditional association proposed by Mantel (1963) seems promising as a test of DIF for pofytomous items when the primary interest is in the between‐group difference in item means, conditional on some measure of ability. The generalized statistic proposed by Mantel and Haenszel may be more useful than Mantel's test when the entire response distributions of the groups are of interest. Simulation results showed that, for both inferential procedures, the studied item should be included in the matching variable, as in the dichotomous case. Descriptive statistics that index the magnitude of DIP were also investigated. Copyright © 1993, Wiley Blackwell. All rights reserved","['belief', 'express', 'performance', 'assessment', 'intrinsically', 'fair', 'multiple‐choice', 'measure', 'form', 'performance', 'assessment', 'fact', 'likely', 'conventional', 'test', 'tap', 'construct‐irrelevant', 'factor', 'assessment', 'differential', 'item', 'function', 'DIF', 'helpful', 'investigate', 'effect', 'subpopulation', 'introduction', 'performance', 'task', 'study', 'extension', 'Mantel‐Haenszel', 'MH', '1959', 'procedure', 'useful', 'assess', 'DIP', 'performance', 'measure', 'explore', 'test', 'conditional', 'association', 'propose', 'Mantel', '1963', 'promise', 'test', 'DIF', 'pofytomous', 'item', 'primary', 'interest', 'between‐group', 'difference', 'item', 'mean', 'conditional', 'measure', 'ability', 'generalized', 'statistic', 'propose', 'Mantel', 'Haenszel', 'useful', 'Mantels', 'test', 'entire', 'response', 'distribution', 'group', 'interest', 'Simulation', 'result', 'inferential', 'procedure', 'study', 'item', 'include', 'matching', 'variable', 'dichotomous', 'case', 'descriptive', 'statistic', 'index', 'magnitude', 'DIP', 'investigate', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['assessment', 'Differential', 'Item', 'Functioning', 'Performance', 'task']",belief express performance assessment intrinsically fair multiple‐choice measure form performance assessment fact likely conventional test tap construct‐irrelevant factor assessment differential item function DIF helpful investigate effect subpopulation introduction performance task study extension Mantel‐Haenszel MH 1959 procedure useful assess DIP performance measure explore test conditional association propose Mantel 1963 promise test DIF pofytomous item primary interest between‐group difference item mean conditional measure ability generalized statistic propose Mantel Haenszel useful Mantels test entire response distribution group interest Simulation result inferential procedure study item include matching variable dichotomous case descriptive statistic index magnitude DIP investigate Copyright © 1993 Wiley Blackwell right reserve,assessment Differential Item Functioning Performance task,0.023533081744049868,0.023601406807603073,0.023370078975250123,0.3565846105490418,0.5729108219240551,0.008664636654988662,0.028424183799228824,0.08293580772894617,0.012688038800985735,0.057350364203032425
Gressard R.P.; Loyd B.H.,A Comparison of Item Sampling Plans in the Application of Multiple Matrix Sampling,1991,28,"This study used a Monte Carlo approach to investigate the effect of item sampling by item stratification on parameter estimation when applying multiple matrix sampling to achievement data. From the results of this study it was concluded that the item sampling method and sampling plan which is a practical compromise in terms of precision and sample size is one based on item stratification by item discrimination and a sampling plan with a moderate number of subtests. This sampling condition provides reasonable precision of the mean and variance estimates but requires only a moderately sized sample. Copyright © 1991, Wiley Blackwell. All rights reserved",A Comparison of Item Sampling Plans in the Application of Multiple Matrix Sampling,"This study used a Monte Carlo approach to investigate the effect of item sampling by item stratification on parameter estimation when applying multiple matrix sampling to achievement data. From the results of this study it was concluded that the item sampling method and sampling plan which is a practical compromise in terms of precision and sample size is one based on item stratification by item discrimination and a sampling plan with a moderate number of subtests. This sampling condition provides reasonable precision of the mean and variance estimates but requires only a moderately sized sample. Copyright © 1991, Wiley Blackwell. All rights reserved","['study', 'Monte', 'Carlo', 'approach', 'investigate', 'effect', 'item', 'sample', 'item', 'stratification', 'parameter', 'estimation', 'apply', 'multiple', 'matrix', 'sample', 'achievement', 'datum', 'result', 'study', 'conclude', 'item', 'sample', 'method', 'sample', 'plan', 'practical', 'compromise', 'term', 'precision', 'sample', 'size', 'base', 'item', 'stratification', 'item', 'discrimination', 'sample', 'plan', 'moderate', 'number', 'subtest', 'sample', 'condition', 'provide', 'reasonable', 'precision', 'mean', 'variance', 'estimate', 'require', 'moderately', 'sized', 'sample', 'Copyright', '©', '1991', 'Wiley', 'Blackwell', 'right', 'reserve']","['Comparison', 'Item', 'Sampling', 'plan', 'Application', 'Multiple', 'Matrix', 'Sampling']",study Monte Carlo approach investigate effect item sample item stratification parameter estimation apply multiple matrix sample achievement datum result study conclude item sample method sample plan practical compromise term precision sample size base item stratification item discrimination sample plan moderate number subtest sample condition provide reasonable precision mean variance estimate require moderately sized sample Copyright © 1991 Wiley Blackwell right reserve,Comparison Item Sampling plan Application Multiple Matrix Sampling,0.8721406351266839,0.03191504649097852,0.03191936461732474,0.03141156774413273,0.03261338602088012,0.10190927461614818,0.0,0.0,0.0,0.01839366870732658
Cizek G.J.,Reconsidering Standards and Criteria,1993,30,"An early debate about the nature of setting standards on educational achievement tests centered on the extent to which resulting standards were arbitrary. Subsequent research in the area has advanced solutions to many practical standard setting problems, but the more fundamental issue regarding the empirical grounding of judgmental standard setting procedures has remained unresolved and largely unaddressed. This article reviews some of the salient elements of the debate about the nature of standard setting on educational assessments and suggests that the dispute can never be satisfactorily resolved within the current paradigm. A reconcep‐tualization of the nature of standard setting is proposed, and suggestions for future research are provided. Copyright © 1993, Wiley Blackwell. All rights reserved",,"An early debate about the nature of setting standards on educational achievement tests centered on the extent to which resulting standards were arbitrary. Subsequent research in the area has advanced solutions to many practical standard setting problems, but the more fundamental issue regarding the empirical grounding of judgmental standard setting procedures has remained unresolved and largely unaddressed. This article reviews some of the salient elements of the debate about the nature of standard setting on educational assessments and suggests that the dispute can never be satisfactorily resolved within the current paradigm. A reconcep‐tualization of the nature of standard setting is proposed, and suggestions for future research are provided. Copyright © 1993, Wiley Blackwell. All rights reserved","['early', 'debate', 'nature', 'set', 'standard', 'educational', 'achievement', 'test', 'center', 'extent', 'result', 'standard', 'arbitrary', 'subsequent', 'research', 'area', 'advanced', 'solution', 'practical', 'standard', 'setting', 'problem', 'fundamental', 'issue', 'regard', 'empirical', 'grounding', 'judgmental', 'standard', 'set', 'procedure', 'remain', 'unresolved', 'largely', 'unaddresse', 'article', 'review', 'salient', 'element', 'debate', 'nature', 'standard', 'setting', 'educational', 'assessment', 'suggest', 'dispute', 'satisfactorily', 'resolve', 'current', 'paradigm', 'A', 'reconcep‐tualization', 'nature', 'standard', 'setting', 'propose', 'suggestion', 'future', 'research', 'provide', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,early debate nature set standard educational achievement test center extent result standard arbitrary subsequent research area advanced solution practical standard setting problem fundamental issue regard empirical grounding judgmental standard set procedure remain unresolved largely unaddresse article review salient element debate nature standard setting educational assessment suggest dispute satisfactorily resolve current paradigm A reconcep‐tualization nature standard setting propose suggestion future research provide Copyright © 1993 Wiley Blackwell right reserve,,0.028528752707524255,0.028434124872751834,0.8857720034454605,0.028320659574008207,0.02894445940025519,0.005970880852448638,0.015370278923930114,1.549874158314821e-05,0.0689992116694334,0.023047416101485637
Budescu D.; Bar‐Hillel M.,To Guess or Not to Guess: A Decision‐Theoretic View of Formula Scoring,1993,30,"Multiple‐choice tests are often scored by formulas under which the respondent's expected score for an item is the same whether he or she omits it or guesses at random. Typically, these formulas are accompanied by instructions that discourage guessing. In this article, we look at test taking from the normative and descriptive perspectives of judgment and decision theory. We show that for a rational test taker, whose goal is the maximization of expected score, answering is either superior or equivalent to omitting–a fact which follows from the scoring formula. For test takers who are not fully rational, or have goals other than the maximization of expected score, it is very hard to give adequate formula scoring instructions, and even the recommendation to answer under partial knowledge is problematic (though generally beneficial). Our analysis derives from a critical look at standard assumptions about the epistemic states, response strategies, and strategic motivations of test takers. In conclusion, we endorse the number‐right scoring rule, which discourages omissions and is robust against variability in respondent motivations, limitations in judgments of uncertainty, and item vagaries. Copyright © 1993, Wiley Blackwell. All rights reserved",To Guess or Not to Guess: A Decision‐Theoretic View of Formula Scoring,"Multiple‐choice tests are often scored by formulas under which the respondent's expected score for an item is the same whether he or she omits it or guesses at random. Typically, these formulas are accompanied by instructions that discourage guessing. In this article, we look at test taking from the normative and descriptive perspectives of judgment and decision theory. We show that for a rational test taker, whose goal is the maximization of expected score, answering is either superior or equivalent to omitting–a fact which follows from the scoring formula. For test takers who are not fully rational, or have goals other than the maximization of expected score, it is very hard to give adequate formula scoring instructions, and even the recommendation to answer under partial knowledge is problematic (though generally beneficial). Our analysis derives from a critical look at standard assumptions about the epistemic states, response strategies, and strategic motivations of test takers. In conclusion, we endorse the number‐right scoring rule, which discourages omissions and is robust against variability in respondent motivations, limitations in judgments of uncertainty, and item vagaries. Copyright © 1993, Wiley Blackwell. All rights reserved","['multiple‐choice', 'test', 'score', 'formula', 'respondent', 'expect', 'score', 'item', 'omit', 'guess', 'random', 'typically', 'formula', 'accompany', 'instruction', 'discourage', 'guess', 'article', 'look', 'test', 'normative', 'descriptive', 'perspective', 'judgment', 'decision', 'theory', 'rational', 'test', 'taker', 'goal', 'maximization', 'expect', 'score', 'answer', 'superior', 'equivalent', 'omitting', '–', 'fact', 'follow', 'scoring', 'formula', 'test', 'taker', 'fully', 'rational', 'goal', 'maximization', 'expect', 'score', 'hard', 'adequate', 'formula', 'scoring', 'instruction', 'recommendation', 'answer', 'partial', 'knowledge', 'problematic', 'generally', 'beneficial', 'analysis', 'derive', 'critical', 'look', 'standard', 'assumption', 'epistemic', 'state', 'response', 'strategy', 'strategic', 'motivation', 'test', 'taker', 'conclusion', 'endorse', 'number‐right', 'scoring', 'rule', 'discourage', 'omission', 'robust', 'variability', 'respondent', 'motivation', 'limitation', 'judgment', 'uncertainty', 'item', 'vagary', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['guess', 'guess', 'decision‐theoretic', 'view', 'Formula', 'Scoring']",multiple‐choice test score formula respondent expect score item omit guess random typically formula accompany instruction discourage guess article look test normative descriptive perspective judgment decision theory rational test taker goal maximization expect score answer superior equivalent omitting – fact follow scoring formula test taker fully rational goal maximization expect score hard adequate formula scoring instruction recommendation answer partial knowledge problematic generally beneficial analysis derive critical look standard assumption epistemic state response strategy strategic motivation test taker conclusion endorse number‐right scoring rule discourage omission robust variability respondent motivation limitation judgment uncertainty item vagary Copyright © 1993 Wiley Blackwell right reserve,guess guess decision‐theoretic view Formula Scoring,0.02464688615321151,0.9016005130806306,0.024713774608941683,0.024353186003855467,0.02468564015336088,0.0,0.060858357574464526,0.0,0.0639136285300889,0.0
Shavelson R.J.; Baxter G.P.; Gao X.,Sampling Variability of Performance Assessments,1993,30,"In this article, performance assessments are cast within a sampling framework. More specifically, a performance assessment is viewed as a sample of student performance drawn from a complex universe defined by a combination of all possible tasks, occasions, raters, and measurement methods. Using generalizability theory, we present evidence bearing on the generalizability and convergent validity of performance assessments sampled from a range of measurement facets and measurement methods. Results at both the individual and school level indicate that task‐sampling variability is the major source ofmeasurment error. Large numbers of tasks are needed to get a reliable measure of mathematics and science achievement at the elementary level. With respect to convergent validity, results suggest that methods do not converge. Students' performance scores, then, are dependent on both the task and method sampled. Copyright © 1993, Wiley Blackwell. All rights reserved",,"In this article, performance assessments are cast within a sampling framework. More specifically, a performance assessment is viewed as a sample of student performance drawn from a complex universe defined by a combination of all possible tasks, occasions, raters, and measurement methods. Using generalizability theory, we present evidence bearing on the generalizability and convergent validity of performance assessments sampled from a range of measurement facets and measurement methods. Results at both the individual and school level indicate that task‐sampling variability is the major source ofmeasurment error. Large numbers of tasks are needed to get a reliable measure of mathematics and science achievement at the elementary level. With respect to convergent validity, results suggest that methods do not converge. Students' performance scores, then, are dependent on both the task and method sampled. Copyright © 1993, Wiley Blackwell. All rights reserved","['article', 'performance', 'assessment', 'cast', 'sample', 'framework', 'specifically', 'performance', 'assessment', 'view', 'sample', 'student', 'performance', 'draw', 'complex', 'universe', 'define', 'combination', 'possible', 'task', 'occasion', 'rater', 'method', 'generalizability', 'theory', 'present', 'evidence', 'bear', 'generalizability', 'convergent', 'validity', 'performance', 'assessment', 'sample', 'range', 'facet', 'method', 'result', 'individual', 'school', 'level', 'indicate', 'task‐sample', 'variability', 'major', 'source', 'ofmeasurment', 'error', 'large', 'number', 'task', 'need', 'reliable', 'measure', 'mathematic', 'science', 'achievement', 'elementary', 'level', 'respect', 'convergent', 'validity', 'result', 'suggest', 'method', 'converge', 'Students', 'performance', 'score', 'dependent', 'task', 'method', 'sample', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,article performance assessment cast sample framework specifically performance assessment view sample student performance draw complex universe define combination possible task occasion rater method generalizability theory present evidence bear generalizability convergent validity performance assessment sample range facet method result individual school level indicate task‐sample variability major source ofmeasurment error large number task need reliable measure mathematic science achievement elementary level respect convergent validity result suggest method converge Students performance score dependent task method sample Copyright © 1993 Wiley Blackwell right reserve,,0.02580974190683098,0.025909616610519014,0.025813859105611982,0.025555129227706103,0.8969116531493319,0.010138842990896417,0.0,0.0,0.012330131752387874,0.17744085974074167
Ruiz‐Primo M.A.; Baxter G.P.; Shavelson R.J.,On the Stability of Performance Assessments,1993,30,"This study examined the stability of scores on two types of performance assessments, an observed hands‐on investigation and a notebook surrogate. Twenty‐nine sixth‐grade students in a hands‐on inquiry‐based science curriculum completed three investigations on two occasions separated by 5 months. Results indicated that: (a) the generalizability across occasions for relative decisions was, on average, moderate for the observed investigations (.52) and the notebooks (.50); (b) the generalizability for absolute decisions was only slightly lower; (c) the major source of measurement error was the person by occasion (residual) interaction; and (d) the procedures students used to carry out the investigations tended to change from one occasion to the other. Copyright © 1993, Wiley Blackwell. All rights reserved",,"This study examined the stability of scores on two types of performance assessments, an observed hands‐on investigation and a notebook surrogate. Twenty‐nine sixth‐grade students in a hands‐on inquiry‐based science curriculum completed three investigations on two occasions separated by 5 months. Results indicated that: (a) the generalizability across occasions for relative decisions was, on average, moderate for the observed investigations (.52) and the notebooks (.50); (b) the generalizability for absolute decisions was only slightly lower; (c) the major source of measurement error was the person by occasion (residual) interaction; and (d) the procedures students used to carry out the investigations tended to change from one occasion to the other. Copyright © 1993, Wiley Blackwell. All rights reserved","['study', 'examine', 'stability', 'score', 'type', 'performance', 'assessment', 'observed', 'hands‐on', 'investigation', 'notebook', 'surrogate', 'twenty‐nine', 'sixth‐grade', 'student', 'hands‐on', 'inquiry‐base', 'science', 'curriculum', 'complete', 'investigation', 'occasion', 'separate', '5', 'month', 'result', 'indicate', 'generalizability', 'occasion', 'relative', 'decision', 'average', 'moderate', 'observe', 'investigation', '52', 'notebook', '50', 'b', 'generalizability', 'absolute', 'decision', 'slightly', 'low', 'c', 'major', 'source', 'error', 'person', 'occasion', 'residual', 'interaction', 'd', 'procedure', 'student', 'carry', 'investigation', 'tend', 'change', 'occasion', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']",,study examine stability score type performance assessment observed hands‐on investigation notebook surrogate twenty‐nine sixth‐grade student hands‐on inquiry‐base science curriculum complete investigation occasion separate 5 month result indicate generalizability occasion relative decision average moderate observe investigation 52 notebook 50 b generalizability absolute decision slightly low c major source error person occasion residual interaction d procedure student carry investigation tend change occasion Copyright © 1993 Wiley Blackwell right reserve,,0.028563014745685542,0.028854422691649142,0.885053454949002,0.028254324792723464,0.029274782820939777,0.0,0.0,0.0,0.012814240282073907,0.10315148713109647
Bridgeman B.; Lewis C.,The Relationship of Essay and Multiple‐Choice Scores With Grades in College Courses,1994,31,"Essay and multiple‐choice scores from Advanced Placement (AP) examinations in American History, European History, English Language and Composition, and Biology were matched with freshman grades in a sample of 32 colleges. Multiple‐choice scores from the American History and Biology examinations were more highly correlated with freshman grade point averages than were essay scores from the same examinations, but essay scores were essentially equivalent to multiple‐choice scores in correlations with course grades in history, English, and biology. In history courses, men and women received comparable grades and had nearly equal scores on the AP essays, but the multiple‐choice scores of men were nearly one half of a standard deviation higher than the scores of women. Copyright © 1994, Wiley Blackwell. All rights reserved",The Relationship of Essay and Multiple‐Choice Scores With Grades in College Courses,"Essay and multiple‐choice scores from Advanced Placement (AP) examinations in American History, European History, English Language and Composition, and Biology were matched with freshman grades in a sample of 32 colleges. Multiple‐choice scores from the American History and Biology examinations were more highly correlated with freshman grade point averages than were essay scores from the same examinations, but essay scores were essentially equivalent to multiple‐choice scores in correlations with course grades in history, English, and biology. In history courses, men and women received comparable grades and had nearly equal scores on the AP essays, but the multiple‐choice scores of men were nearly one half of a standard deviation higher than the scores of women. Copyright © 1994, Wiley Blackwell. All rights reserved","['essay', 'multiple‐choice', 'score', 'Advanced', 'Placement', 'AP', 'examination', 'american', 'History', 'European', 'History', 'English', 'Language', 'Composition', 'Biology', 'match', 'freshman', 'grade', 'sample', '32', 'college', 'multiple‐choice', 'score', 'american', 'History', 'Biology', 'examination', 'highly', 'correlate', 'freshman', 'grade', 'point', 'average', 'essay', 'score', 'examination', 'essay', 'score', 'essentially', 'equivalent', 'multiple‐choice', 'score', 'correlation', 'course', 'grade', 'history', 'English', 'biology', 'history', 'course', 'man', 'woman', 'receive', 'comparable', 'grade', 'nearly', 'equal', 'score', 'AP', 'essay', 'multiple‐choice', 'score', 'man', 'nearly', 'half', 'standard', 'deviation', 'high', 'score', 'woman', 'Copyright', '©', '1994', 'Wiley', 'Blackwell', 'right', 'reserve']","['Relationship', 'Essay', 'Multiple‐Choice', 'Scores', 'Grades', 'College', 'Courses']",essay multiple‐choice score Advanced Placement AP examination american History European History English Language Composition Biology match freshman grade sample 32 college multiple‐choice score american History Biology examination highly correlate freshman grade point average essay score examination essay score essentially equivalent multiple‐choice score correlation course grade history English biology history course man woman receive comparable grade nearly equal score AP essay multiple‐choice score man nearly half standard deviation high score woman Copyright © 1994 Wiley Blackwell right reserve,Relationship Essay Multiple‐Choice Scores Grades College Courses,0.03215807541246257,0.8704232145367061,0.032395438136777824,0.03229134373638814,0.03273192817766515,0.0,0.0,0.0,0.10850329691244526,0.040408302813943385
Nandakumar R.,Simultaneous DIF Amplification and Cancellation: Shealy‐Stout's Test for DIF,1993,30,"The present study investigates the phenomena of simultaneous DIF amplification and cancellation and SIBTEST's role in detecting such. A variety of simulated test data were generated for this purpose. In addition, real test data from various sources were analyzed. The results from both simulated and real test data, as Sheafy and Stout's theory (1993a, 1993b) suggests, show that the SIBTEST is effective in assessing DIF amplification and cancellation (partially or fully) at the test score level. Finally, methodological and substantive implications of DIF amplification and cancellation are discussed. Copyright © 1993, Wiley Blackwell. All rights reserved",Simultaneous DIF Amplification and Cancellation: Shealy‐Stout's Test for DIF,"The present study investigates the phenomena of simultaneous DIF amplification and cancellation and SIBTEST's role in detecting such. A variety of simulated test data were generated for this purpose. In addition, real test data from various sources were analyzed. The results from both simulated and real test data, as Sheafy and Stout's theory (1993a, 1993b) suggests, show that the SIBTEST is effective in assessing DIF amplification and cancellation (partially or fully) at the test score level. Finally, methodological and substantive implications of DIF amplification and cancellation are discussed. Copyright © 1993, Wiley Blackwell. All rights reserved","['present', 'study', 'investigate', 'phenomenon', 'simultaneous', 'dif', 'amplification', 'cancellation', 'sibtest', 'role', 'detect', 'variety', 'simulate', 'test', 'datum', 'generate', 'purpose', 'addition', 'real', 'test', 'datum', 'source', 'analyze', 'result', 'simulated', 'real', 'test', 'datum', 'Sheafy', 'Stouts', 'theory', '1993a', '1993b', 'suggest', 'SIBTEST', 'effective', 'assess', 'DIF', 'amplification', 'cancellation', 'partially', 'fully', 'test', 'score', 'level', 'finally', 'methodological', 'substantive', 'implication', 'dif', 'amplification', 'cancellation', 'discuss', 'Copyright', '©', '1993', 'Wiley', 'Blackwell', 'right', 'reserve']","['simultaneous', 'DIF', 'Amplification', 'Cancellation', 'Shealy‐Stouts', 'Test', 'DIF']",present study investigate phenomenon simultaneous dif amplification cancellation sibtest role detect variety simulate test datum generate purpose addition real test datum source analyze result simulated real test datum Sheafy Stouts theory 1993a 1993b suggest SIBTEST effective assess DIF amplification cancellation partially fully test score level finally methodological substantive implication dif amplification cancellation discuss Copyright © 1993 Wiley Blackwell right reserve,simultaneous DIF Amplification Cancellation Shealy‐Stouts Test DIF,0.8748171977060208,0.03128079254036596,0.031190668506039625,0.03090629611807856,0.031805045129495016,0.0,0.0031037222291080886,0.09929280306116121,0.01749509504133533,0.014426195713824077
Angoff W.H.; Johnson E.G.,The Differential Impact of Curriculum on Aptitude Test Scores,1990,27,"A sample of 22, 923 students who had taken the SAT and the GRE General Test was classified by the four general undergraduate fields of study and by sex. The authors performed several analyses to determine the degree of differential impact that sex and field of study might have on GRE‐Verbal, GRE‐Quantitative, and GRE‐Analytical scores after controlling on SAT‐Verbal and SAT‐Mathematical scores. They found, first, that the correlations of SAT‐Verbal with GRE‐Verbal scores and SAT‐Mathematical with GRE‐Quantitative scores were extremely high, .86 in the total sample and ranging from the low to middle .80s in the eight subgroups. The impact of curriculum and sex, after controlling on SAT scores, was found to be low on GRE‐ Verbal scores but relatively high on GRE‐Quantitative scores, with students in heavily quantitative fields enjoying an advantage over their peers in less quantitative fields of study. The impact was moderate on GRE‐Analytical scores. Further studies designed to “purify” the fields of study and include only clearly verbal fields and clearly mathematical fields showed small additional impact. An additional study indicated a generally slight effect of the institution attended on GRE‐Quantitative scores after controlling for sex, major field of study, and initial ability. Copyright © 1990, Wiley Blackwell. All rights reserved",The Differential Impact of Curriculum on Aptitude Test Scores,"A sample of 22, 923 students who had taken the SAT and the GRE General Test was classified by the four general undergraduate fields of study and by sex. The authors performed several analyses to determine the degree of differential impact that sex and field of study might have on GRE‐Verbal, GRE‐Quantitative, and GRE‐Analytical scores after controlling on SAT‐Verbal and SAT‐Mathematical scores. They found, first, that the correlations of SAT‐Verbal with GRE‐Verbal scores and SAT‐Mathematical with GRE‐Quantitative scores were extremely high, .86 in the total sample and ranging from the low to middle .80s in the eight subgroups. The impact of curriculum and sex, after controlling on SAT scores, was found to be low on GRE‐ Verbal scores but relatively high on GRE‐Quantitative scores, with students in heavily quantitative fields enjoying an advantage over their peers in less quantitative fields of study. The impact was moderate on GRE‐Analytical scores. Further studies designed to “purify” the fields of study and include only clearly verbal fields and clearly mathematical fields showed small additional impact. An additional study indicated a generally slight effect of the institution attended on GRE‐Quantitative scores after controlling for sex, major field of study, and initial ability. Copyright © 1990, Wiley Blackwell. All rights reserved","['sample', '22', '923', 'student', 'SAT', 'GRE', 'General', 'Test', 'classify', 'general', 'undergraduate', 'field', 'study', 'sex', 'author', 'perform', 'analysis', 'determine', 'degree', 'differential', 'impact', 'sex', 'field', 'study', 'GRE‐Verbal', 'GRE‐Quantitative', 'GRE‐Analytical', 'score', 'control', 'sat‐verbal', 'sat‐mathematical', 'score', 'find', 'correlation', 'sat‐verbal', 'gre‐verbal', 'score', 'sat‐mathematical', 'gre‐quantitative', 'score', 'extremely', 'high', '86', 'total', 'sample', 'range', 'low', 'middle', '80', 'subgroup', 'impact', 'curriculum', 'sex', 'control', 'SAT', 'score', 'find', 'low', 'GRE‐', 'Verbal', 'score', 'relatively', 'high', 'gre‐quantitative', 'score', 'student', 'heavily', 'quantitative', 'field', 'enjoy', 'advantage', 'peer', 'quantitative', 'field', 'study', 'impact', 'moderate', 'GRE‐Analytical', 'score', 'study', 'design', '""', 'purify', '""', 'field', 'study', 'include', 'clearly', 'verbal', 'field', 'clearly', 'mathematical', 'field', 'small', 'additional', 'impact', 'additional', 'study', 'indicate', 'generally', 'slight', 'effect', 'institution', 'attend', 'gre‐quantitative', 'score', 'control', 'sex', 'major', 'field', 'study', 'initial', 'ability', 'copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Differential', 'Impact', 'Curriculum', 'Aptitude', 'Test', 'Scores']","sample 22 923 student SAT GRE General Test classify general undergraduate field study sex author perform analysis determine degree differential impact sex field study GRE‐Verbal GRE‐Quantitative GRE‐Analytical score control sat‐verbal sat‐mathematical score find correlation sat‐verbal gre‐verbal score sat‐mathematical gre‐quantitative score extremely high 86 total sample range low middle 80 subgroup impact curriculum sex control SAT score find low GRE‐ Verbal score relatively high gre‐quantitative score student heavily quantitative field enjoy advantage peer quantitative field study impact moderate GRE‐Analytical score study design "" purify "" field study include clearly verbal field clearly mathematical field small additional impact additional study indicate generally slight effect institution attend gre‐quantitative score control sex major field study initial ability copyright © 1990 Wiley Blackwell right reserve",Differential Impact Curriculum Aptitude Test Scores,0.028014740432373033,0.02823074792863982,0.02799105166472665,0.027724574791442573,0.8880388851828179,0.00027758347558104734,0.020401531625024526,0.000681051371443698,0.05999221845058392,0.03550735685637605
De Ayala R.J.; Dodd B.G.; Koch W.R.,A Simulation and Comparison of Flexilevel and Bayesian Computerized Adaptive Testing,1990,27,"Computerized adaptive testing (CAT) is a testing procedure that adapts an examination to an examinee's ability by administering only items of appropriate difficulty for the examinee. In this study, the authors compared Lord's flexilevel testing procedure (flexilevel CAT) with an item response theory‐based CAT using Bayesian estimation of ability (Bayesian CAT). Three flexilevel CATs, which differed in test length (36, 18, and 11 items), and three Bayesian CATs were simulated; the Bayesian CATs differed from one another in the standard error of estimate (SEE) used for terminating the test (0.25, 0.10, and 0.05). Results showed that the flexilevel 36‐ and 18‐item CATs produced ability estimates that may be considered as accurate as those of the Bayesian CAT with SEE = 0.10 and comparable to the Bayesian CAT with SEE = 0.05. The authors discuss the implications for classroom testing and for item response theory‐based CAT. Copyright © 1990, Wiley Blackwell. All rights reserved",A Simulation and Comparison of Flexilevel and Bayesian Computerized Adaptive Testing,"Computerized adaptive testing (CAT) is a testing procedure that adapts an examination to an examinee's ability by administering only items of appropriate difficulty for the examinee. In this study, the authors compared Lord's flexilevel testing procedure (flexilevel CAT) with an item response theory‐based CAT using Bayesian estimation of ability (Bayesian CAT). Three flexilevel CATs, which differed in test length (36, 18, and 11 items), and three Bayesian CATs were simulated; the Bayesian CATs differed from one another in the standard error of estimate (SEE) used for terminating the test (0.25, 0.10, and 0.05). Results showed that the flexilevel 36‐ and 18‐item CATs produced ability estimates that may be considered as accurate as those of the Bayesian CAT with SEE = 0.10 and comparable to the Bayesian CAT with SEE = 0.05. The authors discuss the implications for classroom testing and for item response theory‐based CAT. Copyright © 1990, Wiley Blackwell. All rights reserved","['computerized', 'adaptive', 'testing', 'CAT', 'testing', 'procedure', 'adapt', 'examination', 'examinee', 'ability', 'administer', 'item', 'appropriate', 'difficulty', 'examinee', 'study', 'author', 'compare', 'Lords', 'flexilevel', 'testing', 'procedure', 'flexilevel', 'CAT', 'item', 'response', 'theory‐base', 'CAT', 'bayesian', 'estimation', 'ability', 'Bayesian', 'CAT', 'flexilevel', 'cats', 'differ', 'test', 'length', '36', '18', '11', 'item', 'bayesian', 'cat', 'simulate', 'bayesian', 'cat', 'differ', 'standard', 'error', 'estimate', 'SEE', 'terminate', 'test', '025', '010', '005', 'result', 'flexilevel', '36‐', '18‐item', 'cats', 'produce', 'ability', 'estimate', 'consider', 'accurate', 'Bayesian', 'CAT', 'SEE', '010', 'comparable', 'Bayesian', 'CAT', 'SEE', '005', 'author', 'discuss', 'implication', 'classroom', 'testing', 'item', 'response', 'theory‐base', 'CAT', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Simulation', 'Comparison', 'Flexilevel', 'bayesian', 'Computerized', 'Adaptive', 'Testing']",computerized adaptive testing CAT testing procedure adapt examination examinee ability administer item appropriate difficulty examinee study author compare Lords flexilevel testing procedure flexilevel CAT item response theory‐base CAT bayesian estimation ability Bayesian CAT flexilevel cats differ test length 36 18 11 item bayesian cat simulate bayesian cat differ standard error estimate SEE terminate test 025 010 005 result flexilevel 36‐ 18‐item cats produce ability estimate consider accurate Bayesian CAT SEE 010 comparable Bayesian CAT SEE 005 author discuss implication classroom testing item response theory‐base CAT Copyright © 1990 Wiley Blackwell right reserve,Simulation Comparison Flexilevel bayesian Computerized Adaptive Testing,0.03585583253644214,0.03584390910774916,0.03577745251567708,0.03557050830556644,0.8569522975345651,0.04226142329510184,0.03257060498128463,0.0,0.0,0.0
Braun H.I.; Bennett R.E.; Frye D.; Soloway E.,Scoring Constructed Responses Using Expert Systems,1990,27,"The use of constructed‐response items in large scale standardized testing has been hampered by the costs and difficulties associated with obtaining reliable scores. The advent of expert systems may signal the eventual removal of this impediment. This study investigated the accuracy with which expert systems could score a new, nonmultiple‐choice item type. The item type presents a faulty solution to a computer programming problem and asks the student to correct the solution. This item type was administered to a sample of high school seniors enrolled in an Advanced Placement course in Computer Science who also took the Advanced Placement Computer Science (APCS) examination. Results indicated that the expert systems were able to produce scores for between 82% and 95% of the solutions encountered and to display high agreement with a human reader on the correctness of the solutions. Diagnoses of the specific errors produced by students were less accurate. Correlations with scores on the objective and free‐response sections of the APCS examination were moderate. Implications for additional research and for testing practice are offered. Copyright © 1990, Wiley Blackwell. All rights reserved",Scoring Constructed Responses Using Expert Systems,"The use of constructed‐response items in large scale standardized testing has been hampered by the costs and difficulties associated with obtaining reliable scores. The advent of expert systems may signal the eventual removal of this impediment. This study investigated the accuracy with which expert systems could score a new, nonmultiple‐choice item type. The item type presents a faulty solution to a computer programming problem and asks the student to correct the solution. This item type was administered to a sample of high school seniors enrolled in an Advanced Placement course in Computer Science who also took the Advanced Placement Computer Science (APCS) examination. Results indicated that the expert systems were able to produce scores for between 82% and 95% of the solutions encountered and to display high agreement with a human reader on the correctness of the solutions. Diagnoses of the specific errors produced by students were less accurate. Correlations with scores on the objective and free‐response sections of the APCS examination were moderate. Implications for additional research and for testing practice are offered. Copyright © 1990, Wiley Blackwell. All rights reserved","['constructed‐response', 'item', 'large', 'scale', 'standardized', 'testing', 'hamper', 'cost', 'difficulty', 'associate', 'obtain', 'reliable', 'score', 'advent', 'expert', 'system', 'signal', 'eventual', 'removal', 'impediment', 'study', 'investigate', 'accuracy', 'expert', 'system', 'score', 'new', 'nonmultiple‐choice', 'item', 'type', 'item', 'type', 'present', 'faulty', 'solution', 'computer', 'programming', 'problem', 'ask', 'student', 'correct', 'solution', 'item', 'type', 'administer', 'sample', 'high', 'school', 'senior', 'enrol', 'Advanced', 'Placement', 'course', 'Computer', 'Science', 'Advanced', 'Placement', 'Computer', 'Science', 'APCS', 'examination', 'result', 'indicate', 'expert', 'system', 'able', 'produce', 'score', '82', '95', 'solution', 'encounter', 'display', 'high', 'agreement', 'human', 'reader', 'correctness', 'solution', 'Diagnoses', 'specific', 'error', 'produce', 'student', 'accurate', 'Correlations', 'score', 'objective', 'free‐response', 'section', 'APCS', 'examination', 'moderate', 'implication', 'additional', 'research', 'testing', 'practice', 'offer', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['scoring', 'Constructed', 'Responses', 'Expert', 'Systems']",constructed‐response item large scale standardized testing hamper cost difficulty associate obtain reliable score advent expert system signal eventual removal impediment study investigate accuracy expert system score new nonmultiple‐choice item type item type present faulty solution computer programming problem ask student correct solution item type administer sample high school senior enrol Advanced Placement course Computer Science Advanced Placement Computer Science APCS examination result indicate expert system able produce score 82 95 solution encounter display high agreement human reader correctness solution Diagnoses specific error produce student accurate Correlations score objective free‐response section APCS examination moderate implication additional research testing practice offer Copyright © 1990 Wiley Blackwell right reserve,scoring Constructed Responses Expert Systems,0.02318439798459528,0.02335664149594307,0.9070504177360691,0.02293766878737264,0.023470873996020013,0.0,0.06034699130324015,0.0,0.06292524031100767,0.033789001943787915
Young J.W.,Adjusting the Cumulative GPA Using Item Response Theory,1990,27,"In college admissions, the predictive validity of preadmissions measures such as standardized test scores and high school grades is of wide interest. These measures are most often validated against the criterion of the first‐year grade point average (GPA). However, neither the first‐year GPA nor the four‐year cumulative GPA is an adequate indicator of academic performance through four years of college. In this study, Item Response Theory (IRT) is used to develop a more reliable measure of performance, called an IRT‐based GPA, which is used to estimate the validity of traditional preadmissions information. The data are preadmissions information and course grades for the Class of 1986 at Stanford University (N = 1564). Principal factor analysis is used as a precursor to determine the dimensionality of the course data and to partition courses into approximately unidimensional subsets, each of which is scaled independently. Results show a substantial increase in predictability when the IRT‐based GPA is used instead of the usual GPA. Copyright © 1990, Wiley Blackwell. All rights reserved",Adjusting the Cumulative GPA Using Item Response Theory,"In college admissions, the predictive validity of preadmissions measures such as standardized test scores and high school grades is of wide interest. These measures are most often validated against the criterion of the first‐year grade point average (GPA). However, neither the first‐year GPA nor the four‐year cumulative GPA is an adequate indicator of academic performance through four years of college. In this study, Item Response Theory (IRT) is used to develop a more reliable measure of performance, called an IRT‐based GPA, which is used to estimate the validity of traditional preadmissions information. The data are preadmissions information and course grades for the Class of 1986 at Stanford University (N = 1564). Principal factor analysis is used as a precursor to determine the dimensionality of the course data and to partition courses into approximately unidimensional subsets, each of which is scaled independently. Results show a substantial increase in predictability when the IRT‐based GPA is used instead of the usual GPA. Copyright © 1990, Wiley Blackwell. All rights reserved","['college', 'admission', 'predictive', 'validity', 'preadmission', 'measure', 'standardized', 'test', 'score', 'high', 'school', 'grade', 'wide', 'interest', 'measure', 'validate', 'criterion', 'first‐year', 'grade', 'point', 'average', 'GPA', 'first‐year', 'GPA', 'four‐year', 'cumulative', 'GPA', 'adequate', 'indicator', 'academic', 'performance', 'year', 'college', 'study', 'Item', 'Response', 'Theory', 'IRT', 'develop', 'reliable', 'measure', 'performance', 'irt‐base', 'GPA', 'estimate', 'validity', 'traditional', 'preadmission', 'information', 'datum', 'preadmission', 'information', 'course', 'grade', 'Class', '1986', 'Stanford', 'University', 'N', '1564', 'principal', 'factor', 'analysis', 'precursor', 'determine', 'dimensionality', 'course', 'datum', 'partition', 'course', 'approximately', 'unidimensional', 'subset', 'scale', 'independently', 'result', 'substantial', 'increase', 'predictability', 'irt‐base', 'GPA', 'instead', 'usual', 'GPA', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['adjust', 'cumulative', 'GPA', 'Item', 'Response', 'Theory']",college admission predictive validity preadmission measure standardized test score high school grade wide interest measure validate criterion first‐year grade point average GPA first‐year GPA four‐year cumulative GPA adequate indicator academic performance year college study Item Response Theory IRT develop reliable measure performance irt‐base GPA estimate validity traditional preadmission information datum preadmission information course grade Class 1986 Stanford University N 1564 principal factor analysis precursor determine dimensionality course datum partition course approximately unidimensional subset scale independently result substantial increase predictability irt‐base GPA instead usual GPA Copyright © 1990 Wiley Blackwell right reserve,adjust cumulative GPA Item Response Theory,0.8885216357294055,0.028171877537069116,0.027956334778027708,0.027460294849959037,0.027889857105538814,0.0,0.014802864606518449,0.0,0.01723290682177045,0.0838804651420579
Alexander R.A.,Correction Formulas for Correlations Restricted by Selection on an Unmeasured Variable,1990,27,"In discussions of range-restricted correlation coefficients, two situations are most often addressed. The first is the case where restriction has occurred directly on one of the two variables of interest. The other is termed indirect range restriction to designate the situation where direct restriction has occurred on some third variable. The Thorndike (1947) Case 3 formula provides a means of correcting correlations that have arisen in this latter case when information on the third variable is available. Bryant and Gokhale (1972) gave a formula for
correcting such indirectly restricted correlations when no information is available on the third (directly restricted) variable. This note shows that the Bryant and Gokhale (1972) formula is accurate only in one special instance of indirect range restriction. The more general correction formula is given and demonstrated. Throughout the discussion, r' and S' will refer to the range-restricted correlation and standard deviation, and r and S to their unrestricted counterparts.",Correction Formulas for Correlations Restricted by Selection on an Unmeasured Variable,"In discussions of range-restricted correlation coefficients, two situations are most often addressed. The first is the case where restriction has occurred directly on one of the two variables of interest. The other is termed indirect range restriction to designate the situation where direct restriction has occurred on some third variable. The Thorndike (1947) Case 3 formula provides a means of correcting correlations that have arisen in this latter case when information on the third variable is available. Bryant and Gokhale (1972) gave a formula for
correcting such indirectly restricted correlations when no information is available on the third (directly restricted) variable. This note shows that the Bryant and Gokhale (1972) formula is accurate only in one special instance of indirect range restriction. The more general correction formula is given and demonstrated. Throughout the discussion, r' and S' will refer to the range-restricted correlation and standard deviation, and r and S to their unrestricted counterparts.","['discussion', 'rangerestricte', 'correlation', 'coefficient', 'situation', 'address', 'case', 'restriction', 'occur', 'directly', 'variable', 'interest', 'term', 'indirect', 'range', 'restriction', 'designate', 'situation', 'direct', 'restriction', 'occur', 'variable', 'Thorndike', '1947', 'Case', '3', 'formula', 'provide', 'mean', 'correct', 'correlation', 'arise', 'case', 'information', 'variable', 'available', 'Bryant', 'Gokhale', '1972', 'formula', 'correct', 'indirectly', 'restrict', 'correlation', 'information', 'available', 'directly', 'restrict', 'variable', 'note', 'Bryant', 'Gokhale', '1972', 'formula', 'accurate', 'special', 'instance', 'indirect', 'range', 'restriction', 'general', 'correction', 'formula', 'demonstrate', 'discussion', 'r', 'S', 'refer', 'rangerestricte', 'correlation', 'standard', 'deviation', 'r', 'S', 'unrestricted', 'counterpart']","['correction', 'Formulas', 'Correlations', 'restrict', 'Selection', 'Unmeasured', 'Variable']",discussion rangerestricte correlation coefficient situation address case restriction occur directly variable interest term indirect range restriction designate situation direct restriction occur variable Thorndike 1947 Case 3 formula provide mean correct correlation arise case information variable available Bryant Gokhale 1972 formula correct indirectly restrict correlation information available directly restrict variable note Bryant Gokhale 1972 formula accurate special instance indirect range restriction general correction formula demonstrate discussion r S refer rangerestricte correlation standard deviation r S unrestricted counterpart,correction Formulas Correlations restrict Selection Unmeasured Variable,0.8826639528553364,0.029261397411310796,0.02957773920762049,0.0290020066263585,0.02949490389937379,0.0015040694349841778,0.023432528696418035,0.0004738444659052613,0.009612894446181644,0.010086118940545202
Bolger N.; Kellaghan T.,Method of Measurement and Gender Differences in Scholastic Achievement,1990,27,"Gender differences in scholastic achievement as a function of method of measurement were examined by comparing the performance of 15‐year‐old boys (N = 739) and girls (N = 758) in Irish schools on multiple‐choice tests and free‐response tests (requiring short written answers) of mathematics, Irish, and English achievement. Males performed significantly better than females on multiple‐choice tests compared to their performance on free‐response examinations. An expectation that the gender difference would be larger for the languages and smaller for mathematics because of the superior verbal skills attributed to females was not fulfilled. Copyright © 1990, Wiley Blackwell. All rights reserved",Method of Measurement and Gender Differences in Scholastic Achievement,"Gender differences in scholastic achievement as a function of method of measurement were examined by comparing the performance of 15‐year‐old boys (N = 739) and girls (N = 758) in Irish schools on multiple‐choice tests and free‐response tests (requiring short written answers) of mathematics, Irish, and English achievement. Males performed significantly better than females on multiple‐choice tests compared to their performance on free‐response examinations. An expectation that the gender difference would be larger for the languages and smaller for mathematics because of the superior verbal skills attributed to females was not fulfilled. Copyright © 1990, Wiley Blackwell. All rights reserved","['gender', 'difference', 'scholastic', 'achievement', 'function', 'method', 'examine', 'compare', 'performance', '15‐year‐old', 'boy', 'n', '739', 'girl', 'N', '758', 'irish', 'school', 'multiple‐choice', 'test', 'free‐response', 'test', 'require', 'short', 'write', 'answer', 'mathematic', 'irish', 'english', 'achievement', 'Males', 'perform', 'significantly', 'female', 'multiple‐choice', 'test', 'compare', 'performance', 'free‐response', 'examination', 'expectation', 'gender', 'difference', 'large', 'language', 'small', 'mathematic', 'superior', 'verbal', 'skill', 'attribute', 'female', 'fulfil', 'Copyright', '©', '1990', 'Wiley', 'Blackwell', 'right', 'reserve']","['Method', 'Gender', 'Differences', 'Scholastic', 'Achievement']",gender difference scholastic achievement function method examine compare performance 15‐year‐old boy n 739 girl N 758 irish school multiple‐choice test free‐response test require short write answer mathematic irish english achievement Males perform significantly female multiple‐choice test compare performance free‐response examination expectation gender difference large language small mathematic superior verbal skill attribute female fulfil Copyright © 1990 Wiley Blackwell right reserve,Method Gender Differences Scholastic Achievement,0.028256966605917085,0.8870022291989187,0.02832306175669131,0.027952873553991453,0.028464868884481423,0.0025366040250530673,0.05940018909421759,0.0,0.0038037501297518046,0.04168758516619089
