"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Index Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Shannon G.A.; Cliver B.A.","Shannon, Gregory A. (57191233960); Cliver, Barbara A. (57191231516)","57191233960; 57191231516","An Application of Item Response Theory in the Comparison of Four Conventional Item Discrimination Indices for Criterion‐Referenced Tests","1987","Journal of Educational Measurement","24","4","","347","356","9","2","10.1111/j.1745-3984.1987.tb00285.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112958&doi=10.1111%2fj.1745-3984.1987.tb00285.x&partnerID=40&md5=c7cb06be26eda6886a82789f05867324","Several recent papers have argued for the usefulness of item response theory (IRT) methods of assessing item discrimination power for criterion‐referenced tests (CRTs). Conventional methods continue to be used more widely, however, for reasons that include some practical constraints associated with the use of IRT methods. To provide users with information that may help them to decide on which conventional indices to employ in evaluating CRT items, Spearman rank‐order correlations were computed between IRT‐derived item information functions (llFs) and four conventional discrimination indices: the phi‐coefficient, the B‐index, phi/phi max, and the agreement statistic. The rank‐order correlations between the phi‐coefficient and the llFs were very high, with a median of .96. The remaining conventional indices, with the exception of phi‐over‐phi‐max, also correlated well with the IIF. Theoretical explanations for these relationships are offered. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988112958"
"Willson V.L.","Willson, Victor L. (7004256565)","7004256565","Cognitive and Developmental Effects on Item Performance in Intelligence and Achievement Tests for Young Children","1989","Journal of Educational Measurement","26","2","","103","119","16","5","10.1111/j.1745-3984.1989.tb00322.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119072&doi=10.1111%2fj.1745-3984.1989.tb00322.x&partnerID=40&md5=431faf5c5c28f0dfb2a0d7c8d0d8c781","Performance on items in intelligence and achievement tests can be represented in terms of child development and information processes. Research is reviewed on item performance that supports developmental and information processing effects, particularly in children. Some suggestions for item development in intelligence and achievement tests are presented. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988119072"
"McKinley R.L.","McKinley, Robert L. (57167321600)","57167321600","A Comparison of Six Methods for Combining Multiple IRT Item Parameter Estimates","1988","Journal of Educational Measurement","25","3","","233","246","13","6","10.1111/j.1745-3984.1988.tb00305.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112832&doi=10.1111%2fj.1745-3984.1988.tb00305.x&partnerID=40&md5=0107f7492c66815fca576a719e722f65","Six procedures for combining sets of IRT item parameter estimates obtained from different samples were evaluated using real and simulated response data. In the simulated data analyses, true item and person parameters were used to generate response data for three different‐sized samples. Each sample was calibrated separately to obtain three sets of item parameter estimates for each item. The six procedures for combining multiple estimates were each applied, and the results were evaluated by comparing the true and estimated item characteristic curves. For the real data, the two best methods from the simulation data analyses were applied to three different‐sized samples and the resulting estimated item characteristic curves were compared to the curves obtained when the three samples were combined and calibrated simultaneously. The results support the use of covariance matrix‐weighted averaging and a procedure that involves sample‐size‐weighted averaging of estimated item characteristic curves at the center of the ability distribution Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988112832"
"Strenta A.C.; Elliott R.","Strenta, A. Christopher (6506445419); Elliott, Rogers (24568102400)","6506445419; 24568102400","Differential Grading Standards Revisited","1987","Journal of Educational Measurement","24","4","","281","291","10","38","10.1111/j.1745-3984.1987.tb00280.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988091772&doi=10.1111%2fj.1745-3984.1987.tb00280.x&partnerID=40&md5=45a9574c008cf36ff24673411e365fbd","The differential grading standards described by Goldman and Widawski (1976) exist in the same magnitude and in roughly the same order a decade later in a different kind of institution. Major fields that attract as majors students who score higher on SATs employ stricter grading standards. These differential grading standards serve to attenuate the correlation between SAT scores and grades, which is, even in highly selective institutions, substantial. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988091772"
"Tatsuoka K.K.; Linn R.L.; Tatsuoka M.M.; Yamamoto K.","Tatsuoka, Kikumi K. (6603447775); Linn, Robert L. (56126633100); Tatsuoka, Maurice M. (6506443458); Yamamoto, Kentaro (56130477200)","6603447775; 56126633100; 6506443458; 56130477200","Differential Item Functioning Resulting From The Use of Different Solution Strategies","1988","Journal of Educational Measurement","25","4","","301","319","18","27","10.1111/j.1745-3984.1988.tb00310.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988141315&doi=10.1111%2fj.1745-3984.1988.tb00310.x&partnerID=40&md5=33e29e1b229141cccbf5caec80e39d1c","The present study investigates the degree to which item “bias” techniques can lead to interpretable results when groups are defined in terms of specified differences in the cognitive processes involved in students' problem‐solving strategies. A large group of junior high school students who took a test on subtraction of fractions was divided into two subgroups judged by the rule‐space model to be using different problem‐solving strategies. It was confirmed by use of Mantel‐Haenszel (MH) statistics that these subgroups showed different performances on items with different underlying cognitive tasks. We note that, in our case, we are far from faulting items that show differential item functioning (D1F) between two groups defined in terms of different solution strategies. Indeed, they are “desirable” items, as explained in the discussion section Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988141315"
"Spray J.A.; Ackerman T.A.; Reckase M.D.; Carlson J.E.","Spray, Judith A. (7006830106); Ackerman, Terry A. (16404476400); Reckase, Mark D. (6602075623); Carlson, James E. (57190052912)","7006830106; 16404476400; 6602075623; 57190052912","Effect of the Medium of Item Presentation on Examinee Performance and Item Characteristics","1989","Journal of Educational Measurement","26","3","","261","271","10","28","10.1111/j.1745-3984.1989.tb00332.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090449&doi=10.1111%2fj.1745-3984.1989.tb00332.x&partnerID=40&md5=5f4d77ac476ef10f1da55c5f24d06456","Studies that have investigated differences in examinee performance on items administered in paper‐and‐pencil form or on a computer screen have produced equivocal results. Certain item administration procedures were hypothesized to be among the most important variables causing differences in item performance and ultimately in test scores obtained from these different administration media. A study where these item administration procedures were made as identical as possible for each presentation medium is described. In addition, a methodology is presented for studying the difficulty and discrimination of items under each presentation medium as a post hoc procedure. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988090449"
"GULLICKSON A.R.","GULLICKSON, ARLEN R. (6506885439)","6506885439","TEACHER EDUCATION AND TEACHER‐PERCEIVED NEEDS IN EDUCATIONAL MEASUREMENT AND EVALUATION","1986","Journal of Educational Measurement","23","4","","347","354","7","30","10.1111/j.1745-3984.1986.tb00254.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984520713&doi=10.1111%2fj.1745-3984.1986.tb00254.x&partnerID=40&md5=bdafa4e1f2de2d591e3289ffafcaf394","Professors and teachers were compared relative to their perspectives on preservice educational measurement courses. Twenty‐four professors from different colleges in seven states and 360 teachers from elementary and secondary schools in one midwestern state responded via mailed questionnaire. Professors reported the emphasis given to each of eight topics in preservice educational measurement courses, and teachers reported the emphasis they believed should be given to each topic. In five of the eight content areas, the relative emphases given by professors differed from that recommended by teachers. Major differences emerged in nontest evaluation, statistical analysis, and formative and summative evaluation. Implications of these results are discussed. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84984520713"
"Johnson S.T.; Wallace M.B.","Johnson, Sylvia T. (14325044600); Wallace, Michael B. (12807538800)","14325044600; 12807538800","Characteristics of SAT Quantitative Items Showing Improvement After Coaching Among Black Students From Low‐Income Families: An Exploratory Study","1989","Journal of Educational Measurement","26","2","","133","145","12","6","10.1111/j.1745-3984.1989.tb00324.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988103848&doi=10.1111%2fj.1745-3984.1989.tb00324.x&partnerID=40&md5=34c4bb2c380dd2e557e5da3f755e7abc","The present exploratory investigation is an extension of a 1985 study that evaluated a Scholastic Aptitude Test preparation program involving black urban students. The present study was conducted to determine if there are identifiable characteristics of quantitative items associated with a susceptibility to coaching. Items showing p‐value improvements of. 10 after coaching were compared with items not showing such improvements; comparisons were made in terms of the content, type. format, level of cognitive requirement, and position of the items. Items showing improvement were found across content areas, formats, and type. Findings are discussed in relation to the usefulness and improvement of well‐designed supplemental instructional programs that have the potential to affect criterion performance in mathematics Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988103848"
"WAINER H.","WAINER, HOWARD (7006218234)","7006218234","CAN A TEST BE TOO RELIABLE?","1986","Journal of Educational Measurement","23","2","","171","173","2","4","10.1111/j.1745-3984.1986.tb00243.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007648262&doi=10.1111%2fj.1745-3984.1986.tb00243.x&partnerID=40&md5=68f6c6d2045172798ad4cf59566f8be0","It is shown that summary statistics that are commonly used to measure test quality (reliability, mean rbis, and mean proportion correct) can be seriously misleading. This is demonstrated and explained. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85007648262"
"Webb N.M.; Herman J.L.; Cabello B.","Webb, Noreen M. (7102171983); Herman, Joan L. (7403275946); Cabello, Beverly (6602287829)","7102171983; 7403275946; 6602287829","A Domain‐Referenced Approach to Diagnostic Testing Using Generalizability Theory","1987","Journal of Educational Measurement","24","2","","119","130","11","1","10.1111/j.1745-3984.1987.tb00268.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112968&doi=10.1111%2fj.1745-3984.1987.tb00268.x&partnerID=40&md5=8af27a1e718967cf0e9d4b8eb1c97063","This paper describes a four‐step approach to constructing diagnostic test profiles that provide precise but practical information on students' instructional needs. The approach is based on the specification and analysis of a domain and uses generalizability theory to determine which skills within the domain need to be assessed to diagnose gaps in students' skills and to estimate score profiles. A 64‐item test of pronoun use was constructed to represent 32 categories of usage defined by different combinations of five factors in the domain. Generalizability analyses were conducted to determine the optimal number of categories to be included in students' profiles and the number of items needed for each category, and to produce univariate and multivariate estimates of students' universe scores. Multivariate profiles of universe scores were the most accurate and differed substantially from observed score and univariate universe score profiles. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988112968"
"Norcini J.J.; Lipner R.S.; Langdon L.O.; Strecker C.A.","Norcini, John J. (7006095771); Lipner, Rebecca S. (6603181536); Langdon, Lynn O. (7003564908); Strecker, Carolyn A. (57191232083)","7006095771; 6603181536; 7003564908; 57191232083","A Comparison of Three Variations on a Standard‐Setting Method","1987","Journal of Educational Measurement","24","1","","56","64","8","49","10.1111/j.1745-3984.1987.tb00261.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988091022&doi=10.1111%2fj.1745-3984.1987.tb00261.x&partnerID=40&md5=29d4beb86ee2cbb01d1a15d803d0c9f3","The purpose of this study was to determine whether two variations on the typical Angoff group standard‐setting process would produce sufficiently consistent results to recommend their use. Judgments obtained from a group of experts during a meeting were compared with judgments gathered from the same group before and after the meeting. The results indicate that differences between passing scores obtained with the three variations are relatively small, but those gathered before the meeting were less consistent than ratings gathered during and after the meeting. These results imply that judgments gathered after an initial traditional group‐process session can provide an efficient alternative mechanism for setting cutting scores using the Angoff method. This research was supported by The American Board of Internal Medicine, but does not necessarily reflect its opinions or policies. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988091022"
"Raven J.","Raven, John (7103305535)","7103305535","The Raven Progressive Matrices: A Review of National Norming Studies and Ethnic and Socioeconomic Variation Within the United States","1989","Journal of Educational Measurement","26","1","","1","16","15","118","10.1111/j.1745-3984.1989.tb00314.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032069726&doi=10.1111%2fj.1745-3984.1989.tb00314.x&partnerID=40&md5=3b84c651dbd370bd843afbce344fa4f5","In this paper, some recent results relating to the stability o f scores on the Raven Progressive Matrices Test for different subgroups within and between the United Kingdom, the United States, and other Western societies are summarised. Subsequent sections deal with variation over time. A possible explanation for the variation in norms over time and between ethnic groups within the United States is offered. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85032069726"
"Haertel E.H.","Haertel, Edward H. (6602745554)","6602745554","Using Restricted Latent Class Models to Map the Skill Structure of Achievement Items","1989","Journal of Educational Measurement","26","4","","301","321","20","301","10.1111/j.1745-3984.1989.tb00336.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128977&doi=10.1111%2fj.1745-3984.1989.tb00336.x&partnerID=40&md5=8b413cad99f80d04640a62045d652cf1","This paper presents a new method for using certain restricted latent class models, referred to as binary skills models, to determine the skills required by a set o f test items. The method is applied to reading achievement data from a nationally representative sample o f fourth‐grade students and offers useful perspectives on test structure and examinee ability, distinct from those provided by other methods o f analysis. Models fitted to small, overlapping sets o f items are integrated into a common skill map, and the nature o f each skill is then inferred from the characteristics o f the items for which it is required. The reading comprehension items examined conform closely to a unidimensional scale with six discrete skill levels that range from an inability to comprehend or match isolated words in a reading passage to the abilities required to integrate passage content with general knowledge and to recognize the main ideas o f the most difficult passages on the test. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988128977"
"Zeidner M.","Zeidner, Moshe (7004524025)","7004524025","Sociocultural Differences in Examinees' Attitudes Toward Scholastic Ability Exams","1988","Journal of Educational Measurement","25","1","","67","76","9","6","10.1111/j.1745-3984.1988.tb00292.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119915&doi=10.1111%2fj.1745-3984.1988.tb00292.x&partnerID=40&md5=698fe4dbb2d9407612c5b74280dbf5a0","The major aim of the present study was threefold: (a) to compare the test attitudes and perceptions o f examinees of varying sociocultural group membership toward verbal and nonverbal standardized ability tests; (b) to determine the degree of covariation between test attitudes and test scores; and (c) to delineate the properties and potential applications of a test attitude or feedback inventory specifically designed to assess examinees’ perceptions of key situational variables in the test context. The feedback inventory was administered to a sample of 259 seventh grade students in Israel immediately following standardized group scholastic ability testing procedures. On the whole, few meaningful group differences in test attitudes were observed by social class, ethnicity, or sex. However, a nonverbal test was generally rated more favorably than a verbal test, among varying sociocultural and sex subgroups. Considered together, test attitude scales share a meaningful proportion o f variance with the test score on both verbal and nonverbal tests. However, in view o f the negligible ethnic and social class differences in test attitudes and the nonsignificant interaction between test attitudes and background variables, the data provide little support for the situational bias claim Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988119915"
"Wilcox R.R.","Wilcox, Rand R. (7202527113)","7202527113","Confidence Intervals for True Scores Under an Answer‐Until‐Correct Scoring Procedure","1987","Journal of Educational Measurement","24","3","","263","269","6","0","10.1111/j.1745-3984.1987.tb00279.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053789&doi=10.1111%2fj.1745-3984.1987.tb00279.x&partnerID=40&md5=712c4e4d5f59727396c3f8fe055fbba6","Under an answer‐until‐correct scoring procedure, many measurement problems can be solved when certain cognitive models of examinee behavior can be assumed (Wilcox, 1983). Point estimates of true score under these models are available, but the problem of obtaining a confidence interval has never been addressed. Two simple methods for obtaining a confidence interval are suggested that give good results when the sample size is reasonably large, say, greater than or equal to 20, and when true score is not too close to zero or one. A third procedure is suggested that can also be used to get slightly better results where again the sample size is assumed to be reasonably large and true score is not too close to zero or one. For small sample sizes or situations where true score is close to zero or one, a fourth procedure is described that always gives conservative results. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988053789"
"Sehmitt A.P.","Sehmitt, Alieia P. (57191231584)","57191231584","Language and Cultural Characteristics That Explain Differential Item Functioning for Hispanic Examinees on the Scholastic Aptitude Test","1988","Journal of Educational Measurement","25","1","","1","13","12","27","10.1111/j.1745-3984.1988.tb00287.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053669&doi=10.1111%2fj.1745-3984.1988.tb00287.x&partnerID=40&md5=1c5a2ceb624adbb96c50bdc71f2906f4","The standardization methodology was used to help identify item characteristics that might explain differential item functioning among Hispanics on the Scholastic Aptitude Test. Results indicated that true cognates or words with a common root in English and Spanish and content of special interest for Hispanics seemed to help Hispanics performance. Limited occurrence of false cognates (words that appear to be cognates but have different meanings in both languages) and of homographs (words that are spelled alike but have different meanings in English) restricted their evaluation. Nevertheless, examination of items with false cognates or homographs gave some evidence indicating that their occurrence might make items unexpectedly more difficult for Hispanic examinees Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988053669"
"I‐leaning G.","I‐leaning, Grant (57191232238)","57191232238","Does the Rasch Model Really Work for Multiple‐Choice Items? Take Another Look: A Response to Divgi","1989","Journal of Educational Measurement","26","1","","91","97","6","7","10.1111/j.1745-3984.1989.tb00321.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119874&doi=10.1111%2fj.1745-3984.1989.tb00321.x&partnerID=40&md5=32b9621b2ebe5505e272f7e27caba523","Divgi's (1986) study concludes, largely on the basis of a proposed test o f fit that is designed to be more powerful than Wright and Panchapakesan's (I 969) test, that the Rasch model is never appropriate for use with multiple‐choice type test items. This paper is an attempt to refute the conclusions of Divgi's study. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988119874"
"Stanley J.C.","Stanley, Julian C. (7402185684)","7402185684","Note About Possible Bias Resulting When Under‐Statisticized Studies are Excluded from Meta‐Analyses","1987","Journal of Educational Measurement","24","1","","72","76","4","2","10.1111/j.1745-3984.1987.tb00263.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988057862&doi=10.1111%2fj.1745-3984.1987.tb00263.x&partnerID=40&md5=e8217dc99fe6fe359d7949c6c9d9679a","Reviews and meta‐analyses of research on a given topic may exclude' a sizable percentage of reports because they do not lend themselves to the type of summarizing procedures used. If the excluded articles contain relevant information, this may bias the conclusions of the analysis. It seems likely that, when computing statistics from their data, researchers will need to consider this aspect. A simple illustration of how that can sometimes be done readily is presented. A robust correlation coefficient easily computable from published data is shown to indicate a sizable relationship that is contrary to the main conclusion of a meta‐analysis. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988057862"
"Kolen M.J.","Kolen, Michael J. (6603925839)","6603925839","Defining Score Scales in Relation to Measurement Error","1988","Journal of Educational Measurement","25","2","","97","110","13","16","10.1111/j.1745-3984.1988.tb00295.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002695333&doi=10.1111%2fj.1745-3984.1988.tb00295.x&partnerID=40&md5=8e95b33edd3ee28c421d166382696dbb","Scale scores for educational tests can be made more interpretable by incorporating score precision information at the time the score scale is established. Methods for incorporating this information are examined that are applicable to testing situations with number‐correct scoring. Both linear and nonlinear methods are described. These methods can be used to construct score scales that discourage the overinterpretation of small differences in scores. The application of the nonlinear methods also results in scale scores that have nearly equal error variability along the score scale and that possess the property that adding a specified number of points to and subtracting the same number of points from any examinee's scale score produces an approximate two‐sided confidence interval with a specified coverage. These nonlinear methods use an arcsine transformation to stabilize measurement error variance for transformed scores. The methods are compared through the use of illustrative examples. The effect of rounding on measurement error variability is also considered and illustrated using stanines Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0002695333"
"Smith R.L.; Smith J.K.","Smith, Robert L. (58263685800); Smith, Jeffrey K. (15047002900)","58263685800; 15047002900","Differential Use of Item Information by Judges Using Angoff and Nedeisky Procedures","1988","Journal of Educational Measurement","25","4","","259","274","15","35","10.1111/j.1745-3984.1988.tb00307.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053716&doi=10.1111%2fj.1745-3984.1988.tb00307.x&partnerID=40&md5=8ee3d713fb769902debb0089f824e5cd","Competency examinations in a variety of domains require setting a minimum standard of performance. This study examines the issue of whether judges using the two most popular methods for setting cut scores (Angoff and Nedelsky methods) use different sources of information when making their judgments. Thirty‐one judges were assigned randomly to the two methods to set cut scores for a high school graduation test in reading comprehension. These ratings were then related to characteristics of the items as well as to empirically obtained p values. Results indicate that judges using the Angoff method use a wider variety of information and yield estimates closer to the actual p values. The characteristics of items used in the study were effective predictors of judges’ ratings, but were far less effective in predicting p values Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988053716"
"Rowley G.L.","Rowley, Glenn L. (7006131497)","7006131497","Assessing Error in Behavioral Data: Problems of Sequencing","1989","Journal of Educational Measurement","26","3","","273","284","11","4","10.1111/j.1745-3984.1989.tb00333.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988104036&doi=10.1111%2fj.1745-3984.1989.tb00333.x&partnerID=40&md5=3fec88751de22818d8a3f96bac10ceff","Both classical test theory and generalizability theory focus on measurement error as a group property. Thus, common estimates o f errors o f measurement are developed for all members o f a group. But with behavioral data, unlike specifically test data, it is sometimes possible to estimate error separately for each individual. This enables one to ask questions about the relationships between error o f measurement and other characteristics o f the individual. Consequently, it also makes possible the use of regression techniques to call upon group data to improve the estimates o f individual measurement error. The focus on the individual also lays bare the possibility o f sequencing effects, and it is shown that, even in the absence o f trend, autocorrelation can cause standard procedures to grossly underestimate the magnitude o f measurement error. Classroom observation data are examined for autocorrelation, and recommendations are made about the scheduling o f data collection so as to minimize its effects. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988104036"
"Millman J.; Westman R.S.","Millman, Jason (25954577400); Westman, Ronald S. (57225235039)","25954577400; 57225235039","Computer‐Assisted Writing of Achievement Test Items: Toward a Future Technology","1989","Journal of Educational Measurement","26","2","","177","190","13","18","10.1111/j.1745-3984.1989.tb00327.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090383&doi=10.1111%2fj.1745-3984.1989.tb00327.x&partnerID=40&md5=cb2a01c1b8bdfaee9377fada495eed40","Five appproaches to writing test items with computer assistance are descibed. The authors propose a scheme for modeling knowledge by a set of structures and outline a system for implementing it. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988090383"
"McMorris R.F.; DeMers L.P.; Schwarz S.P.","McMorris, Robert F. (6603270899); DeMers, Lawrence P. (57191235318); Schwarz, Shirley P. (8971962600)","6603270899; 57191235318; 8971962600","Attitudes, Behaviors, and Reasons for Changing Responses Following Answer‐Changing Instruction","1987","Journal of Educational Measurement","24","2","","131","143","12","30","10.1111/j.1745-3984.1987.tb00269.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073686&doi=10.1111%2fj.1745-3984.1987.tb00269.x&partnerID=40&md5=df284269c917816cc4ad3a7ca6cff891","Contrary to the adage warning against changing test answers, mean gain from changing has been an invariant research finding. Consistency of this gain was tested for students instructed about the research results, and composition of the gain was analyzed by examining the students' reasons for changing. Students in six graduate measurement classes instructed about the answer‐changing literature responded to three exams and a questionnaire. Mean gain remained positive and consistent with gain for previously studied uninstructed groups; amount of change was also stable. “Rethinking the item and conceptualizing a better answer” was the most frequent reason given for changing. “Rereading the item and better understanding the question” was the second most cited reason, followed by “rereading/rethinking” combined, and “making a clerical error.” For each frequently used reason, wrong‐to‐right (WR) changes were in the majority. Implications for research and practice are discussed. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988073686"
"Baglin R.F.","Baglin, Roger F. (57190929399)","57190929399","Group Scores: A Rejoinder to Burket","1988","Journal of Educational Measurement","25","1","","77","79","2","0","10.1111/j.1745-3984.1988.tb00293.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074930&doi=10.1111%2fj.1745-3984.1988.tb00293.x&partnerID=40&md5=afeea6102bf2c9b36ad4a82a7a29f74d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988074930"
"BAGLIN R.F.","BAGLIN, ROGER F. (57190929399)","57190929399","A PROBLEM IN CALCULATING GROUP SCORES ON NORM‐REFERENCED TESTS","1986","Journal of Educational Measurement","23","1","","57","58","1","4","10.1111/j.1745-3984.1986.tb00234.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988126183&doi=10.1111%2fj.1745-3984.1986.tb00234.x&partnerID=40&md5=4a81868549e297a58f22ce5f8f942462","Norm‐referenced standardized achievement tests are designed, and commonly used, for obtaining group scores. Various methods are used to calculate and express group scores in terms of common derived scores, such as percent ile ranks. Publishers' scaled scores are ordinarily used in these procedures, with the result that the group scores can possess anomalous characteristics. The group scores can vary widely, depending on not only the measure of central tendency but also the type of derived score employed. A reason for this situation is hypothesized to be the use of inappropriate statistical procedures to develop publishers' scaled scores. Practitioners need to be aware of this problem and to document their procedures when calculating and reporting group scores. Test publishers are urged to avoid the use of scaling procedures that are seen as responsible for this problem. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988126183"
"Mandeville G.K.","Mandeville, Garrett K. (7801432799)","7801432799","School Effectiveness Indices Revisited: Cross‐Year Stability","1988","Journal of Educational Measurement","25","4","","349","356","7","31","10.1111/j.1745-3984.1988.tb00313.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072911&doi=10.1111%2fj.1745-3984.1988.tb00313.x&partnerID=40&md5=0844d6d78b5db7a90b179aac7242167f","School effectiveness indices (SEIs) based on residuals from regressing test performance onto prior test performance and a socioeconomic status (SES) measure were obtained for 2 consecutive years for 431 elementary schools. The resulting SEIs were found to be reasonably stable year to year, the correlations ranging from. 34 to .66, depending on grade level (1–4) and subject (reading and mathematics). To aid in the identification of the factors that affect the stability of school achievement, correlations of the SEIs across subjects and grade levels were obtained also. It was determined that SEIs reflecting the performance of students at the same grade level were relatively stable, whether the same or different students were involved. However, SEIs reflecting the performance of students at different grade levels were very unstable. This suggests that grade‐within‐school effects dominate whatever global school effects operate in elementary schools. Implications for effective schools research, the design of school recognition/reward programs, and research and measurement specialists in general are discussed Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988072911"
"Wainer H.; Kiely G.L.","Wainer, Howard (7006218234); Kiely, Gerard L. (57191234237)","7006218234; 57191234237","Item Clusters and Computerized Adaptive Testing: A Case for Testlets","1987","Journal of Educational Measurement","24","3","","185","201","16","436","10.1111/j.1745-3984.1987.tb00274.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988141297&doi=10.1111%2fj.1745-3984.1987.tb00274.x&partnerID=40&md5=d45bedc0d88c2b661b4214dcdaff020f","It is observed that many sorts of difficulties may preclude the uneventful construction of tests by a computerized algorithm, such as those currently in favor in Computerized Adaptive Testing (CAT). In this essay we discuss a number of these problems, as well as some possible avenues of solution. We conclude with the development of the “testlet,” a bundle of items that can be arranged either hierarchically or linearly, thus maintaining the efficiency of an adaptive test while keeping the quality control of test construction that is possible currently only with careful expert scrutiny. Performance on the separate testlets is aggregated to yield ability estimates. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988141297"
"MeCornack R.L.; McLeod M.M.","MeCornack, Robert L. (57191231094); McLeod, Mary M. (57216062677)","57191231094; 57216062677","Gender Bias in the Prediction of College Course Performance","1988","Journal of Educational Measurement","25","4","","321","331","10","24","10.1111/j.1745-3984.1988.tb00311.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988077086&doi=10.1111%2fj.1745-3984.1988.tb00311.x&partnerID=40&md5=6b51653b8e1ab16bfd0e23a8c1394285","Is the relationship of college grades to the traditional predictors of aptitude test scores and high school grades different for men and women? The usual gender bias of underpredicting the grade point averages of women may result from gender‐related course selection effects. This study controlled course selection effects by predicting single course grades rather than a composite grade from several courses. In most of the large introductory courses studied, no gender bias was found that would hold up on cross‐validation in a subsequent semester. Usually, it was counterproductive to adjust grade predictions according to gender. Grade point average was predicted more accurately than single course grades Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988077086"
"DORANS N.J.","DORANS, NEIL J. (6602289148)","6602289148","THE IMPACT OF ITEM DELETION ON EQUATING CONVERSIONS AND REPORTED SCORE DISTRIBUTIONS","1986","Journal of Educational Measurement","23","3","","245","264","19","8","10.1111/j.1745-3984.1986.tb00250.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973080286&doi=10.1111%2fj.1745-3984.1986.tb00250.x&partnerID=40&md5=e01f749eacd48323ea2867924f2534a5","A formal analysis of the effects of item deletion on equating/scaling functions and reported score distributions is presented. There are two components of the present analysis: analytical and empirical. The analytical decomposition demonstrates how the effects of item characteristics, test properties, individual examinee responses, and rounding rules combine to produce the item deletion effect on the equating/scaling function and candidate scores, In addition to demonstrating how the deleted item's psychometric characteristics can affect the equating function, the analytical component of the report examines the effects of not scoring versus scoring all options correct, the effects of re‐equating versus not re‐equating, and the interaction between the decision to re‐equate or to not re‐equate and the scoring option chosen for the flawed item. The empirical portion of the report uses data from the May 1982 administration of the SA T, which contained the circles item, to illustrate the effects of item deletion on reported score distributions and equating functions. The empirical data verify what the analytical decomposition predicts. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84973080286"
"Burket G.R.","Burket, George R. (6602105926)","6602105926","Group Scores: A Response to Baglin","1987","Journal of Educational Measurement","24","2","","175","178","3","1","10.1111/j.1745-3984.1987.tb00273.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988060659&doi=10.1111%2fj.1745-3984.1987.tb00273.x&partnerID=40&md5=c3a1c31d73d4e49355f3c8bae44bafa1","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988060659"
"Albanese M.A.","Albanese, Mark A. (7006286551)","7006286551","The Projected Impact of the Correction for Guessing on Individual Scores","1988","Journal of Educational Measurement","25","2","","149","157","8","12","10.1111/j.1745-3984.1988.tb00299.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988129722&doi=10.1111%2fj.1745-3984.1988.tb00299.x&partnerID=40&md5=5765a8e73f004af8c81924c8f0fb4b6f","This article presents estimates of the effects of the use of formula scoring on an individual examinee's score. The results of this analysis suggest that under plausible assumptions, using test characteristics derived from several studies, some examinees would increase their scores by one half standard deviation or more if they were to answer items omitted under formula directions Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988129722"
"Stiggins R.J.; Griswold M.M.; Wikelund K.R.","Stiggins, Richard J. (6602224104); Griswold, Maggie Miller (57191234508); Wikelund, Karen Reed (57191234177)","6602224104; 57191234508; 57191234177","Measuring Thinking Skills Through Classroom Assessment","1989","Journal of Educational Measurement","26","3","","233","246","13","30","10.1111/j.1745-3984.1989.tb00330.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058855&doi=10.1111%2fj.1745-3984.1989.tb00330.x&partnerID=40&md5=701d04ba5f1b5ca5fc24a93acb653d8e","The classroom assessment procedures o f 36 teachers in grades 2 to 12 were studied in depth to determine the extent to which they measure students” higher order thinking skills in mathematics, science, social studies, and language arts. A wide variety o f assessment documents were analyzed, teachers were observed asking oral questions in their classrooms, and each teacher was interviewed. The results revealed that paper‐and‐pencil assessment documents were dominated by recall questions across all grade levels. However, inference was assessed also, especially in mathematics. Oral questions tended to tap recall too, with analysis and inference reflected to some extent. Across grades, subjects, and forms o f assessment, comparison and evaluation questions were rare. Although these teachers had been trained to teach thinking skills to some extent, they were less often trained to assess such skills. Those who were trained tended to ask a higher proportion o f thinking skills questions than those who were not. The training implications o f the results are discussed. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988058855"
"Crocker L.; Llabre M.; Miller M.D.","Crocker, Linda (7003863257); Llabre, Maria (7003996654); Miller, M. David (55757783023)","7003863257; 7003996654; 55757783023","The Generalizability of Content Validity Ratings","1988","Journal of Educational Measurement","25","4","","287","299","12","24","10.1111/j.1745-3984.1988.tb00309.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128752&doi=10.1111%2fj.1745-3984.1988.tb00309.x&partnerID=40&md5=35ef39a5c9fc82a41cf505ca1e0063dd","The problem of assessing the content validity (or relevance) of standardized achievement tests is considered within the framework of generalizability theory. Four illustrative designs are described that may be used to assess test‐item fit to a curriculum. For each design, appropriate variance components are identified for making relative and absolute item (or test) selection decisions. Special consideration is given to use of these procedures for determining the number of raters and/or schools needed in a content‐validation decisionmaking study. Application of these procedures is illustrated using data from an international assessment of mathematics achievement Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988128752"
"Elliott R.; Strenta A.C.","Elliott, Rogers (24568102400); Strenta, A. Christopher (6506445419)","24568102400; 6506445419","Effects of Improving the Reliability of the GPA on Prediction Generally and on Comparative Predictions for Gender and Race Particularly","1988","Journal of Educational Measurement","25","4","","333","347","14","63","10.1111/j.1745-3984.1988.tb00312.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988121048&doi=10.1111%2fj.1745-3984.1988.tb00312.x&partnerID=40&md5=208e27e037cc357b6548e67b0420ceba","The reliability of a method of adjusting grade point averages for differences in departmental grading standards was examined, as were the effects of such adjustments on the predictive validity of high school grades, SAT scores, and achievement test scores. The index of differential grading standards for all on‐time graduates of the Dartmouth College class of 1986 was quite reliable, and its use in adjusting grade averages increased predictive validity, reduced its erosion over years, reduced the apparent underprediction of women, and improved predictions for blacks. Differential group enrollment in courses in the science division seems to account for much of the effect of adjustment on grades. Improvement in the reliability of the criterial grade averages also was shown to have similar effects on gender and race prediction in another data set Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988121048"
"WEBB N.M.; HERMAN J.L.; CABELLO B.","WEBB, NOREEN M. (7102171983); HERMAN, JOAN L. (7403275946); CABELLO, BEVERLY (6602287829)","7102171983; 7403275946; 6602287829","DIAGNOSING STUDENTS' ERRORS FROM THEIR RESPONSE SELECTIONS IN LANGUAGE ARTS","1986","Journal of Educational Measurement","23","2","","163","170","7","1","10.1111/j.1745-3984.1986.tb00242.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988054530&doi=10.1111%2fj.1745-3984.1986.tb00242.x&partnerID=40&md5=fab68042f638935cbec3cdf778d05897","This set of studies examined the consistency of student response patterns on a test of language arts, as a first step toward designing a computerized adaptive test to diagnose errors. A diagnostic domain‐referenced language arts test was designed so that the choice of response would immediately point to a specific misconception in pronoun usage. This direct correspondence between error and diagnosis was designed to facilitate classroom instruction and remediation. Analysis of students' response choices on matched items and analysis of students' rationales for selecting their responses showed that student behavior was not always consistent and could be used to diagnose some errors but not others. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988054530"
"Bennett R.E.; Rock D.A.; Novatkoski I.","Bennett, Randy Elliot (7402440584); Rock, Donald A. (7102591023); Novatkoski, Inge (57191232055)","7402440584; 7102591023; 57191232055","Differential Item Functioning on the SAT‐M Braille Edition","1989","Journal of Educational Measurement","26","1","","67","79","12","12","10.1111/j.1745-3984.1989.tb00319.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007334947&doi=10.1111%2fj.1745-3984.1989.tb00319.x&partnerID=40&md5=c2bac7785f33ef05b5988473678b6ec3","This study attempted to pinpoint the causes of differential item difficulty for blind students taking the braille edition of the Scholastic Aptitude Test's Mathematical section (SAT‐M). The study method involved reviewing the literature to identify factors that might cause differential item functioning for these examinees, forming item categories based on these factors, identifying categories that functioned differentially, and assessing the functioning o f the items comprising deviant categories to determine if the differential effect was pervasive. Results showed an association between selected item categories and differential functioning, particularly for items that included figures in the stimulus, items for which spatial estimation was helpful in eliminating at least two of the options, and items that presented figures that were small or medium in size. The precise meaning of this association was unclear, however, because some items from the suspected categories functioned normally, factors other than the hypothesized ones might have caused the observed aberrant item behavior, and the differential difficulty might reflect real population differences in relevant content knowledge Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0007334947"
"Kane M.T.","Kane, Michael T. (36088969800)","36088969800","On the Use of IRT Models With Judgmental Standard Setting Procedures","1987","Journal of Educational Measurement","24","4","","333","345","12","24","10.1111/j.1745-3984.1987.tb00284.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988103878&doi=10.1111%2fj.1745-3984.1987.tb00284.x&partnerID=40&md5=2676c22ce406822bdaae8330477cec10","In judgmental standard setting procedures (e.g., the Angoff procedure), expert raters establish minimum pass levels (MPLs) for test items, and these MPLs are then combined to generate a passing score for the test. As suggested by Van der Linden (1982), item response theory (IRT) models may be useful in analyzing the results of judgmental standard setting studies. This paper examines three issues relevant to the use of lRT models in analyzing the results of such studies. First, a statistic for examining the fit of MPLs, based on judges' ratings, to an IRT model is suggested. Second, three methods for setting the passing score on a test based on item MPLs are analyzed; these analyses, based on theoretical models rather than empirical comparisons among the three methods, suggest that the traditional approach (i.e., setting the passing score on the test equal to the sum of the item MPLs) does not provide the best results. Third, a simple procedure, based on generalizability theory, for examining the sources of error in estimates of the passing score is discussed. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988103878"
"EMBRETSON S.; SCHNEIDER L.M.; ROTH D.L.","EMBRETSON, SUSAN (6603914093); SCHNEIDER, LISA M. (56232354800); ROTH, DAVID L. (13309995400)","6603914093; 56232354800; 13309995400","MULTIPLE PROCESSING STRATEGIES AND THE CONSTRUCT VALIDITY OF VERBAL REASONING TESTS","1986","Journal of Educational Measurement","23","1","","13","32","19","11","10.1111/j.1745-3984.1986.tb00231.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988068804&doi=10.1111%2fj.1745-3984.1986.tb00231.x&partnerID=40&md5=cb1f1e7f09afd8d1aa02e5c78a0882e9","This study examines the influence of processing strategies, and the associated metacomponents that determine when to apply them, on the construct validity of a verbal reasoning test. Three strategies for solving verbal analogy items were examined: a rule‐oriented strategy, an association strategy, and a partial rule strategy. Construct validity was studied in two separate stages: construct representation and nomothetic span. For construct representation, evidence was obtained that all three strategies, and their related metacomponents, are associated with performance on analogy items. For nomothetic span, the current study found that all three strategies contribute to individual differences in verbal reasoning and to the predictive validity of the test. The results of this study also point to the utility of metacomponents as constructs for describing and understanding test performance. Implications of the results for test development and theories of aptitude are elaborated. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988068804"
"LORD F.M.","LORD, FREDERIC M. (16463899300)","16463899300","MAXIMUM LIKELIHOOD AND BAYESIAN PARAMETER ESTIMATION IN ITEM RESPONSE THEORY","1986","Journal of Educational Measurement","23","2","","157","162","5","72","10.1111/j.1745-3984.1986.tb00241.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979601298&doi=10.1111%2fj.1745-3984.1986.tb00241.x&partnerID=40&md5=ec47ed16d2c2ffd533f1d7a667aeccb6","Advantages and disadvantages of joint maximum likelihood, marginal maximum likelihood, and Bayesian methods of parameter estimation in item response theory are discussed and compared. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84979601298"
"BENTON S.L.; KIEWRA K.A.","BENTON, STEPHEN L. (7006442912); KIEWRA, KENNETH A. (6603636291)","7006442912; 6603636291","MEASURING THE ORGANIZATIONAL ASPECTS OF WRITING ABILITY","1986","Journal of Educational Measurement","23","4","","377","386","9","6","10.1111/j.1745-3984.1986.tb00257.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010523127&doi=10.1111%2fj.1745-3984.1986.tb00257.x&partnerID=40&md5=1645d2220b8674bb6423dfe8b48ec0bf","The present study assessed the relationship among holistic writing ability, the Test of Standard Written English (TSWE), and the following tests of organizational ability: anagram solving, word reordering, sentence reordering, and paragraph assembly. Based upon a sample of 105 undergraduate students, the main findings were that writing ability, as measured by the holistic method of scoring, was significantly correlated with performance on the TSWE and the four tests of organizational ability. A composite score on all four organizational tests was found to have the highest zero‐order correlation with the measure of writing ability. A stepwise regression analysis, with the measure of writing ability as the criterion, also indicated that the composite score explained a significant proportion of the variance beyond that explained by the TSWE. The results are discussed in terms of the Kintsch and van Dijk model of strategic discourse processing, which suggests that different organizational strategies operate at the levels of words, sentences, and paragraphs. It is concluded that tests assessing organizational strategies ought to be included in assessments of writing ability. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85010523127"
"Wilcox R.R.; Wilcox K.T.; Chung J.","Wilcox, Rand R. (7202527113); Wilcox, Karen Thompson (7005707462); Chung, Jacob (57191233022)","7202527113; 7005707462; 57191233022","A Note on Decisionmaking Processes for Multiple‐Choice Test Items","1988","Journal of Educational Measurement","25","3","","247","250","3","0","10.1111/j.1745-3984.1988.tb00306.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074676&doi=10.1111%2fj.1745-3984.1988.tb00306.x&partnerID=40&md5=26c06d47065f27e15dbc6659a593fcf8","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988074676"
"Braun H.I.","Braun, Henry I. (7202368777)","7202368777","A New Approach to Avoiding Problems of Scale in Interpreting Trends in Mental Measurement Data","1988","Journal of Educational Measurement","25","3","","171","191","20","12","10.1111/j.1745-3984.1988.tb00301.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988075005&doi=10.1111%2fj.1745-3984.1988.tb00301.x&partnerID=40&md5=7eb4cc8aac2826691a76fd452380620e","Extracting policy‐relevant information from large national surveys of educational achievement is ordinarily a nontrivial task. It is made more treacherous when the data are expressed on scales that are not uniquely determined. The paper begins with a critical analysis of a recent attempt to interpret the findings on reading achievement obtained by the National Assessment of Educational Progress (NAEP). It then describes a new approach to the quantification and interpretation of change and demonstrates its appropriateness for repeated cross‐sectional designs such as NAEP. Limitations imposed by the survey design and the nature of the measurements are highlighted Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988075005"
"MEHRENS W.A.; PHILLIPS S.E.","MEHRENS, WILLIAM A. (6602621148); PHILLIPS, S.E. (7402028180)","6602621148; 7402028180","DETECTING IMPACTS OF CURRICULAR DIFFERENCES IN ACHIEVEMENT TEST DATA","1986","Journal of Educational Measurement","23","3","","185","196","11","22","10.1111/j.1745-3984.1986.tb00244.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988070196&doi=10.1111%2fj.1745-3984.1986.tb00244.x&partnerID=40&md5=1fbf44014049deb843e5b80685e7281c","Standardized tests are designed to measure broad goals. But many professionals have been concerned with the lack of fairly specific matches between items (or objectives) on a test and the curriculum (instruction). This study assessed the differences in standardized test scores resulting from curricular differences in two school systems. The degree of curriculum‐test match for reading and math in grades 3 and 6 was based on ratings of that match by qualified district personnel. Further, results of using different textbook series were analyzed. The dependent variables of test and subtest scores were analyzed using a two‐factor MANCOVA where textbook series and school personnel ratings were the two factors, and pretest scores and percent eligible for Aid to Families with Dependent Children (AFDC) were the covariates. None of the multivariate F tests were significant at the .05 level. It was concluded that neither the curricular match as judged by district personnel or the textbook series used had a significant impact on standardized test scores. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988070196"
"SAWYER R.","SAWYER, RICHARD (7201516630)","7201516630","USING DEMOGRAPHIC SUBGROUP AND DUMMY VARIABLE EQUATIONS TO PREDICT COLLEGE FRESHMAN GRADE AVERAGE","1986","Journal of Educational Measurement","23","2","","131","145","14","14","10.1111/j.1745-3984.1986.tb00239.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016181694&doi=10.1111%2fj.1745-3984.1986.tb00239.x&partnerID=40&md5=e6d90627cc1293ddce50a45ecb3c1ea3","The purpose of this study was to determine whether adjustments for the differential prediction observed among sex, racial/ethnic, or age subgroups in one freshman class at a college could be used to improve prediction accuracy for these subgroups in future freshman classes. For older students, dummy variable and separate subgroup prediction equations were found, on cross‐validation, to be more accurate than the total group equations. For sex subgroups, dummy variable and separate subgroup equations were only moderately effective in improving prediction accuracy. For racial/ethnic subgroups, they were more often than not less accurate, on cross‐validation, than total group equations. Among all three kinds of demographic subgroupings, shifts over time in colleges' mean grade averages were found to be a more important source of prediction bias than differential prediction. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85016181694"
"Doolittle A.E.; Cleary T.A.","Doolittle, Allen E. (57065318700); Cleary, T. Anne (7005072219)","57065318700; 7005072219","Gender‐Based Differential Item Performance in Mathematics Achievement Items","1987","Journal of Educational Measurement","24","2","","157","166","9","50","10.1111/j.1745-3984.1987.tb00271.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106750&doi=10.1111%2fj.1745-3984.1987.tb00271.x&partnerID=40&md5=c53a0834e5a2cbe0b7257f0e888b50c8","A procedure for the detection of differential item performance (DIP) is used to investigate the relationships between characteristics of mathematics achievement items and gender differences in performance. Eight randomly equivalent samples of high school seniors were each given a unique form of the ACT Assessment Mathematics Usage Test (ACTM). Students without requisite mathematics courses were deleted from the samples to reduce the confounding effects of differences in instruction at the high school level. Signed measures of DIP were obtained for each item in the eight ACTM forms. These DIP estimates were then analyzed in a 6 × 8 (item category by form) experimental design. A significant item category effect was found indicating a relationship between item characteristics and gender‐based DIP. Predictions, based on previous research about the categories of items that would contribute to gender‐based DIP, were supported: Geometry and mathematics reasoning items were relatively more difficult for female examinees and the more algorithmic, computation‐oriented items were relatively easier. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988106750"
"Zwick R.; Ercikan K.","Zwick, Rebecca (7004200859); Ercikan, Kadriye (6603174172)","7004200859; 6603174172","Analysis of Differential Item Functioning in the NAEP History Assessment","1989","Journal of Educational Measurement","26","1","","55","66","11","107","10.1111/j.1745-3984.1989.tb00318.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002911440&doi=10.1111%2fj.1745-3984.1989.tb00318.x&partnerID=40&md5=1c94248d66ff5ffe5cec63665fbfcda5","The Mantel‐Haenszel approach for investigating differential item functioning was applied to U.S. history items that were administered as part o f the National Assessment o f Educational Progress, On some items, blacks, Hispanics, and females performed more poorly than other students, conditional on number‐right score. It was hypothesized that this resulted, in part, from the fact that ethnic and gender groups differed in their exposure to the material included in the assessment. Supplementary Mantel‐Haenszel analyses were undertaken in which the number o f historical periods studied, as well as score. was used as a conditioning variable. Contrary to expectation, the additional conditioning did not lead to a reduction in the number o f DIF items. Both methodological and substantive explanations for this unexpected result were explored. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0002911440"
"Bock R.D.; Murakl E.; Pfeiffenberger W.","Bock, R. Darrell (55436391100); Murakl, Eiji (57191235062); Pfeiffenberger, Will (6504760166)","55436391100; 57191235062; 6504760166","Item Pool Maintenance in the Presence of Item Parameter Drift","1988","Journal of Educational Measurement","25","4","","275","285","10","79","10.1111/j.1745-3984.1988.tb00308.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099050&doi=10.1111%2fj.1745-3984.1988.tb00308.x&partnerID=40&md5=770619446f1c85391bab4f3833122e3d","Differential linear drift of item location parameters over a 10 ‐year period is demonstrated in data from the College Board Physics Achievement Test. The relative direction of drift is associated with the content of the items and reflects changing emphasis in the physics curricula of American secondary schools. No evidence of drift of discriminating power parameters was found. Statistical procedures for detecting, estimating, and accounting for item parameter drift in item pools for long‐term testing programs are proposed Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988099050"
"Bennett R.E.; Rock D.A.; Kaplan B.A.","Bennett, Randy Elliot (7402440584); Rock, Donald A. (7102591023); Kaplan, Bruce A. (55422892100)","7402440584; 7102591023; 55422892100","SAT Differential Item Performance for Nine Handicapped Groups","1987","Journal of Educational Measurement","24","1","","41","55","14","30","10.1111/j.1745-3984.1987.tb00260.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984453034&doi=10.1111%2fj.1745-3984.1987.tb00260.x&partnerID=40&md5=970877d3d46129448bd8f9fe716b7fb7","The purpose of this study was to identify broad classes of items that behave differentially for handicapped examinees taking special, extended‐time administrations of the Scholastic Aptitude Test (SA T). To identify these item classes, the performance of nine handicapped groups and one nonhandicapped group on each of two forms of the SAT was investigated through a two‐stage procedure. The first stage centered on the performance of item clusters. Individual items composing clusters showing questionable performance were then examined. This two‐stage procedure revealed little indication of differentially functioning item classes. However, some notable instances of differential performance at the item level were detected, the most serious of which affected visually impaired students taking the braille edition of the test. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84984453034"
"CHASE C.I.","CHASE, CLINTON I. (7102500238)","7102500238","ESSAY TEST SCORING: INTERACTION OF RELEVANT VARIABLES","1986","Journal of Educational Measurement","23","1","","33","41","8","70","10.1111/j.1745-3984.1986.tb00232.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963332750&doi=10.1111%2fj.1745-3984.1986.tb00232.x&partnerID=40&md5=f220e7e6f3b9b0459744afd831daece7","In studies of essay tests, a single independent variable, such as penmanship, is often observed and conclusions are made about the relevance of that variable in scoring tests. But in this study it is hypothesized that the readers of an essay respond to a variable in terms of its context with other variables, that is, readers may be more tolerant of poor penmanship for males than females, especially if expectations for the writer are involved. In this investigation sex, race, reader expectation, and quality of handwriting were crossed to study their interaction effects. A single student essay was contrived. Each of 80 readers was given the essay plus a student's record card, which identified the student as a high or low achiever (expectancy), black or white, male or female. The essay was written in either good or poor handwriting. A reader read only one essay under one combination of conditions. The results showed complex interactions of expectations, writing, and sex within race. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84963332750"
"Albanese M.A.; Sabers D.L.","Albanese, Mark A. (7006286551); Sabers, Darrell L. (6701710722)","7006286551; 6701710722","Multiple True‐False Items: A Study of Interitem Correlations, Scoring Alternatives, and Reliability Estimation","1988","Journal of Educational Measurement","25","2","","111","123","12","29","10.1111/j.1745-3984.1988.tb00296.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988120948&doi=10.1111%2fj.1745-3984.1988.tb00296.x&partnerID=40&md5=fb742650833342eef07e266d5f12b831","Intercorrelations among multiple true‐false items were examined to determine to what extent each true‐false option can be treated as independent. Results from 157 health science students and 170 medical students showed that correlations between true‐false options associated with the same stem were from 2.6 to 7.0 times larger than those from different stems. This suggests that results from previous research indicating that each true‐false option could be treated as an independent item cannot be generalized to other tests and examinee populations without supporting evidence. Four scoring methods were explored which varied chance success levels and scoring for partial knowledge. The results showed that scoring methods incorporating partial knowledge were more reliable and possessed greater concurrent and predictive validity than those minimizing chance success. Methods for computing reliability estimates were compared and suggestions were offered regarding practical use Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988120948"
"Subkoviak M.J.","Subkoviak, Michael J. (6506489478)","6506489478","A Practitioner's Guide to Computation and Interpretation of Reliability Indices for Mastery Tests","1988","Journal of Educational Measurement","25","1","","47","55","8","60","10.1111/j.1745-3984.1988.tb00290.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032069360&doi=10.1111%2fj.1745-3984.1988.tb00290.x&partnerID=40&md5=b2de4cc42c5791ebe7b74d607c2a4d5a","From the perspective of teachers and test makers at the district or state level, current methods for obtaining reliability indices for mastery tests like the agreement coefficient and kappa coefficient are quite laborious. For example, some methods require two test administrations, whereas single administration approaches involve complex statistical procedures and require access to appropriate computer software. The present paper offers practitioners tables from which agreement and kappa coefficients can be read directly. Further‐more, because these indices differ from traditional reliability coefficients, the issue of what constitutes acceptable values of agreement and kappa coefficients is also addressed Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85032069360"
"ANGOFF W.H.; COWELL W.R.","ANGOFF, WILLIAM H. (16648034100); COWELL, WILLIAM R. (57195040559)","16648034100; 57195040559","AN EXAMINATION OF THE ASSUMPTION THAT THE EQUATING OF PARALLEL FORMS IS POPULATION‐INDEPENDENT","1986","Journal of Educational Measurement","23","4","","327","345","18","23","10.1111/j.1745-3984.1986.tb00253.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988081654&doi=10.1111%2fj.1745-3984.1986.tb00253.x&partnerID=40&md5=c6f6f9ce30eae3995f130c3c88bd4a51","Linear conversions were developed relating scores on two recent forms of the homogeneous GRE Quantitative Test (GRE‐Q) and the specially constituted heterogeneous GRE Verbal‐plus‐Quantitative Test (GRE‐V+Q), using randomly equivalent groups of about 13, 000 taking each form. Specially defined homogeneous subpopulations were then identified, and conversions between scores on the two forms were again calculated, this time based on l000‐case samples drawn at random from the subpopulations. Finally, in order to develop empirical measures of equating error, I00 samples of 1, 000 cases each were drawn at random from the two total groups and used to calculate 100 conversions between scores on the two forms. The conversions based on the specially selected subpopulations were then compared with the total‐group conversions and evaluated in terms of the empirical standard errors. The results showed that the conversions for the subpopulations agreed with the total‐group conversion quite satisfactorily for the GRE‐Q and almost as well for the GRE‐V+Q. It was concluded that the data clearly support the assumption of population independence for homogeneous tests, but not quite so clearly for heterogeneous tests. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988081654"
"Powers D.E.","Powers, Donald E. (7202818498)","7202818498","Who Benefits Most From Preparing for a “Coachable” Admissions Test?","1987","Journal of Educational Measurement","24","3","","247","262","15","17","10.1111/j.1745-3984.1987.tb00278.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099175&doi=10.1111%2fj.1745-3984.1987.tb00278.x&partnerID=40&md5=83df2de361666bc41b29e6684a591eab","A previous study of the initial, preoperational version of the Graduate Record Examinations (GRE) analytical ability measure (Powers & Swinton, 1984) revealed practically and statistically significant effects of test familiarization on analytical test scores. (Two susceptible item types were subsequently removed from the test.) Data from this study were reanalyzed for evidence of differential effects for subgroups of examinees classified by age, ethnicity, degree aspiration, English language dominance, and performance on other sections of the GRE General Test. The results suggested little, if any, difference among subgroups of examinees with respect to their response to the particular kind of test preparation considered in the study. Within the limits of the data, no particular subgroup appeared to benefit significantly more or significantly less than any other subgroup. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988099175"
"Chalifour C.L.; Powers D.E.","Chalifour, Clark L. (57191233685); Powers, Donald E. (7202818498)","57191233685; 7202818498","The Relationship of Content Characteristics of GRE Analytical Reasoning Items to Their Difficulties and Discriminations","1989","Journal of Educational Measurement","26","2","","120","132","12","17","10.1111/j.1745-3984.1989.tb00323.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122271&doi=10.1111%2fj.1745-3984.1989.tb00323.x&partnerID=40&md5=921755f2deee0562f93fd9a0ff942b0d","In actual test development practice, the number o f test items that must be developed and pretested is typically greater, and sometimes much greater, than the number that is eventually judged suitable for use in operational test forms. This has proven to be especially true for one item type–analytical reasoning‐that currently forms the bulk of the analytical ability measure of the GRE General Test. This study involved coding the content characteristics of some 1,400 GRE analytical reasoning items. These characteristics were correlated with indices of item difficulty and discrimination. Several item characteristics were predictive of the difficulty of analytical reasoning items. Generally, these same variables also predicted item discrimination, but to a lesser degree. The results suggest several content characteristics that could be considered in extending the current specifications for analytical reasoning items. The use of these item features may also contribute to greater efficiency in developing such items. Finally, the influence of these various characteristics also provides a better understanding of the construct validity of the analytical reasoning item type. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988122271"
"Green B.F.; Crone C.R.; Folk V.G.","Green, Bert F. (7401825047); Crone, Carolyn R. (57191234380); Folk, Valerie Greaud (6507575516)","7401825047; 57191234380; 6507575516","A Method for Studying Differential Distractor Functioning","1989","Journal of Educational Measurement","26","2","","147","160","13","39","10.1111/j.1745-3984.1989.tb00325.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988129065&doi=10.1111%2fj.1745-3984.1989.tb00325.x&partnerID=40&md5=157563cd7cd2daaaa017fa2c7c2e681b","A method of analyzing test item responses is advocated to examine differential item functioning through distractor choices of those who answer an item incorrectly. The analysis, called Differential Distractor Functioning, uses log‐linear models of a three‐way contingency table to examine whether there is an interaction of population subgroup and option choice when ability is held constant. The analysis is explained and is exemplified in an analysis of the Verbal portion of a recent Scholastic Aptitude Test. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988129065"
"Kane M.T.; Kingshury C.; Colton D.; Estes C.","Kane, Michael T. (36088969800); Kingshury, Carole (57191232357); Colton, Dean (7005083059); Estes, Carmen (7005765539)","36088969800; 57191232357; 7005083059; 7005765539","Combining Data on Criticality and Frequency in Developing Test Plans for Licensure and Certification Examinations","1989","Journal of Educational Measurement","26","1","","17","27","10","36","10.1111/j.1745-3984.1989.tb00315.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040298734&doi=10.1111%2fj.1745-3984.1989.tb00315.x&partnerID=40&md5=e4330397cd8e7f391e3fa3453f853504","Job analysis is a critical component in evaluating the validity of many high‐stakes testing programs, particularly those used for licensure or certification. The ratings of criticality and frequency of various activities that are derived from such job analyses can be combined in a number of ways. This paper develops a multiplicative model as a natural and effective way to combine ratings o f frequency and criticality in order to obtain estimates of the relative importance of different activities for practice. An example of the model's use is presented. The multiplicative model incorporates adjustments to ensure that the effective weights of frequency and criticality are appropriate. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0040298734"
"ROWE D.W.","ROWE, DEBORAH W. (7201668048)","7201668048","DOES RESEARCH SUPPORT THE USE OF “PURPOSE QUESTIONS” ON READING COMPREHENSION TESTS?","1986","Journal of Educational Measurement","23","1","","43","55","12","6","10.1111/j.1745-3984.1986.tb00233.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012447067&doi=10.1111%2fj.1745-3984.1986.tb00233.x&partnerID=40&md5=e64e8e8d6eebddd77666c98db3641dae","The inclusion of “purpose questions” before passages in reading comprehension tests is evaluated in light of recent research related to schema theory and adjunct questions. Schema‐theoretic studies of prepassage cues to the theme of a reading selection suggest that purpose questions may serve a schema activating function. Recent studies of higher level (nonliteral) prepassage questions also support this view. Conclusions commonly drawn from studies using literal level prepassage questions cannot be applied directly to the use of purpose questions on the Metropolitan Achievement Tests. While research supports the use of purpose questions, gaps and weaknesses in the available data exist. Suggestions for further study are provided. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85012447067"
"Tatsuoka K.K.","Tatsuoka, Kikumi K. (6603447775)","6603447775","Validation of Cognitive Sensitivity for Item Response Curves","1987","Journal of Educational Measurement","24","3","","233","245","12","17","10.1111/j.1745-3984.1987.tb00277.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988123003&doi=10.1111%2fj.1745-3984.1987.tb00277.x&partnerID=40&md5=3a29edf324af8effb6cf43a2275b750d","This study sought a scientific way to examine whether item response curves are influenced systematically by the cognitive processes underlying solution of the items in a procedural domain (addition of fractions). Starting from an expert teacher's logical task analysis and prediction of various erroneous rules and sources of misconceptions, an error diagnostic program was developed. This program was used to carry out an error analysis of test performance by three samples of students. After the cognitive structure of the subtasks was validated by a majority of the students, the items were characterized by their underlying subtask patterns. It was found that item response curves for items in the same categories were significantly more homogeneous than those in different categories. In other words, underlying cognitive subtasks appeared to systematically influence the slopes and difficulties of item response curves. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988123003"
"Powell B.; Steelman L.C.","Powell, Brian (7202173513); Steelman, Lala Carr (7003565358)","7202173513; 7003565358","On State SAT Research: A Response to Wainer","1987","Journal of Educational Measurement","24","1","","84","89","5","4","10.1111/j.1745-3984.1987.tb00266.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928455132&doi=10.1111%2fj.1745-3984.1987.tb00266.x&partnerID=40&md5=addcc76fd27fce507ffdc7e96ad7ec4a","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84928455132"
"Miller M.D.; Linn R.L.","Miller, M. David (55757783023); Linn, Robert L. (56126633100)","55757783023; 56126633100","Invariance of Item Characteristic Functions With Variations in Instructional Coverage","1988","Journal of Educational Measurement","25","3","","205","219","14","28","10.1111/j.1745-3984.1988.tb00303.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988077139&doi=10.1111%2fj.1745-3984.1988.tb00303.x&partnerID=40&md5=2cd423b933e7cbb015699f3598fde017","An assumption of item response theory is that a person's score is a function of the item response parameters and the person's ability. In this paper, the effect of variations in instructional coverage on item characteristic functions is examined. Using data from the Second International Mathematics Study (1985), curriculum clusters were formed based on teachers’ ratings of their students’ opportunities to learn the items on a test. After forming curriculum clusters, item response curves were compared using signed and unsigned sum of squared differences. Some of the differences in the item response curves between curriculum clusters were found to be large, but better performance was not necessarily related to greater opportunity to learn. The item response curve differences were much larger than differences reported in prior studies based on comparisons of black and white students. Implications of the findings for applications of item response theory to educational achievement test data are discussed Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988077139"
"Wilcox R.R.; Wilcox K.T.","Wilcox, Rand R. (7202527113); Wilcox, Karen Thompson (7005707462)","7202527113; 7005707462","Models of Decisionmaking Processes for Multiple‐Choice Test Items: An Analysis of Spatial Ability","1988","Journal of Educational Measurement","25","2","","125","136","11","1","10.1111/j.1745-3984.1988.tb00297.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988075070&doi=10.1111%2fj.1745-3984.1988.tb00297.x&partnerID=40&md5=7eeb3967551172cfd575e721fab21a52","Latent class models of decisionmaking processes related to multiple‐choice test items are extremely important and useful in mental test theory. However, building realistic models or studying the robustness of existing models is very difficult. One problem is that there are a limited number of empirical studies that address this issue. The purpose of this paper is to describe and illustrate how latent class models, in conjunction with the answer‐until‐correct format, can be used to examine the strategies used by examinees for a specific type of task. In particular, suppose an examinee responds to a multiple‐choice test item designed to measure spatial ability, and the examinee gets the item wrong. This paper empirically investigates various latent class models of the strategies that might be used to arrive at an incorrect response. The simplest model is a random guessing model, but the results reported here strongly suggest that this model is unsatisfactory. Models for the second attempt of an item, under an answer‐until‐correct scoring procedure, are proposed and found to give a good fit to data in most situations. Some results on strategies used to arrive at the first choice are also discussed Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988075070"
"Angoff W.H.","Angoff, William H. (16648034100)","16648034100","Does Guessing Really Help?","1989","Journal of Educational Measurement","26","4","","323","336","13","13","10.1111/j.1745-3984.1989.tb00337.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090407&doi=10.1111%2fj.1745-3984.1989.tb00337.x&partnerID=40&md5=0ccb4fb82e806cbf48a9353ef0ae148c","This study examines the claim that attempting, or guessing at, more items yields improved formula scores. Two samples of students who had taken a form of the SA T‐ Verbal consisting of three parallel half‐hour sections, were used to form the following scores on each of the three sections: the number of attempts, a guessing index, the formula score, and (indirectly) an approximation to an ability score. Correlations were obtained separately for the two samples between the attempts, and the guessing index, on one section, the formula score on a second section, and ability as measured by the third section. The partial correlations obtained hovered near zero, suggesting, contrary to conventional opinion, that, on average, attempting more items and guessing are not helpful in yielding higher formula scores, and that, therefore, formula scoring is not generally disadvantageous to the student who is less willing to guess and attempt an item that he or she is not sure of. On closer examination, however, it became clear that the advantages of guessing depend, at least in part, on the ability of the examinee. Although the relationship is generally quite weak, it is apparently the case that more able examinees do tend to profit somewhat from guessing, and would therefore be disadvantaged by their reluctance to guess. On the other hand, less able examinees may lower their scores i f they guess. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988090407"
"WAINER H.","WAINER, HOWARD (7006218234)","7006218234","FIVE PITFALLS ENCOUNTERED WHILE TRYING TO COMPARE STATES ON THEIR SAT SCORES","1986","Journal of Educational Measurement","23","1","","69","81","12","22","10.1111/j.1745-3984.1986.tb00235.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988082594&doi=10.1111%2fj.1745-3984.1986.tb00235.x&partnerID=40&md5=b3057202ef3c0133ad4b92477bdf6706","Recent research (Page & Feifs, 1985; Powell & Steelman, 1984; Steelman & Powell, 1985) attempts to draw inferences about the relative standing of the states on the basis of mean SAT scores. These papers point out that this cannot be done without statistically adjusting for various differences among the states. In this paper I identify five serious errors that, when made, call into question the validity of such inferences. In addition, I describe some plausible ways to avoid the errors. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988082594"
"Frary R.B.","Frary, Robert B. (6602858608)","6602858608","The Effect of Inappropriate Omissions on Formula Scores: A Simulation Study","1989","Journal of Educational Measurement","26","1","","41","53","12","2","10.1111/j.1745-3984.1989.tb00317.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956137521&doi=10.1111%2fj.1745-3984.1989.tb00317.x&partnerID=40&md5=8a64f64d9c591651a32f96c6511bbebf","Responses to a 50‐item, four‐choice test were simulated for 1,000 examinees under conventional formula‐scoring instructions. One hundred ninety‐two simulation runs reflected variations in the average level o f item difficulty, the extent to which examinees tended to omit inappropriately (when the formulascoring directions recommended guessing), the extent to which they were misinformed (classified correct answers as distractors), the extent to which they guessed contrary to the formula‐scoring instructions, the extent to which examinee ability and tendency to omit inappropriately were correlated, the examinee ability level at which misinformation was most prevalent, and the extent to which item difficulty was related to the probability that an examinee would be misinformed. For each examinee, formula scores and expected formula scores were determined allowing and not allowing inappropriate omissions. Under certain conditions, failure to guess as recommended by the formula‐scoring instructions produced nontrivial proportions o f examinees with expected score losses o f one or more points. These conditions were a test o f at least moderate difficulty, a low level for the tendency to be misinformed, and at least a moderate level for the tendency to omit inappropriately. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-77956137521"
"Willms J.D.; Raudenbush S.W.","Willms, J. Douglas (7003798144); Raudenbush, Stephen W. (7003383172)","7003798144; 7003383172","A Longitudinal Hierarchical Linear Model for Estimating School Effects and Their Stability","1989","Journal of Educational Measurement","26","3","","209","232","23","185","10.1111/j.1745-3984.1989.tb00329.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988123007&doi=10.1111%2fj.1745-3984.1989.tb00329.x&partnerID=40&md5=b34d488606a9ace606fb0a0d8df8df26","This paper presents a general longitudinal model for estimating school effects and their stability. Previous research on the stability of school performance over successive years has produced inconsistent findings. We argue that the findings have been inconsistent for at least two reasons: researchers have estimated different types of school effects, and they have not distinguished between instability due to true changes in school performance and instability due to measurement and sampling error. We describe two different types of school effects, each relevant to a different policy audience, and we present a longitudinal model that is capable of separating true changes in school effects from sampling and measurement error. The model also provides a means for estimating the effects of school policies and practices while controlling statistically for the effects of factors exogenous to the schooling system. This paper provides an example of the approach based on data describing two cohorts of students from one Education Authority in Scotland. It concludes with a discussion of the limitations of the model and implications for those collecting indicators of school performance for planning and evaluation purposes. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988123007"
"Hirsch T.M.","Hirsch, Thomas M. (57064906200)","57064906200","Multidimensional Equating","1989","Journal of Educational Measurement","26","4","","337","349","12","11","10.1111/j.1745-3984.1989.tb00338.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122557&doi=10.1111%2fj.1745-3984.1989.tb00338.x&partnerID=40&md5=e87e0d1d78da111613d221a631c84f6b","Equatings were performed on both simulated and real data sets using the common‐examinee design and two abilities for each examinee (i.e., two dimensions). Item and ability parameter estimates were found by using the Multidimensional Item Response Theory Estimation (MIRTE) program. The amount of equating error was evaluated by a comparison of the mean difference and the mean absolute difference between the true scores and ability estimates found on both tests for the common examinees used in the equating. The results indicated that effective equating, as measured by comparability o f true scores, was possible with the techniques used in this study. When the stability o f the ability estimates was examined, unsatisfactory results were found. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988122557"
"Phillips S.E.; Mehrens W.A.","Phillips, S.E. (7402028180); Mehrens, William A. (6602621148)","7402028180; 6602621148","Curricular Differences and Unidimensionality of Achievement Test Data: An Exploratory Analysis","1987","Journal of Educational Measurement","24","1","","1","16","15","8","10.1111/j.1745-3984.1987.tb00258.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988117779&doi=10.1111%2fj.1745-3984.1987.tb00258.x&partnerID=40&md5=ea1a79bf0abd06f5ddb520b902be9b11","The purpose of this study was to investigate whether a linear factor analytic method commonly used to investigate violation of the item response theory (IRT) unidimensionality assumption is sensitive to measurable curricular differences within a school district and to examine the possibility of differential item performance for groups of students receiving different instruction. For grades 3 and 6 in reading and mathematics, personnel from two midwestern school systems that regularly administer standardized achievement tests identified the formal textbook series used and provided ratings of test‐instructional match for each school building (classroom). For both districts, the factor analysis results suggested no differences in percentages of variance for large first factors and relatively small second factors across ratings or series groups. The IRT analyses indicated little, if any, differential item performance for curricular subgroups. Thus, the impact of factors that might be related to curricular differences was judged to be minor. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988117779"
"Thissen D.; Steinberg L.; Fitzpatrick A.R.","Thissen, David (7003712685); Steinberg, Lynne (7005545940); Fitzpatrick, Anne R. (7004620831)","7003712685; 7005545940; 7004620831","Multiple‐Choice Models: The Distractors Are Also Part of the Item","1989","Journal of Educational Measurement","26","2","","161","176","15","73","10.1111/j.1745-3984.1989.tb00326.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988129071&doi=10.1111%2fj.1745-3984.1989.tb00326.x&partnerID=40&md5=48d0340547352ff26fb231c93146c94a","This paper describes an item response model for multiple‐choice items and illustrates its application in item analysis. The model provides parametric and graphical summaries of the performance of each alternative associated with a multiple‐choice item; the summaries describe each alternative's relationship to the proficiency being measured. The interpretation of the parameters of the multiple‐choice model and the use of the model in item analysis are illustrated using data obtained from a pilot test of mathematics achievement items. The use of such item analysis for the detection of flawed items, for item design and development, and for test construction is discussed. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988129071"
"Marsh H.W.","Marsh, Herbert W. (7201585638)","7201585638","The Hierarchical Structure of Self‐Concept and the Application of Hierarchical Confirmatory Factor Analysis","1987","Journal of Educational Measurement","24","1","","17","39","22","231","10.1111/j.1745-3984.1987.tb00259.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938026245&doi=10.1111%2fj.1745-3984.1987.tb00259.x&partnerID=40&md5=8beff813ef0c7bf494ad6040b0391970","This investigation (a) tested the ability of an a priori hierarchical structure of self‐concept derived from the Shavelson model to explain responses to the Self Description Questionnaire (SDQ) III, and (b) demonstrated the application and problems with the use of hierarchical confirmatory factor analysis (HCFA). A first‐order factor analysis clearly identified all 13 facets of self‐concept that the SDQ 111 is designed to measure. A series of HCFA models supported the separation of the 13 SDQ III facets of self‐concept into academic and nonacademic components, and the academic facets into math/academic and verbal/academic components. However, support for separation of nonacademic facets into the physical, social, and moral second‐order factors was not strong. Third‐order HCFA models resulted in a clearly defined hierarchical general self‐concept that was substantially related to Esteem and to physical, social, and emotional components of self‐concept, but not to the academic and moral components. However, the hierarchy was so weak that first‐order facets could not be accurately inferred from the higher order facets. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84938026245"
"ALBANESE M.A.","ALBANESE, MARK A. (7006286551)","7006286551","THE CORRECTION FOR GUESSING: A FURTHER ANALYSIS OF ANGOFF AND SCHRADER","1986","Journal of Educational Measurement","23","3","","225","235","10","10","10.1111/j.1745-3984.1986.tb00248.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010192804&doi=10.1111%2fj.1745-3984.1986.tb00248.x&partnerID=40&md5=b5c7bab68e9825b1f2a38d88895f338d","This study reexamines results reported by Angoff and Schrader (1981, 1984) regarding formula directions and rights directions for standardized tests. In that study, it was concluded that the two scoring directions were essentially equivalent. In this study, methodological concerns are discussed and additional data analyses undertaken. Among the methodological concerns discussed are the potential problems in using volunteers for the College Board phase of the study and the likelihood of treatment contamination in the Graduate Management Admissions Test (GMAT) phase. Estimates of success rates of the rights directions group on items omitted and not reached by the formula group, were beyond chance levels by 0.6 % to 13.0 % for the College Board tests and at or below chance levels for the GMAT. Alternative interpretations of the data and suggestions for additional research are proposed. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85010192804"
"Norcini J.J.; Shea J.A.; Ping J.C.","Norcini, John J. (7006095771); Shea, Judy A. (35572343300); Ping, James C. (57191231686)","7006095771; 35572343300; 57191231686","A Note on the Application of Multiple Matrix Sampling to Standard Setting","1988","Journal of Educational Measurement","25","2","","159","164","5","9","10.1111/j.1745-3984.1988.tb00300.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112482&doi=10.1111%2fj.1745-3984.1988.tb00300.x&partnerID=40&md5=8e8f6a0bb78f554b73f1fe5b4a4f2fd9","In many of the methods currently proposed for standard setting, all experts are asked to judge all items, and the standard is taken as the mean of their judgments. When resources are limited, gathering the judgments of all experts in a single group can become impractical. Multiple matrix sampling (MMS) provides an alternative. This paper applies MMS to a variation on Angoff's method (1971) of standard setting. A pool of 36 experts and 190 items were divided randomly into 5 groups, and estimates of borderline examinee performance were acquired. Results indicated some variability in the cutting scores produced by the individual groups, but the variance components were reasonably well estimated. The standard error of the cutting score was very small, and the width of the 90% confidence interval around it was only 1.3 items. The reliability of the final cutting score was.98 Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988112482"
"Thissen D.; Steinberg L.; Mooney J.A.","Thissen, David (7003712685); Steinberg, Lynne (7005545940); Mooney, Jo Ann (57191233404)","7003712685; 7005545940; 57191233404","Trace Lines for Testlets: A Use of Multiple‐Categorical‐Response Models","1989","Journal of Educational Measurement","26","3","","247","260","13","133","10.1111/j.1745-3984.1989.tb00331.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988109768&doi=10.1111%2fj.1745-3984.1989.tb00331.x&partnerID=40&md5=9aeac827c6b9fc433dfe37d05062326d","It is not always convenient or appropriate to construct tests in which individual items are fungible. There are situations in which small clusters of items (testlets) are the units that are assembled to create a test. Using data from a test of reading comprehension constructed of four passages with several questions following each passage, we show that local independence fails at the level of the individual questions. The questions following each passage, however, constitute a testlet. We discuss the application to testlet scoring of some multiple‐category models originally developed for individual items, In the example examined, the concurrent validity of the testlet scoring equaled or exceeded that of individual‐item‐level scoring Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988109768"
"Masters G.N.","Masters, Geofferey N. (16448815200)","16448815200","Item Discrimination: When More Is Worse","1988","Journal of Educational Measurement","25","1","","15","29","14","69","10.1111/j.1745-3984.1988.tb00288.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090461&doi=10.1111%2fj.1745-3984.1988.tb00288.x&partnerID=40&md5=921747b215fffe5fe3f9b4f9d70da983","High item discrimination can be a symptom o f a special kind of measurement disturbance introduced by an item that gives persons o f high ability a special advantage over and above their higher abilities. This type o f disturbance, which can be interpreted as a form o f item “bias,” can be encouraged by methods that routinely interpret highly discriminating items as the “best” items on a test and may be compounded by procedures that weight items by their discrimination. The type of measurement disturbance described and illustrated in this paper occurs when an item is sensitive to individual differences on a second, undesired dimension that is positively correlated with the variable intended to be measured. Possible secondary influences o f this type include opportunity to learn, opportunity to answer, and test wiseness Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988090461"
"Klockars A.J.; Yamagishi M.","Klockars, Alan J. (6603819320); Yamagishi, Midori (58331964000)","6603819320; 58331964000","The Influence of Labels and Positions in Rating Scales","1988","Journal of Educational Measurement","25","2","","85","96","11","55","10.1111/j.1745-3984.1988.tb00294.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002924303&doi=10.1111%2fj.1745-3984.1988.tb00294.x&partnerID=40&md5=bb8ca8c44b8df8dd5e5387bd762f89bb","On occasion, rating scales are used in which the verbal labels are compressed to saturate one end of the response continuum. In these scales, differences arise between the normal meaning of the label and its scalar position. This paper investigated the influence of the label and the position in defining the meaning of the labeled position. Three forms of labeled rating scale were constructed where the labels FAIR and GOOD were systematically moved. Subjects judged the desirability of premarked responses on the scales. In addition, two forms were used to determine the desirability of the unlabeled scale position and the desirability of the label independent of a rating scale. When label and position differed in meaning, the labeled position was rated as a compromise between the scale value of the label and the position Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0002924303"
"Norcini J.J.","Norcini, John J. (7006095771)","7006095771","The Answer Key as a Source of Error in Examinations for Professionals","1987","Journal of Educational Measurement","24","4","","321","331","10","6","10.1111/j.1745-3984.1987.tb00283.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106417&doi=10.1111%2fj.1745-3984.1987.tb00283.x&partnerID=40&md5=db8583ff1fdfe5b121522b52e0585912","There has been a growing emphasis on written examinations that assess the ability of physicians and teachers to make decisions in the absence of protocols for action—a crucial aspect of professional competence. A characteristic of such tests is controversy, even among experts, about what constitutes the correct response to some of the items. This paper studied the impact of variability in answer keys, constructed using the aggregate method, on total errors of measurement. Results indicated that several scorers made a sizable contribution to reduction in measurement error and that scorers or groups of scorers who each developed the answer key for a subset of items produced better results than a single group that developed the answer key for all items. Implications of judgment tests scored using the aggregate method are described for teacher and physician certification. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988106417"
"Roberts D.M.","Roberts, Dennis M. (55465556800)","55465556800","Limitations of the Score‐Difference Method in Detecting Cheating in Recognition Test Situations","1987","Journal of Educational Measurement","24","1","","77","81","4","6","10.1111/j.1745-3984.1987.tb00264.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072551&doi=10.1111%2fj.1745-3984.1987.tb00264.x&partnerID=40&md5=df7a54e68580a71830f2c32bf0f9f305","An examination is made of a score‐difference model for the detection of cheating based on the difference between two scores for an examinee: one based on the appropriate scoring key and another based on an alternative, inappropriate key. The normal approximation to the binomial distribution model is used to predict characteristics of chance test score distributions and difference score distributions. Results show that students' difference scores can vary widely by chance alone, even for short tests. It is argued that the score‐difference method could falsely accuse many students as cheaters. It is suggested that the score‐difference method is seriously flawed and has little to recommend it. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988072551"
"Mehrens W.A.; Phillips S.E.","Mehrens, William A. (6602621148); Phillips, S.E. (7402028180)","6602621148; 7402028180","Sensitivity of Item Difficulties to Curricular Validity","1987","Journal of Educational Measurement","24","4","","357","370","13","11","10.1111/j.1745-3984.1987.tb00286.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988057715&doi=10.1111%2fj.1745-3984.1987.tb00286.x&partnerID=40&md5=f0853b126815b196390cf8fa53ca34d2","The issue of the match of what is taught to what is tested recently has received increased emphasis. Many recent studies have examined the extent of the mismatch between a local set of objectives/instruction and the content of a nationally standardized test. Phillips and Mehrens (1985, 1987; see also Mehrens & Phillips, 1986) have conducted a series of studies that show that, within a district, different textbook series and informal curricula generally have no significant impact on test total, objective, or item scores. The present study explored further the curricular validity differences across textbooks and the relationship between item statistics and measures of curricular validity. In particular, this study sought to determine whether item p‐values appear to be related to measures of curricular validity based on three mathematics textbook series used at a given grade and in a previous grade. The results of this study indicated that the textbooks differed somewhat in content coverage when using a 180‐cell matrix classification. However, these differences were not great, especially when the textbooks in both grades 5 and 6 were considered. Further, all three series covered almost all of the 53 cells in the matrix covered by the Stanford Achievement Test, and the differences that did exist in textbook content coverage had no observable relationship to differences in item p‐values. In addition, the mean difficulty level of the Stanford Achievement Test items classified by cell were similar for students using the different textbooks, despite the differences among textbooks in location, presentation, and organization of content. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988057715"
"Divgi D.R.","Divgi, D.R. (36961737400)","36961737400","Reply to Andrich and Henning","1989","Journal of Educational Measurement","26","3","","295","299","4","3","10.1111/j.1745-3984.1989.tb00335.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072833&doi=10.1111%2fj.1745-3984.1989.tb00335.x&partnerID=40&md5=7020264e0a1d9628f214c01886807f44","This paper discusses Andrich's (1989) criticisms of three points made by Divgi (1986). Some important details omitted in Divgi's earlier paper concerning the inconsistency of the unconditional estimator of Rasch difficulties are provided. In the cases of specific objectivity and person‐fit statistics, Andrich criticizes positions Divgi never took or meant to imply. The paper also answers some criticisms by Henning (1989). Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988072833"
"Mandeville G.K.; Anderson L.W.","Mandeville, Garrett K. (7801432799); Anderson, Lorin W. (8696562300)","7801432799; 8696562300","The Stability of School Effectiveness Indices Across Grade Levels and Subject Areas","1987","Journal of Educational Measurement","24","3","","203","216","13","45","10.1111/j.1745-3984.1987.tb00275.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988141412&doi=10.1111%2fj.1745-3984.1987.tb00275.x&partnerID=40&md5=ca6c3d1efee4f8f0a85f4f8b98eb1f24","School effectiveness indices (SEIs) based on regressing achievement test performance onto earlier test performance and a socioeconomic status (SES) measure were obtained for eight subject–grade level combinations for a large sample of elementary schools. School means based on longitudinally matched student scores comprised the data set used in the analysis. The resulting SEIs were found to be somewhat unstable across subject areas (reading and mathematics) and very unstable across grade levels (1 through 4). Grade‐to‐grade correlations of the SEIs measuring mathematics performance, although small, tended to be statistically significant, whereas those measuring reading performance were generally nonsignificant. Thus, school effects may be more readily discernible in some subject areas than in others. Implications for research on effective schools and for the design of school recognition programs based on student test performance are discussed. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988141412"
"ANGOFF W.H.; SCHRADER W.B.","ANGOFF, WILLIAM H. (16648034100); SCHRADER, WILLIAM B. (57205964659)","16648034100; 57205964659","A REJOINDER TO ALBANESE, “THE CORRECTION FOR GUESSING: A FURTHER ANALYSIS OF ANGOFF AND SCHRADER”","1986","Journal of Educational Measurement","23","3","","237","243","6","2","10.1111/j.1745-3984.1986.tb00249.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988070533&doi=10.1111%2fj.1745-3984.1986.tb00249.x&partnerID=40&md5=e80a20f5dacf95c8cf864d15e6e2b4fb","Albanese is critical of the Angoff‐Schrader (1984) study for several reasons, mainly, the fact that it draws some of its data from volunteer student‐subjects. If this criticism is valid, and if we therefore decide to exclude all studies based on volunteer subjects, we necessarily discontinue virtually all experimentation. Albanese recognizes this, but offers no workable substitute for experimental studies. We find, on the other hand, no evidence that the use of volunteer subjects impaired the soundness of our conclusions. Albanese offers an index of his own construction for use in evaluating the Angoff‐Schrader data, and in applying it, comes to a different interpretation from that given by the authors. However, we find serious statistical flaws in his index when applied to our data. For example, we observe that when it is applied to small samples it yields uninterpretable results. After careful consideration of Albanese's reanalyses and interpretations, we stand firm in our original conclusion that formula scores are essentially invariant under different testing directions. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988070533"
"Wainer H.","Wainer, Howard (7006218234)","7006218234","The Future of Item Analysis","1989","Journal of Educational Measurement","26","2","","191","208","17","27","10.1111/j.1745-3984.1989.tb00328.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128720&doi=10.1111%2fj.1745-3984.1989.tb00328.x&partnerID=40&md5=06c188943eecbc207a6ccd1daf74b01c","This paper reviews the role of the item in test construction and suggests some new methods of item analysis. A look at dynamic, graphical item analysis is provided that utilizes the advantages of modern high speed, highly interactive computing. Several illustrations are provided Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988128720"
"Cook L.L.; Eignor D.R.; Taft H.L.","Cook, Linda L. (7201872374); Eignor, Daniel R. (6507584415); Taft, Hessy L. (57209030828)","7201872374; 6507584415; 57209030828","A Comparative Study of the Effects of Recency of Instruction on the Stability of IRT and Conventional Item Parameter Estimates","1988","Journal of Educational Measurement","25","1","","31","45","14","44","10.1111/j.1745-3984.1988.tb00289.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090989&doi=10.1111%2fj.1745-3984.1988.tb00289.x&partnerID=40&md5=da609ace29aca8bf6ebf38fb1fba5585","A potential concern for individuals interested in using item response theory (IRT) with achievement test data is that such tests have been specifically designed to measure content areas related to course curriculum and students taking the tests at different points in their coursework may not constitute samples from the same population. In this study, data were obtained from three administrations of two forms of a Biology achievement test. Data from the newer of the two forms were collected at a spring administration, made up of high school sophomores just completing the Biology course, and at a fall administration, made up mostly of seniors who completed their instruction in the course from 6–18 months prior to the test administration. Data from the older form, already on scale, were collected at only a fall administration, where the sample was comparable to the newer form fall sample. IRT and conventional item difficulty parameter estimates for the common items across the two forms were compared for each of the two form/sample combinations. In addition, conventional and IRT score equatings were performed between the new and old forms for each o f the form sample combinations. Widely disparate results were obtained between the equatings based on the two form/sample combinations. Conclusions are drawn about the use o f both classical test theory and IRT in situations such as that studied, and implications o f the results for achievement test validity are also discussed Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988090989"
"Seheuneman J.D.","Seheuneman, Janiee Dowd (57191235247)","57191235247","An Experimental, Exploratory Study of Causes of Bias in Test Items","1987","Journal of Educational Measurement","24","2","","97","118","21","38","10.1111/j.1745-3984.1987.tb00267.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988057830&doi=10.1111%2fj.1745-3984.1987.tb00267.x&partnerID=40&md5=b941ae1b3c23dd9b14e45acae7cfd555","This study evaluated 16 hypotheses, subsumed under 7 more general hypotheses, concerning possible sources of bias in test items for black and white examinees on the Graduate Record Examination General Test (GRE). Items were developed in pairs that were varied according to a particular hypothesis, with each item from a pair administered in different forms of an experimental portion of the GRE. Data were analyzed using log linear methods. Ten of the 16 hypotheses showed interactions between group membership and the item version indicating a differential effect of the item manipulation on the performance of black and white examinees. The complexity of some of the interactions found, however, suggested that uncontrolled factors were also differentially affecting performance. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988057830"
"Dorans N.J.; Livingston S.A.","Dorans, Neil J. (6602289148); Livingston, Samuel A. (35864363200)","6602289148; 35864363200","Male‐Female Differences in SAT‐Verbal Ability Among Students of High SAT‐Mathematical Ability","1987","Journal of Educational Measurement","24","1","","65","71","6","11","10.1111/j.1745-3984.1987.tb00262.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988084584&doi=10.1111%2fj.1745-3984.1987.tb00262.x&partnerID=40&md5=6cee3460a5a8585a05390dca7c843068","It has been suggested that females who score extremely high on the mathematical portions of the Scholastic Aptitude Test (SAT) do so because they have very high verbal skills, whereas some males score extremely high on the mathematical portion despite their relatively low verbal skills. This hypothesis was investigated with data from two SAT administrations by comparing the conditional distributions of SAT‐Verbal (SAT‐V), given SAT‐Mathematical (SAT‐M), for males and females. Evidence for and against the hypothesis was observed. As implied by the hypothesis, the males had lower conditional SAT‐V mean scores. However, in contradiction to the hypothesis, the males did not have greater conditional variances of SAT‐V given SAT‐M. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988084584"
"Shaw D.G.; Huffman M.D.; Haviland M.G.","Shaw, Dale G. (7403341840); Huffman, Michael D. (57191232124); Haviland, Mark G. (7006595235)","7403341840; 57191232124; 7006595235","Grouping Continuous Data in Discrete Intervals: Information Loss and Recovery","1987","Journal of Educational Measurement","24","2","","167","173","6","9","10.1111/j.1745-3984.1987.tb00272.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058881&doi=10.1111%2fj.1745-3984.1987.tb00272.x&partnerID=40&md5=13dd558f465ec1ebeafbf01897983902","Information loss occurs when continuous data are grouped in discrete intervals. After calculating the squared correlation coefficients between continuous data and corresponding grouped data for four population distributions, the effects of population distribution, number of intervals, and interval width on information loss and recover), were examined. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988058881"
"van den Bergh H.; Eiting M.H.","van den Bergh, Huub (7005230156); Eiting, Mindert H. (41261471500)","7005230156; 41261471500","A Method of Estimating Rater Reliability","1989","Journal of Educational Measurement","26","1","","29","40","11","32","10.1111/j.1745-3984.1989.tb00316.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122228&doi=10.1111%2fj.1745-3984.1989.tb00316.x&partnerID=40&md5=870c29eeb0e1ff8bafde7e7a57b48a92","A method for assessing rater reliability by means of a design of overlapping rater teams is presented. The products to be rated are split randomly into m disjoint subsamples, m equaling the number of raters. Each rater rates at least two subsamples according to a prefixed design. The covariances or correlations of the ratings can be analyzed with LISREL models, resulting in estimates of the rater reliabilities. Models in which the rater reliabilities are congeneric, tauequivalent, or parallel can be tested. We address problems concerning the identification and the degrees of freedom of the models and present two examples based on essay ratings. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988122228"
"Norcini J.J.; Shea J.A.; Kanya D.T.","Norcini, John J. (7006095771); Shea, Judy A. (35572343300); Kanya, D. Theresa (57191232075)","7006095771; 35572343300; 57191232075","The Effect of Various Factors on Standard Setting","1988","Journal of Educational Measurement","25","1","","57","65","8","50","10.1111/j.1745-3984.1988.tb00291.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988113066&doi=10.1111%2fj.1745-3984.1988.tb00291.x&partnerID=40&md5=f0224d2f0726c0f17d548e3441034aa0","This paper reports two studies of standard setting using Angoff's method. Results of the first study suggest that specialization within broad content areas does not affect an expert's estimates of the performance of the borderline group. This is reassuring because the knowledge base of many professions is so large that no individual can be considered an expert in all aspects of it. Results of the second study support the recommendation that performance data be provided during the standard‐setting process. They are frequently used by experts, but will not have an impact on the standard unless the distribution of item difficulties is skewed markedly. It also increases the correspondence between p‐values and estimates of borderline group performance, thereby reducing errors in pass/fail decisions. Overall, the results support recommendations often made in standard‐setting literature, but they need to be replicated with other groups of experts Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988113066"
"Birenbaum M.; Tatsuoka K.K.","Birenbaum, Menucha (6701441757); Tatsuoka, Kikumi K. (6603447775)","6701441757; 6603447775","Effects of “On‐Line” Test Feedback on the Seriousness of Subsequent Errors","1987","Journal of Educational Measurement","24","2","","145","155","10","21","10.1111/j.1745-3984.1987.tb00270.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112947&doi=10.1111%2fj.1745-3984.1987.tb00270.x&partnerID=40&md5=807459594cd2e56b77e0dddf9fe01661","The present study examined the effect of three modes of feedback on the seriousness of error types committed on a post‐test. The measure of seriousness of error types, or wrong rules, indicates to what extent the wrong rules deviate from the right rule. An on‐line, free‐response, six‐item pre‐test covering addition of signed numbers was administered to 263 eighth graders. Upon completion of the pre‐test, the students were randomly assigned to receive one of the following kinds of feedback to responses to a second six‐item test: (a) information about whether or not the response was correct, (b) the correct answer, or (c) the correct rule for solving the problem. The effect of the feedback mode on the seriousness of errors committed on a third six‐item test, the post‐test, was found to be differential and dependent upon the seriousness of errors committed on the pre‐test. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988112947"
"HANSON R.A.; MCMORRIS R.F.; BAILEY J.D.","HANSON, RALPH A. (24444925800); MCMORRIS, ROBERT F. (6603270899); BAILEY, JERRY D. (36854449600)","24444925800; 6603270899; 36854449600","DIFFERENCES IN INSTRUCTIONAL SENSITIVITY BETWEEN ITEM FORMATS AND BETWEEN ACHIEVEMENT TEST ITEMS","1986","Journal of Educational Measurement","23","1","","1","12","11","7","10.1111/j.1745-3984.1986.tb00230.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964920639&doi=10.1111%2fj.1745-3984.1986.tb00230.x&partnerID=40&md5=6cd6e36b7c05e67756bf6e265d05b31e","Building achievement tests which are sensitive to the instructional effects of school programs concerns both practitioners and researchers in education. To produce such tests, empirical procedures to guide item selection are needed. In this paper, an operational framework and a set of empirical procedures for this task are presented. Within this framework, item sensitivity is linked to instructional implementation. A simple components of variance model has been used to provide actual estimates of instructional sensitivity. These procedures are illustrated using data from a comparative study of alternative item formats for a criterion‐referenced test. Even when items were closely matched to instructional content specifications, important differences in instructional sensitivity emerged. These differences were found between the same items presented in different formats as well as between different items presented within the same format. Implications of these results for developing criterion‐referenced achievement tests are discussed. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84964920639"
"Smith R.M.","Smith, Richard M. (55727406500)","55727406500","Assessing Partial Knowledge in Vocabulary","1987","Journal of Educational Measurement","24","3","","217","231","14","14","10.1111/j.1745-3984.1987.tb00276.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106787&doi=10.1111%2fj.1745-3984.1987.tb00276.x&partnerID=40&md5=69f2643e21fc14828358acfe2d7a0bc5","This study reports an attempt to assess partial knowledge in vocabulary. Fifty multiple‐choice vocabulary items were constructed so that the incorrect choices followed the stages of vocabulary acquisition defined by O'Connor (1940). Ability estimates based on Rasch dichotomous and polychotomous models were compared to determine if there were any gains in validity or reliability as a result of using the polychotomous scoring model rather than the dichotomous scoring model. An attempt was also made to determine the appropriateness of O'Connor's stage theory of vocabulary acquisition for predicting the type of errors that examinees of differing ability would make on the test items. The results indicate that the reliability and concurrent validity of the polychotomous scoring of a subset of items that fit the polychotomous scoring model were significantly higher than those for dichotomous scoring of the same subset of items. The results also indicate moderate support for O'Connor's theory of vocabulary acquisition. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988106787"
"Tatsuoka K.K.; Birenbaum M.; Arnold J.","Tatsuoka, Kikumi K. (6603447775); Birenbaum, Menucha (6701441757); Arnold, Jerry (57191235461)","6603447775; 6701441757; 57191235461","On the Stability of Students’Rules of Operation for Solving Arithmetic Problems","1989","Journal of Educational Measurement","26","4","","351","361","10","4","10.1111/j.1745-3984.1989.tb00339.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053649&doi=10.1111%2fj.1745-3984.1989.tb00339.x&partnerID=40&md5=9d803387d9d96502ca34640831275f81","The purpose o f this study was to examine the consistency with which students applied procedural rules for solving signed‐number operations across identical items presented in different orders. A test with 64 open‐ended items was administered to 161 eighth graders. The test consisted o f two 32‐item subtests containing identical items. The items in each subtest were in random order. Students’responses to each subtest were compared with respect to the identified underlying rules o f operation used to solve each problem set. The results indicated that inconsistent rule application was common among students who had not mastered signed‐number arithmetic operations. In contrast, mastery level students, those who use the right rules, show a stable pattern o f rule application in signed‐number arithmetic. These results are discussed in light of the hypothesis testing approach to the learning process. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988053649"
"Hills J.R.; Subhiyah R.G.; Hirsch T.M.","Hills, John R. (16516588500); Subhiyah, Raja G. (6602766392); Hirsch, Thomas M. (57064906200)","16516588500; 6602766392; 57064906200","Equating Minimum‐Competency Tests: Comparisons of Methods","1988","Journal of Educational Measurement","25","3","","221","231","10","3","10.1111/j.1745-3984.1988.tb00304.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988109776&doi=10.1111%2fj.1745-3984.1988.tb00304.x&partnerID=40&md5=8ca1ef69ba99c06a852ace7f721df65e","The 1986 scores from Florida's Statewide Student Assessment Test, Part II (SSAT‐II), a minimum‐competency test required for high school graduation in Florida, were placed on the scale of the 1984 scores from that test using five different equating procedures. For the highest scoring 84 % of the students, four of the five methods yielded results within 1.5 raw‐score points of each other. They would be essentially equally satisfactory in this situation, in which the tests were made parallel item by item in difficulty and content and the groups of examinees were population cohorts separated by only 2 years. Also, the results from six different lengths of anchor items were compared. Anchors of 25, 20, 15, or 10 randomly selected items provided equatings as effective as 30 items using the concurrent IRT equating method, but an anchor of 5 randomly selected items did not Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988109776"
"McLarty J.R.; Noble A.C.; Huntley R.M.","McLarty, Joyce R. (16436338000); Noble, A. Candace (57191235455); Huntley, Renee M. (36841931200)","16436338000; 57191235455; 36841931200","Effects of Item Wording on Sex Bias","1989","Journal of Educational Measurement","26","3","","285","293","8","3","10.1111/j.1745-3984.1989.tb00334.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112547&doi=10.1111%2fj.1745-3984.1989.tb00334.x&partnerID=40&md5=4ee38edc87e3f51a149e2f5a144eba92","This study examined the effects of gender‐related item‐wording changes on the performance of male and female examinees. Mathematics word problems and English language items were created in neuter, male, and female versions. Items were administered to randomly equivalent samples of about 300 high school juniors and seniors. Loglinear analysis was used to assess the impact of item gender and its interaction with examinee sex on the difficulty and discrimination of each item in each context. No items were found to have sex bias in either context. Mathematics items did not have different difficulty or discrimination in the three gender versions. Neither mathematics nor English items had different discrimination levels in the three gender‐related versions. Some English items, however, were found to have different difficulty levels in the three gender‐related versions. These difficulty differences were not systematic.” none of the three gender versions appeared consistently more or less difficult than the others. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988112547"
"Gohmann S.F.","Gohmann, Stephan F. (6602115689)","6602115689","Comparing State SAT Scores: Problems, Biases, and Corrections","1988","Journal of Educational Measurement","25","2","","137","148","11","4","10.1111/j.1745-3984.1988.tb00298.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988075015&doi=10.1111%2fj.1745-3984.1988.tb00298.x&partnerID=40&md5=7f49bcc322fb49a1de4d2e42005c62b1","Comparing SAT scores among states using regression analysis leads to biased results because states differ in the proportion of students taking the exam. When the proportion of students taking the exam is included in the regression equation, the results can be biased because of misspecifieation bias. A method intended to correct for selection bias is presented, and empirical results suggest that sample selection bias is present in SAT score regressions. Regression equations and state rankings are compared between the selection‐corrected equation and equations for which the selection problem is not addressed. The proposed method is one of many available as possible solutions to the selection problem. Alternative methods may produce different results Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988075015"
"Houston W.M.; Noviek M.R.","Houston, Walter M. (7006739594); Noviek, Melvin R. (57191232046)","7006739594; 57191232046","Race‐Based Differential Prediction in Air Force Technical Training Programs","1987","Journal of Educational Measurement","24","4","","309","320","11","17","10.1111/j.1745-3984.1987.tb00282.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988104081&doi=10.1111%2fj.1745-3984.1987.tb00282.x&partnerID=40&md5=b70d43b8c77ae3ec70f0d828218376fd","Bayesian Johnson‐Neyman methodology was used to investigate differential prediction by race in United States Air Force technical training programs. Meaningful Johnson‐Neyman regions of differences were found in eight of nine comparisons. In all nine training courses the regressions for blacks were flatter than for whites and the race‐differentiated regression lines intersected within the range of predictor test scores. In six cases the cut‐score for course qualification was within the Johnson‐Neyman region, and in every case the bias was negative for blacks. It is noted that if the cut score had been set substantially higher the bias would have been positive for blacks in all cases. This analysis may explain why earlier studies that averaged bias across the predictor distribution yielded mixed results. It is hypothesized that the consistent results obtained here are a consequence of the lower predictability found in the black subpopulation. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988104081"
"GARG R.; BOSS M.W.; CARLSON J.E.","GARG, RASHMI (7202772544); BOSS, MARVIN W. (24382414900); CARLSON, JAMES E. (57190052912)","7202772544; 24382414900; 57190052912","A COMPARISON OF EXAMINEE SAMPLING AND MULTIPLE MATRIX SAMPLING IN TEST DEVELOPMENT","1986","Journal of Educational Measurement","23","2","","119","130","11","2","10.1111/j.1745-3984.1986.tb00238.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988109891&doi=10.1111%2fj.1745-3984.1986.tb00238.x&partnerID=40&md5=4c5c8f2c3756f9225b364a7453f8e35a","For the purpose of obtaining data to use in test development, multiple matrix sampling (MMS) plans were compared to examinee sampling plans. Data were simulated for examinees, sampled from a population with a normal distribution of ability, responding to items selected from an item universe. Three item universes were considered: one that would produce a normal distribution of test scores, one a moderately platykurtic distribution, and one a very platykurtic distribution. When comparing sampling plans, total numbers of observations were held constant. No differences were found among plans in estimating item difficulty. Examinee sampling produced better estimates of item discrimination, test reliability, and test validity. As total number of observations increased, estimates improved considerably, especially for those MMS plans with larger subtest sizes. Larger numbers of observations were needed for tests designed to produce a normal distribution of test scores. With an adequate number of observations, MMS is seen as an alternative to examinee sampling in test development. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988109891"
"Secolsky C.","Secolsky, Charles (36998734900)","36998734900","On the Direct Measurement of Face Validity: A Comment on Nevo","1987","Journal of Educational Measurement","24","1","","82","83","1","17","10.1111/j.1745-3984.1987.tb00265.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-74649085638&doi=10.1111%2fj.1745-3984.1987.tb00265.x&partnerID=40&md5=f103a0b43072cf5aa9ba1a2499ad5d4e","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-74649085638"
"Andrich D.","Andrich, David (7004058358)","7004058358","Statistical Reasoning in Psychometric Models and Educational Measurement","1989","Journal of Educational Measurement","26","1","","81","90","9","7","10.1111/j.1745-3984.1989.tb00320.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058891&doi=10.1111%2fj.1745-3984.1989.tb00320.x&partnerID=40&md5=6afa26bbe8f2ff4b1db28cf53c8fbdd7","This paper explores three points raised by Divgi (1986) in order to explicate the important distinction between deterministic and statistical reasoning in the application of statistical models to educational measurement. The three issues concern the relationship between data and esimation equations, the distinction between parameters and parameter estimates, and the power of tests of fit of responses across the ability continuum. Copyright © 1989, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988058891"
"Zwick R.","Zwick, Rebecca (7004200859)","7004200859","Assessing the Dimensionality of NAEP Reading Data","1987","Journal of Educational Measurement","24","4","","293","308","15","47","10.1111/j.1745-3984.1987.tb00281.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099165&doi=10.1111%2fj.1745-3984.1987.tb00281.x&partnerID=40&md5=0138b0bc3a7f79150221fcc10a8bd089","The reading data from the 1983–84 National Assessment of Educational Progress survey were scaled using a unidimensional item response theory model. To determine whether the responses to the reading items were consistent with unidimensionality, the full‐information factor analysis method developed by Bock and associates (1985) and Rosenbaum's (1984) test of unidimensionality, conditional (local) independence, and monotonicity were applied. Full‐information factor analysis involves the assumption of a particular item response function; the number of latent variables required to obtain a reasonable fit to the data is then determined. The Rosenbaum method provides a test of the more general hypothesis that the data can be represented by a model characterized by unidimensionality, conditional independence, and monotonicity. Results of both methods indicated that the reading items could be regarded as measures of a single dimension. Simulation studies were conducted to investigate the impact of balanced incomplete block (BIB) spiraling, used in NAEP to assign items to students, on methods of dimensionality assessment. In general, conclusions about dimensionality were the same for BIB‐spiraled data as for complete data. Copyright © 1987, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988099165"
"Reckase M.D.; Ackerman T.A.; Carlson J.E.","Reckase, Mark D. (6602075623); Ackerman, Terry A. (16404476400); Carlson, James E. (57190052912)","6602075623; 16404476400; 57190052912","Building a Unidimensional Test Using Multidimensional Items","1988","Journal of Educational Measurement","25","3","","193","203","10","71","10.1111/j.1745-3984.1988.tb00302.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988113076&doi=10.1111%2fj.1745-3984.1988.tb00302.x&partnerID=40&md5=d37924f5e6fefef30015e6fe43e20a02","This paper demonstrates, both theoretically and empirically, using both simulated and real test data, that sets of items can be selected that meet the unidimensionality assumption of most item response theory models even though they require more than one ability for a correct response. Sets of items that measure the same composite of abilities as defined by multidimensional item response theory are shown to meet the unidimensionality assumption. A method for identifying such item sets is also presented Copyright © 1988, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988113076"
"WEISS D.J.; KINGSBURY G.G.","WEISS, DAVID J. (55333410500); KINGSBURY, G. GAGE (12800030300)","55333410500; 12800030300","APPLICATION OF COMPUTERIZED ADAPTIVE TESTING TO EDUCATIONAL PROBLEMS","1984","Journal of Educational Measurement","21","4","","361","375","14","272","10.1111/j.1745-3984.1984.tb01040.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000318735&doi=10.1111%2fj.1745-3984.1984.tb01040.x&partnerID=40&md5=99b92e9db2248cd49639712482d16cfa","Three applications of computerized adaptive testing (CAT) to help solve problems encountered in educational settings are described and discussed. Each of these applications makes use of item response theory to select test questions from an item pool to estimate a student's achievement level and its precision. These estimates may then be used in conjunction with certain testing strategies to facilitate certain educational decisions. The three applications considered are (a) adaptive mastery testing for determining whether or not a student has mastered a particular content area, (b) adaptive grading for assigning grades to students, and (c) adaptive self‐referenced testing for estimating change in a student's achievement level. Differences between currently used classroom procedures and these CAT procedures are discussed. For the adaptive mastery testing procedure, evidence from a series of studies comparing conventional and adaptive testing procedures is presented showing that the adaptive procedure results in more accurate mastery classifications than do conventional mastery tests, while using fewer test questions. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0000318735"
"FRISBIE D.A.; SWEENEY D.C.","FRISBIE, DAVID A. (7003704007); SWEENEY, DARYL C. (57195040003)","7003704007; 57195040003","THE RELATIVE MERITS OF MULTIPLE TRUE‐FALSE ACHIEVEMENT TESTS","1982","Journal of Educational Measurement","19","1","","29","35","6","31","10.1111/j.1745-3984.1982.tb00112.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005364679&doi=10.1111%2fj.1745-3984.1982.tb00112.x&partnerID=40&md5=f352af2f13b55ed382f5bf6b53cfd12c","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85005364679"
"MCMORRIS R.F.; URBACH S.L.; CONNOR M.C.","MCMORRIS, ROBERT F. (6603270899); URBACH, SANDRA L. (57195044884); CONNOR, MICHAEL C. (57195042395)","6603270899; 57195044884; 57195042395","EFFECTS OF INCORPORATING HUMOR IN TEST ITEMS","1985","Journal of Educational Measurement","22","2","","147","155","8","27","10.1111/j.1745-3984.1985.tb01054.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346923302&doi=10.1111%2fj.1745-3984.1985.tb01054.x&partnerID=40&md5=86c776395961467be24b329f92d718da","Two matched forms of a 50 item multiple‐choice grammar test were developed. Twenty items designed to be humorous were included in one form. Test forms were randomly assigned to 126 eighth graders who received the test plus alternate forms of a questionnaire. Inclusion of humorous items did not affect grammar scores on matched humorous/nonhumorous items nor on common post‐treatment items, nor did inclusion affect results of anxiety measures. Students favored inclusion of humor on tests, judged effects of humor positively, and estimated humorous items to be easier. Humor did not lower performance but was sought by the students. Potential for more valid and humane measurement is discussed. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0346923302"
"WOLFLE L.M.; ROBERTSHAW D.","WOLFLE, LEE M. (6507547609); ROBERTSHAW, DIANNE (16478026400)","6507547609; 16478026400","RACIAL DIFFERENCES IN MEASUREMENT ERROR IN EDUCATIONAL ACHIEVEMENT MODELS","1983","Journal of Educational Measurement","20","1","","39","49","10","6","10.1111/j.1745-3984.1983.tb00188.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990443866&doi=10.1111%2fj.1745-3984.1983.tb00188.x&partnerID=40&md5=e2d79336fe657187cf55ef535591a6e8","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84990443866"
"BEUK C.H.","BEUK, CEES H. (57195043205)","57195043205","A METHOD FOR REACHING A COMPROMISE BETWEEN ABSOLUTE AND RELATIVE STANDARDS IN EXAMINATIONS","1984","Journal of Educational Measurement","21","2","","147","152","5","37","10.1111/j.1745-3984.1984.tb00226.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141988000&doi=10.1111%2fj.1745-3984.1984.tb00226.x&partnerID=40&md5=abc6dc2b94ee771d1be6dbac4cc7ce29","Examining committees often need to reach a compromise between absolute and relative standards. Unfortunately, the way in which the compromise is achieved is usually unclear. This paper proposes a systematic method for reaching a compromise. In this method, the estimated passing score (level of minimum knowledge) is assumed to be related to the expected pass rate (percentage of successful candidates) through a simple linear function. The examination results define a function relating the percentage of candidates who would be successful given a specified passing score to the passing score. The intersection of both functions gives the required compromise. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0141988000"
"HOPKINS K.D.; GEORGE C.A.; WILLIAMS D.D.","HOPKINS, KENNETH D. (25952508400); GEORGE, CATHERINE A. (57195041046); WILLIAMS, DAVID D. (56161789800)","25952508400; 57195041046; 56161789800","THE CONCURRENT VALIDITY OF STANDARDIZED ACHIEVEMENT TESTS BY CONTENT AREA USING TEACHERS' RATINGS AS CRITERIA","1985","Journal of Educational Measurement","22","3","","177","182","5","49","10.1111/j.1745-3984.1985.tb01056.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040248781&doi=10.1111%2fj.1745-3984.1985.tb01056.x&partnerID=40&md5=f35c1b2bc0d24c040932b44b3381a21c","To assess the concurrent validity of standardized achievement tests using teachers' ratings (and rankings) of pupils' academic achievement as criteria, 42 teachers evaluated each of their students (n = 1,032) in each of five major curricular areas prior to the administration of a battery of standardized achievement tests. The teachers were directed to rate each student's proficiency disregarding attendance, attitude, deportment, and so on. Within‐class correlation coefficients were computed to eliminate rater leniency bias. The standardized achievement tests were found to have substantial concurrent validity in reading, math, language arts, science, and social studies. The normalized teacher ranks yielded significantly higher validity coefficients than did the ratings, although the magnitude of the difference was small. The concurrent validity coefficients for language arts, reading, and math were significantly higher than those in science and social studies. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0040248781"
"FLYNN J.R.","FLYNN, JAMES R. (7403246691)","7403246691","IQ GAINS AND THE BINET DECREMENTS","1984","Journal of Educational Measurement","21","3","","283","290","7","29","10.1111/j.1745-3984.1984.tb01035.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011618513&doi=10.1111%2fj.1745-3984.1984.tb01035.x&partnerID=40&md5=7fe5928e9891eb71a1d7b9cd65682374","Thorndike compiled Stanford‐Binet data which made it appear that children aged 6 and under have made greater IQ gains than older children and that this pattern dominated the whole period from 1932 to 1971–72. Therefore, he sought causal factors likely to affect preschoolers more than others, for example, TV in general and educational TV in particular. A wide array of data show that the atypical gains of young children are either an artifact of sampling error or totally antedate 1947, ruling out TV as an age‐specific factor. This data also suggest that Americans have gained about 12 IQ points from 1932 to 1972 with verbal gains being a point lower and performance gains a point higher. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85011618513"
"STIGGINS R.J.; BRIDGEFORD N.J.","STIGGINS, RICHARD J. (6602224104); BRIDGEFORD, NANCY J. (57035981500)","6602224104; 57035981500","THE ECOLOGY OF CLASSROOM ASSESSMENT","1985","Journal of Educational Measurement","22","4","","271","286","15","96","10.1111/j.1745-3984.1985.tb01064.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954630369&doi=10.1111%2fj.1745-3984.1985.tb01064.x&partnerID=40&md5=9ef770a5f3ac1dbb924598df5c458488","Research on classroom assessment has tended to focus on standardized tests and has paid minimal attention to teacher‐developed assessments. As a result, we have a narrow understanding of the classroom assessment environment. This study was designed to broaden that understanding by exploring the nature and quality of teacher‐developed assessment. Teachers from a range of grades, subjects, and school districts described their patterns of test use, concerns about assessment, and use of performance assessment by completing an extensive questionnaire. When responses are summarized across teachers, the results suggest that the foundation and structure of classroom assessment consists primarily of teacher‐developed assessments, with performance assessment serving as one of the key assessment tools. Teachers are concerned about assessment and know that improvement may be needed, but they will need help to bring about necessary changes. Action plans are suggested for enhancing the quality of teacher‐developed tests. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84954630369"
"LIVINGSTON S.A.","LIVINGSTON, SAMUEL A. (35864363200)","35864363200","ESTIMATION OF THE CONDITIONAL STANDARD ERROR OF MEASUREMENT FOR STRATIFIED TESTS","1982","Journal of Educational Measurement","19","2","","135","138","3","9","10.1111/j.1745-3984.1982.tb00122.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005245742&doi=10.1111%2fj.1745-3984.1982.tb00122.x&partnerID=40&md5=9000408398fe4980fc706873a5529c83","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85005245742"
"TATSUOKA K.K.","TATSUOKA, KIKUMI K. (6603447775)","6603447775","RULE SPACE: AN APPROACH FOR DEALING WITH MISCONCEPTIONS BASED ON ITEM RESPONSE THEORY","1983","Journal of Educational Measurement","20","4","","345","354","9","642","10.1111/j.1745-3984.1983.tb00212.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024968994&doi=10.1111%2fj.1745-3984.1983.tb00212.x&partnerID=40&md5=7159a9e61be706a18d479ed4c16d6aa1","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85024968994"
"HARNISCH D.L.","HARNISCH, DELWYN L. (6506625886)","6506625886","ITEM RESPONSE PATTERNS: APPLICATIONS FOR EDUCATIONAL PRACTICE","1983","Journal of Educational Measurement","20","2","","191","206","15","34","10.1111/j.1745-3984.1983.tb00199.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007197632&doi=10.1111%2fj.1745-3984.1983.tb00199.x&partnerID=40&md5=a44153ee4a1f0f8c4f3a6dc4a5a0f294","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0007197632"
"HARDY R.A.","HARDY, ROY A. (57156839300)","57156839300","MEASURING INSTRUCTIONAL VALIDITY: A REPORT OF AN INSTRUCTIONAL VALIDITY STUDY FOR THE ALABAMA HIGH SCHOOL GRADUATION EXAMINATION","1984","Journal of Educational Measurement","21","3","","291","301","10","1","10.1111/j.1745-3984.1984.tb01036.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935791464&doi=10.1111%2fj.1745-3984.1984.tb01036.x&partnerID=40&md5=3de85e6d4dbb58314fc3dae1d2fdaa2a","Prior to the initial administration of the Alabama High School Graduation Exam, this study was conducted to determine to what extent competencies to be measured by the exam were being taught in the public schools of Alabama. Teachers of grades 7, 8, 9, and 10 were asked to report the proportion of students in their classes who had received instruction on each competency. The survey identifies a number of competencies that were not being extensively taught and provides insight as to possible reasons certain competencies are not uniformly taught. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84935791464"
"HAERTEL E.","HAERTEL, EDWARD (6602745554)","6602745554","DETECTION OF A SKILL DICHOTOMY USING STANDARDIZED ACHIEVEMENT TEST ITEMS","1984","Journal of Educational Measurement","21","1","","59","72","13","10","10.1111/j.1745-3984.1984.tb00221.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010706136&doi=10.1111%2fj.1745-3984.1984.tb00221.x&partnerID=40&md5=d658c40d6e1e5d00cff979f2eaa66f4d","Multiple‐choice reading comprehension items from a conventional, norm‐referenced reading comprehension test are successfully analyzed using a simple latent class model. A classification rule for assigning respondents to “mastery” or “nonmastery” states is presented which simplifies the scoring procedure of Macready and Dayton (1977). A procedure is also derived for estimating the “true,” or “disattenuated,” latent cross‐classification of masters versus nonmasters for two tests, and illustrated using two sets of items from the same content domain. Results support the use of latent class, state mastery models with more heterogeneous item pools than has been advocated by previous authors. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0010706136"
"BAKER E.L.; HERMAN J.L.","BAKER, EVA L. (7401660666); HERMAN, JOAN L. (7403275946)","7401660666; 7403275946","TASK STRUCTURE DESIGN: BEYOND LINKAGE","1983","Journal of Educational Measurement","20","2","","149","164","15","12","10.1111/j.1745-3984.1983.tb00196.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988071871&doi=10.1111%2fj.1745-3984.1983.tb00196.x&partnerID=40&md5=b7d1fb99440c9e829f7d40ebae99a87f","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988071871"
"SECOLSKY C.","SECOLSKY, CHARLES (36998734900)","36998734900","USING EXAMINEE JUDGMENTS FOR DETECTING INVALID ITEMS ON TEACHER‐MADE CRITERION‐REFERENCED TESTS","1983","Journal of Educational Measurement","20","1","","51","63","12","2","10.1111/j.1745-3984.1983.tb00189.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010492605&doi=10.1111%2fj.1745-3984.1983.tb00189.x&partnerID=40&md5=20de52754d50cd040e9f205e32aadb0b","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85010492605"
"HALADYNA T.M.; ROID G.H.","HALADYNA, THOMAS M. (6602737797); ROID, GALE H. (6602571692)","6602737797; 6602571692","A COMPARISON OF TWO APPROACHES TO CRITERION‐REFERENCED TEST CONSTRUCTION","1983","Journal of Educational Measurement","20","3","","271","282","11","8","10.1111/j.1745-3984.1983.tb00205.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984527347&doi=10.1111%2fj.1745-3984.1983.tb00205.x&partnerID=40&md5=b934de316ef6fb5486c8b282b7f282f3","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84984527347"
"STENNER A.J.; SMITH M., III; BURDICK D.S.","STENNER, A. JACKSON (55270479900); SMITH, MALBERT (56410183300); BURDICK, DONALD S. (7005772489)","55270479900; 56410183300; 7005772489","TOWARD A THEORY OF CONSTRUCT DEFINITION","1983","Journal of Educational Measurement","20","4","","305","316","11","78","10.1111/j.1745-3984.1983.tb00209.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-60649091511&doi=10.1111%2fj.1745-3984.1983.tb00209.x&partnerID=40&md5=cad3202aeb88962a1d0d0ff7e2d02d1d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-60649091511"
"BUDESCU D.","BUDESCU, DAVID (7003588697)","7003588697","EFFICIENCY OF LINEAR EQUATING AS A FUNCTION OF THE LENGTH OF THE ANCHOR TEST","1985","Journal of Educational Measurement","22","1","","13","20","7","21","10.1111/j.1745-3984.1985.tb01045.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988076805&doi=10.1111%2fj.1745-3984.1985.tb01045.x&partnerID=40&md5=62643cd45611d24630d92aa7e2c59350","One of the most widely used methods for equating multiple parallel forms of a test is to incorporate a common set of anchor items in all its operational forms. Under appropriate assumptions it is possible to derive a linear equation for converting raw scores from one operational form to the others. The present note points out that the single most important determinant of the efficiency of the equating process is the magnitude of the correlation between the anchor test and the unique components of each form. It is suggested to use some monotonic function of this correlation as a measure of the equating efficiency, and a simple model relating the relative length of the anchor test and the test reliability to this measure of efficiency is presented. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988076805"
"KLEIN L.W.; JARJOURA D.","KLEIN, LAWRENCE W. (57195043090); JARJOURA, DAVID (7004918700)","57195043090; 7004918700","THE IMPORTANCE OF CONTENT REPRESENTATION FOR COMMON‐ITEM EQUATING WITH NONRANDOM GROUPS","1985","Journal of Educational Measurement","22","3","","197","206","9","45","10.1111/j.1745-3984.1985.tb01058.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001705766&doi=10.1111%2fj.1745-3984.1985.tb01058.x&partnerID=40&md5=c4b3eb82ad8b0d5d5ca8c604af85b65b","The equating accuracy of content‐representative anchors versus nonrepresen‐tative but substantially longer anchors is compared. Content representation was defined as a match between anchors and total test of the percentage of items in each of several content areas. Through a chain of equatings it was found that content representation in anchors was critical for the testing program studied. The results are explained in terms of differences in mean profiles (by content area) of the nonrandom groups used for equating. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0001705766"
"GREEN B.F.; BOCK R.D.; HUMPHREYS L.G.; LINN R.L.; RECKASE M.D.","GREEN, BERT F. (7401825047); BOCK, R. DARRELL (55436391100); HUMPHREYS, LLOYD G. (56238275200); LINN, ROBERT L. (56126633100); RECKASE, MARK D. (6602075623)","7401825047; 55436391100; 56238275200; 56126633100; 6602075623","TECHNICAL GUIDELINES FOR ASSESSING COMPUTERIZED ADAPTIVE TESTS","1984","Journal of Educational Measurement","21","4","","347","360","13","257","10.1111/j.1745-3984.1984.tb01039.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977036370&doi=10.1111%2fj.1745-3984.1984.tb01039.x&partnerID=40&md5=62e1b17ede16572fdf4d0468567f83f3","Guidelines are proposed for evaluating a computerized adaptive test. Topics include dimensionality, measurement error, validity, estimation of item parameters, item pool characteristics and human factors. Equating CAT and conventional tests is considered and matters of equity are addressed. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84977036370"
"MESSICK S.","MESSICK, SAMUEL (16538215700)","16538215700","THE PSYCHOLOGY OF EDUCATIONAL MEASUREMENT","1984","Journal of Educational Measurement","21","3","","215","237","22","176","10.1111/j.1745-3984.1984.tb01030.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973713134&doi=10.1111%2fj.1745-3984.1984.tb01030.x&partnerID=40&md5=769e81486654cd98ecec489fabcc4217","Because school learning entails not just accretion of knowledge but the structuring and restructuring of knowledge and cognitive skills, the conception and construction of educational achievement measures must be cast in developmental terms. And because student characteristics as well as social and educational experiences influence current performance, the interpretation and implications of educational achievement measures must be relative to intrapersonal and situational contexts. These points imply a strategy of comprehensive assessment in context that focuses on the processes and structures involved in subject‐matter competence as moderated in performance by personal and environmental influences. This article addresses in detail both the nature of developing competence and its measurement in terms of context‐dependent task performance. Construct‐irrelevant task difficulty that might jeopardize the meaning of test scores as well as construct‐irrelevant influences that might jeopardize implications for action are taken into account via the comprehensive measurement of relevant contextual factors. Comprehensive assessment in context thus facilitates valid interpretations of the meaning and implications of ability and achievement scores in particular instances, thereby lightening the interpretive and ethical burdens on test users and enhancing the validity of test use. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84973713134"
"WAINER H.","WAINER, HOWARD (7006218234)","7006218234","AN EXPLORATORY ANALYSIS OF PERFORMANCE ON THE SAT","1984","Journal of Educational Measurement","21","2","","81","91","10","2","10.1111/j.1745-3984.1984.tb00222.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934170229&doi=10.1111%2fj.1745-3984.1984.tb00222.x&partnerID=40&md5=1b068978fbec95d817ad8e011ca85da2","Methods of exploratory data analysis are used to decompose data tables that portray the performance of various ethnic groups on the Scholastic Aptitude Test (SAT). These analyses show the size and structure of the differences in performance among the ethnic groups studied, the nature of changes across time, and the interactions between ethnic group membership and time. Suggestions regarding the clear display of results from data in this form are made and illustrated. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84934170229"
"MILLS C.N.","MILLS, CRAIG N. (55951945400)","55951945400","A COMPARISON OF THREE METHODS OF ESTABLISHING CUT‐OFF SCORES ON CRITERION‐REFERENCED TESTS","1983","Journal of Educational Measurement","20","3","","283","292","9","22","10.1111/j.1745-3984.1983.tb00206.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008753263&doi=10.1111%2fj.1745-3984.1983.tb00206.x&partnerID=40&md5=06d2226a3c896a9981de7b54166e0b52","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85008753263"
"FRARY R.B.","FRARY, ROBERT B. (6602858608)","6602858608","MULTIPLE‐CHOICE VERSUS FREE‐RESPONSE: A SIMULATION STUDY","1985","Journal of Educational Measurement","22","1","","21","31","10","8","10.1111/j.1745-3984.1985.tb01046.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038272328&doi=10.1111%2fj.1745-3984.1985.tb01046.x&partnerID=40&md5=241a05b5c71ad76f5ccdd033464fbd05","Responses to a 40‐item test were simulated for 150 examinees under free‐response and multiple‐choice formats. The simulation was replicated three times for each of 30 variations reflecting format and the extent to which examinees were (a) misinformed, (b) successful in guessing free‐response answers, and (c) able to recognize with assurance correct multiple‐choice options that they could not produce under free‐response testing. Internal consistency reliability (KR20) estimates were consistently higher for the free‐response score sets, even when the free‐response item difficulty indices were augmented to yield mean scores comparable to those from multiple‐choice testing. In addition, all test score sets were correlated with four randomly generated sets of unit‐normal measures, whose intercorrelations ranged from moderate to strong. These measures served as criteria because one of them had been used as the basic ability measure in the simulation of the test score sets. Again, the free‐response score sets yielded superior results even when tests of equal difficulty were compared. The guessing and recognition factors had little or no effect on reliability estimates or correlations with the criteria. The extent of misinformation affected only multiple‐choice score KR20′s (more misinformation—higher KR20′s). Although free‐response tests were found to be generally superior, the extent of their advantage over multiple‐choice was judged sufficiently small that other considerations might justifiably dictate format choice. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0038272328"
"HANNA G.","HANNA, GILA (16446992400)","16446992400","THE USE OF A FACTOR‐ANALYTIC MODEL FOR ASSESSING THE VALIDITY OF GROUP COMPARISONS","1984","Journal of Educational Measurement","21","2","","191","199","8","3","10.1111/j.1745-3984.1984.tb00229.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007142498&doi=10.1111%2fj.1745-3984.1984.tb00229.x&partnerID=40&md5=fe5fac8bbec9aecba1c5284b935e32e1","This paper assesses the validity of a comparison of mean test scores for two groups of students, and of a longitudinal comparison of means within each group. Using LISREL, confirmatory factor analyses were carried out (a) to test the hypotheses of similar factor patterns, equal units of measurement, and equal accuracy of measurement between the two groups, and (b) to estimate the correlation between the latent traits measured by two successive test administrations in each group. The results indicate (a) that a comparison of the group means may be invalid because, although the factor pattern was the same for both groups, the factors were not measured in the same units, and (b) that longitudinal comparisons within each group are seriously complicated by evidence of structural change. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0007142498"
"DORANS N.J.; KULICK E.","DORANS, NEIL J. (6602289148); KULICK, EDWARD (15045114600)","6602289148; 15045114600","DEMONSTRATING THE UTILITY OF THE STANDARDIZATION APPROACH TO ASSESSING UNEXPECTED DIFFERENTIAL ITEM PERFORMANCE ON THE SCHOLASTIC APTITUDE TEST","1986","Journal of Educational Measurement","23","4","","355","368","13","233","10.1111/j.1745-3984.1986.tb00255.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948083640&doi=10.1111%2fj.1745-3984.1986.tb00255.x&partnerID=40&md5=9b71a6c69a8807766fefc9ea137f6dbe","The standardization method for assessing unexpected differential item performance or differential item functioning (DIF) is introduced. The principal findings of the first five studies that have used this approach on the Scholastic Aptitude Test are presented. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84948083640"
"VEALE J.R.","VEALE, JAMES R. (57214437671)","57214437671","ASSESSING CULTURAL BIAS USING FOIL RESPONSE DATA: CULTURAL VARIATION","1983","Journal of Educational Measurement","20","3","","249","258","9","10","10.1111/j.1745-3984.1983.tb00203.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053310&doi=10.1111%2fj.1745-3984.1983.tb00203.x&partnerID=40&md5=88bd4e100ca0625eb12ca8bf7d745983","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988053310"
"KANE M.T.","KANE, MICHAEL T. (36088969800)","36088969800","THE ROLE OF RELIABILITY IN CRITERION‐REFERENCED TESTS","1986","Journal of Educational Measurement","23","3","","221","224","3","19","10.1111/j.1745-3984.1986.tb00247.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040191861&doi=10.1111%2fj.1745-3984.1986.tb00247.x&partnerID=40&md5=39b91a7cbe5fc7d374d07868b5c0713f","In discussion of the properties of criterion‐referenced tests, it is often assumed that traditional reliability indices, particularly those based on internal consistency, are not relevant. However, if the measurement errors involved in using an individual's observed score on a criterion‐referenced test to estimate his or her universe scores on a domain of items are compared to errors of an a priori procedure that assigns the same universe score (the mean observed test score) to all persons, the test‐based procedure is found to improve the accuracy of universe score estimates only if the test reliability is above 0.5. This suggests that criterion‐referenced tests with low reliabilities generally will have limited use in estimating universe scores on domains of items. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0040191861"
"YEN W.M.","YEN, WENDY M. (7102684621)","7102684621","THE CHOICE OF SCALE FOR EDUCATIONAL MEASUREMENT: AN IRT PERSPECTIVE","1986","Journal of Educational Measurement","23","4","","299","325","26","81","10.1111/j.1745-3984.1986.tb00252.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000258408&doi=10.1111%2fj.1745-3984.1986.tb00252.x&partnerID=40&md5=bd74d430906f1d144cf8e0d141092288","Two methods of constructing equal‐interval scales for educational achievement are discussed: Thurstone's absolute scaling method and Item Response Theory (IRT). Alternative criteria for choosing a scale are contrasted. It is argued that clearer criteria are needed for judging the appropriateness and usefulness of alternative scaling procedures, and more information is needed about the qualities of the different scales that are available. In answer to this second need, some examples are presented of how IRT can be used to examine the properties of scales: It is demonstrated that for observed score scales in common use (i.e., any scores that are influenced by measurement error), (a) systematic errors can be introduced when comparing growth at selected percentiles, and (b) normalizing observed scores will not necessarily produce a scale that is linearly related to an underlying normally distributed true trait. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0000258408"
"KOK F.G.; MELLENBERGH G.J.; VAN DER FLIER H.","KOK, FRANK G. (57195041861); MELLENBERGH, GIDEON J. (7003739438); VAN DER FLIER, HENK (6603918318)","57195041861; 7003739438; 6603918318","DETECTING EXPERIMENTALLY INDUCED ITEM BIAS USING THE ITERATIVE LOGIT METHOD","1985","Journal of Educational Measurement","22","4","","295","303","8","38","10.1111/j.1745-3984.1985.tb01066.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015535196&doi=10.1111%2fj.1745-3984.1985.tb01066.x&partnerID=40&md5=896c8c3570cf7ce0a09b3632b473f6be","A test for mental arithmetic was constructed, consisting of items written in Dutch (the subjects’ native language), Spanish, and Roman numerals. A group of 286 subjects received some information on Spanish numerals. The group was randomly split into a Spanish Group and a Roman Group. The Spanish Group received further instruction on Spanish numerals, while the Roman Group got instruction on Roman numerals. Checks on the experimental manipulations showed that the Spanish Group had better knowledge of Spanish numerals than the Roman Group, whereas the Roman Group had better knowledge of Roman numerals. From the total test two subtests were constructed: a 30‐item Dutch/Spanish subtest (15 items in Dutch and 15 in Spanish), and a 25‐item Dutch/Roman subtest (15 items in Dutch and 10 in Roman). The Dutch items were unbiased between the Spanish and Roman groups, whereas the Spanish items of the Dutch/Spanish subtest were biased against the Roman Group, and the Roman items of the Dutch/Roman subtest were biased against the Spanish Group. The iterative logit method was applied to the two subtests. The method showed very good results in detecting biased items. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85015535196"
"TATSUOKA K.K.; TATSUOKA M.M.","TATSUOKA, KIKUMI K. (6603447775); TATSUOKA, MAURICE M. (6506443458)","6603447775; 6506443458","SPOTTING ERRONEOUS RULES OF OPERATION BY THE INDIVIDUAL CONSISTENCY INDEX","1983","Journal of Educational Measurement","20","3","","221","230","9","66","10.1111/j.1745-3984.1983.tb00201.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977023819&doi=10.1111%2fj.1745-3984.1983.tb00201.x&partnerID=40&md5=d1fd134790ffb5a70b667f993ebee843","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84977023819"
"SPENCER B.D.","SPENCER, BRUCE D. (7201938598)","7201938598","ON INTERPRETING TEST SCORES AS SOCIAL INDICATORS: STATISTICAL CONSIDERATIONS","1983","Journal of Educational Measurement","20","4","","317","333","16","21","10.1111/j.1745-3984.1983.tb00210.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012278496&doi=10.1111%2fj.1745-3984.1983.tb00210.x&partnerID=40&md5=ad6b83500cf479a7fc84775be13f475b","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85012278496"
"GOLDSTEIN H.","GOLDSTEIN, HARVEY (7203025236)","7203025236","MEASURING CHANGES IN EDUCATIONAL ATTAINMENT OVER TIME: PROBLEMS AND POSSIBILITIES","1983","Journal of Educational Measurement","20","4","","369","377","8","62","10.1111/j.1745-3984.1983.tb00214.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001587566&doi=10.1111%2fj.1745-3984.1983.tb00214.x&partnerID=40&md5=202797c4e7f794122e8e4ffc83349c26","[No abstract available]","","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-0001587566"
"VAN DER FLIER H.; MELLENBERGH G.J.; ADÈR H.J.; WIJN M.","VAN DER FLIER, HENK (6603918318); MELLENBERGH, GIDEON J. (7003739438); ADÈR, HERMAN J. (7004454872); WIJN, MARINA (57195041704)","6603918318; 7003739438; 7004454872; 57195041704","AN ITERATIVE ITEM BIAS DETECTION METHOD","1984","Journal of Educational Measurement","21","2","","131","145","14","35","10.1111/j.1745-3984.1984.tb00225.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984458627&doi=10.1111%2fj.1745-3984.1984.tb00225.x&partnerID=40&md5=ed831e216f7edda7146bc107d3037a78","Two strategies for assessing item bias are discussed: methods that compare (transformed) item difficulties unconditional on ability level and methods that compare the probabilities of correct response conditional on ability level. In the present study, the logit model was used to compare the probabilities of correct response to an item by members of two groups, these probabilities being conditional on the observed score. Here the observed score serves as an indicator of ability level. The logit model was iteratively applied: In the Tth iteration, the T items with the highest value of the bias statistic are excluded from the test, and the observed score indicator of ability for the (T + 1)th iteration is computed from the remaining items. This method was applied to simulated data. The results suggest that the iterative logit method is a substantial improvement on the noniterative one, and that the iterative method is very efficient in detecting biased and unbiased items. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84984458627"
"SCHURR K.T.; HENRIKSEN L.W.","SCHURR, K. TERRY (56508241900); HENRIKSEN, L.W. (19734275800)","56508241900; 19734275800","EFFECTS OF ITEM SEQUENCING AND GROUPING IN LOW‐INFERENCE TYPE QUESTIONNAIRES","1983","Journal of Educational Measurement","20","4","","379","391","12","7","10.1111/j.1745-3984.1983.tb00215.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973706164&doi=10.1111%2fj.1745-3984.1983.tb00215.x&partnerID=40&md5=d4c9bb62e4b81066f9c6bd7e3e3f08eb","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973706164"
"BLOK H.","BLOK, H. (7005274990)","7005274990","ESTIMATING THE RELIABILITY, VALIDITY, AND INVALIDITY OF ESSAY RATINGS","1985","Journal of Educational Measurement","22","1","","41","52","11","24","10.1111/j.1745-3984.1985.tb01048.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002292198&doi=10.1111%2fj.1745-3984.1985.tb01048.x&partnerID=40&md5=780fcd60bc8215ec7f35fab174dab6ca","In an essay rating study multiple ratings may be obtained by having different raters judge essays or by having the same rater(s) repeat the judging of essays. An important question in the analysis of essay ratings is whether multiple ratings, however obtained, may be assumed to represent the same true scores. When different raters judge the same essays only once, it is impossible to answer this question. In this study 16 raters judged 105 essays on two occasions; hence, it was possible to test assumptions about true scores within the framework of linear structural equation models. It emerged that the ratings of a given rater on the two occasions represented the same true scores. However, the ratings of different raters did not represent the same true scores. The estimated intercorrelations of the true scores of different raters ranged from .415 to .910. Parameters of the best fitting model were used to compute coefficients of reliability, validity, and invalidity. The implications of these coefficients are discussed. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0002292198"
"PLAKE B.S.; ANSORGE C.J.; PARKER C.S.; LOWRY S.R.","PLAKE, BARBARA S. (6603689848); ANSORGE, CHARLES J. (6602229861); PARKER, CLAIRE S. (57189651481); LOWRY, STEVEN R. (55415347100)","6603689848; 6602229861; 57189651481; 55415347100","EFFECTS OF ITEM ARRANGEMENT, KNOWLEDGE OF ARRANGEMENT TEST ANXIETY AND SEX ON TEST PERFORMANCE","1982","Journal of Educational Measurement","19","1","","49","57","8","18","10.1111/j.1745-3984.1982.tb00114.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005400963&doi=10.1111%2fj.1745-3984.1982.tb00114.x&partnerID=40&md5=6f9206224265e8754dbbf498719092f5","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85005400963"
"WRIGHT B.D.; BELL S.R.","WRIGHT, BENJAMIN D. (7402346612); BELL, SUSAN R. (57195044877)","7402346612; 57195044877","ITEM BANKS: WHAT, WHY, HOW","1984","Journal of Educational Measurement","21","4","","331","345","14","61","10.1111/j.1745-3984.1984.tb01038.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970743575&doi=10.1111%2fj.1745-3984.1984.tb01038.x&partnerID=40&md5=07e63c38d59d387c6a3a147e68791025","In this paper, we discuss curricular implications of item banking and its practical value to teachers and students, and list a variety of working banks with their sources. We also review the psychometric basis of item banking, outline a family of computer programs for accomplishing banking, and give the equations necessary to build a bank. We conclude with a discussion of item quality control and examples of items that misfit because of miskeying, guessing, or carelessness. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84970743575"
"NEVO B.","NEVO, BARUCH (6602948460)","6602948460","FACE VALIDITY REVISITED","1985","Journal of Educational Measurement","22","4","","287","293","6","226","10.1111/j.1745-3984.1985.tb01065.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001718163&doi=10.1111%2fj.1745-3984.1985.tb01065.x&partnerID=40&md5=d9a44361cc70e39c1d24bdb4326b2f89","The literature of the concept of face validity is surveyed. A definition of face validity involving four facets is proposed. Empirical data suggesting that face validity is a characteristic of tests that can be validly and reliably measured are presented. Implications for research and for practical use are discussed. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0001718163"
"LISSITZ R.W.; WILLHOFT J.L.","LISSITZ, ROBERT W. (6602902644); WILLHOFT, JOSEPH L. (35325811300)","6602902644; 35325811300","A METHODOLOGICAL STUDY OF THE TORRANCE TESTS OF CREATIVITY","1985","Journal of Educational Measurement","22","1","","1","11","10","61","10.1111/j.1745-3984.1985.tb01044.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007797734&doi=10.1111%2fj.1745-3984.1985.tb01044.x&partnerID=40&md5=74ca708e42ac2eb5b32139d0ba04e5c5","The sensitivity of creative thinking as measured by the Torrance Tests with regard to an experimenter‐induced response set was investigated. The 198 subjects were divided into four groups. Before performing the Unusual Uses Activity (Verbal Form A), each group was given a unique set of instructions. Group I received standard instructions and acted as a control. Other treatments were varied in terms of the types of responses that were encouraged. Group II was encouraged to be “practical and reasonable”; Group III was encouraged to list “as many ideas” as possible; and Group IV was encouraged to include all “unusual, weird, or illogical” ideas. Discriminant analysis yeilded two significant functions, suggesting that univariate analysis of the Torrance scales of Fluency. Flexibility, and Originality can be misleading. A multivariate Dunnett test showed that Torrance Tests were highly sensitive to brief experimenter manipulation. A very modest induced attitude shift resulted in a shift of up to approximately a standard deviation in the creativity scores. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85007797734"
"DIVGI D.R.","DIVGI, D.R. (36961737400)","36961737400","DOES THE RASCH MODEL REALLY WORK FOR MULTIPLE CHOICE ITEMS? NOT IF YOU LOOK CLOSELY","1986","Journal of Educational Measurement","23","4","","283","298","15","53","10.1111/j.1745-3984.1986.tb00251.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001255373&doi=10.1111%2fj.1745-3984.1986.tb00251.x&partnerID=40&md5=b18319a59768045e6d2e373d11901d42","This paper discusses various issues involved in using the Rasch model with multiple choice tests. By presenting a modified test that is much more powerful, the value of Wright and Panchapakesan's test as evidence of model fit is shown to be questionable. According to the new test, the model failed to fit 68% of the items in the Anchor Test Study. Effects of such misfit on test equating are demonstrated. Results of some past studies purporting to support the Rasch model are shown to be irrelevant, or to yield the conclusion that the Rasch model did not fit the data. Issues like “objectivity” and consistent estimation are shown to be unimportant in selection of a latent trait model. Thus, available evidence shows the Rasch model to be unsuitable for multiple choice items. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0001255373"
"YEN W.M.","YEN, WENDY M. (7102684621)","7102684621","OBTAINING MAXIMUM LIKELIHOOD TRAIT ESTIMATES FROM NUMBER‐CORRECT SCORES FOR THE THREE‐PARAMETER LOGISTIC MODEL","1984","Journal of Educational Measurement","21","2","","93","111","18","41","10.1111/j.1745-3984.1984.tb00223.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954601329&doi=10.1111%2fj.1745-3984.1984.tb00223.x&partnerID=40&md5=4e96e4ad2f1bcd35148713e1b9016458","A procedure is presented for obtaining maximum likelihood trait estimates from number‐correct (NC) scores for the three‐parameter logistic model. The procedure produces an NC score to trait estimate conversion table, which can be used when the hand scoring of tests is desired or when item response pattern (IP) scoring is unacceptable for other (e.g., political) reasons. Simulated data are produced for four 20‐item and four 40‐item tests of varying difficulties. These data indicate that the NC scoring procedure produces trait estimates that are tau‐equivalent to the IP trait estimates (i.e., they are expected to have the same mean for all groups of examinees), but the NC trait estimates have higher standard errors of measurement than IP trait estimates. Data for six real achievement tests verify that the NC trait estimates are quite similar to the IP trait estimates but have higher empirical standard errors than IP trait estimates, particularly for low‐scoring examinees. Analyses in the estimated true score metric confirm the conclusions made in the trait metric. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-79954601329"
"LORD F.M.","LORD, FREDERIC M. (16463899300)","16463899300","STANDARD ERRORS OF MEASUREMENT AT DIFFERENT ABILITY LEVELS","1984","Journal of Educational Measurement","21","3","","239","243","4","43","10.1111/j.1745-3984.1984.tb01031.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010721136&doi=10.1111%2fj.1745-3984.1984.tb01031.x&partnerID=40&md5=fc76a7638a093d4cbf4248e7f7cccd7c","Four methods are outlined for estimating or approximating from a single test administration the standard error of measurement of number‐right test score at specified ability levels or cutting scores. The methods are illustrated and compared on one set of real test data. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85010721136"
"POWERS D.E.; ALDERMAN D.L.","POWERS, DONALD E. (7202818498); ALDERMAN, DONALD L. (16482270900)","7202818498; 16482270900","EFFECTS OF TEST FAMILIARIZATION ON SAT PERFORMANCE","1983","Journal of Educational Measurement","20","1","","71","79","8","10","10.1111/j.1745-3984.1983.tb00191.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007397352&doi=10.1111%2fj.1745-3984.1983.tb00191.x&partnerID=40&md5=81fe0d3af0bc6044008a54647ce81328","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0007397352"
"DE GRUIJTER D.N.M.","DE GRUIJTER, DATO N. M. (6507363394)","6507363394","COMPROMISE MODELS FOR ESTABLISHING EXAMINATION STANDARDS","1985","Journal of Educational Measurement","22","4","","263","269","6","41","10.1111/j.1745-3984.1985.tb01063.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988896280&doi=10.1111%2fj.1745-3984.1985.tb01063.x&partnerID=40&md5=4f0edb0bbe35cc40e63709982523c1a2","Cutoff scores based on absolute standards can be unacceptable in terms of the number of failures they produce. Cutoff scores based on relative standards, that is, cutoff scores set to achieve a fixed percentage of failures, can be unacceptable because an acceptable performance level for passed examinees cannot be guaranteed. In some situations one can improve upon an absolute standard using a compromise model, which draws on the information in the observed score distribution for a test to adjust the standard. Three compromise models are described and compared in this article. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988896280"
"PAGE E.B.; FEIFS H.","PAGE, ELLIS B. (7103396107); FEIFS, HELMUTS (57195041193)","7103396107; 57195041193","SAT SCORES AND AMERICAN STATES: SEEKING FOR USEFUL MEANING","1985","Journal of Educational Measurement","22","4","","305","312","7","13","10.1111/j.1745-3984.1985.tb01067.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988095680&doi=10.1111%2fj.1745-3984.1985.tb01067.x&partnerID=40&md5=f1692826329a3cbd71e2a4759c761392","Despite difficulties, states frequently seek to compare their test performance with that of other states, or with the published national norms. All states have some students taking the Scholastic Aptitude Test (SAT) exams for college admissions, but because the numbers vary widely, when the state means are made public by the College Boards, there is debate about their interpretation. Here the 50 state SAT means are the criterion in a multivariate model, together with the state variables of percent taking the SATs (very influential), percent of minority in the state, average income, unemployment rate, and per pupil expenditure. Regression is analyzed (without expenditure) to examine the state residuals compared with the “expected” scores. And briefly noted is the difficulty of using alternative tests for such comparisons, as these are currently normed, selected, and used. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988095680"
"FREEMAN D.J.; BELLI G.M.; PORTER A.C.; FLODEN R.E.; SCHMIDT W.H.; SCHWILLE J.R.","FREEMAN, DONALD J. (59099021400); BELLI, GABRIELLA M. (7004998951); PORTER, ANDREW C. (57210728301); FLODEN, ROBERT E. (6505832077); SCHMIDT, WILLIAM H. (7404055781); SCHWILLE, JOHN R. (16314452700)","59099021400; 7004998951; 57210728301; 6505832077; 7404055781; 16314452700","THE INFLUENCE OF DIFFERENT STYLES OF TEXTBOOK USE ON INSTRUCTIONAL VALIDITY OF STANDARDIZED TESTS","1983","Journal of Educational Measurement","20","3","","259","270","11","15","10.1111/j.1745-3984.1983.tb00204.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-3242843560&doi=10.1111%2fj.1745-3984.1983.tb00204.x&partnerID=40&md5=b0478388019bbad9c645aee05b833bfd","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-3242843560"
"CUMMINGS O.W.","CUMMINGS, OLIVER W. (25023283300)","25023283300","DIFFERENTIAL MEASUREMENT OF READING COMPREHENSION SKILLS FOR STUDENTS WITH DISCREPANT SUBSKILL PROFILES","1982","Journal of Educational Measurement","19","1","","59","66","7","0","10.1111/j.1745-3984.1982.tb00115.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024957651&doi=10.1111%2fj.1745-3984.1982.tb00115.x&partnerID=40&md5=6c0f95460348e0c62d21cd952499a8a4","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85024957651"
"MARSH H.W.; O'NEILL R.","MARSH, HERBERT W. (7201585638); O'NEILL, ROSALIE (57195040215)","7201585638; 57195040215","SELF DESCRIPTION QUESTIONNAIRE III: THE CONSTRUCT VALIDITY OF MULTIDIMENSIONAL SELF‐CONCEPT RATINGS BY LATE ADOLESCENTS","1984","Journal of Educational Measurement","21","2","","153","174","21","468","10.1111/j.1745-3984.1984.tb00227.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024958225&doi=10.1111%2fj.1745-3984.1984.tb00227.x&partnerID=40&md5=b2e14227a96bcec3b6d3e37e3e87b2c9","This research examines the validity of self‐concept interpretations of scores from a new instrument for use with university‐aged respondents. The Self Description Questionnaire III (SDQ III) was designed to measure 13 factors of self‐concept, and these dimensions were identified with conventional and confirmatory factor analyses. In two different studies, the reliabilities of the 13 factors were high (median alpha = 0.89) and correlations among the factors were low (median r = 0.09). Correlations among a wide variety of validity criteria and the multiple dimensions of self‐concept measured by the SDQ III formed a logical and theoretically consistent pattern of relationships. Academic achievement measures in language and mathematics were substantially correlated with self‐concepts in the same areas but not with other self‐concept factors. Ratings by significant others for all 13 SDQ HI scales were substantially correlated with the measures of corresponding self‐concepts, but were not substantially correlated with the measures of noncorresponding self‐concepts. These findings offer strong support for the construct validity of both self‐concept and interpretations based upon the SDQ III. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85024958225"
"CURTIS M.E.","CURTIS, MARY E. (36830367100)","36830367100","READING THEORY AND THE ASSESSMENT OF READING ACHIEVEMENT","1983","Journal of Educational Measurement","20","2","","133","147","14","19","10.1111/j.1745-3984.1983.tb00195.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978582890&doi=10.1111%2fj.1745-3984.1983.tb00195.x&partnerID=40&md5=6ec04956dccf90d9ff607d4419bd0d31","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84978582890"
"TALLMADGE G.K.","TALLMADGE, G. KASTEN (6507242322)","6507242322","RUMORS REGARDING THE DEATH OF THE EQUIPERCENTILE ASSUMPTION MAY HAVE BEEN GREATLY EXAGGERATED","1985","Journal of Educational Measurement","22","1","","33","39","6","3","10.1111/j.1745-3984.1985.tb01047.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970413336&doi=10.1111%2fj.1745-3984.1985.tb01047.x&partnerID=40&md5=080337fd916704eb649dffa7b76613b2","Powers, Slaughter, and Helmick (1983) recently analyzed selection, pretest, and posttest scores collected from large numbers of students in two cohorts. These analyses led them to conclude that the equipercentile assumption underlying norm‐referenced evaluation methodology is “inappropriate.” Re‐examination of the data suggests that there is strong support for the validity of the equipercentile assumption in the selection and pretest scores they present. The observed “gains” from pre‐ to posttests are better attributed to stakeholder bias, posttests that match the curriculum content too closely, or a combination of these two factors than to inappropriateness of the equipercentile assumption. Annual testing where the posttest from one year also serves as the pretest for the next is suggested as a promising solution to both of the cited threats to the internal validity of norm‐referenced evaluations. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84970413336"
"BEJAR I.I.","BEJAR, ISAAC I. (6602577229)","6602577229","EDUCATIONAL DIAGNOSTIC ASSESSMENT","1984","Journal of Educational Measurement","21","2","","175","189","14","37","10.1111/j.1745-3984.1984.tb00228.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350402761&doi=10.1111%2fj.1745-3984.1984.tb00228.x&partnerID=40&md5=ae234e6278627d85feaae5ba2c7cf9cd","A strong demand currently exists for testing instruments that are capable of providing more informative and diagnostic results than typical tests offer. This paper reviews approaches that have been proposed for educational diagnostic assessment. Two major approaches are identified: (a) deficit assessment, which focuses on weaknesses of the student, and (b) error analysis, which focuses on the kinds of errors the student commits. This paper also reviews recent work related to diagnostic assessment that is based on the integration of methods from cognitive psychology and artificial intelligence. It is concluded that the development of powerful diagnostic instruments may require a reexamination of existing psychometric models and possibly the development of alternative ones. It is also pointed out that the traditional approach to the specification of content in terms of static taxonomies may not be appropriate given the dynamic and sequential nature of diagnostic assessment. Finally, it is noted that the psychometric and content demands of diagnostic assessment all but require test admininstration by computer. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-70350402761"
"LINN R.L.","LINN, ROBERT L. (56126633100)","56126633100","SELECTION BIAS: MULTIPLE MEANINGS","1984","Journal of Educational Measurement","21","1","","33","47","14","36","10.1111/j.1745-3984.1984.tb00219.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019499154&doi=10.1111%2fj.1745-3984.1984.tb00219.x&partnerID=40&md5=09fabe81accf782a279db71ca291f044","The common approach to studies of predictive bias is analyzed within the context of a conceptual model in which predictors and criterion measures are viewed as fallible indicators of idealized qualifications. Selection on the basis of idealized qualifications depends on acceptance of a meritocratic principle, a principle which is itself subject to debate. Even with acceptance of this principle, however, it is shown that there will generally be a wide range of uncertainty as the result of the fallible nature of the indicators. Only when boundary conditions defined by Birnbaum (1979) are exceeded will the results be unambiguous. An example of such a case is presented. It is argued, however, that such instances are probably somewhat rare. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85019499154"
"HAMBLETON R.K.; DE GRUIJTER D.N.M.","HAMBLETON, RONALD K. (7006242264); DE GRUIJTER, DATO N. M. (6507363394)","7006242264; 6507363394","APPLICATION OF ITEM RESPONSE MODELS TO CRITERION‐REFERENCED TEST ITEM SELECTION","1983","Journal of Educational Measurement","20","4","","355","367","12","22","10.1111/j.1745-3984.1983.tb00213.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005234098&doi=10.1111%2fj.1745-3984.1983.tb00213.x&partnerID=40&md5=7c258925f24b5272e0b05f8d6c8d630a","[No abstract available]","","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85005234098"
"BUDESCU D.V.; NEVO B.","BUDESCU, DAVID V. (7003588697); NEVO, BARUCH (6602948460)","7003588697; 6602948460","OPTIMAL NUMBER OF OPTIONS: AN INVESTIGATION OF THE ASSUMPTION OF PROPORTIONALITY","1985","Journal of Educational Measurement","22","3","","183","196","13","32","10.1111/j.1745-3984.1985.tb01057.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973817319&doi=10.1111%2fj.1745-3984.1985.tb01057.x&partnerID=40&md5=f488ad076a4ed70ed5b9c22828ae276e","One assumption common to all models for determining the optimal number of options per item (e. g., Lord, 1977) is that total testing time is proportional to the number of items and the number of options per item. Therefore, under this assumption given a fixed testing time, the test can be shortened or lengthened by deleting or adding a proportional number of options. The present study examines the validity of this assumption in three tests which were administered with 2, 3, 4, and 5 options per item. The number of items attempted in the first 10 and 15 minutes of the testing session and the time needed to complete the tests were recorded. Thus, the rate of performance for both fixed time and fixed test length was analyzed. A strong and consistently negative relationship between rate of performance and the number of options was detected in all tests. Thus, the empirical results did not support the assumption of proportionality. Furthermore, the data indicated that the method by which options are deleted can play a role in this context. A more realistic assumption of generalized proportionality, proposed by Grier (1976), was supported by the results from a Mathematical Reasoning test, but was only partially supported for a Vocabulary and a Reading Comprehension test. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84973817319"
"TAYLOR C.; WHITE K.R.","TAYLOR, CIE (57189398626); WHITE, KARL R. (7402810353)","57189398626; 7402810353","THE EFFECT OF REINFORCEMENT AND TRAINING ON GROUP STANDARDIZED TEST BEHAVIOR","1982","Journal of Educational Measurement","19","3","","199","209","10","3","10.1111/j.1745-3984.1982.tb00128.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005399336&doi=10.1111%2fj.1745-3984.1982.tb00128.x&partnerID=40&md5=1fce6eb8be3816dda250e888471d3586","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85005399336"
"NITKO A.J.; HSU T.‐C.","NITKO, ANTHONY J. (11938980400); HSU, TSE‐CHI (36786004200)","11938980400; 36786004200","A COMPREHENSIVE MICROCOMPUTER SYSTEM FOR CLASSROOM TESTING","1984","Journal of Educational Measurement","21","4","","377","390","13","5","10.1111/j.1745-3984.1984.tb01041.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984429051&doi=10.1111%2fj.1745-3984.1984.tb01041.x&partnerID=40&md5=f020a2c9ff30930409499cb2e251f1c8","This paper describes a comprehensive system of microcomputer programs that was developed as a tool for teachers to use for (a) improving instruction and classroom tests, and (b) evaluating and reporting student learning. The programs implement many of the suggestions made in standard educational measurement textbooks for how to use data from tests to improve instructional decision‐making in the classroom and to improve quality of classroom testing. The paper describes the three main components of the system: (a) a student data‐base with associated information retrieval and computational programs, (b) a test item analysis and evaluation component that can be used for guiding instruction, and (c) a test item banking and test assembly component. In the course of the description, several microcomputer program design considerations are discussed. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84984429051"
"MASTERS G.N.","MASTERS, GEOFFEREY N. (16448815200)","16448815200","CONSTRUCTING AN ITEM BANK USING PARTIAL CREDIT SCORING","1984","Journal of Educational Measurement","21","1","","19","32","13","14","10.1111/j.1745-3984.1984.tb00218.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993790439&doi=10.1111%2fj.1745-3984.1984.tb00218.x&partnerID=40&md5=48c9234a579fe58e861d903a41a5ff19","A method for banking test items scored in several ordered response categories is described. Each item is seen as an ordered sequence of steps, and test forms are equated using the estimated difficulties of the steps in their shared items. Procedures for analyzing the internal consistency of individual links and for analyzing the coherence of an entire linking structure are described. The methodology is used to link six forms of a mathematics problem solving test. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84993790439"
"CROSS L.H.; FRARY R.B.; KELLY P.P.; SMALL R.C.; IMPARA J.C.","CROSS, LAWRENCE H. (7103098312); FRARY, ROBERT B. (6602858608); KELLY, PATRICIA P. (57195044732); SMALL, ROBERT C. (57024571500); IMPARA, JAMES C. (6602233011)","7103098312; 6602858608; 57195044732; 57024571500; 6602233011","ESTABLISHING MINIMUM STANDARDS FOR ESSAYS: BLIND VERSUS INFORMED RÉVIEWS","1985","Journal of Educational Measurement","22","2","","137","146","9","10","10.1111/j.1745-3984.1985.tb01053.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072492&doi=10.1111%2fj.1745-3984.1985.tb01053.x&partnerID=40&md5=469d734905e16be35506418507abd5c1","This study was undertaken to evaluate two procedures for establishing a minimum performance standard for the essay subtest of the National Teacher Examinations (NTE) Communication Skills test. Twenty public school teachers and 20 teacher educators were randomly assigned to either a “blind” or “informed” review panel. Both panels were directed to evaluate the same set of 12 sample essays. Those assigned to the informed panel were apprised of the scores previously awarded to each essay and were asked to decide upon the lowest score they would judge to be acceptable for a beginning teacher. Those assigned to the blind panel were not informed of the scores and were asked to judge whether each essay was at least minimally acceptable for a beginning teacher. Following their blind reviews, the scores were revealed to this group, and they were also asked to indicate the lowest score they would judge to be acceptable. No significant mean differences were observed for the standards associated with blind and informed reviews, but a significant mean difference was observed between the standards set by teachers and teacher educators; the former being more stringent. Based upon criteria other than mean differences, it was concluded that the preferred procedure for setting standards on essays should involve a blind review followed by an informed review. This strategy was subsequently implemented in a state‐wide validation study in Virginia, and the results of this full‐scale study are presented along with results reported for a study using informed reviews in another state. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988072492"
"CROSS L.H.; IMPARA J.C.; FRARY R.B.; JAEGER R.M.","CROSS, LAWRENCE H. (7103098312); IMPARA, JAMES C. (6602233011); FRARY, ROBERT B. (6602858608); JAEGER, RICHARD M. (7201646840)","7103098312; 6602233011; 6602858608; 7201646840","A COMPARISON OF THREE METHODS FOR ESTABLISHING MINIMUM STANDARDS ON THE NATIONAL TEACHER EXAMINATIONS","1984","Journal of Educational Measurement","21","2","","113","129","16","61","10.1111/j.1745-3984.1984.tb00224.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001541786&doi=10.1111%2fj.1745-3984.1984.tb00224.x&partnerID=40&md5=93e846a6c6b5234e6c5d66a306823341","Minimum standards were established for the National Teacher Examinations (NTE) area examinations in mathematics and in elementary education by independent panels of teacher educators who had been instructed in the use of either the Angoff, Nedelsky, or Jaeger procedures. Of these three procedures, only the Jaeger method requires that normative data be provided to the judges when evaluating the items. However, it was of interest to study the effect such information would have upon the standards obtained using the other two methods. Therefore, the design incorporated three sequential review sessions with the level of normative information different for each. A three‐factor ANOVA revealed significant main effects for methods and sessions but not for subject area. None of the interactions was significant. The anticipated failure rates, the psychometric characteristics of the ratings, and other factors suggest that the Angoff procedure, as modified during the second session of this study, yields the most defensible standards for the NTE area examinations. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0001541786"
"LINN R.L.","LINN, ROBERT L. (56126633100)","56126633100","PEARSON SELECTION FORMULAS: IMPLICATIONS FOR STUDIES OF PREDICTIVE BIAS AND ESTIMATES OF EDUCATIONAL EFFECTS IN SELECTED SAMPLES","1983","Journal of Educational Measurement","20","1","","1","15","14","32","10.1111/j.1745-3984.1983.tb00185.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346519176&doi=10.1111%2fj.1745-3984.1983.tb00185.x&partnerID=40&md5=55a407e01983221aefbddefec9ccfb8f","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0346519176"
"JARADAT D.; SAWAGED S.","JARADAT, DERAR (57201657424); SAWAGED, SARI (57195045056)","57201657424; 57195045056","THE SUBSET SELECTION TECHNIQUE FOR MULTIPLE‐CHOICE TESTS: AN EMPIRICAL INQUIRY","1986","Journal of Educational Measurement","23","4","","369","376","7","13","10.1111/j.1745-3984.1986.tb00256.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011002787&doi=10.1111%2fj.1745-3984.1986.tb00256.x&partnerID=40&md5=1108f9e5cfed421e2923f5158f4d268e","The impact of the Subset Selection Technique (SST) for administering and scoring multiple‐choice items on certain properties of a test was compared with that of the two other commonly used methods, the Number Right (NR) and the Correction for Guessing Formula (CFG). Under SST, examinees are instructed to select any number of response alternatives, the objective being to include the correct answer in the chosen set. The effects of each scoring method on the psychometric properties of a test and on the performance of examinees with different achievement levels and/or risk‐taking propensities were investigated. Results indicated that SST outperformed the other two methods, producing not only higher reliability and validity coefficients for the test, but doing so without favoring high risk takers. The superiority of SST may be attributed to two interrelated factors: the efficiency of the technique in controlling for guessing and the encouragement provided examinees to use their partial knowledge in responding. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0011002787"
"MARSH H.W.; HOCEVAR D.","MARSH, HERBERT W. (7201585638); HOCEVAR, DENNIS (7003755443)","7201585638; 7003755443","CONFIRMATORY FACTOR ANALYSIS OF MULTITRAIT‐MULTIMETHOD MATRICES","1983","Journal of Educational Measurement","20","3","","231","248","17","111","10.1111/j.1745-3984.1983.tb00202.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0008785323&doi=10.1111%2fj.1745-3984.1983.tb00202.x&partnerID=40&md5=44c39b183695261181d56ab2e7ae3da8","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0008785323"
"DORANS N.J.; KINGSTON N.M.","DORANS, NEIL J. (6602289148); KINGSTON, NEAL M. (25930959400)","6602289148; 25930959400","THE EFFECTS OF VIOLATIONS OF UNIDIMENSIONALITY ON THE ESTIMATION OF ITEM AND ABILITY PARAMETERS AND ON ITEM RESPONSE THEORY EQUATING OF THE GRE VERBAL SCALE","1985","Journal of Educational Measurement","22","4","","249","262","13","39","10.1111/j.1745-3984.1985.tb01062.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014558567&doi=10.1111%2fj.1745-3984.1985.tb01062.x&partnerID=40&md5=65fdb61200b06d70f3d1790e3447ff75","One of the major assumptions of item response theory (IRT)models is that performance on a set of items is unidimensional, that is, the probability of successful performance by examinees on a set of items can be modeled by a mathematical model that has only one ability parameter. In practice, this strong assumption is likely to be violated. An important pragmatic question to consider is: What are the consequences of these violations? In this research, evidence is provided of violations of unidimensionality on the verbal scale of the GRE Aptitude Test, and the impact of these violations on IRT equating is examined. Previous factor analytic research on the GRE Aptitude Test suggested that two verbal dimensions, discrete verbal (analogies, antonyms, and sentence completions)and reading comprehension, existed. Consequently, the present research involved two separate calibrations (homogeneous) of discrete verbal items and reading comprehension items as well as a single calibration (heterogeneous) of all verbal item types. Thus, each verbal item was calibrated twice and each examinee obtained three ability estimates: reading comprehension, discrete verbal, and all verbal. The comparability of ability estimates based on homogeneous calibrations (reading comprehension or discrete verbal) to each other and to the all‐verbal ability estimates was examined. The effects of homogeneity of item calibration pool on estimates of item discrimination were also examined. Then the comparability of IRT equatings based on homogeneous and heterogeneous calibrations was assessed. The effects of calibration homogeneity on ability parameter estimates and discrimination parameter estimates are consistent with the existence of two highly correlated verbal dimensions. IRT equating results indicate that although violations of unidimensionality may have an impact on equating, the effect may not be substantial. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85014558567"
"SUBKOVIAK M.J.; MACK J.S.; IRONSON G.H.; CRAIG R.D.","SUBKOVIAK, MICHAEL J. (6506489478); MACK, JOANNE S. (57195041749); IRONSON, GAIL H. (7006098861); CRAIG, ROBERT D. (57195041648)","6506489478; 57195041749; 7006098861; 57195041648","EMPIRICAL COMPARISON OF SELECTED ITEM BIAS DETECTION PROCEDURES WITH BIAS MANIPULATION","1984","Journal of Educational Measurement","21","1","","49","58","9","21","10.1111/j.1745-3984.1984.tb00220.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009379221&doi=10.1111%2fj.1745-3984.1984.tb00220.x&partnerID=40&md5=fa1fae84f686dc41ca479a0e4ff61f5c","Biased test items were intentionally imbedded within a set of test items, and the resulting instrument was administered to large samples of blacks and whites. Three popular item bias detection procedures were then applied to the data: (1) the three‐parameter item characteristic curve procedure, (2) the chi‐square method, and (3) the transformed item difficulty approach. The three‐parameter item characteristic curve procedure proved most effective at detecting the intentionally biased test items; and the chi‐square method was viewed as the best alternative. The transformed item difficulty approach has certain limitations yet represents a practical alternative if sample size, lack of computer facilities, or the like preclude the use of the other two procedures. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0009379221"
"AIRASIAN P.W.; MADAUS G.F.","AIRASIAN, PETER W. (6505790573); MADAUS, GEORGE F. (6602679298)","6505790573; 6602679298","LINKING TESTING AND INSTRUCTION: POLICY ISSUES","1983","Journal of Educational Measurement","20","2","","103","118","15","39","10.1111/j.1745-3984.1983.tb00193.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988129229&doi=10.1111%2fj.1745-3984.1983.tb00193.x&partnerID=40&md5=bca69f5da9918254cf6f78b10c4eb6fc","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988129229"
"MILLMAN J.; ARTER J.A.","MILLMAN, JASON (25954577400); ARTER, JUDITH A. (57011176300)","25954577400; 57011176300","ISSUES IN ITEM BANKING","1984","Journal of Educational Measurement","21","4","","315","330","15","22","10.1111/j.1745-3984.1984.tb01037.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013346979&doi=10.1111%2fj.1745-3984.1984.tb01037.x&partnerID=40&md5=0b46dd7340119ec65c9505d7424a504e","An item bank is defined as a relatively large collection of easily accessible test questions. A wide variety of item bank schemes that meet this relatively unrestricted definition is illustrated. Advantages and disadvantages of item banking and the conditions under which item banks have the most potential value are identified. An extensive list of questions to be asked in designing item banking systems is provided. The following five questions were singled out for further discussion: How many items should be in the bank? Should users develop their own item collections or use the collections of others? How should the items be classified? Should items be calibrated? Will each test have different items or will the same test be administered to all? Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-85013346979"
"RUDNER L.M.","RUDNER, LAWRENCE M. (6701657821)","6701657821","INDIVIDUAL ASSESSMENT ACCURACY","1983","Journal of Educational Measurement","20","3","","207","219","12","30","10.1111/j.1745-3984.1983.tb00200.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007261199&doi=10.1111%2fj.1745-3984.1983.tb00200.x&partnerID=40&md5=2f3334168cab4242d29dc33d8d218766","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0007261199"
"SCHMIDT W.H.","SCHMIDT, WILLIAM H. (7404055781)","7404055781","CONTENT BIASES IN ACHIEVEMENT TESTS","1983","Journal of Educational Measurement","20","2","","165","178","13","13","10.1111/j.1745-3984.1983.tb00197.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011172687&doi=10.1111%2fj.1745-3984.1983.tb00197.x&partnerID=40&md5=2d2cf1d2d8c714bdb25d02a72137dc4d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85011172687"
"LINN R.L.","LINN, ROBERT L. (56126633100)","56126633100","TESTING AND INSTRUCTION: LINKS AND DISTINCTIONS","1983","Journal of Educational Measurement","20","2","","179","189","10","21","10.1111/j.1745-3984.1983.tb00198.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039118503&doi=10.1111%2fj.1745-3984.1983.tb00198.x&partnerID=40&md5=30074c0c7d4880132d353e7e612fdc7e","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0039118503"
"FRASER B.J.","FRASER, BARRY J. (7005454351)","7005454351","DEVELOPMENT OF SHORT FORMS OF SEVERAL CLASSROOM ENVIRONMENT SCALES","1982","Journal of Educational Measurement","19","3","","221","227","6","45","10.1111/j.1745-3984.1982.tb00130.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004826228&doi=10.1111%2fj.1745-3984.1982.tb00130.x&partnerID=40&md5=c66a94a8f3abdd040084288ab7887065","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85004826228"
"SCOTT M.; HATFIELD J.G.","SCOTT, M.M (56652388800); HATFIELD, JAMES G. (57195043968)","56652388800; 57195043968","LEMS OF ANALYST AND OBSERVER AGREEMENT IN NATURALISTIC NARRATIVE DATA","1985","Journal of Educational Measurement","22","3","","207","218","11","24","10.1111/j.1745-3984.1985.tb01059.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000480885&doi=10.1111%2fj.1745-3984.1985.tb01059.x&partnerID=40&md5=fe32ad6da0149462e5a16244d1ea08ee","The use of naturalistic narrative data is increasing but without increasing methodological rigor. Assumptions underlying such methods prohibit an investigator from interrupting the natural habitat by use of such standard laboratory procedures as equal observation intervals for all subjects, equal behavior units, or other artificial equalization of the behavioral data stream. Data analytic techniques commonly used in standard laboratory research present problems in the analysis of naturalistic data of this type inasmuch as most assume equal observation intervals, equal unit intervals, or some other fundamental character of equality. These problems are particularly acute with respect to estimates of analyst and observer agreement. This paper addresses these problems and discusses the advantages and disadvantages of several possible solutions. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0000480885"
"POWERS S.; SLAUGHTER H.; HELMICK C.","POWERS, STEPHEN (7203083098); SLAUGHTER, HELEN (6506115031); HELMICK, CHERYL (57195040014)","7203083098; 6506115031; 57195040014","A TEST OF THE EQUIPERCENTILE HYPOTHESIS OF THE TIERS NORM‐REFERENCED MODEL","1983","Journal of Educational Measurement","20","3","","299","302","3","5","10.1111/j.1745-3984.1983.tb00208.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973836337&doi=10.1111%2fj.1745-3984.1983.tb00208.x&partnerID=40&md5=feabc71f2b0a1c29348068969e2998ca","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973836337"
"JONES L.V.; DAVENPORT E.C., JR.; BRYSON A.; BEKHUIS T.; ZWlCK R.","JONES, LYLE V. (7403624294); DAVENPORT, ERNEST C. (7006557222); BRYSON, ALOHA (57190483018); BEKHUIS, TANJA (36773437800); ZWlCK, REBECCA (57195042827)","7403624294; 7006557222; 57190483018; 36773437800; 57195042827","MATHEMATICS AND SCIENCE TEST SCORES AS RELATED TO COURSES TAKEN IN HIGH SCHOOL AND OTHER FACTORS","1986","Journal of Educational Measurement","23","3","","197","208","11","19","10.1111/j.1745-3984.1986.tb00245.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040574479&doi=10.1111%2fj.1745-3984.1986.tb00245.x&partnerID=40&md5=eabac3e61d9571b85b3da48e17baf447","For the 1980 sophomore cohort from High School and Beyond (National Opinion Research Center, 1983a, 1983b), a strong relation was found between senior‐year mathematics achievement test score and the number of high school mathematics courses taken at the level of Algebra I and above. Unexpectedly, the relation was slightly stronger for courses the students reported taking than for courses as recorded on the student's high school transcript. The strength of relation was reduced but remained substantial after a covariance adjustment for student socioeconomic status (SES), sophomore‐year verbal achievement test score, and sophomore‐year mathematics test score. In contrast, the relation between senior‐year science test score and the number of high school science courses taken was far weaker than for mathematics, both before and after covariance adjustment. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0040574479"
"HAMBLETON R.K.; MILLS C.N.; SIMON R.","HAMBLETON, RONALD K. (7006242264); MILLS, CRAIG N. (55951945400); SIMON, ROBERT (57189396382)","7006242264; 55951945400; 57189396382","DETERMINING THE LENGTHS FOR CRITERION‐REFERENCED TESTS","1983","Journal of Educational Measurement","20","1","","27","38","11","6","10.1111/j.1745-3984.1983.tb00187.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746339255&doi=10.1111%2fj.1745-3984.1983.tb00187.x&partnerID=40&md5=884883abc4407af771001195a057e83a","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-33746339255"
"HUGHES D.C.; KEELING B.; TUCK B.F.","HUGHES, DAVID C. (57213111654); KEELING, BRIAN (6701629267); TUCK, BRYAN F. (7005647113)","57213111654; 6701629267; 7005647113","EFFECTS OF ACHIEVEMENT EXPECTATIONS AND HANDWRITING QUALITY ON SCORING ESSAYS","1983","Journal of Educational Measurement","20","1","","65","70","5","37","10.1111/j.1745-3984.1983.tb00190.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001896536&doi=10.1111%2fj.1745-3984.1983.tb00190.x&partnerID=40&md5=8e259385285d9dc403806a2d81271eba","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0001896536"
"FRISBIE D.A.; DRUVA C.A.","FRISBIE, DAVID A. (7003704007); DRUVA, CYNTHIA A. (57191119781)","7003704007; 57191119781","ESTIMATING THE RELIABILITY OF MULTIPLE TRUE‐FALSE TESTS","1986","Journal of Educational Measurement","23","2","","99","105","6","13","10.1111/j.1745-3984.1986.tb00236.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038590131&doi=10.1111%2fj.1745-3984.1986.tb00236.x&partnerID=40&md5=409d8a36f54f79bdb747b74225e2482b","This study was designed to examine the level of dependence within multiple true‐false (MTF) test item clusters by computing sets of item intercorrelations with data from a test composed of both MTF and multiple choice (MC) items. It was posited that internal analysis reliability estimates for MTF tests would be spurious due to elevated MTF within‐cluster intercorrelations. Results showed that, on the average, MTF within‐cluster dependence was no greater than that found between MTF items from different clusters, between MC items, or between MC and MTF items. But item for item, there was greater dependence between items within the same cluster than between items of different clusters. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0038590131"
"CHASE C.I.","CHASE, CLINTON I. (7102500238)","7102500238","ESSAY TEST SCORES AND READING DIFFICULTY","1983","Journal of Educational Measurement","20","3","","293","297","4","12","10.1111/j.1745-3984.1983.tb00207.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011219510&doi=10.1111%2fj.1745-3984.1983.tb00207.x&partnerID=40&md5=6db46bab201e6ef41011f383dd548be7","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85011219510"
"HAERTEL E.; CALFEE R.","HAERTEL, EDWARD (6602745554); CALFEE, ROBERT (24605978700)","6602745554; 24605978700","SCHOOL ACHIEVEMENT: THINKING ABOUT WHAT TO TEST","1983","Journal of Educational Measurement","20","2","","119","132","13","15","10.1111/j.1745-3984.1983.tb00194.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988115801&doi=10.1111%2fj.1745-3984.1983.tb00194.x&partnerID=40&md5=082310c02693402dfc5d0749440bba85","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988115801"
"VALENCIA R.R.; RANKIN R.J.","VALENCIA, RICHARD R. (7005533880); RANKIN, RICHARD J. (7102093640)","7005533880; 7102093640","FACTOR ANALYSIS OF THE K–ABC FOR GROUPS OF ANGLO AND MEXICAN AMERICAN CHILDREN","1986","Journal of Educational Measurement","23","3","","209","219","10","15","10.1111/j.1745-3984.1986.tb00246.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001161861&doi=10.1111%2fj.1745-3984.1986.tb00246.x&partnerID=40&md5=6a71da7bcc7e88bc6cd77cd56a950717","Factor analyses of the Kaufman Assessment Battery for Children (K–ABC) were performed on separate groups of Anglo (n = 100) and Mexican American (n = 100) fifth‐grade children to determine the comparability of underlying structures and to examine the existence of possible bias in construct validity of the K–ABC for each group. The subjects were selected from two contiguous school districts and stratified by grade, socioeconomic status (SES), and language status. A principal factor analysis produced two factors in the Mental Processing area and three factors in the total battery (Mental Processing plus Achievement areas) for the two groups. Examination of the coefficients of congruence indicated factorial similarity across four of five factors. Furthermore, the factors that emerged for the Anglo and Mexican American groups corresponded to the scales comprising the K–ABC. The results of the investigation provide some evidence for the construct validity of the K–ABC for the two ethnic groups. Further test bias research on the K–ABC is recommended. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0001161861"
"LINN R.L.; HASTINGS C.N.","LINN, ROBERT L. (56126633100); HASTINGS, C. NICHOLAS (57024743900)","56126633100; 57024743900","A META ANALYSIS OF THE VALIDITY OF PREDICTORS OF PERFORMANCE IN LAW SCHOOL","1984","Journal of Educational Measurement","21","3","","245","259","14","27","10.1111/j.1745-3984.1984.tb01032.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000014539&doi=10.1111%2fj.1745-3984.1984.tb01032.x&partnerID=40&md5=e1ed9bc21757b99c7759cb8e96d3abcd","Although there is considerable evidence that the Law School Admission Test (LSAT) and the undergraduate grade‐point average (UGPA) have a useful degree of predictive validity, there is also a large variation in the magnitude of the coefficients across schools. Understanding this variation has important implications for the use and interpretation of results of a validity study conducted at an individual school. A meta analysis of the validity results and data on applicants to 154 law schools was conducted in an effort to better understand this observed variation. The standard deviation (SD) on the LSAT and the correlation between the LSAT and UGPA for accepted students at each law school accounted for 58.5% of the between‐school variance in the multiple correlations of these two predictors with first‐year average grade in law school. Sampling error accounted for an additional 12% of the variance. Hence, only a small fraction of the between‐school variability in validities remains to be explained by other statistical artifacts of situational specificity factors. Mean validities and 90% credibility values for four adjustment procedures are reported as are the mean observed validities for different combinations of predictors. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0000014539"
"HUGHES D.C.; KEELING B.","HUGHES, DAVID C. (57213111654); KEELING, BRIAN (6701629267)","57213111654; 6701629267","THE USE OF MODEL ESSAYS TO REDUCE CONTEXT EFFECTS IN ESSAY SCORING","1984","Journal of Educational Measurement","21","3","","277","281","4","17","10.1111/j.1745-3984.1984.tb01034.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039149829&doi=10.1111%2fj.1745-3984.1984.tb01034.x&partnerID=40&md5=e280d362c5a88261d4913843ab7d4725","Several studies have shown the existence of context effects in the scoring of essays. That is, essays receive higher marks when preceded by poor quality scripts than when preceded by good quality scripts. The present study investigated the effectiveness of providing scorers with model essays to reduce the influence of context. Context effects persisted despite the use of model essays during scoring. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0039149829"
"McCAULEY D.E., JR.; COLBERG M.","McCAULEY, DONALD E. (57195040263); COLBERG, MAGDA (24517514500)","57195040263; 24517514500","TRANSPORTABILITY OF DEDUCTIVE MEASUREMENT ACROSS CULTURES","1983","Journal of Educational Measurement","20","1","","81","92","11","2","10.1111/j.1745-3984.1983.tb00192.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024944162&doi=10.1111%2fj.1745-3984.1983.tb00192.x&partnerID=40&md5=2529d4087e9a73fb6945da833876471f","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85024944162"
"SHEPARD L.A.; CAMILLI G.; WILLIAMS D.M.","SHEPARD, LORRIE A. (23147241200); CAMILLI, GREGORY (7003383989); WILLIAMS, DAVID M. (57195043407)","23147241200; 7003383989; 57195043407","VALIDITY OF APPROXIMATION TECHNIQUES FOR DETECTING ITEM BIAS","1985","Journal of Educational Measurement","22","2","","77","105","28","56","10.1111/j.1745-3984.1985.tb01050.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988117027&doi=10.1111%2fj.1745-3984.1985.tb01050.x&partnerID=40&md5=f2ab55feb7f1e5a953c4ed07e83789d5","The purpose of this research was to recommend an item bias procedure when the number of minority examinees is too small to use preferred three‐parameter IRT methods. The chi‐square, Angoff delta‐plot, andpseudo‐IRT indices were compared with both real and simulated data. For the real test data a criterion of known bias had been established by cross‐validated IRT‐3 results. The findings from the Math Test and the simulated test were consistent. The pseudo‐IRT approach was best (measured by both correlations and percent agreement) in delecting criterion bias. The chi‐square was close in accuracy to the pseudo‐IRT index. The Angoff delta‐plot method was found to be inadequate on both heuristic and empirical grounds. In extreme cases it even identified items as biased against whites that were simulated to be biased against blacks. However, a modified Angoff index, where p‐value differences were regressed on item point biserials (and the residualized values used as the index), was nearly as good as the chi‐square in identifying known bias. A final caution was offered regarding the use of item bias techniques. The statistical flags should never be used mechanically to discard items; rather they should be used to inspect items for possible differences in meaning. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988117027"
"POWERS D.E.","POWERS, DONALD E. (7202818498)","7202818498","EFFECTS OF COACHING ON GRE APTITUDE TEST SCORES","1985","Journal of Educational Measurement","22","2","","121","136","15","24","10.1111/j.1745-3984.1985.tb01052.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988098340&doi=10.1111%2fj.1745-3984.1985.tb01052.x&partnerID=40&md5=cd8f23218be37e91dc1c324619fa5431","Test preparation activities were determined for a large representative sample of Graduate Record Examination (GRE) Aptitude Test takers. About 3% of these examinees had attended formal coaching programs for one or more sections of the test. After adjusting for differences in the background characteristics of coached and uncoached students, effects on test scores were related to the length and the type of programs offered. The effects on GRE verbal ability scores were not significantly related to the amount of coaching examinees received, and quantitative coaching effects increased slightly but not significantly with additional coaching. Effects on analytical ability scores, on the other hand, were related significantly to the length of coaching programs, through improved performance on two analytical item types, which have since been deleted from the test. Overall, the data suggest that, when compared with the two highly susceptible item types that have been removed from the GRE Aptitude Test, the test item types in the current version of the test (now called the GRE General Test) appear to show relatively little susceptibility to formal coaching experiences of the kinds considered here. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988098340"
"BAKER F.B.","BAKER, FRANK B. (7202513736)","7202513736","TECHNOLOGY AND TESTING: STATE OF THE ART AND TRENDS FOR THE FUTURE","1984","Journal of Educational Measurement","21","4","","399","406","7","2","10.1111/j.1745-3984.1984.tb01043.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970340206&doi=10.1111%2fj.1745-3984.1984.tb01043.x&partnerID=40&md5=cf91263fa68fca1bed0244d5b135864b","Until recently, computers and computer related technology have been used in testing primarily to provide large scale programs with cost effective scoring and reporting. However, the “microcomputer revolution” is beginning to have a significant impact on how technology is being employed in testing. The present paper reviews a number of existing applications where microcomputers have been successfully employed in the context of testing. Then, future trends in both technology and instruction are examined for possible impact upon testing. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84970340206"
"JACKSON D.N.; HOLDEN R.R.; LOCKLIN R.H.; MARKS E.","JACKSON, DOUGLAS N. (7404288191); HOLDEN, RONALD R. (7102188239); LOCKLIN, RALPH H. (57195040650); MARKS, EDMOND (25954348600)","7404288191; 7102188239; 57195040650; 25954348600","TAXONOMY OF VOCATIONAL INTERESTS OF ACADEMIC MAJOR AREAS","1984","Journal of Educational Measurement","21","3","","261","275","14","7","10.1111/j.1745-3984.1984.tb01033.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009125385&doi=10.1111%2fj.1745-3984.1984.tb01033.x&partnerID=40&md5=d1ca91938301dc1e1c9b2164a8e6a063","This study appraised the degree to which student profiles on the Jackson Vocational Interest Survey (JVIS) for different academic majors could be clustered in a meaningful way. From an initial sample of 10,134 students, a matrix of mean scores for 131 academic majors on each of 34 JVIS basic interest scales was computed. This matrix was subjected to a singular value decomposition with subsequent orthogonal and oblique rotations of 17 reference axes. The 17 clusters so defined reflected distinct sets of academic major fields, with separate clusters for majors in engineering, computer science, performing arts, communicative arts, human services, and others. Male and female groups entered into the definition of every cluster. Based on the salient representations of academic majors on these reference axes, modal cluster profiles were computed and decomposed into five orthogonal higher‐order dimensions. The implications of these findings for the psychology of occupational choice, career development, and vocational interest measurement are outlined. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0009125385"
"VAN DE VIJVER F.J.R.; POORTINGA Y.H.","VAN DE VIJVER, FONS J. R. (7003643938); POORTINGA, YPE H. (7004708804)","7003643938; 7004708804","A COMMENT ON McCAULEY AND COLBERG'S CONCEPTION OF CROSS‐CULTURAL TRANSPORTABILITY OF TESTS","1985","Journal of Educational Measurement","22","2","","157","161","4","1","10.1111/j.1745-3984.1985.tb01055.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965783071&doi=10.1111%2fj.1745-3984.1985.tb01055.x&partnerID=40&md5=0e287f044573f8008f4d17805bee83ab","Recently McCauley and Colberg described a theory of transportability, an approach to devising test items for cross‐cultural comparative research that are free from unwanted cultural effects. They also provided data to demonstrate the feasibility of their approach. In this comment it is argued that the transportability notion resembles earlier cross‐cultural work and does not add new insights into the vexed problem of cross‐cultural comparison. It is also stated that McCauley and Colberg's statistical checks do not preclude the possibility of bias. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84965783071"
"GAMACHE L.M.; NOVICK M.R.","GAMACHE, LEANN M. (57195041483); NOVICK, MELVIN R. (16513922700)","57195041483; 16513922700","CHOICE OF VARIABLES AND GENDER DIFFERENTIATED PREDICTION WITHIN SELECTED ACADEMIC PROGRAMS","1985","Journal of Educational Measurement","22","1","","53","70","17","13","10.1111/j.1745-3984.1985.tb01049.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009891569&doi=10.1111%2fj.1745-3984.1985.tb01049.x&partnerID=40&md5=a62bb37596b1ec2a7474953995c7e101","The existence of differential prediction of 2‐year grade point average is reported for gender groups within programs of study at a large state university. Johnson‐Neyman analyses indicate regions on the predictor score scales where differential prediction has practical impact within each program. A significant portion of students is generally affected. It is shown that careful selection of a subset of variables reduces differential prediction while maintaining predictive potency. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0009891569"
"McARTHUR D.L.; CHOPPIN B.H.","McARTHUR, DAVID L. (57529045900); CHOPPIN, BRUCE H. (6506847898)","57529045900; 6506847898","COMPUTERIZED DIAGNOSTIC TESTING","1984","Journal of Educational Measurement","21","4","","391","397","6","10","10.1111/j.1745-3984.1984.tb01042.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984119652&doi=10.1111%2fj.1745-3984.1984.tb01042.x&partnerID=40&md5=1bba634cf6e3f70ac655e4a657137e18","Efforts to use computers for diagnostic testing in education require appropriate psychometric and technological strategies and, equally important, good theoretical foundations as to what constitutes a diagnosis and how it is to be discerned in an examinee's performance. Although at present this combination is infrequent in education, for some years it has been in evidence in computer‐managed diagnosis in medicine. For comparable success in the field of education, it is argued that artificial intelligence algorithms may provide a suitable avenue for developing diagnostic testing further. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84984119652"
"SEDDON G.M.","SEDDON, G.M. (6701422029)","6701422029","THE MEASUREMENT AND PROPERTIES OF DIVERGENT THINKING ABILITY AS A SINGLE COMPOUND ENTITY","1983","Journal of Educational Measurement","20","4","","393","402","9","12","10.1111/j.1745-3984.1983.tb00216.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007818000&doi=10.1111%2fj.1745-3984.1983.tb00216.x&partnerID=40&md5=220c4908bf621c5c80b361763bda7030","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85007818000"
"EBEL R.L.","EBEL, ROBERT L. (16050366400)","16050366400","PROPOSED SOLUTIONS TO TWO PROBLEMS OF TEST CONSTRUCTION","1982","Journal of Educational Measurement","19","4","","267","278","11","14","10.1111/j.1745-3984.1982.tb00133.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005203201&doi=10.1111%2fj.1745-3984.1982.tb00133.x&partnerID=40&md5=c8b1fe72e398ba488d86ed8a561e80bd","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85005203201"
"PHILLIPS S.E.","PHILLIPS, S.E. (7402028180)","7402028180","THE EFFECTS OF THE DELETION OF MISFITTING PERSONS ON VERTICAL EQUATING VIA THE RASCH MODEL","1986","Journal of Educational Measurement","23","2","","107","118","11","8","10.1111/j.1745-3984.1986.tb00237.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007338399&doi=10.1111%2fj.1745-3984.1986.tb00237.x&partnerID=40&md5=b0ed68b31f433585c74293b36b50e13a","The purpose of the study was to compare Rasch model equatings of multilevel achievement test data before and after the deletion of misfitting persons. The Rasch equatings were also compared with an equating obtained using the equipercentile method. No basis could be found in the results for choosing between the two Rasch equatings. The deletion of misfitting persons produced minor improvements in Rasch model fit to the data. Both Rasch equatings produced results that differed from the results of the equipercentile equating. The Rasch data also indicated that the misfitting persons deleted in the second Rasch equating tended to be from the lower portion of the achievement distribution, suggesting that they may have been guessing. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0007338399"
"ANGOFF W.H.; SCHRADER W.B.","ANGOFF, WILLIAM H. (16648034100); SCHRADER, WILLIAM B. (57205964659)","16648034100; 57205964659","A STUDY OF HYPOTHESES BASIC TO THE USE OF RIGHTS AND FORMULA SCORES","1984","Journal of Educational Measurement","21","1","","1","17","16","20","10.1111/j.1745-3984.1984.tb00217.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988079977&doi=10.1111%2fj.1745-3984.1984.tb00217.x&partnerID=40&md5=e387902ac0133c9daf74406df6c8ad07","The hypothesis that some students, when tested under formula directions, omit items about which they have useful partial knowledge implies that such directions are not as fair as rights directions, especially to those students who are less inclined to guess. This hypothesis may be called the differential effects hypothesis. An alternative hypothesis states that examinees would perform no better than chance expectation on items that they would omit under formula directions but would answer under rights directions. This may be called the invariance hypothesis. Experimental data on this question were obtained by conducting special test administrations of College Board SAT‐verbal and Chemistry tests and by including experimental tests in a Graduate Management Admission Test administration. The data provide a basis for evaluating the two hypotheses and for assessing the effects of directions on the reliability and parallelism of scores for sophisticated examinees taking professionally developed tests. Results support the invariance hypothesis rather than the differential effects hypothesis. Copyright © 1984, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988079977"
"MILLER M.D.","MILLER, M. DAVID (55757783023)","55757783023","TIME ALLOCATION AND PATTERNS OF ITEM RESPONSE","1986","Journal of Educational Measurement","23","2","","147","156","9","12","10.1111/j.1745-3984.1986.tb00240.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007260631&doi=10.1111%2fj.1745-3984.1986.tb00240.x&partnerID=40&md5=764040dbc967265a56826c93557bfa4c","A measure of student patterns of item response is usually interpreted as a measure of carelessness or guessing. However, when aggregated to the class level, it was shown that the index identifies classes that have a poor match between test content and instructional coverage. In particular, Sato's (1980) caution index identified classes whose pattern of time allocation across content areas was atypical. In addition, it was found that the mean caution index for a class can best be interpreted knowing the within‐class standard deviation of the index. Copyright © 1986, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0007260631"
"BIRENBAUM M.; SHAW D.J.","BIRENBAUM, MENUCHA (6701441757); SHAW, DORIS J. (57195042453)","6701441757; 57195042453","TASK SPECIFICATION CHART: A KEY TO A BETTER UNDERSTANDING OF TEST RESULTS","1985","Journal of Educational Measurement","22","3","","219","230","11","11","10.1111/j.1745-3984.1985.tb01060.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988115372&doi=10.1111%2fj.1745-3984.1985.tb01060.x&partnerID=40&md5=67bba8c06c63aaa66310c4ccd1adf426","A task specification chart (TSC) that integrates the content facets and the procedural steps of a specified task is suggested as a tool for designing a test and for interpreting its results. An instructional unit for adding and subtracting fractions was used to demonstrate the design and application of the TSC. To evaluate its efficacy, a test based on the TSC was administered to two independent samples. Accounts of the variance of item difficulty indices and errors in students’ responses were used as the criteria for this evaluation. The results indicated that a very high percentage of variance in item difficulty indices was accounted for by item characteristics representing different task components. The typology of errors constructed on the basis of the TSC proved to be an efficient tool for identifying erroneous rules of operation underlying students' response patterns on the test. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988115372"
"ROGOSA D.R.; WILLETT J.B.","ROGOSA, DAVID R. (16465325600); WILLETT, JOHN B. (35561044200)","16465325600; 35561044200","DEMONSTRATING THE RELIABILITY THE DIFFERENCE SCORE IN THE MEASUREMENT OF CHANGE","1983","Journal of Educational Measurement","20","4","","335","343","8","287","10.1111/j.1745-3984.1983.tb00211.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935443438&doi=10.1111%2fj.1745-3984.1983.tb00211.x&partnerID=40&md5=a25751f60595600d6fecf7dbe2bc8bcf","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84935443438"
"BENSON J.; HOCEVAR D.","BENSON, JERI (7401802146); HOCEVAR, DENNIS (7003755443)","7401802146; 7003755443","THE IMPACT OF ITEM PHRASING ON THE VALIDITY OF ATTITUDE SCALES FOR ELEMENTARY SCHOOL CHILDREN","1985","Journal of Educational Measurement","22","3","","231","240","9","155","10.1111/j.1745-3984.1985.tb01061.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001987115&doi=10.1111%2fj.1745-3984.1985.tb01061.x&partnerID=40&md5=ea32e83e2580cc040be497af577b6671","The purpose of the study was to examine the effect of item phrasing on the validity of a Likert‐type attitude scale. Three content similar scales were composed of 15 items, either all positive, all negative, or a mixture of positive and negative items. Five hundred twenty‐two students in grades 4–6 responded to one of the three forms. Results from the all positive and negative forms indicated that item means, variances, and factor structures differed significantly. Inspection of item means suggested that it was difficult for the students to indicate agreement by disagreeing with a negative statement. Analyses of the mixed phrasing form indicated factors based upon item phrasing, not item content. Taken together, the results suggest that the technique of balancing item phrasing when used with elementary students appears to affect adversely the validity of attitude measurement. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-0001987115"
"JOHNSON S.; BELL J.F.","JOHNSON, SANDRA (55466788600); BELL, JOHN F. (57198402998)","55466788600; 57198402998","EVALUATING AND PREDICTING SURVEY EFFICIENCY USING GENERALIZABILITY THEORY","1985","Journal of Educational Measurement","22","2","","107","119","12","12","10.1111/j.1745-3984.1985.tb01051.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988064245&doi=10.1111%2fj.1745-3984.1985.tb01051.x&partnerID=40&md5=3d40f25d886f7956b94ff7a0a90d676e","A long‐term science performance monitoring program began in England, Wales, and Northern Ireland in 1980 with the first in an initial series of annual national sample surveys of the science performances of II‐, 13‐ and 15‐year‐old pupils. The assessment framework underlying this program is process‐oriented, consisting of a number of subcategories of science activity, some of which are assessed in practical mode. Pupils are randomly selected for testing according to a complex sampling scheme. Questions are also selected randomly to represent the various subcategories. From the start of this program, it was intended to appeal to generalizability theory for a suitable estimation paradigm, and in this paper some preliminary applications of G‐theory are described. The results of these applications would suggest that computerized question‐banking, domain‐sampling of questions, and G‐theory together provide a useful new technology for this kind of performance monitoring exercise. The issue of interpretability might still remain a problem, however, unless the question domains can be clearly defined, and can be reflected in the question pools with consistency over time. Copyright © 1985, Wiley Blackwell. All rights reserved","","","Article","Final","","Scopus","2-s2.0-84988064245"
"BIRENBAUM M.; TATSUOKA K.K.","BIRENBAUM, MENUCHA (6701441757); TATSUOKA, KIKUMI K. (6603447775)","6701441757; 6603447775","THE EFFECT OF A SCORING SYSTEM BASED ON THE ALGORITHM UNDERLYING THE STUDENTS’RESPONSE PATTERNS ON THE DIMENSIONALITY OF ACHIEVEMENT TEST DATA OF THE PROBLEM SOLVING TYPE","1983","Journal of Educational Measurement","20","1","","17","26","9","15","10.1111/j.1745-3984.1983.tb00186.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988063710&doi=10.1111%2fj.1745-3984.1983.tb00186.x&partnerID=40&md5=97a3e88da126763981709178fa5b9309","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988063710"
"VAN DER LINDEN W.J.","VAN DER LINDEN, WIM J. (55409657500)","55409657500","A LATENT TRAIT METHOD FOR DETERMINING INTRAJUDGE INCONSISTENCY IN THE ANGOFF AND NEDELSKY TECHNIQUES OF STANDARD SETTING","1982","Journal of Educational Measurement","19","4","","295","308","13","60","10.1111/j.1745-3984.1982.tb00135.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988140907&doi=10.1111%2fj.1745-3984.1982.tb00135.x&partnerID=40&md5=d9c7eb0a9009e1a6b40f7467df765063","[No abstract available]","","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84988140907"
"HUYNH H.; SAUNDERS J.C.","HUYNH, HUYNH (16512875100); SAUNDERS, JOSEPH C. (36932795300)","16512875100; 36932795300","ACCURACY OF TWO PROCEDURES FOR ESTIMATING RELIABILITY OF MASTERY TESTS","1980","Journal of Educational Measurement","17","4","","351","358","7","14","10.1111/j.1745-3984.1980.tb00836.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037927338&doi=10.1111%2fj.1745-3984.1980.tb00836.x&partnerID=40&md5=bc1b63741cc18440b8268873eb55dffb","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0037927338"
"WILD C.L.; DURSO R.; RUBIN D.B.","WILD, CHERYL L. (57190902463); DURSO, ROBIN (57195042820); RUBIN, DONALD B. (7202307156)","57190902463; 57195042820; 7202307156","EFFECT OF INCREASED TEST‐TAKING TIME ON TEST SCORES BY ETHNIC GROUP, YEARS OUT OF SCHOOL, AND SEX","1982","Journal of Educational Measurement","19","1","","19","28","9","31","10.1111/j.1745-3984.1982.tb00111.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953072875&doi=10.1111%2fj.1745-3984.1982.tb00111.x&partnerID=40&md5=9cef65d8e34686c9ceaa241352cfa64f","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84953072875"
"LEINHARDT G.; SEEWALD A.M.","LEINHARDT, GAEA (6602602646); SEEWALD, ANDREA MAR (6506451932)","6602602646; 6506451932","STUDENT‐LEVEL OBSERVATION OF BEGINNING READING","1981","Journal of Educational Measurement","18","3","","171","177","6","6","10.1111/j.1745-3984.1981.tb00851.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005406756&doi=10.1111%2fj.1745-3984.1981.tb00851.x&partnerID=40&md5=fec4b2cf4bf13ada5d6aa644894f7b28","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85005406756"
"BROWN C.","BROWN, CHARLES (57036025200)","57036025200","A NOTE ON THE DETERMINATION OF “ACCEPTABLE” PERFORMANCE IN THORNDIKE'S STANDARD OF FAIR SELECTION","1980","Journal of Educational Measurement","17","3","","203","209","6","3","10.1111/j.1745-3984.1980.tb00827.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347779585&doi=10.1111%2fj.1745-3984.1980.tb00827.x&partnerID=40&md5=a9ad4ce0b852a774f7baea3b9b9f0bb4","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0347779585"
"DALY J.A.; DICKSON‐MARKMAN F.","DALY, JOHN A. (7401992187); DICKSON‐MARKMAN, FRAN (56994855200)","7401992187; 56994855200","CONTRAST EFFECTS IN EVALUATING ESSAYS","1982","Journal of Educational Measurement","19","4","","309","316","7","33","10.1111/j.1745-3984.1982.tb00136.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001058749&doi=10.1111%2fj.1745-3984.1982.tb00136.x&partnerID=40&md5=fdd11a4e4e0e17900ef28c3ec5605a65","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0001058749"
"LINN R.L.; HARNISCH D.L.","LINN, ROBERT L. (56126633100); HARNISCH, DELWYN L. (6506625886)","56126633100; 6506625886","INTERACTIONS BETWEEN ITEM CONTENT AND GROUP MEMBERSHIP ON ACHIEVEMENT TEST ITEMS","1981","Journal of Educational Measurement","18","2","","109","118","9","80","10.1111/j.1745-3984.1981.tb00846.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965460461&doi=10.1111%2fj.1745-3984.1981.tb00846.x&partnerID=40&md5=89736f1cf7e81dcfdf9aa5294e8f7997","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84965460461"
"QUELLMALZ E.S.; CAPELL F.J.; CHOU C.‐P.","QUELLMALZ, EDYS S. (6507884970); CAPELL, FRANK J. (6603476685); CHOU, CHIH‐PING (7403592838)","6507884970; 6603476685; 7403592838","EFFECTS OF DISCOURSE AND RESPONSE MODE ON THE MEASUREMENT OF WRITING COMPETENCE","1982","Journal of Educational Measurement","19","4","","241","258","17","44","10.1111/j.1745-3984.1982.tb00131.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984529818&doi=10.1111%2fj.1745-3984.1982.tb00131.x&partnerID=40&md5=318bd5523134c7d58b99e388d85d5164","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84984529818"
"HOLMES S.E.","HOLMES, SUSAN E. (57189651135)","57189651135","UNIDIMENSIONALITY AND VERTICAL EQUATING WITH THE RASCH MODEL","1982","Journal of Educational Measurement","19","2","","139","147","8","22","10.1111/j.1745-3984.1982.tb00123.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988059637&doi=10.1111%2fj.1745-3984.1982.tb00123.x&partnerID=40&md5=ac96758e613703afe2ef20849016df33","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988059637"
"RUDNER L.M.; GETSON P.R.; KNIGHT D.L.","RUDNER, LAWRENCE M. (6701657821); GETSON, PAMELA R. (57207904278); KNIGHT, DAVID L. (57195045326)","6701657821; 57207904278; 57195045326","A MONTE CARLO COMPARISON OF SEVEN BIASED ITEM DETECTION TECHNIQUES","1980","Journal of Educational Measurement","17","1","","1","10","9","43","10.1111/j.1745-3984.1980.tb00810.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988070635&doi=10.1111%2fj.1745-3984.1980.tb00810.x&partnerID=40&md5=4051cfbd9e30c35dd6e5ceabf2b8a250","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988070635"
"BEJAR I.I.","BEJAR, ISAAC I. (6602577229)","6602577229","A PROCEDURE FOR INVESTIGATING THE UNIDIMENSIONALITY OF ACHIEVEMENT TESTS BASED ON ITEM PARAMETER ESTIMATES","1980","Journal of Educational Measurement","17","4","","283","296","13","53","10.1111/j.1745-3984.1980.tb00832.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002259022&doi=10.1111%2fj.1745-3984.1980.tb00832.x&partnerID=40&md5=90fb1f692b13a850abd1f12b8e0aa0fd","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0002259022"
"KOLEN M.J.; WHITNEY D.R.","KOLEN, MICHAEL J. (6603925839); WHITNEY, DOUGLAS R. (7102971846)","6603925839; 7102971846","COMPARISON OF FOUR PROCEDURES FOR EQUATING THE TESTS OF GENERAL EDUCATIONAL DEVELOPMENT","1982","Journal of Educational Measurement","19","4","","279","293","14","12","10.1111/j.1745-3984.1982.tb00134.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988084884&doi=10.1111%2fj.1745-3984.1982.tb00134.x&partnerID=40&md5=c2fdb1fef53033dd6451a9499a31ec5c","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988084884"
"BERGAN J.R.","BERGAN, JOHN R. (23379867100)","23379867100","MEASURING OBSERVER AGREEMENT USING THE QUASI‐INDEPENDENCE CONCEPT","1980","Journal of Educational Measurement","17","1","","59","69","10","4","10.1111/j.1745-3984.1980.tb00815.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84972661601&doi=10.1111%2fj.1745-3984.1980.tb00815.x&partnerID=40&md5=e92ba183061d7e0d62ca7f5564614e94","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84972661601"
"ANDERSON R.E.; WELCH W.W.; HARRIS L.J.","ANDERSON, RONALD E. (57213583728); WELCH, WAYNE W. (7203028295); HARRIS, LINDA J. (57195043690)","57213583728; 7203028295; 57195043690","METHODOLOGICAL CONSIDERATIONS IN THE DEVELOPMENT OF INDICATORS OF ACHIEVEMENT IN DATA FROM THE NATIONAL ASSESSMENT","1982","Journal of Educational Measurement","19","2","","113","124","11","2","10.1111/j.1745-3984.1982.tb00120.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987254302&doi=10.1111%2fj.1745-3984.1982.tb00120.x&partnerID=40&md5=31d51e7d10f5927b86c939262e66c7a3","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84987254302"
"FORSYTH R.A.; SPRATT K.F.","FORSYTH, ROBERT A. (57029929200); SPRATT, KEVIN F. (7007115669)","57029929200; 7007115669","MEASURING PROBLEM SOLVING ABILITY IN MATHEMATICS WITH MULTIPLE‐CHOICE ITEMS: THE EFFECT OF ITEM FORMAT ON SELECTED ITEM AND TEST CHARACTERISTICS","1980","Journal of Educational Measurement","17","1","","31","43","12","9","10.1111/j.1745-3984.1980.tb00812.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007182129&doi=10.1111%2fj.1745-3984.1980.tb00812.x&partnerID=40&md5=950475ced17873c417caf700b2ec681d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0007182129"
"KEOGH B.K.; PULLIS M.E.; CADWELL J.","KEOGH, BARBARA K. (7103302004); PULLIS, MICHAEL E. (24353233800); CADWELL, JOEL (25949498700)","7103302004; 24353233800; 25949498700","A SHORT FORM OF THE TEACHER TEMPERAMENT QUESTIONNAIRE","1982","Journal of Educational Measurement","19","4","","323","329","6","78","10.1111/j.1745-3984.1982.tb00138.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970256169&doi=10.1111%2fj.1745-3984.1982.tb00138.x&partnerID=40&md5=c5e0d805462bc324b3c239b3b2a34d43","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84970256169"
"ZOREF L.; WILLIAMS P.","ZOREF, LESLIE (57224229728); WILLIAMS, PAUL (57024298400)","57224229728; 57024298400","A LOOK AT CONTENT BIAS IN IQ TESTS","1980","Journal of Educational Measurement","17","4","","313","322","9","19","10.1111/j.1745-3984.1980.tb00834.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009325409&doi=10.1111%2fj.1745-3984.1980.tb00834.x&partnerID=40&md5=97674f7e4b06626eee3b385017378bd7","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0009325409"
"BERK R.A.","BERK, RONALD A. (7102907826)","7102907826","A CONSUMERs' GUIDE TO CRITERION‐REFERENCED TEST RELIABILITY","1980","Journal of Educational Measurement","17","4","","323","349","26","27","10.1111/j.1745-3984.1980.tb00835.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988111860&doi=10.1111%2fj.1745-3984.1980.tb00835.x&partnerID=40&md5=f040f90b442cad84ac9c45db690016ad","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988111860"
"BRIDGEMAN B.","BRIDGEMAN, BRENT (7005526936)","7005526936","GENERALITY OF A “FAST” OR “SLOW” TEST‐TAKING STYLE ACROSS A VARIETY OF COGNITIVE TASKS","1980","Journal of Educational Measurement","17","3","","211","217","6","4","10.1111/j.1745-3984.1980.tb00828.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011651478&doi=10.1111%2fj.1745-3984.1980.tb00828.x&partnerID=40&md5=f673208cbb94712e83c2081be34eaf06","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0011651478"
"KOLEN M.J.","KOLEN, MICHAEL J. (6603925839)","6603925839","COMPARISON OF TRADITIONAL AND ITEM RESPONSE THEORY METHODS FOR EQUATING TESTS","1981","Journal of Educational Measurement","18","1","","1","11","10","66","10.1111/j.1745-3984.1981.tb00838.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001732408&doi=10.1111%2fj.1745-3984.1981.tb00838.x&partnerID=40&md5=d2091dea95f4df1218f7bd46db436a92","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0001732408"
"GANOPOLE S.J.","GANOPOLE, SELINA J. (57025171500)","57025171500","THE FUNDAMENTAL READING COMPETENCIES TEST","1980","Journal of Educational Measurement","17","1","","71","74","3","0","10.1111/j.1745-3984.1980.tb00816.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024936945&doi=10.1111%2fj.1745-3984.1980.tb00816.x&partnerID=40&md5=feea58164ab4c9a6c571873b7bd611a5","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85024936945"
"WALLER M.I.","WALLER, MICHAEL I. (57189177949)","57189177949","A PROCEDURE FOR COMPARING LOGISTIC LATENT TRAIT MODELS","1981","Journal of Educational Measurement","18","2","","119","125","6","9","10.1111/j.1745-3984.1981.tb00847.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973704109&doi=10.1111%2fj.1745-3984.1981.tb00847.x&partnerID=40&md5=74b94098a02a8692df7ac5289c0d36f4","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973704109"
"WILCOX R.R.","WILCOX, RAND R. (7202527113)","7202527113","SOME NEW RESULTS ON AN ANSWER‐UNTIL‐CORRECT SCORING PROCEDURE","1982","Journal of Educational Measurement","19","1","","67","74","7","19","10.1111/j.1745-3984.1982.tb00116.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040737650&doi=10.1111%2fj.1745-3984.1982.tb00116.x&partnerID=40&md5=b666a11b09d31c6c0338a8507e658bc1","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0040737650"
"GROSS A.L.; SHULMAN V.","GROSS, ALAN L. (8361184600); SHULMAN, VIVIAN (22948847300)","8361184600; 22948847300","THE APPLICABILITY OF THE BETA BINOMIAL MODEL FOR CRITERION REFERENCED TESTING","1980","Journal of Educational Measurement","17","3","","195","201","6","6","10.1111/j.1745-3984.1980.tb00826.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074783&doi=10.1111%2fj.1745-3984.1980.tb00826.x&partnerID=40&md5=0320c00c1dbac871e06370dfdae2e019","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988074783"
"SEDDON G.M.; CHOKOTHO N.C.J.; MERRITT R.","SEDDON, G.M. (6701422029); CHOKOTHO, N.C.J. (56632467500); MERRITT, R. (57195041539)","6701422029; 56632467500; 57195041539","THE IDENTIFICATION OF RADEX PROPERTIES IN OBJECTIVE TEST ITEMS","1981","Journal of Educational Measurement","18","3","","155","170","15","1","10.1111/j.1745-3984.1981.tb00850.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987228095&doi=10.1111%2fj.1745-3984.1981.tb00850.x&partnerID=40&md5=736c6698ec4683bb9c02351155e3f558","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84987228095"
"WARDROP J.L.; ANDERSON T.H.; HIVELY W.; HASTINGS C.N.; ANDERSON R.I.; MULLER K.E.","WARDROP, JAMES L. (6701862970); ANDERSON, THOMAS H. (56971798700); HIVELY, WELLS (57833839100); HASTINGS, C. NICHOLAS (57024743900); ANDERSON, RICHARD I. (57195040757); MULLER, KEITH E. (7403204717)","6701862970; 56971798700; 57833839100; 57024743900; 57195040757; 7403204717","A FRAMEWORK FOR ANALYZING THE INFERENCE STRUCTURE OF EDUCATIONAL ACHIEVEMENT TESTS","1982","Journal of Educational Measurement","19","1","","1","18","17","1","10.1111/j.1745-3984.1982.tb00110.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973818998&doi=10.1111%2fj.1745-3984.1982.tb00110.x&partnerID=40&md5=9bd81b3c2b7636989862708150a49199","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973818998"
"CUDECK R.","CUDECK, ROBERT (6701660563)","6701660563","A COMPARATIVE STUDY OF INDICES FOR INTERNAL CONSISTENCY","1980","Journal of Educational Measurement","17","2","","117","130","13","7","10.1111/j.1745-3984.1980.tb00820.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973827072&doi=10.1111%2fj.1745-3984.1980.tb00820.x&partnerID=40&md5=972fc78de3e18cce8d037076a2db7130","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973827072"
"GAY L.R.","GAY, LORRAINE R. (35101931600)","35101931600","THE COMPARATIVE EFFECTS OF MULTIPLE‐CHOICE VERSUS SHORT‐ANSWER TESTS ON RETENTION","1980","Journal of Educational Measurement","17","1","","45","50","5","41","10.1111/j.1745-3984.1980.tb00813.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994927429&doi=10.1111%2fj.1745-3984.1980.tb00813.x&partnerID=40&md5=1ffd13d589b4fed4882acdb3d261262d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84994927429"
"REYNOLDS C.R.","REYNOLDS, CECIL R. (7402331624)","7402331624","AN EXAMINATION FOR BIAS IN A PRESCHOOL TEST BATTERY ACROSS RACE AND SEX","1980","Journal of Educational Measurement","17","2","","137","146","9","16","10.1111/j.1745-3984.1980.tb00822.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999405291&doi=10.1111%2fj.1745-3984.1980.tb00822.x&partnerID=40&md5=8938978475dd1563070fd5d97e35ac2a","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84999405291"
"WEBB N.M.; SHAVELSON R.J.","WEBB, NOREEN M. (7102171983); SHAVELSON, RICHARD J. (35613093400)","7102171983; 35613093400","MULTIVARIATE GENERALIZABILITY OF GENERAL EDUCATIONAL DEVELOPMENT RATINGS","1981","Journal of Educational Measurement","18","1","","13","22","9","23","10.1111/j.1745-3984.1981.tb00839.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001729995&doi=10.1111%2fj.1745-3984.1981.tb00839.x&partnerID=40&md5=8425c19eefeba39030257d87f996510a","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0001729995"
"KOFFLER S.L.","KOFFLER, STEPHEN L. (56987695000)","56987695000","A COMPARISON OF APPROACHES FOR SETTING PROFICIENCY STANDARDS","1980","Journal of Educational Measurement","17","3","","167","178","11","33","10.1111/j.1745-3984.1980.tb00824.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0012089754&doi=10.1111%2fj.1745-3984.1980.tb00824.x&partnerID=40&md5=1c5e373d92d7efc4b0183fd955e3f0d2","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0012089754"
"LOYD B.H.; HOOVER H.D.","LOYD, BRENDA H. (6601961198); HOOVER, H.D. (56600179100)","6601961198; 56600179100","VERTICAL EQUATING USING THE RASCH MODEL","1980","Journal of Educational Measurement","17","3","","179","193","14","165","10.1111/j.1745-3984.1980.tb00825.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963469754&doi=10.1111%2fj.1745-3984.1980.tb00825.x&partnerID=40&md5=4da090a366a11e26cd67d3bb7b6e2adf","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84963469754"
"YEN W.M.","YEN, WENDY M. (7102684621)","7102684621","THE EXTENT, CAUSES AND IMPORTANCE OF CONTEXT EFFECTS ON ITEM PARAMETERS FOR TWO LATENT TRAIT MODELS","1980","Journal of Educational Measurement","17","4","","297","311","14","45","10.1111/j.1745-3984.1980.tb00833.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84972743451&doi=10.1111%2fj.1745-3984.1980.tb00833.x&partnerID=40&md5=2fcba1cd6eedcf8d2ae2b0b9f42d8001","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84972743451"
"BIRENBAUM M.; TATSUOKA K.K.","BIRENBAUM, MENUCHA (6701441757); TATSUOKA, KIKUMI K. (6603447775)","6701441757; 6603447775","ON THE DIMENSIONALITY OF ACHIEVEMENT TEST DATA","1982","Journal of Educational Measurement","19","4","","259","266","7","20","10.1111/j.1745-3984.1982.tb00132.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988130171&doi=10.1111%2fj.1745-3984.1982.tb00132.x&partnerID=40&md5=0e942056c35d090ecacdc05ce37b1015","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988130171"
"SKAKUN E.N.; KLING S.","SKAKUN, ERNEST N. (6601962136); KLING, SAMUEL (7003902280)","6601962136; 7003902280","COMPARABILITY OF METHODS FOR SETTING STANDARDS","1980","Journal of Educational Measurement","17","3","","229","235","6","30","10.1111/j.1745-3984.1980.tb00830.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996769815&doi=10.1111%2fj.1745-3984.1980.tb00830.x&partnerID=40&md5=ccb5a2ec58417ba37496408d483c3dbf","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84996769815"
"HUGHES D.C.; KEELING B.; TUCK B.F.","HUGHES, DAVID C. (57213111654); KEELING, BRIAN (6701629267); TUCK, BRYAN F. (7005647113)","57213111654; 6701629267; 7005647113","THE INFLUENCE OF CONTEXT POSITION AND SCORING METHOD ON ESSAY SCORING","1980","Journal of Educational Measurement","17","2","","131","134","3","18","10.1111/j.1745-3984.1980.tb00821.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039742278&doi=10.1111%2fj.1745-3984.1980.tb00821.x&partnerID=40&md5=17465366ae697567eb16106bc934cd6d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0039742278"
"HAMILTON L.C.","HAMILTON, LAWRENCE C. (7101728566)","7101728566","SEX DIFFERENCES IN SELF‐REPORT ERRORS: A NOTE OF CAUTION","1981","Journal of Educational Measurement","18","4","","221","228","7","6","10.1111/j.1745-3984.1981.tb00855.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-19744376733&doi=10.1111%2fj.1745-3984.1981.tb00855.x&partnerID=40&md5=0ad08ed18b7839840e7a015a6158dc84","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-19744376733"
"TALLMADGE G.K.","TALLMADGE, G. KASTEN (6507242322)","6507242322","AN EMPIRICAL ASSESSMENT OF NORM‐REFERENCED EVALUATION METHODOLOGY","1982","Journal of Educational Measurement","19","2","","97","112","15","12","10.1111/j.1745-3984.1982.tb00119.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002024079&doi=10.1111%2fj.1745-3984.1982.tb00119.x&partnerID=40&md5=719636351ff2eba6db32d96eb7b6c062","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0002024079"
"HALADYNA T.; ROID G.","HALADYNA, TOM (6602737797); ROID, GALE (6602571692)","6602737797; 6602571692","THE ROLE OF INSTRUCTIONAL SENSITIVITY IN THE EMPIRICAL REVIEW OF CRITERION‐REFERENCED TEST ITEMS","1981","Journal of Educational Measurement","18","1","","39","53","14","27","10.1111/j.1745-3984.1981.tb00841.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011971812&doi=10.1111%2fj.1745-3984.1981.tb00841.x&partnerID=40&md5=cb745bc71cc5f61685e1c966b0c26727","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0011971812"
"WARD W.C.; FREDERIKSEN N.; CARLSON S.B.","WARD, WILLIAM C. (7202733418); FREDERIKSEN, NORMAN (7003357783); CARLSON, SYBIL B. (24486532200)","7202733418; 7003357783; 24486532200","CONSTRUCT VALIDITY OF FREE‐RESPONSE AND MACHINE‐SCORABLE FORMS OF A TEST","1980","Journal of Educational Measurement","17","1","","11","29","18","47","10.1111/j.1745-3984.1980.tb00811.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988092120&doi=10.1111%2fj.1745-3984.1980.tb00811.x&partnerID=40&md5=61d246cfd76d5526a291ddf84b0d3f77","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988092120"
"SIROTNIK K.A.","SIROTNIK, KENNETH A. (56360886700)","56360886700","PSYCHOMETRIC IMPLICATIONS OF THE UNIT‐OF‐ANALYSIS PROBLEM (WITH EXAMPLES FROM THE MEASUREMENT OF ORGANIZATIONAL CLIMATE)","1980","Journal of Educational Measurement","17","4","","245","282","37","94","10.1111/j.1745-3984.1980.tb00831.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970291272&doi=10.1111%2fj.1745-3984.1980.tb00831.x&partnerID=40&md5=453a9958c42663ac33d4c0dc4582ecac","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84970291272"
"JAEGER R.M.","JAEGER, RICHARD M. (7201646840)","7201646840","SOME EXPLORATORY INDICES FOR SELECTION OF A TEST EQUATING METHOD","1981","Journal of Educational Measurement","18","1","","23","38","15","12","10.1111/j.1745-3984.1981.tb00840.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944532776&doi=10.1111%2fj.1745-3984.1981.tb00840.x&partnerID=40&md5=4690e5d48d9f7f4ffe0c8dd9cc969ac0","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-24944532776"
"SMITH J.K.","SMITH, JEFFREY K. (15047002900)","15047002900","CONVERGING ON CORRECT ANSWERS: A PECULIARITY OF MULTIPLE CHOICE ITEMS","1982","Journal of Educational Measurement","19","3","","211","220","9","24","10.1111/j.1745-3984.1982.tb00129.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948868543&doi=10.1111%2fj.1745-3984.1982.tb00129.x&partnerID=40&md5=3215c8849c8afd24b41aacc1ce5aa879","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84948868543"
"BAGLIN R.F.","BAGLIN, ROGER F. (57190929399)","57190929399","DOES “NATIONALLY” NORMED REALLY MEAN NATIONALLY?","1981","Journal of Educational Measurement","18","2","","97","107","10","10","10.1111/j.1745-3984.1981.tb00845.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950272272&doi=10.1111%2fj.1745-3984.1981.tb00845.x&partnerID=40&md5=5bc38f15039885a345c1b1782ccb91bc","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84950272272"
"ASKEGAARD L.D.; UMILA B.V.","ASKEGAARD, LEWIS D. (57190055445); UMILA, BENWARDO V. (57195041019)","57190055445; 57195041019","AN EMPIRICAL INVESTIGATON OF THE APPLICABILITY OF MULTIPLE MATRIX SAMPLING TO THE METHOD OF RANK ORDER","1982","Journal of Educational Measurement","19","3","","193","197","4","1","10.1111/j.1745-3984.1982.tb00127.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952749667&doi=10.1111%2fj.1745-3984.1982.tb00127.x&partnerID=40&md5=9607fc04e99bc3cf7487382085c73f53","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84952749667"
"WlNNE P.H.; BELFRY M.J.","WlNNE, PHILIP H. (57195040680); BELFRY, M. JOAN (57195040398)","57195040680; 57195040398","INTERPRETIVE PROBLEMS WHEN CORRECTING FOR ATTENUATION","1982","Journal of Educational Measurement","19","2","","125","134","9","34","10.1111/j.1745-3984.1982.tb00121.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970472161&doi=10.1111%2fj.1745-3984.1982.tb00121.x&partnerID=40&md5=174401a2962d824eaa82849dbcf3ac5d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84970472161"
"BURTON N.W.","BURTON, NANCY W. (25624524300)","25624524300","STABILITY OF THE NATIONAL ASSESSMENT SCORING METHODS","1980","Journal of Educational Measurement","17","2","","95","104","9","1","10.1111/j.1745-3984.1980.tb00818.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973714933&doi=10.1111%2fj.1745-3984.1980.tb00818.x&partnerID=40&md5=5a30704ed83b3c1f76edc516fe0744d6","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973714933"
"LEINHARDT G.; SEEWALD A.M.","LEINHARDT, GAEA (6602602646); SEEWALD, ANDREA MAR (6506451932)","6602602646; 6506451932","OVERLAP: WHAT'S TESTED, WHAT'S TAUGHT?","1981","Journal of Educational Measurement","18","2","","85","96","11","46","10.1111/j.1745-3984.1981.tb00844.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951576068&doi=10.1111%2fj.1745-3984.1981.tb00844.x&partnerID=40&md5=9bd65be024e6f072f87c49a7712b9d42","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84951576068"
"ROWLEY G.L.","ROWLEY, GLENN L. (7006131497)","7006131497","HISTORICAL ANTECEDENTS OF THE STANDARD‐SETTING DEBATE: AN INSIDE ACCOUNT OF THE MINIMAL‐BEARDEDNESS CONTROVERSY","1982","Journal of Educational Measurement","19","2","","87","95","8","3","10.1111/j.1745-3984.1982.tb00118.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542769150&doi=10.1111%2fj.1745-3984.1982.tb00118.x&partnerID=40&md5=19518de9cfbc2311c732506556a4ca0d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-1542769150"
"PENG C.J.; SUBKOVIAK M.J.","PENG, CHAO‐YING J. (57682674400); SUBKOVIAK, MICHAEL J. (6506489478)","57682674400; 6506489478","A NOTE ON HUYNH'S NORMAL APPROXIMATION PROCEDURE FOR ESTIMATING CRITERION‐REFERENCED RELIABILITY","1980","Journal of Educational Measurement","17","4","","359","368","9","20","10.1111/j.1745-3984.1980.tb00837.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001409770&doi=10.1111%2fj.1745-3984.1980.tb00837.x&partnerID=40&md5=adca61b37881e672962b0607bdb816ed","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0001409770"
"MOSS P.A.; COLE N.S.; KHAMPALIKIT C.","MOSS, PAMELA A. (57217792160); COLE, NANCY S. (38961260400); KHAMPALIKIT, CHOOSAK (57032978000)","57217792160; 38961260400; 57032978000","A COMPARISON OF PROCEDURES TO ASSESS WRITTEN LANGUAGE SKILLS AT GRADES 4, 7, AND 10","1982","Journal of Educational Measurement","19","1","","37","47","10","34","10.1111/j.1745-3984.1982.tb00113.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925979085&doi=10.1111%2fj.1745-3984.1982.tb00113.x&partnerID=40&md5=55e90ce68adb8884212dfc584560e9fa","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84925979085"
"BLISS L.B.","BLISS, LEONARD B. (57195040195)","57195040195","A TEST OF LORD'S ASSUMPTION REGARDING EXAMINEE GUESSING BEHAVIOR ON MULTIPLE‐CHOICE TESTS USING ELEMENTARY SCHOOL STUDENTS","1980","Journal of Educational Measurement","17","2","","147","152","5","45","10.1111/j.1745-3984.1980.tb00823.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988135259&doi=10.1111%2fj.1745-3984.1980.tb00823.x&partnerID=40&md5=86ebf510008f97819b45332985e0beeb","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84988135259"
"CARDINET J.; TOURNEUR Y.; ALLAL L.","CARDINET, JEAN (16545382900); TOURNEUR, YVAN (35955152500); ALLAL, LINDA (23033009400)","16545382900; 35955152500; 23033009400","EXTENSION OF GENERALIZABILITY THEORY AND ITS APPLICATIONS IN EDUCATIONAL MEASUREMENT","1981","Journal of Educational Measurement","18","4","","183","204","21","57","10.1111/j.1745-3984.1981.tb00852.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001523579&doi=10.1111%2fj.1745-3984.1981.tb00852.x&partnerID=40&md5=0028c89d716242d23e2152914cdc0a5f","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0001523579"
"WEITZMAN R.A.","WEITZMAN, R.A. (57225813638)","57225813638","THE PREDICTION OF COLLEGE ACHIEVEMENT BY THE SCHOLASTIC APTITUDE TEST AND THE HIGH SCHOOL RECORD","1982","Journal of Educational Measurement","19","3","","179","191","12","11","10.1111/j.1745-3984.1982.tb00126.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962160143&doi=10.1111%2fj.1745-3984.1982.tb00126.x&partnerID=40&md5=0052f83b306714f18189ae875f03c611","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84962160143"
"MARASCUILO L.A.; SLAUGHTER R.E.","MARASCUILO, LEONARD A. (6505945302); SLAUGHTER, ROBERT E. (57195041674)","6505945302; 57195041674","STATISTICAL PROCEDURES FOR IDENTIFYING POSSIBLE SOURCES OF ITEM BIAS BASED ON x2 STATISTICS","1981","Journal of Educational Measurement","18","4","","229","248","19","36","10.1111/j.1745-3984.1981.tb00856.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015582412&doi=10.1111%2fj.1745-3984.1981.tb00856.x&partnerID=40&md5=24b349bffd4c2ceabc748e85d18812bb","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85015582412"
"SMITH P.L.","SMITH, PHILIP L. (57028742000)","57028742000","GAINING ACCURACY IN GENERALIZABILITY THEORY: USING MULTIPLE DESIGNS","1981","Journal of Educational Measurement","18","3","","147","154","7","16","10.1111/j.1745-3984.1981.tb00849.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994934583&doi=10.1111%2fj.1745-3984.1981.tb00849.x&partnerID=40&md5=6aabdcd11c86db6e18254fb62706b89d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84994934583"
"JAEGER R.M.; WOLF M.B.","JAEGER, RICHARD M. (7201646840); WOLF, MARIAN B. (57195040334)","7201646840; 57195040334","THE EFFECT OF STIMULUS FORMAT ON DISCRIMINABILITY IN SCHOOL SURVEYS","1982","Journal of Educational Measurement","19","3","","163","178","15","1","10.1111/j.1745-3984.1982.tb00125.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973749994&doi=10.1111%2fj.1745-3984.1982.tb00125.x&partnerID=40&md5=97bd0a1ebdb2af627b5d085f6e86d088","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84973749994"
"DONLON T.F.","DONLON, THOMAS F. (7006205084)","7006205084","UNINTERPRETABLE SCORES: THEIR IMPLICATIONS FOR TESTING PRACTICE","1981","Journal of Educational Measurement","18","4","","213","219","6","0","10.1111/j.1745-3984.1981.tb00854.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024935013&doi=10.1111%2fj.1745-3984.1981.tb00854.x&partnerID=40&md5=1de2999811c365ec33fe959ea7ebd8d0","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85024935013"
"HARNISCH D.L.; LINN R.L.","HARNISCH, DELWYN L. (6506625886); LINN, ROBERT L. (56126633100)","6506625886; 56126633100","ANALYSIS OF ITEM RESPONSE PATTERNS. QUESTIONABLE TEST DATA AND DISSIMILAR CURRICULUM PRACTICES","1981","Journal of Educational Measurement","18","3","","133","146","13","104","10.1111/j.1745-3984.1981.tb00848.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984503331&doi=10.1111%2fj.1745-3984.1981.tb00848.x&partnerID=40&md5=d839ae711b76e0cb9321483074713fa2","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84984503331"
"WHITELY S.E.","WHITELY, SUSAN E. (16493742700)","16493742700","MEASURING APTITUDE PROCESSES WITH MULTICOMPONENT LATENT TRAIT MODELS","1981","Journal of Educational Measurement","18","2","","67","84","17","15","10.1111/j.1745-3984.1981.tb00843.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0011524156&doi=10.1111%2fj.1745-3984.1981.tb00843.x&partnerID=40&md5=fe57f01689cd7ab79709fc154f309a2d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0011524156"
"KIRSCH I.S.; GUTHRIE J.T.","KIRSCH, IRWIN S. (7102863524); GUTHRIE, JOHN T. (7201546630)","7102863524; 7201546630","CONSTRUCT VALIDITY OF FUNCTIONAL READING TESTS","1980","Journal of Educational Measurement","17","2","","81","93","12","13","10.1111/j.1745-3984.1980.tb00817.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0040802259&doi=10.1111%2fj.1745-3984.1980.tb00817.x&partnerID=40&md5=928b67dbae760ae9e9dd47071b02b67d","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0040802259"
"LEVIN J.R.; SERLIN R.C.","LEVIN, JOEL R. (7403286430); SERLIN, RONALD C. (7004623219)","7403286430; 7004623219","APPROXIMATE SAMPLE SIZE AND POWER DETERMINATION FOR SCHEFFÈ CONTRASTS","1981","Journal of Educational Measurement","18","1","","55","57","2","4","10.1111/j.1745-3984.1981.tb00842.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946973136&doi=10.1111%2fj.1745-3984.1981.tb00842.x&partnerID=40&md5=fcbc31337cf89d4eb88efd0236d3a6f7","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84946973136"
"SIROTNIK K.A.","SIROTNIK, KENNETH A. (56360886700)","56360886700","ASSESSING ATTITUDINAL CONGRUENCE: A CASE FOR ABSOLUTE (AS WELL AS RELATIVE) INDICES","1981","Journal of Educational Measurement","18","4","","205","212","7","1","10.1111/j.1745-3984.1981.tb00853.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937104644&doi=10.1111%2fj.1745-3984.1981.tb00853.x&partnerID=40&md5=4fc68353ddee07059956983e904c2c23","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84937104644"
"ZIMMERMAN D.W.; WILLIAMS R.H.","ZIMMERMAN, DONALD W. (7202940240); WILLIAMS, RICHARD H. (7409603589)","7202940240; 7409603589","GAIN SCORES IN RESEARCH CAN BE HIGHLY RELIABLE","1982","Journal of Educational Measurement","19","2","","149","154","5","108","10.1111/j.1745-3984.1982.tb00124.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000754520&doi=10.1111%2fj.1745-3984.1982.tb00124.x&partnerID=40&md5=b21db54d1021ddabc902a23fcd1cca26","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0000754520"
"IRVIN L.K.; HALPERN A.S.; LANDMAN J.T.","IRVIN, LARRY K. (6701331308); HALPERN, ANDREW S. (7103054785); LANDMAN, JANET T. (7005091321)","6701331308; 7103054785; 7005091321","ASSESSMENT OF RETARDED STUDENT ACHIEVEMENT WITH STANDARDIZED TRUE/FALSE AND MULTIPLE‐CHOICE TESTS","1980","Journal of Educational Measurement","17","1","","51","58","7","1","10.1111/j.1745-3984.1980.tb00814.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952157829&doi=10.1111%2fj.1745-3984.1980.tb00814.x&partnerID=40&md5=2366c01bdd399a3ed99eef240405c4b8","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84952157829"
"KNAPP L.; KNAPP R.R.","KNAPP, LILA (36916953600); KNAPP, ROBERT R. (25950129400)","36916953600; 25950129400","CLUSTERED OCCUPATIONAL INTEREST MEASUREMENT BASED ON SEX‐BALANCED INVENTORY ITEMS","1982","Journal of Educational Measurement","19","1","","75","81","6","1","10.1111/j.1745-3984.1982.tb00117.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449682413&doi=10.1111%2fj.1745-3984.1982.tb00117.x&partnerID=40&md5=3dd0bdc1478d020ea599885cee750f87","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-70449682413"
"LAM T.C.M.; KLOCKARS A.J.","LAM, TONY C. M. (57220607063); KLOCKARS, ALAN J. (6603819320)","57220607063; 6603819320","ANCHOR POINT EFFECTS ON THE EQUIVALENCE OF QUESTIONNAIRE ITEMS","1982","Journal of Educational Measurement","19","4","","317","322","5","74","10.1111/j.1745-3984.1982.tb00137.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0007921046&doi=10.1111%2fj.1745-3984.1982.tb00137.x&partnerID=40&md5=f4448a12cae9974810fff68de2ce5c69","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-0007921046"
"LAMB R.R.; PREDIGER D.J.","LAMB, RICHARD R. (24518907900); PREDIGER, DALE J. (6602111339)","24518907900; 6602111339","CONSTRUCT VALIDITY OF RAW SCORE AND STANDARD SCORE REPORTS OF VOCATIONAL INTERESTS","1980","Journal of Educational Measurement","17","2","","107","114","7","0","10.1111/j.1745-3984.1980.tb00819.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024949602&doi=10.1111%2fj.1745-3984.1980.tb00819.x&partnerID=40&md5=730c1ec7385560f3160b034f6a9463cc","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-85024949602"
"HOGAN T.P.; MISHLER C.","HOGAN, THOMAS P. (57203054607); MISHLER, CAROL (16469387500)","57203054607; 16469387500","RELATIONSHIPS BETWEEN ESSAY TESTS AND OBJECTIVE TESTS OF LANGUAGE SKILLS FOR ELEMENTARY SCHOOL STUDENTS","1980","Journal of Educational Measurement","17","3","","219","227","8","20","10.1111/j.1745-3984.1980.tb00829.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925926325&doi=10.1111%2fj.1745-3984.1980.tb00829.x&partnerID=40&md5=713e303c1b229bd0ab4e3d96a4fc5b84","[No abstract available]","","","Article","Final","","Scopus","2-s2.0-84925926325"
