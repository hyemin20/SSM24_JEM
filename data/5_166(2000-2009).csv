Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
Güler N.; Penfield R.D.,"Güler, Neşe (34768155700); Penfield, Randall D. (6601923478)",34768155700; 6601923478,A comparison of the logistic regression and contingency table methods for simultaneous detection of uniform and nonuniform dif,2009,Journal of Educational Measurement,46,3,,314,329,15,20,10.1111/j.1745-3984.2009.00083.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749092277&doi=10.1111%2fj.1745-3984.2009.00083.x&partnerID=40&md5=98a2613f407a5e4f752614bca022852f,"In this study, we investigate the logistic regression (LR), Mantel-Haenszel (MH), and Breslow-Day (BD) procedures for the simultaneous detection of both uniform and nonuniform differential item functioning (DIF). A simulation study was used to assess and compare the Type I error rate and power of a combined decision rule (CDR), which assesses DIF using a combination of the decisions made with BD and MH to those of LR. The results revealed that while the Type I error rate of CDR was consistently below the nominal alpha level, the Type I error rate of LR was high for the conditions having unequal ability distributions. In addition, the power of CDR was consistently higher than that of LR across all forms of DIF. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-69749092277
Kim S.,"Kim, Seonghoon (23060961000)",23060961000,A comparative study of IRT fixed parameter calibration methods,2006,Journal of Educational Measurement,43,4,,355,381,26,66,10.1111/j.1745-3984.2006.00021.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845423981&doi=10.1111%2fj.1745-3984.2006.00021.x&partnerID=40&md5=350ffeec9114ddd2e19bc84a104cb769,"This article provides technical descriptions of five fixed parameter calibration (FPC) methods, which were based on marginal maximum likelihood estimation via the EM algorithm, and evaluates them through simulation. The five FPC methods described are distinguished from each other by how many times they update the prior ability distribution and by how many EM cycles they use. Specifically, the five FPC methods included no prior weights updating and one EM cycle (NWU-OEM) or multiple EM cycles (NWU-MEM), one prior weights updating and one EM cycle (OWU-OEM) or multiple EM cycles (OWU-MEM), and multiple weights updating and multiple EM cycles (MWU-MEM) methods. All the five FPC methods were evaluated in terms of recovery of the underlying ability distribution and item parameters. An important factor in the simulation was three different ability (normal) distributions - N(0, 1), N(0.5, 1.2 2), and N(1, 1.4 2)-for FPC groups, with the fixed item parameters obtained with a reference N(0, 1) group. Only the MWU-MEM method appeared to perform properly under all the three distributions. Under the N(0, 1) distribution, the NWU-MEM and OWU-MEM methods also appeared to perform properly. Under the N(0.5, 1.2 2), and N(1, 1.4 2) distributions, however, the four methods other than the MWU-MEM method resulted in some or severe under-estimation in the recovery.",,,Article,Final,,Scopus,2-s2.0-33845423981
Dorans N.J.,"Dorans, Neil J. (6602289148)",6602289148,Using subpopulation invariance to assess test score equity,2004,Journal of Educational Measurement,41,1,,43,68,25,71,10.1111/j.1745-3984.2004.tb01158.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142760069&doi=10.1111%2fj.1745-3984.2004.tb01158.x&partnerID=40&md5=27996028dc15e310d782fbf9c570cc9c,"Score equity assessment (SEA) is introduced, and placed within a fair assessment context that includes differential prediction or fair selection and differential item functioning. The notion of subpopulation invariance of linking functions is central to the assessment of score equity, just as it has been for differential item functioning and differential prediction. Advanced Placement (AP) data are used for illustrative purposes. The use of multiple-choice and constructed response items in AP provides an opportunity to observe a case where subpopulation invariance of linking functions does not hold (U.S. History), and a case in which it does hold (Calculus AB). The lack of invariance for U.S. History might be attributed to several sources. The role of SEA in assessing the fairness of test assembly processes is discussed.",,,Article,Final,,Scopus,2-s2.0-3142760069
Finch H.,"Finch, Holmes (12767650300)",12767650300,Comparison of the performance of varimax and promax rotations: Factor structure recovery for dichotomous items,2006,Journal of Educational Measurement,43,1,,39,52,13,117,10.1111/j.1745-3984.2006.00003.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645115335&doi=10.1111%2fj.1745-3984.2006.00003.x&partnerID=40&md5=29af090b5e6489d7727e38143c677cae,"Nonlinear factor analysis is a tool commonly used by measurement specialists to identify both the presence and nature of multidimensionality in a set of test items, an important issue given that standard Item Response Theory models assume a unidimensional latent structure. Results from most factor-analytic algorithms include loading matrices, which are used to link items with factors. Interpretation of the loadings typically occurs after they have been rotated in order to amplify the presence of simple structure. The purpose of this simulation study is to compare the ability of two commonly used methods of rotation, Varimax and Promax, in terms of their ability to correctly link items to factors and to identify the presence of simple structure. Results suggest that the two approaches are equally able to recover the underlying factor structure, regardless of the correlations among the factors, though the oblique method is better able to identify the presence of a ""simple structure."" These results suggest that for identifying which items are associated with which factors, either approach is effective, but that for identifying simple structure when it is present, the oblique method is preferable. © Copyright 2006 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-33645115335
Meyer J.P.; Setzer J.C.,"Meyer, J. Patrick (9278489400); Setzer, J. Carl (26039416900)",9278489400; 26039416900,A comparison of bridging methods in the analysis of NAEP trends with the new race subgroup definitions,2009,Journal of Educational Measurement,46,1,,104,128,24,2,10.1111/j.1745-3984.2009.01071.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349197545&doi=10.1111%2fj.1745-3984.2009.01071.x&partnerID=40&md5=dc752bfdbd9121fa930732393b19a311,"Recent changes to federal guidelines for the collection of data on race and ethnicity allow respondents to select multiple race categories. Redefining race subgroups in this manner poses problems for research spanning both sets of definitions. NAEP long-term trends have used the single-race subgroup definitions for over thirty years. Little is known about the effects of redefining race subgroups on these trends. Bridging methods for reconciling the single and multiple race definitions have been developed. These methods treat single-race subgroup membership as unknown or missing. A simulation study was conducted to determine the effectiveness of four bridging methods: multiple imputation logistic regression, multiple imputation probabilistic whole assignment, deterministic whole assignment - smallest group, and deterministic whole assignment - largest group. Only the first of these methods incorporates covariate information about examinees into the bridging procedure. The other three methods only use information contained in the race item response. The simulation took into account the percentage of biracial examinees and the missing data mechanism. Results indicated that the multiple imputation logistic regression was often the best performing method. Given that all K-12 and higher education institutions will be required to use the multiple-race definitions by 2009, implications for No Child Left Behind and other federally mandated reporting are discussed. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-61349197545
Gierl M.J.; Leighton J.P.; Tan X.,"Gierl, Mark J. (6701316189); Leighton, Jacqueline P. (7101772605); Tan, Xuan (14067968700)",6701316189; 7101772605; 14067968700,Evaluating DETECT classification accuracy and consistency when data display complex structure,2006,Journal of Educational Measurement,43,3,,265,289,24,8,10.1111/j.1745-3984.2006.00016.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747268078&doi=10.1111%2fj.1745-3984.2006.00016.x&partnerID=40&md5=3fd856eb9926f8a28deeb2f72c119788,"DETECT, the acronym for Dimensionality Evaluation To Enumerate Contributing Traits, is an innovative and relatively new nonparametric dimensionality assessment procedure used to identify mutually exclusive, dimensionally homogeneous clusters of items using a genetic algorithm (Zhang & Stout, 1999). Because the clusters of items are mutually exclusive, this procedure is most useful when the data display approximate simple structure. In many testing situations, however, data display a complex multidimensional structure. The purpose of the current study was to evaluate DETECT item classification accuracy and consistency when the data display different degrees of complex structure using both simulated and real data. Three variables were manipulated in the simulation study: The percentage of items displaying complex structure (10%, 30%, and 50%), the correlation between dimensions (.00, .30, .60, .75, and .90), and the sample size (500, 1,000, and 1,500). The results from the simulation study reveal that DETECT can accurately and consistently cluster items according to their true underlying dimension when as many as 30% of the items display complex structure, if the correlation between dimensions is less than or equal to .75 and the sample size is at least 1,000 examinees. If 50% of the items display complex structure, then the correlation between dimensions should be less than or equal to .60 and the sample size be, at least, 1,000 examinees. When the correlation between dimensions is .90, DETECT does not work well with any complex dimensional structure or sample size. Implications for practice and directions for future research are discussed.",,,Article,Final,,Scopus,2-s2.0-33747268078
Prowker A.; Camilli G.,"Prowker, Adam (16317319100); Camilli, Gregory (7003383989)",16317319100; 7003383989,Looking beyond the overall scores of NAEP assessments: Applications of generalized linear mixed modeling for exploring value-added item difficulty effects,2007,Journal of Educational Measurement,44,1,,69,87,18,4,10.1111/j.1745-3984.2007.00027.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249004367&doi=10.1111%2fj.1745-3984.2007.00027.x&partnerID=40&md5=18b449f6f6c4336d05ae220365326f0f,"The central idea of differential item functioning (DIF) is to examine differences between two groups at the item level while controlling for overall proficiency. This approach is useful for examining hypotheses at a finer-grain level than are permitted by a total test score. The methodology proposed in this paper is also aimed at estimating differences at the item rather than the overall score level, yet with the innovation where item-level differences for many groups simultaneously are the focus. This is a straightforward generalization of DIF as variance rather than one or several group differences; conceptually, this can be referred to as item difficulty variation (IDV). When instruction is of interest, and ""groups"" is a unit at which instruction is determined or delivered, then IDV signals value-added effects that can be influenced by either demographic or instructional variables. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34249004367
Monahan P.O.; Lee W.-C.; Ankenmann R.D.,"Monahan, Patrick O. (35600384300); Lee, Won-Chan (57203094500); Ankenmann, Robert D. (6603355304)",35600384300; 57203094500; 6603355304,Generating dichotomous item scores with the four-parameter beta compound binomial model,2007,Journal of Educational Measurement,44,3,,211,225,14,2,10.1111/j.1745-3984.2007.00035.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547971664&doi=10.1111%2fj.1745-3984.2007.00035.x&partnerID=40&md5=aa01486d65c0013e932e760e03af2bf8,"A Monte Carlo simulation technique for generating dichotomous item scores is presented that implements (a) a psychometric model with different explicit assumptions than traditional parametric item response theory (IRT) models, and (b) item characteristic curves without restrictive assumptions concerning mathematical form. The four-parameter beta compound-binomial (4PBCB) strong true score model (with two-term approximation to the compound binomial) is used to estimate and generate the true score distribution. The nonparametric item-true score step functions are estimated by classical item difficulties conditional on proportion-correct total score. The technique performed very well in replicating inter-item correlations, item statistics (point-biserial correlation coefficients and item proportion-correct difficulties), first four moments of total score distribution, and coefficient alpha of three real data sets consisting of educational achievement test scores. The technique replicated real data (including subsamples of differing proficiency) as well as the three-parameter logistic (3PL) IRT model (and much better than the 1PL model) and is therefore a promising alternative simulation technique. This 4PBCB technique may be particularly useful as a more neutral simulation procedure for comparing methods that use different IRT models. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34547971664
Lockwood J.R.; McCaffrey D.F.; Hamilton L.S.; Stecher B.; Le V.-N.; Martinez J.F.,"Lockwood, J.R. (7102841578); McCaffrey, Daniel F. (7005049755); Hamilton, Laura S. (7101728700); Stecher, Brian (7004000864); Le, Vi-Nhuan (7006849811); Martinez, José Felipe (22958122100)",7102841578; 7005049755; 7101728700; 7004000864; 7006849811; 22958122100,The sensitivity of value-added teacher effect estimates to different mathematics achievement measures,2007,Journal of Educational Measurement,44,1,,47,67,20,102,10.1111/j.1745-3984.2007.00026.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249084724&doi=10.1111%2fj.1745-3984.2007.00026.x&partnerID=40&md5=74d4fc5b89889cfb3b0052673272514b,"Using longitudinal data from a cohort of middle school students from a large school district, we estimate separate ""value-added"" teacher effects for two subscales of a mathematics assessment under a variety of statistical models varying in form and degree of control for student background characteristics. We find that the variation in estimated effects resulting from the different mathematics achievement measures is large relative to variation resulting from choices about model specification, and that the variation within teachers across achievement measures is larger than the variation across teachers. These results suggest that conclusions about individual teachers' performance based on value-added models can be sensitive to the ways in which student achievement is measured. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34249084724
Wainer H.; Wang X.A.; Skorupski W.P.; Bradlow E.T.,"Wainer, Howard (7006218234); Wang, X.A. (57219369196); Skorupski, William P. (8661013100); Bradlow, Eric T. (7004487843)",7006218234; 57219369196; 8661013100; 7004487843,A Bayesian method for evaluating passing scores: The PPoP curve,2005,Journal of Educational Measurement,42,3,,271,281,10,13,10.1111/j.1745-3984.2005.00014.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644458281&doi=10.1111%2fj.1745-3984.2005.00014.x&partnerID=40&md5=ff0b73244fd74c03373b91eac8fba04d,"In this note, we demonstrate an interesting use of the posterior distributions (and corresponding posterior samples of proficiency) that are yielded by fitting a fully Bayesian test scoring model to a complex assessment. Specifically, we examine the efficacy of the test in combination with the specific passing score that was chosen through expert judgment, or, in general, any external a priori criterion. In addition, we study the robustness of the test's efficacy with respect to choice of the passing score.",,,Article,Final,,Scopus,2-s2.0-24644458281
Van Der Linden W.J.,"Van Der Linden, Wim J. (55409657500)",55409657500,A comparison of item-selection methods for adaptive tests with content constraints,2005,Journal of Educational Measurement,42,3,,283,302,19,28,10.1111/j.1745-3984.2005.00015.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644498561&doi=10.1111%2fj.1745-3984.2005.00015.x&partnerID=40&md5=6f7385f33f12173d1480709a7266c1d3,"In test assembly, a fundamental difference exists between algorithms that select a test sequentially or simultaneously. Sequential assembly allows us to optimize an objective function at the examinee's ability estimate, such as the test information function in computerized adaptive testing. But it leads to the non-trivial problem of how to realize a set of content constraints on the test - a problem more naturally solved by a simultaneous item-selection method. Three main item-selection methods in adaptive testing offer solutions to this dilemma. The spiraling method moves item selection across categories of items in the pool proportionally to the numbers needed from them. Item selection by the weighted-deviations method (WDM) and the shadow test approach (STA) is based on projections of the future consequences of selecting an item. These two methods differ in that the former calculates a projection of a weighted sum of the attributes of the eventual test and the latter a projection of the test itself. The pros and cons of these methods are analyzed. An empirical comparison between the WDM and STA was conducted for an adaptive version of the Law School Admission Test (LSAT), which showed equally good item-exposure rates but violations of some of the constraints and larger bias and inaccuracy of the ability estimator for the WDM.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-24644498561
Henson R.; Templin J.; Douglas J.,"Henson, Robert (14622483200); Templin, Jonathan (23010693000); Douglas, Jeffrey (7403213236)",14622483200; 23010693000; 7403213236,Using efficient model based sum-scores for conducting skills diagnoses,2007,Journal of Educational Measurement,44,4,,361,376,15,29,10.1111/j.1745-3984.2007.00044.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36349014314&doi=10.1111%2fj.1745-3984.2007.00044.x&partnerID=40&md5=fbfd48aeff812be7c48cf96a073cd2b7,"Consider test data, a specified set of dichotomous skills measured by the test, and an IRT cognitive diagnosis model (ICDM). Statistical estimation of the data set using the ICDM can provide examinee estimates of mastery for these skills, referred to generally as attributes. With such detailed information about each examinee, future instruction can be tailored specifically for each student, often referred to as formative assessment. However, use of such cognitive diagnosis models to estimate skills in classrooms can require computationally intensive and complicated statistical estimation algorithms, which can diminish the breadth of applications of attribute level diagnosis. We explore the use of sum-scores (each attribute measured by a sum-score) combined with estimated model-based sum-score mastery/nonmastery cutoffs as an easy-to-use and intuitive method to estimate attribute mastery in classrooms and other settings where simple skills diagnostic approaches are desirable. Using a simulation study of skills diagnosis test settings and assuming a test consisting of a model-based calibrated set of items, correct classification rates (CCRs) are compared among four model-based approaches for estimating attribute mastery, namely using full model-based estimation and three different methods of computing sum-scores (simple sum-scores, complex sum-scores, and weighted complex sum-scores) combined with model-based mastery sum-score cutoffs. In summary, the results suggest that model-based sum-scores and mastery cutoffs can be used to estimate examinee attribute mastery with only moderate reductions in CCRs in comparison with the full model-based estimation approach. Certain topics are mentioned that are currently being investigated, especially applications in classroom and textbook settings. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-36349014314
Gorin J.S.,"Gorin, Joanna S. (9249493900)",9249493900,Manipulating processing difficulty of reading comprehension questions: The feasibility of verbal item generation,2005,Journal of Educational Measurement,42,4,,351,373,22,49,10.1111/j.1745-3984.2005.00020.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30344484274&doi=10.1111%2fj.1745-3984.2005.00020.x&partnerID=40&md5=f34b9776233329222ea4faaff9514300,"Based on a previously validated cognitive processing model of reading comprehension, this study experimentally examines potential generative components of text-based multiple-choice reading comprehension test questions. Previous research (Embretson & Wetzel, 1987; Gorin & Embretson, 2005; Sheehan & Ginther, 2001) shows text encoding and decision processes account for significant proportions of variance in item difficulties. In the current study, Linear Logistic Latent Trait Model (LLTM; Fischer, 1973) parameter estimates of experimentally manipulated items are examined to further verify the impact of encoding and decision processes on item difficulty. Results show that manipulation of some passage features, such as increased use of negative wording, significantly increases item difficulty in some cases, whereas others, such as altering the order of information presentation in a passage, did not significantly affect item difficulty, but did affect reaction time. These results suggest that reliable changes in difficulty and response time through algorithmic manipulation of certain task features is feasible. However, non-significant results for several manipulations highlight potential challenges to item generation in establishing direct links between theoretically relevant item features and individual item processing. Further examination of these relationships will be informative to item writers as well as test developers interested in the feasibility of item generation as an assessment tool.",,,Article,Final,,Scopus,2-s2.0-30344484274
Lamprianou I.; Boyle B.,"Lamprianou, Iasonas (7801460815); Boyle, Bill (12239706400)",7801460815; 12239706400,Accuracy of measurement in the context of mathematics National Curriculum tests in England for ethnic minority pupils and pupils who speak English as an additional language,2004,Journal of Educational Measurement,41,3,,239,259,20,19,10.1111/j.1745-3984.2004.tb01164.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6344289311&doi=10.1111%2fj.1745-3984.2004.tb01164.x&partnerID=40&md5=dde4b74f9915cc831907b938170116ab,"Research has suggested that inappropriate or misfitting response patterns may have detrimental effects on the quality and validity of measurement. It has been suggested that factors like language and ethnic background are related to the generation of misfitting response patterns, but the empirical research on this is rather poor. This research analyzes data from three testing cycles of the National Curriculum tests in mathematics in England using the Rasch model. It was found that pupils having English as an additional language and pupils belonging to ethnic minorities are significantly more likely to generate aberrant response patterns. However, within the groups of pupils belonging to ethnic minorities, those who speak English as an additional language are not significantly more likely to generate misfitting response patterns. This may indicate that the ethnic background effect is more significant than the effect of the first language spoken. The results suggest that pupils having English as an additional language and pupils belonging to ethnic minorities are mismeasured significantly more than the remainder of pupils by taking the mathematics National Curriculum tests. More research is needed to generalize the results to other subjects and contexts.",,,Article,Final,,Scopus,2-s2.0-6344289311
Yao L.; Boughton K.,"Yao, Lihua (24460828900); Boughton, Keith (6602432238)",24460828900; 6602432238,Multidimensional linking for tests with mixed item types,2009,Journal of Educational Measurement,46,2,,177,197,20,15,10.1111/j.1745-3984.2009.00076.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749121294&doi=10.1111%2fj.1745-3984.2009.00076.x&partnerID=40&md5=1cb245d4a09473a0c1dd6a429bbf77da,"Numerous assessments contain a mixture of multiple choice (MC) and constructed response (CR) item types and many have been found to measure more than one trait. Thus, there is a need for multidimensional dichotomous and polytomous item response theory (IRT) modeling solutions, including multidimensional linking software. For example, multidimensional item response theory (MIRT) may have a promising future in subscale score proficiency estimation, leading toward a more diagnostic orientation, which requires the linking of these subscale scores across different forms and populations. Several multidimensional linking studies can be found in the literature; however, none have used a combination of MC and CR item types. Thus, this research explores multidimensional linking accuracy for tests composed of both MC and CR items using a matching test characteristic/response function approach. The two-dimensional simulation study presented here used real data-derived parameters from a large-scale statewide assessment with two subscale scores for diagnostic profiling purposes, under varying conditions of anchor set lengths (6, 8, 16, 32, 60), across 10 population distributions, with a mixture of simple versus complex structured items, using a sample size of 3,000. It was found that for a well chosen anchor set, the parameters recovered well after equating across all populations, even for anchor sets composed of as few as six items. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-66749121294
Gierl M.J.; Cui Y.; Zhou J.,"Gierl, Mark J. (6701316189); Cui, Ying (35208098100); Zhou, Jiawen (23571309100)",6701316189; 35208098100; 23571309100,Reliability and attribute-based scoring in cognitive diagnostic assessment,2009,Journal of Educational Measurement,46,3,,293,313,20,27,10.1111/j.1745-3984.2009.00082.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749107348&doi=10.1111%2fj.1745-3984.2009.00082.x&partnerID=40&md5=4e6cd2ca992c373f3e13708c9ad6a9fc,"The attribute hierarchy method (AHM) is a psychometric procedure for classifying examinees' test item responses into a set of structured attribute patterns associated with different components from a cognitive model of task performance. Results from an AHM analysis yield information on examinees' cognitive strengths and weaknesses. Hence, the AHM can be used for cognitive diagnostic assessment. The purpose of this study is to introduce and evaluate a new concept for assessing attribute reliability using the ratio of true score variance to observed score variance on items that probe specific cognitive attributes. This reliability procedure is evaluated and illustrated using both simulated data and student response data from a sample of algebra items taken from the March 2005 administration of the SAT. The reliability of diagnostic scores and the implications for practice are also discussed. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-69749107348
Betebenner D.W.; Shang Y.; Xiang Y.; Zhao Y.; Yue X.,"Betebenner, Damian W. (6602136214); Shang, Yi (36721772300); Xiang, Yun (36830174200); Zhao, Yan (24282144100); Yue, Xiaohui (24281971800)",6602136214; 36721772300; 36830174200; 24282144100; 24281971800,The impact of performance level misclassification on the accuracy and precision of percent at performance level measures,2008,Journal of Educational Measurement,45,2,,119,137,18,11,10.1111/j.1745-3984.2007.00056.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949130768&doi=10.1111%2fj.1745-3984.2007.00056.x&partnerID=40&md5=98f65b2c1264ad0bba2694948d5ebaf4,"No Child Left Behind (NCLB) performance mandates, embedded within state accountability systems, focus school AYP (adequate yearly progress) compliance squarely on the percentage of students at or above proficient. The singular importance of this quantity for decision-making purposes has initiated extensive research into percent proficient as a measure of school quality. In particular, technical discussions have scrutinized the impact of sampling, measurement, and other sources of error on percent proficient statistics. In this article, we challenge the received orthodoxy that measurement error associated with individual students' scores is inconsequential for aggregate percent proficient statistics. Synthesizing current classification accuracy research with techniques from randomized response designs, we establish results which specify the extent to which measurement error - manifest as performance level misclassifications - produces bias and increases error variability for percent at performance level statistics. The results have direct relevance for the design of coherent and fair accountability systems based upon assessment outcomes. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-43949130768
Muckle T.J.; Karabatsos G.,"Muckle, Timothy J. (26644402100); Karabatsos, George (6603507883)",26644402100; 6603507883,Hierarchical generalized linear models for the analysis of judge ratings,2009,Journal of Educational Measurement,46,2,,198,219,21,14,10.1111/j.1745-3984.2009.00078.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749116633&doi=10.1111%2fj.1745-3984.2009.00078.x&partnerID=40&md5=65110b5cc161551c30ec34228a531e4a,"It is known that the Rasch model is a special two-level hierarchical generalized linear model (HGLM). This article demonstrates that the many-faceted Rasch model (MFRM) is also a special case of the two-level HGLM, with a random intercept representing examinee ability on a test, and fixed effects for the test items, judges, and possibly other facets. This perspective suggests useful modeling extensions of the MFRM. For example, in the HGLM framework it is possible to model random effects for items and judges in order to assess their stability across examinees. The MFRM can also be extended so that item difficulty and judge severity are modeled as functions of examinee characteristics (covariates), for the purposes of detecting differential item functioning and differential rater functioning. Practical illustrations of the HGLM are presented through the analysis of simulated and real judge-mediated data sets involving ordinal responses. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-66749116633
Li Y.H.; Lissitz R.W.,"Li, Yuan H. (8724941600); Lissitz, Robert W. (6602902644)",8724941600; 6602902644,Applications of the analytically derived asymptotic standard errors of item response theory item parameter estimates,2004,Journal of Educational Measurement,41,2,,85,117,32,16,10.1111/j.1745-3984.2004.tb01109.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544331650&doi=10.1111%2fj.1745-3984.2004.tb01109.x&partnerID=40&md5=cb4de351639ec0030ffe1aaa73979279,"The analytically derived asymptotic standard errors (SEs) of maximum likelihood (ML) item estimates can be approximated by a mathematical function without examinees' responses to test items, and the empirically determined SEs of marginal maximum likelihood estimation (MMLE)/Bayesian item estimates can be obtained when the same set of items is repeatedly estimated from the simulation (or resampling) test data. The latter method will result in rather stable and accurate SE estimates as the number of replications increases, but requires cumbersome and time-consuming calculations. Instead of using the empirically determined method, the adequacy of using the analytical-based method in predicting the SEs for item parameter estimates was examined by comparing results produced from both approaches. The results indicated that the SEs yielded from both approaches were, in most cases, very similar, especially when they were applied to a generalized partial credit model. This finding encourages test practitioners and researchers to apply the analytically asymptotic SEs of item estimates to the context of item-linking studies, as well as to the method of quantifying the SEs of equating scores for the item response theory (IRT) true-score method. Three-dimensional graphical presentation for the analytical SEs of item estimates as the bivariate function of item difficulty together with item discrimination was also provided for a better understanding of several frequently used IRT models.",,,Article,Final,,Scopus,2-s2.0-4544331650
Armstrong R.D.; Shi M.,"Armstrong, Ronald D. (7401712476); Shi, Min (35190979500)",7401712476; 35190979500,Model-free CUSUM methods for person fit,2009,Journal of Educational Measurement,46,4,,408,428,20,12,10.1111/j.1745-3984.2009.00090.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549160340&doi=10.1111%2fj.1745-3984.2009.00090.x&partnerID=40&md5=e24d85571bcc80e2fd6e2cac9f3d2844,This article demonstrates the use of a new class of model-free cumulative sum (CUSUM) statistics to detect person fit given the responses to a linear test. The fundamental statistic being accumulated is the likelihood ratio of two probabilities. The detection performance of this CUSUM scheme is compared to other model-free person-fit statistics found in the literature as well as an adaptation of another CUSUM approach. The study used both simulated responses and real response data from a large-scale standardized admission test. © 2009 by the National Council on Measurement in Education.,,,Article,Final,,Scopus,2-s2.0-71549160340
Almond R.G.; DiBello L.V.; Moulder B.; Zapata-Rivera J.-D.,"Almond, Russell G. (7003588488); DiBello, Louis V. (23008058600); Moulder, Brad (6506077346); Zapata-Rivera, Juan-Diego (57225684096)",7003588488; 23008058600; 6506077346; 57225684096,Modeling diagnostic assessments with bayesian networks,2007,Journal of Educational Measurement,44,4,,341,359,18,55,10.1111/j.1745-3984.2007.00043.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348987259&doi=10.1111%2fj.1745-3984.2007.00043.x&partnerID=40&md5=2f5a1148c7056f6ad037cb9cd0fa247f,"This paper defines Bayesian network models and examines their applications to IRT-based cognitive diagnostic modeling. These models are especially suited to building inference engines designed to be synchronous with the finer grained student models that arise in skills diagnostic assessment. Aspects of the theory and use of Bayesian network models are reviewed, as they affect applications to diagnostic assessment. The paper discusses how Bayesian network models are set up with expert information, improved and calibrated from data, and deployed as evidence-based inference engines. Aimed at a general educational measurement audience, the paper illustrates the flexibility and capabilities of Bayesian networks through a series of concrete examples, and without extensive technical detail. Examples are provided of proficiency spaces with direct dependencies among proficiency nodes, and of customized evidence models for complex tasks. This paper is intended to motivate educational measurement practitioners to learn more about Bayesian networks from the research literature, to acquire readily available Bayesian network software, to perform studies with real and simulated data sets, and to look for opportunities in educational settings that may benefit from diagnostic assessment fueled by Bayesian network modeling. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-36348987259
Randall J.; Engelhard Jr. G.,"Randall, Jennifer (23996208700); Engelhard Jr., George (7003970969)",23996208700; 7003970969,Examining teacher grades using rasch measurement theory,2009,Journal of Educational Measurement,46,1,,1,18,17,29,10.1111/j.1745-3984.2009.01066.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349153033&doi=10.1111%2fj.1745-3984.2009.01066.x&partnerID=40&md5=2c48a071a0db547834b888e5dd0cb4e6,"In this study, we present an approach to questionnaire design within educational research based on Guttman's mapping sentences and Many-Facet Rasch Measurement Theory. We designed a 54-item questionnaire using Guttman's mapping sentences to examine the grading practices of teachers. Each item in the questionnaire represented a unique student scenario that was graded by teachers. Three focus groups of elementary (N = 5), middle (N = 4), and high school (N = 2) teachers examined the scenarios for clarity, comprehensiveness, and ease of understanding. Based on the suggestions of the focus groups, the revised questionnaires were completed by 516 public school teachers located in a major metropolitan county in the Southeast. The grades assigned by the teachers to the scenarios were analyzed using the FACETS computer program. The results of the analyses suggest that teachers primarily assign grades on the basis of student achievement as expected, although for some teachers other facets (ability, behavior, and effort) may play a role in final grade assignment. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-61349153033
De La Torre J.,"De La Torre, Jimmy (22940257400)",22940257400,An empirically based method of Q-matrix validation for the DINA model: Development and applications,2008,Journal of Educational Measurement,45,4,,343,362,19,202,10.1111/j.1745-3984.2008.00069.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049152984&doi=10.1111%2fj.1745-3984.2008.00069.x&partnerID=40&md5=8c119ffcd117993d21df1e0ce6997cc6,"Most model fit analyses in cognitive diagnosis assume that a Q matrix is correct after it has been constructed, without verifying its appropriateness. Consequently, any model misfit attributable to the Q matrix cannot be addressed and remedied. To address this concern, this paper proposes an empirically based method of validating a Q matrix used in conjunction with the DINA model. The proposed method can be implemented with other considerations such as substantive information about the items, or expert knowledge about the domain, to produce a more integrative framework of Q-matrix validation. The paper presents the theoretical foundation for the proposed method, develops an algorithm for its practical implementation, and provides real and simulated data applications to examine its viability. Relevant issues regarding the implementation of the method are discussed. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-57049152984
De La Torre J.; Karelitz T.M.,"De La Torre, Jimmy (22940257400); Karelitz, Tzur M. (6508018994)",22940257400; 6508018994,Impact of diagnosticity on the adequacy of models for cognitive diagnosis under a linear attribute structure: A simulation study,2009,Journal of Educational Measurement,46,4,,450,469,19,18,10.1111/j.1745-3984.2009.00092.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549122894&doi=10.1111%2fj.1745-3984.2009.00092.x&partnerID=40&md5=e4aa5b835c03e2ce015f2c2b8056b4be,"Compared to unidimensional item response models (IRMs), cognitive diagnostic models (CDMs) based on latent classes represent examinees' knowledge and item requirements using discrete structures. This study systematically examines the viability of retrofitting CDMs to IRM-based data with a linear attribute structure. The study utilizes a procedure to make the IRM and CDM frameworks comparable and investigates how estimation accuracy is affected by test diagnosticity and the match between the true and fitted models. The study shows that comparable results can be obtained when highly diagnostic IRM data are retrofitted with CDM, and vice versa, retrofitting CDMs to IRM-based data in some conditions can result in considerable examinee misclassification, and model fit indices provide limited indication of the accuracy of item parameter estimation and attribute classification. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-71549122894
Li Y.H.; Schafer W.D.,"Li, Yuan H. (8724941600); Schafer, William D. (7202157839)",8724941600; 7202157839,Increasing the homogeneity of CAT's item-exposure rates by minimizing or maximizing varied target functions while assembling shadow tests,2005,Journal of Educational Measurement,42,3,,245,269,24,11,10.1111/j.1745-3984.2005.00013.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644481951&doi=10.1111%2fj.1745-3984.2005.00013.x&partnerID=40&md5=2fa0db7105ab849e4cfc636ad5139545,"A computerized adaptive testing (CAT) algorithm that has the potential to increase the homogeneity of CAT's item-exposure rates without significantly sacrificing the precision of ability estimates was proposed and assessed in the shadow-test (van der Linden & Reese, 1998) CAT context. This CAT algorithm was formed by a combination of maximizing or minimizing varied target functions while assembling shadow tests. There were four target functions to be separately used in the first, second, third, and fourth quarter test of CAT. The elements to be used in the four functions were associated with (a) a random number assigned to each item, (b) the absolute difference between an examinee's current ability estimate and an item difficulty, (c) the absolute difference between an examinee's current ability estimate and an optimum item difficulty, and (d) item information. The results indicated that this combined CAT fully utilized all the items in the pool, reduced the maximum exposure rates, and achieved more homogeneous exposure rates. Moreover, its precision in recovering ability estimates was similar to that of the maximum item-information method. The combined CAT method resulted in the best overall results compared with the other individual CAT item-selection methods. The findings from the combined CAT are encouraging. Future uses are discussed.",,,Article,Final,,Scopus,2-s2.0-24644481951
Penfield R.D.; Algina J.,"Penfield, Randall D. (6601923478); Algina, James (7003768166)",6601923478; 7003768166,A generalized DIF effect variance estimator for measuring unsigned differential test functioning in mixed format tests,2006,Journal of Educational Measurement,43,4,,295,312,17,57,10.1111/j.1745-3984.2006.00018.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845444462&doi=10.1111%2fj.1745-3984.2006.00018.x&partnerID=40&md5=f5c67cc015cdbf8dbf2dab93a781f695,"One approach to measuring unsigned differential test functioning is to estimate the variance of the differential item functioning (DIF) effect across the items of the test. This article proposes two estimators of the DIF effect variance for tests containing dichotomous and polytomous items. The proposed estimators are direct extensions of the noniterative estimators developed by Camilli and Penfield (1997) for tests composed of dichotomous items. A small simulation study is reported in which the statistical properties of the generalized variance estimators are assessed, and guidelines are proposed for interpreting values of DIF effect variance estimators.",,,Article,Final,,Scopus,2-s2.0-33845444462
Clauser B.E.; Mee J.; Baldwin S.G.; Margolis M.J.; Dillon G.F.,"Clauser, Brian E. (7003595460); Mee, Janet (23498092200); Baldwin, Su G. (35112775500); Margolis, Melissa J. (7101753505); Dillon, Gerard F. (57209265036)",7003595460; 23498092200; 35112775500; 7101753505; 57209265036,Judges' Use of Examinee Performance Data in an Angoff Standard-Setting Exercise for a Medical Licensing Examination: An Experimental Study,2009,Journal of Educational Measurement,46,4,,390,407,17,44,10.1111/j.1745-3984.2009.00089.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549121966&doi=10.1111%2fj.1745-3984.2009.00089.x&partnerID=40&md5=b24583de1b30f57e34b6efc54ca13bff,"Although the Angoff procedure is among the most widely used standard setting procedures for tests comprising multiple-choice items, research has shown that subject matter experts have considerable difficulty accurately making the required judgments in the absence of examinee performance data. Some authors have viewed the need to provide performance data as a fatal flaw for the procedure; others have considered it appropriate for experts to integrate performance data into their judgments but have been concerned that experts may rely too heavily on the data. There have, however, been relatively few studies examining how experts use the data. This article reports on two studies that examine how experts modify their judgments after reviewing data. In both studies, data for some items were accurate and data for other items had been manipulated. Judges in both studies substantially modified their judgments whether the data were accurate or not. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-71549121966
Zimmerman D.W.,"Zimmerman, Donald W. (7202940240)",7202940240,The reliability of difference scores in populations and samples,2009,Journal of Educational Measurement,46,1,,19,42,23,13,10.1111/j.1745-3984.2009.01067.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349143105&doi=10.1111%2fj.1745-3984.2009.01067.x&partnerID=40&md5=40e0f4c332040fe626c045514dfb6c0e,"This study was an investigation of the relation between the reliability of difference scores, considered as a parameter characterizing a population of examinees, and the reliability estimates obtained from random samples from the population. The parameters in familiar equations for the reliability of difference scores were redefined in such a way that determinants of reliability in both populations and samples become more transparent. Computer simulation was used to find sample values and to plot frequency distributions of various correlations and variance ratios relevant to the reliability of differences. The shape of frequency distributions resulting from the simulations and the means and standard deviations of these distributions reveal the extent to which reliability estimates based on sample data can be expected to meaningfully represent population reliability. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-61349143105
Roussos L.A.; Templin J.L.; Henson R.A.,"Roussos, Louis A. (6603805095); Templin, Jonathan L. (23010693000); Henson, Robert A. (14622483200)",6603805095; 23010693000; 14622483200,Skills diagnosis using IRT-based latent class models,2007,Journal of Educational Measurement,44,4,,293,311,18,56,10.1111/j.1745-3984.2007.00040.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348974087&doi=10.1111%2fj.1745-3984.2007.00040.x&partnerID=40&md5=40a76fba2339b20637d57c399332dd90,"This article describes a latent trait approach to skills diagnosis based on a particular variety of latent class models that employ item response functions (IRFs) as in typical item response theory (IRT) models. To enable and encourage comparisons with other approaches, this description is provided in terms of the main components of any psychometric approach: the ability model and the IRF structure; review of research on estimation, model checking, reliability, validity, equating, and scoring; and a brief review of real data applications. In this manner the article demonstrates that this approach to skills diagnosis has built a strong initial foundation of research and resources available to potential users. The outlook for future research and applications is discussed with special emphasis on a call for pilot studies and concomitant increased validity research. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-36348974087
Jang E.E.; Roussos L.,"Jang, Eunice Eunhee (16316110800); Roussos, Louis (6603805095)",16316110800; 6603805095,An investigation into the dimensionality of TOEFL using conditional covariance-based nonparametric approach,2007,Journal of Educational Measurement,44,1,,1,21,20,43,10.1111/j.1745-3984.2007.00024.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249096246&doi=10.1111%2fj.1745-3984.2007.00024.x&partnerID=40&md5=09c29a251b24a98d0b2bcf4d711fb59b,This article reports two studies to illustrate methodologies for conducting a conditional covariance-based nonparametric dimensionality assessment using data from two forms of the Test of English as a Foreign Language (TOEFL). Study 1 illustrates how to assess overall dimensionality of the TOEFL including all three subtests. Study 2 is aimed at illustrating how to conduct dimensionality analyses for a testlet-based test by focusing on the Reading Comprehension (RC) section in combination with item content analyses and hypothesis testing. The results of Study 1 indicated that both TOEFL forms involve two dominant dimensions corresponding to the Listening Comprehension section and the combination of the Reading Comprehension section and Structure and Written Expression section. The extensive RC analyses from Study 2 revealed strong evidence that a significant amount of the RC multidimensionality came from testlet effects. Confirmatory analyses coupled with exploratory cluster analyses and substantive item content analyses further identified dimensionality structure having to do with reading subskills. © 2007 by the National Council on Measurement in Education.,,,Article,Final,,Scopus,2-s2.0-34249096246
Liu J.; Low A.C.,"Liu, Jinghua (35210488600); Low, Albert C. (25723469000)",35210488600; 25723469000,A comparison of the kernel equating method with traditional equating methods using SAT® data,2008,Journal of Educational Measurement,45,4,,309,323,14,6,10.1111/j.1745-3984.2008.00067.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56949108872&doi=10.1111%2fj.1745-3984.2008.00067.x&partnerID=40&md5=174dd41277541f6932a6cd3896c4fd2d,"This study applied kernel equating (KE) in two scenarios: equating to a very similar population and equating to a very different population, referred to as a distant population, using SAT® data. The KE results were compared to the results obtained from analogous traditional equating methods in both scenarios. The results indicate that KE results are comparable to the results of other methods. Further, the results show that when the two populations taking the two tests are similar on the anchor score distributions, different equating methods yield the same or very similar results, even though they have different assumptions. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-56949108872
Yang W.-L.,"Yang, Wen-Ling (7407758617)",7407758617,Sensitivity of linkings between AP multiple-choice scores and composite scores to geographical region: An illustration of checking for population invariance,2004,Journal of Educational Measurement,41,1,,33,41,8,25,10.1111/j.1745-3984.2004.tb01157.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142730598&doi=10.1111%2fj.1745-3984.2004.tb01157.x&partnerID=40&md5=64f7776c352f3164837bea87dc636242,"Educational Testing Service This application study investigates whether the multiple-choice to composite linking functions that determine Advanced Placement Program exam grades remain invariant over subgroups defined by region. Three years of test data from an AP exam are used to study invariance across regions. The study focuses on two questions: (a) How invariant are grade thresholds across regions? and (b) Do the small sample sizes for some regional groups present particular problems for assessing thresholds invariance? The equatability index proposed by Dorans and Holland (2000) is employed to evaluate the invariance of the linking functions, and cross-classification is used to evaluate the invariance of the composite cut scores. Overall, the linkings across regions seem to hold up reasonably well Nevertheless, more exams need to be examined.",,,Article,Final,,Scopus,2-s2.0-3142730598
Korobko O.B.; Glas C.A.W.; Bosker R.J.; Luyten J.W.,"Korobko, Oksana B. (24281423000); Glas, Cees A. W. (6701681156); Bosker, Roel J. (6506252135); Luyten, Johan W. (58065950000)",24281423000; 6701681156; 6506252135; 58065950000,Comparing the difficulty of examination subjects with item response theory,2008,Journal of Educational Measurement,45,2,,139,157,18,26,10.1111/j.1745-3984.2007.00057.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949130148&doi=10.1111%2fj.1745-3984.2007.00057.x&partnerID=40&md5=3a7b1909e5cb142b7b138f4d164fb5de,"Methods are presented for comparing grades obtained in a situation where students can choose between different subjects. It must be expected that the comparison between the grades is complicated by the interaction between the students' pattern and level of proficiency on one hand, and the choice of the subjects on the other hand. Three methods based on item response theory (IRT) for the estimation of proficiency measures that are comparable over students and subjects are discussed: a method based on a model with a unidimensional representation of proficiency, a method based on a model with a multidimensional representation of proficiency, and a method based on a multidimensional representation of proficiency where the stochastic nature of the choice of examination subjects is explicitly modeled. The methods are compared using the data from the Central Examinations in Secondary Education in the Netherlands. The results show that the unidimensional IRT model produces unrealistic results, which do not appear when using the two multidimensional IRT models. Further, it is shown that both the multidimensional models produce acceptable model fit. However, the model that explicitly takes the choice process into account produces the best model fit. © 2008 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-43949130148
Kim J.-S.,"Kim, Jee-Seon (8849255200)",8849255200,Using the distractor categories of multiple-choice items to improve IRT linking,2006,Journal of Educational Measurement,43,3,,193,213,20,7,10.1111/j.1745-3984.2006.00013.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747308471&doi=10.1111%2fj.1745-3984.2006.00013.x&partnerID=40&md5=ee154790fb6ac3a7b12576297d30981e,"Simulation and real data studies are used to investigate the value of modeling multiple-choice distractors on item response theory linking. Using the characteristic curve linking procedure for Bock's (1972) nominal response model presented by Kim and Hanson (2002), all-category linking (i.e., a linking based on all category characteristic curves of the linking items) is compared against correct-only (CO) linking (i.e., linking based on the correct category characteristic curves only) using a common-item nonequivalent groups design. The CO linking is shown to represent an approximation to what occurs when using a traditional correct/incorrect item response model for linking. Results suggest that the number of linking items needed to achieve an equivalent level of linking precision declines substantially when incorporating the distractor categories.",,,Article,Final,,Scopus,2-s2.0-33747308471
Kim S.; Lee W.-C.,"Kim, Seonghoon (23060961000); Lee, Won-Chan (57203094500)",23060961000; 57203094500,An extension of four IRT linking methods for mixed-format tests,2006,Journal of Educational Measurement,43,1,,53,76,23,29,10.1111/j.1745-3984.2006.00004.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645126684&doi=10.1111%2fj.1745-3984.2006.00004.x&partnerID=40&md5=67c46e4025220385a49a4092a441fd75,"Under item response theory (IRT), linking proficiency scales from separate calibrations of multiple forms of a test to achieve a common scale is required in many applications. Four IRT linking methods including the mean/mean, mean/sigma, Haebara, and Stocking-Lord methods have been presented for use with single-format tests. This study extends the four linking methods to a mixture of unidimensional IRT models for mixed-format tests. Each linking method extended is intended to handle mixed-format tests using any mixture of the following five IRT models: the three-parameter logistic, graded response, generalized partial credit, nominal response (NR), and multiple-choice (MC) models. A simulation study is conducted to investigate the performance of the four linking methods extended to mixed-format tests. Overall, the Haebara and Stocking-Lord methods yield more accurate linking results than the mean/mean and mean/sigma methods. When the NR model or the MC model is used to analyze data from mixed-format tests, limitations of the mean/mean, mean/sigma, and Stocking-Lord methods are described. © Copyright 2006 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-33645126684
Kim S.; Von Davier A.A.; Haberman S.,"Kim, Sooyeon (56668293600); Von Davier, Alina A. (6506976799); Haberman, Shelby (14048312500)",56668293600; 6506976799; 14048312500,Small-sample equating using a synthetic linking function,2008,Journal of Educational Measurement,45,4,,325,342,17,25,10.1111/j.1745-3984.2008.00068.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049160452&doi=10.1111%2fj.1745-3984.2008.00068.x&partnerID=40&md5=01df4c5077838f460da285ab95af68c3,"This study addressed the sampling error and linking bias that occur with small samples in a nonequivalent groups anchor test design. We proposed a linking method called the synthetic function, which is a weighted average of the identity function and a traditional equating function (in this case, the chained linear equating function). Specifically, we compared the synthetic, identity, and chained linear functions for various-sized samples from two types of national assessments. One design used a highly reliable test and an external anchor, and the other used a relatively low-reliability test and an internal anchor. The results from each of these methods were compared to the criterion equating function derived from the total samples with respect to linking bias and error. The study indicated that the synthetic functions might be a better choice than the chained linear equating method when samples are not large and, as a result, unrepresentative. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-57049160452
Penfield R.D.,"Penfield, Randall D. (6601923478)",6601923478,An odds ratio approach for assessing differential distractor functioning effects under the nominal response model,2008,Journal of Educational Measurement,45,3,,247,269,22,23,10.1111/j.1745-3984.2008.00063.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849131862&doi=10.1111%2fj.1745-3984.2008.00063.x&partnerID=40&md5=0d4ccf1d3f6b742100c55b7b719b8a58,"Investigations of differential distractor functioning (DDF) can provide valuable information concerning the location and possible causes of measurement invariance within a multiple-choice item. In this article, I propose an odds ratio estimator of the DDF effect as modeled under the nominal response model. In addition, I propose a simultaneous distractor-level (SDL) test of invariance based on the results of the distractor-level tests of DDF. The results of a simulation study indicated that the DDF effect estimator maintained good statistical properties under a variety of conditions, and the SDL test displayed substantially higher power than the traditional Mantel-Haenszel test of no DIF when the DDF effect varied in magnitude and/or size across the distractors. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-50849131862
Finkelman M.; Nering M.L.; Roussos L.A.,"Finkelman, Matthew (24558635500); Nering, Michael L. (6602422746); Roussos, Louis A. (6603805095)",24558635500; 6602422746; 6603805095,A conditional exposure control method for multidimensional adaptive testing,2009,Journal of Educational Measurement,46,1,,84,103,19,21,10.1111/j.1745-3984.2009.01070.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349118689&doi=10.1111%2fj.1745-3984.2009.01070.x&partnerID=40&md5=1a6a67de772bc9b5292c489e22404d05,"In computerized adaptive testing (CAT), ensuring the security of test items is a crucial practical consideration. A common approach to reducing item theft is to define maximum item exposure rates, i.e., to limit the proportion of examinees to whom a given item can be administered. Numerous methods for controlling exposure rates have been proposed for tests employing the unidimensional 3-PL model. The present article explores the issues associated with controlling exposure rates when a multidimensional item response theory (MIRT) model is utilized and exposure rates must be controlled conditional upon ability. This situation is complicated by the exponentially increasing number of possible ability values in multiple dimensions. The article introduces a new procedure, called the generalized Stocking-Lewis method, that controls the exposure rate for students of comparable ability as well as with respect to the overall population. A realistic simulation set compares the new method with three other approaches: Kullback-Leibler information with no exposure control, Kullback-Leibler information with unconditional Sympson-Hetter exposure control, and random item selection. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-61349118689
Gierl M.J.; Bisanz J.; Bisanz G.L.; Boughton K.A.,"Gierl, Mark J. (6701316189); Bisanz, Jeffrey (6701791781); Bisanz, Gay L. (6603029928); Boughton, Keith A. (6602432238)",6701316189; 6701791781; 6603029928; 6602432238,Identifying Content and Cognitive Skills that Produce Gender Differences in Mathematics: A Demonstration of the Multidimensionality-Based DIF Analysis Paradigm,2003,Journal of Educational Measurement,40,4,,281,306,25,47,10.1111/j.1745-3984.2003.tb01148.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842764845&doi=10.1111%2fj.1745-3984.2003.tb01148.x&partnerID=40&md5=64b08aece45716adc175ec794aa4b33d,"Progress has been made in developing statistical methods for identifying DIF items, but procedures to aid with the substantive interpretations of these items have lagged behind. To overcome this problem, Roussos and Stout (1996) proposed a multidimensionality-based DIF analysis paradigm. We illustrate and evaluate an application of this framework as it applied to the study of gender differences in mathematics. Four characteristics distinguish this study from previous research: the substantive analysis was guided by past research on the content and cognitive-related sources of gender differences in mathematics achievement, as presented in the taxonomy by Gallagher, De Lisi, Holst, McGillicuddy-De Lisi, Morely, and Cahalan (2000); the substantive analysis was conducted by reviewers who were highly knowledgeable about the cognitive strategies students use to solve math problems; three statistical methods were used to test hypotheses about gender differences, including SIBTEST, DIMTEST, and multiple linear regression; and the data were from a curriculum-based achievement test developed with the goal of minimizing obvious, content-related gender differences. We show that the framework can lead to clearly interpretable results and we highlight both the strengths and weaknesses of applying the Roussos and Stout framework to the study of group differences.",,,Article,Final,,Scopus,2-s2.0-1842764845
Pommerich M.; Segall D.O.,"Pommerich, Mary (6602421150); Segall, Daniel O. (6603798099)",6602421150; 6603798099,Local dependence in an operational CAT: Diagnosis and implications,2008,Journal of Educational Measurement,45,3,,201,223,22,9,10.1111/j.1745-3984.2008.00061.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849140447&doi=10.1111%2fj.1745-3984.2008.00061.x&partnerID=40&md5=a767d760b24eb0a0e2a1239065f2b9d4,"The accuracy of CAT scores can be negatively affected by local dependence if the CAT utilizes parameters that are misspecified due to the presence of local dependence and/or fails to control for local dependence in responses during the administration stage. This article evaluates the existence and effect of local dependence in a test of Mathematics Knowledge. Diagnostic tools were first used to evaluate the existence of local dependence in items that were calibrated under a 3PL model. A simulation study was then used to evaluate the effect of local dependence on the precision of examinee CAT scores when the 3PL model was used for selection and scoring. The diagnostic evaluation showed strong evidence for local dependence. The simulation suggested that local dependence in parameters had a minimal effect on CAT score precision, while local dependence in responses had a substantial effect on score precision, depending on the degree of local dependence present. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-50849140447
Penfield R.D.,"Penfield, Randall D. (6601923478)",6601923478,Assessing differential step functioning in polytomous items using a common odds ratio estimator,2007,Journal of Educational Measurement,44,3,,187,210,23,40,10.1111/j.1745-3984.2007.00034.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547966148&doi=10.1111%2fj.1745-3984.2007.00034.x&partnerID=40&md5=ecb51a8e9066208de366ecbb15c5798a,"Many statistics used in the assessment of differential item functioning (DIF) in polytomous items yield a single item-level index of measurement invariance that collapses information across all response options of the polytomous item. Utilizing a single item-level index of DIF can, however, be misleading if the magnitude or direction of the DIF changes across the steps underlying the polytomous response process. A more comprehensive approach to examining measurement invariance in polytomous item formats is to examine invariance at the level of each step of the polytomous item, a framework described in this article as differential step functioning (DSF). This article proposes a nonparametric DSF estimator that is based on the Mantel-Haenszel common odds ratio estimator (Mantel & Haenszel, 1959), which is frequently implemented in the detection of DIF in dichotomous items. A simulation study demonstrated that when the level of DSF varied in magnitude or sign across the steps underlying the polytomous response options, the DSF-based approach typically provided a more powerful and accurate test of measurement invariance than did corresponding item-level DIF estimators. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34547966148
Finch H.; Habing B.,"Finch, Holmes (12767650300); Habing, Brian (6508232117)",12767650300; 6508232117,Comparison of NOHARM and DETECT in item cluster recovery: Counting dimensions and allocating items,2005,Journal of Educational Measurement,42,2,,149,169,20,15,10.1111/j.1745-3984.2005.00008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644458071&doi=10.1111%2fj.1745-3984.2005.00008&partnerID=40&md5=0f84e70e724d6049bc33a4353ce84416,"This study examines the performance of a new method for assessing and characterizing dimensionality in test data using the NOHARM model, and comparing it with DETECT. Dimensionality assessment is carried out using two goodness-of-fit statistics that are compared to reference x 2 distributions. A Monte Carlo study is used with item parameters based on a statewide basic skills assessment and the SAT. Other factors that are varied include the correlation among the latent traits, the number of items, the number of subjects, skewness of the latent traits, and the presence or absence of guessing. The performance of the two procedures is judged by the accuracy in determining the number of underlying dimensions, and the degree to which items are correctly clustered together. Results indicate that the new, NOHARM-based method appears to perform comparably to DETECT in terms of simultaneously finding the correct number of dimensions and clustering items correctly. NOHARM is generally better able to determine the number of underlying dimensions, but less able to group items together, than DETECT. When errors in item cluster assignment are made, DETECT is more likely to incorrectly separate items while NOHARM more often incorrectly groups them together.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-21644458071
Andries Van Der Ark L.; Emons W.H.M.; Sijtsma K.,"Andries Van Der Ark, L. (6602260003); Emons, Wilco H. M. (6603733674); Sijtsma, Klaas (7004185286)",6602260003; 6603733674; 7004185286,Detecting answer copying using alternate test forms and seat locations in small-scale examinations,2008,Journal of Educational Measurement,45,2,,99,117,18,7,10.1111/j.1745-3984.2007.00055.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949141784&doi=10.1111%2fj.1745-3984.2007.00055.x&partnerID=40&md5=b4d5c942bdafc68df00440a674b44489,"Two types of answer-copying statistics for detecting copiers in small-scale examinations are proposed. One statistic identifies the ""copier- source"" pair, and the other in addition suggests who is copier and who is source. Both types of statistics can be used when the examination has alternate test forms. A simulation study shows that the statistics do not depend on the total-test score. Another simulation study compares the statistics with two known statistics, and shows that they have substantial power. The new statistics are applied to data from a small-scale examination (N = 230) with two alternate test forms. Auxiliary information on the seat location of the examinees and the test scores of the examinees was used to determine whether or not examinees could be suspected. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-43949141784
Holland P.W.; Sinharay S.; Von Davier A.A.; Han N.,"Holland, Paul W. (7202792268); Sinharay, Sandip (6602980064); Von Davier, Alina A. (6506976799); Han, Ning (23972730600)",7202792268; 6602980064; 6506976799; 23972730600,An approach to evaluating the missing data assumptions of the chain and post-stratification equating methods for the NEAT design,2008,Journal of Educational Measurement,45,1,,17,43,26,17,10.1111/j.1745-3984.2007.00050.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549127201&doi=10.1111%2fj.1745-3984.2007.00050.x&partnerID=40&md5=8f0fdcdd415f4da17afba253cea5bd4e,"Two important types of observed score equating (OSE) methods for the non-equivalent groups with Anchor Test (NEAT) design are chain equating (CE) and post-stratification equating (PSE). CE and PSE reflect two distinctly different ways of using the information provided by the anchor test for computing OSE functions. Both types of methods include linear and nonlinear equating functions. In practical situations, it is known that the PSE and CE methods will give different results when the two groups of examinees differ on the anchor test. However, given that both types of methods are justified as OSE methods by making different assumptions about the missing data in the NEAT design, it is difficult to conclude which, if either, of the two is more correct in a particular situation. This study compares the predictions of the PSE and CE assumptions for the missing data using a special data set for which the usually missing data are available. Our results indicate that in an equating setting where the linking function is decidedly non-linear and CE and PSE ought to be different, both sets of predictions are quite similar but those for CE are slightly more accurate. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-40549127201
Zwick R.; Greif Green J.,"Zwick, Rebecca (7004200859); Greif Green, Jennifer (56750617000)",7004200859; 56750617000,"New perspectives on the correlation of SAT scores, high school grades, and socioeconomic factors",2007,Journal of Educational Measurement,44,1,,23,45,22,60,10.1111/j.1745-3984.2007.00025.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249103355&doi=10.1111%2fj.1745-3984.2007.00025.x&partnerID=40&md5=6c77f6ec386717988d3d154a986fecc1,"In studies of the SAT, correlations of SAT scores, high school grades, and socioeconomic factors (SES) are usually obtained using a university as the unit of analysis. This approach obscures an important structural aspect of the data: The high school grades received by a given institution come from a large number of high schools, all of which have potentially different grading standards. SAT scores, on the other hand, can be assumed to have the same meaning across high schools. Our analyses of a large national sample show that, when pooled within-high-school analyses are applied, high school grades and class rank have larger correlations with family income and education than is evident in the results of typical analyses, and SAT scores have smaller associations with socioeconomic factors. SAT scores and high school grades, therefore, have more similar associations with SES than they do when only the usual across-high-school correlations are considered. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34249103355
Bridgeman B.; Cline F.,"Bridgeman, Brent (7005526936); Cline, Frederick (6603743489)",7005526936; 6603743489,Effects of differentially time-consuming tests on computer-adaptive test scores,2004,Journal of Educational Measurement,41,2,,137,148,11,33,10.1111/j.1745-3984.2004.tb01111.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544309331&doi=10.1111%2fj.1745-3984.2004.tb01111.x&partnerID=40&md5=c4e25362f34ec64fc244d3571868f6bf,"Time limits on some computer-adaptive tests (CATs) are such that many examinees have difficulty finishing, and some examinees may be administered tests with more time-consuming items than others. Results from over 100,000 examinees suggested that about half of the examinees must guess on the final six questions of the analytical section of the Graduate Record Examination if they were to finish before time expires. At the higher-ability levels, even more guessing was required because the questions administered to higher-ability examinees were typically more time consuming. Because the scoring model is not designed to cope with extended strings of guesses, substantial errors in ability estimates can be introduced when CATs have strict time limits. Furthermore, examinees who are administered tests with a disproportionate number of time-consuming items appear to get lower scores than examinees of comparable ability who are administered tests containing items that can be answered more quickly, though the issue is very complex because of the relationship of time and difficulty, and the multidimensionality of the test.",,,Article,Final,,Scopus,2-s2.0-4544309331
Oshima T.C.; Raju N.S.; Nanda A.O.,"Oshima, T.C. (55672664200); Raju, Nambury S. (7007121164); Nanda, Alice O. (12445047100)",55672664200; 7007121164; 12445047100,A new method for assessing the statistical significance in the Differential Functioning of Items and Tests (DFIT) framework,2006,Journal of Educational Measurement,43,1,,1,17,16,42,10.1111/j.1745-3984.2006.00001.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645118836&doi=10.1111%2fj.1745-3984.2006.00001.x&partnerID=40&md5=26d4da9badcda5593f0ebcea25a96608,"A new item parameter replication method is proposed for assessing the statistical significance of the noncompensatory differential item functioning (NCDIF) index associated with the differential functioning of items and tests framework. In this new method, a cutoff score for each item is determined by obtaining a (1 -α) percentile rank score from a frequency distribution of NCDIF values under the no-DIF condition by generating a large number of item parameters based on the item parameter estimates and their variance-covariance structures from a computer program such as BIILOG-MG3. This cutoff for each item can be used as the basis for determining whether a given NCDIF index is significantly different from zero. This new method has definite advantages over the current method and yields cutoff values that are tailored to a particular data set and a particular item. A Monte Carlo assessment of this new method is presented and discussed. © Copyright 2006 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-33645118836
Wollack J.A.; Cohen A.S.; Wells C.S.,"Wollack, James A. (6701334965); Cohen, Allan S. (55465451100); Wells, Craig S. (24167495500)",6701334965; 55465451100; 24167495500,A Method for Maintaining Scale Stability in the Presence of Test Speededness,2003,Journal of Educational Measurement,40,4,,307,330,23,42,10.1111/j.1745-3984.2003.tb01149.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842815176&doi=10.1111%2fj.1745-3984.2003.tb01149.x&partnerID=40&md5=36c9935e09226f09f67c6a837a69d2ca,"Administering tests under time constraints may result in poorly estimated item parameters, particularly for items at the end of the test (Douglas, Kim, Habing, & Gao, 1998; Oshima, 1994). Bolt, Cohen, and Wollack (2002) developed an item response theory mixture model to identify a latent group of examinees for whom a test is overly speeded, and found that item parameter estimates for end-of-test items in the nonspeeded group were similar to estimates for those same items when administered earlier in the test. In this study, we used the Bolt et al. (2002) method to study the effect of removing speeded examinees on the stability of a score scale over an 11-year period. Results indicated that using only the nonspeeded examinees for equating and estimating item parameters provided a more unidimensional scale, smaller effects of item parameter drift (including fewer drifting items), and less scale drift (i.e., bias) and variability (i.e., root mean squared errors) when compared to the total group of examinees.",,,Article,Final,,Scopus,2-s2.0-1842815176
Lei P.-W.; Chen S.-Y.; Yu L.,"Lei, Pui-Wa (7005391935); Chen, Shu-Ying (8344121900); Yu, Lan (55488246300)",7005391935; 8344121900; 55488246300,Comparing methods of assessing differential item functioning in a computerized adaptive testing environment,2006,Journal of Educational Measurement,43,3,,245,264,19,14,10.1111/j.1745-3984.2006.00015.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747293913&doi=10.1111%2fj.1745-3984.2006.00015.x&partnerID=40&md5=1617439011fd6db1a4a184b3c05f3a52,"Mantel-Haenszel and SIBTEST, which have known difficulty in detecting non-unidirectional differential item functioning (DIF), have been adapted with some success for computerized adaptive testing (CAT). This study adapts logistic regression (LR) and the item-response-theory-likelihood-ratio test (IRT-LRT), capable of detecting both unidirectional and non-unidirectional DIP, to the CAT environment in which pretest items are assumed to be seeded in CATs but not used for trait estimation. The proposed adaptation methods were evaluated with simulated data under different sample size ratios and impact conditions in terms of Type I error, power, and specificity in identifying the form of DIF. The adapted LR and IRT-LRT procedures are more powerful than the CAT version of SIBTEST for non-unidirectional DIF detection. The good Type I error control provided by IRT-LRT under extremely unequal sample sizes and large impact is encouraging. Implications of these and other findings are discussed.",,,Article,Final,,Scopus,2-s2.0-33747293913
Briggs D.C.; Wilson M.,"Briggs, Derek C. (7202334195); Wilson, Mark (55547134983)",7202334195; 55547134983,Generalizability in item response modeling,2007,Journal of Educational Measurement,44,2,,131,155,24,45,10.1111/j.1745-3984.2007.00031.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247880236&doi=10.1111%2fj.1745-3984.2007.00031.x&partnerID=40&md5=e2c19e8093882b325cbb8b712d0911a7,"An approach called generalizability in item response modeling (GIRM) is introduced in this article. The GIRM approach essentially incorporates the sampling model of generalizability theory (GT) into the scaling model of item response theory (IRT) by making distributional assumptions about the relevant measurement facets. By specifying a random effects measurement model, and taking advantage of the flexibility of Markov Chain Monte Carlo (MCMC) estimation methods, it becomes possible to estimate GT variance components simultaneously with traditional IRT parameters. It is shown how GT and IRT can be linked together, in the context of a single-facet measurement design with binary items. Using both simulated and empirical data with the software WinBUGS, the GIRM approach is shown to produce results comparable to those from a standard GT analysis, while also producing results from a random effects IRT model. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34247880236
Skaggs G.,"Skaggs, Gary (20436934200)",20436934200,Accuracy of random groups equating with very small samples,2005,Journal of Educational Measurement,42,4,,309,330,21,50,10.1111/j.1745-3984.2005.00018.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30344465785&doi=10.1111%2fj.1745-3984.2005.00018.x&partnerID=40&md5=d4b6be093ea1c05ac21be0406aefcef4,"This study investigated the effectiveness of equating with very small samples using the random groups design. Of particular interest was equating accuracy at specific scores where performance standards might be set. Two sets of simulations were carried out, one in which the two forms were identical and one in which they differed by a tenth of a standard deviation in overall difficulty. These forms were equated using mean equating, linear equating, unsmoothed equipercentile equating, and equipercentile equating using two through six moments of log-linear presmoothing with samples of 25, 50, 75, 100, 150, and 200. The results indicated that identity equating was preferable to any equating method when samples were as small as 25. For samples of 50 and above, the choice of an equating method over identity equating depended on the location of the passing score relative to examinee performance. If passing scores were located below the mean, where data were sparser, mean equating produced the smallest percentage of misclassified examinees. For passing scores near the mean, all methods produced similar results with linear equating being the most accurate. For passing scores above the mean, equipercentile equating with 2-and 3-moment presmoothing were the best equating methods. Higher levels of presmoothing did not improve the results.",,,Article,Final,,Scopus,2-s2.0-30344465785
Schulz E.M.; Betebenner D.; Ahn M.,"Schulz, E. Matthew (57213119158); Betebenner, Damian (6602136214); Ahn, Meeyeon (7103352157)",57213119158; 6602136214; 7103352157,Hierarchical logistic regression in course placement,2004,Journal of Educational Measurement,41,3,,271,286,15,4,10.1111/j.1745-3984.2004.tb01166.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6344270066&doi=10.1111%2fj.1745-3984.2004.tb01166.x&partnerID=40&md5=2d4989cc395370f97de6a20ff9b9f873,"Whether hierarchical logistic regression can reduce the sample size requirement for estimating optimal cutoff scores in a course placement service where predictive validity is measured by a threshold utility function is explored. Data from courses with varying class size were randomly partitioned into two halves per course. Non-hierarchical and hierarchical analyses were performed on each half. Compared to their nonhierarchical counterparts, hierarchically estimated cutoff scores from different halves were more stable and predicted course outcomes in the other half more accurately. These differences were mostpronounced with small samples. Sample size requirements for developing cutoff scores for course placement can be substantially reduced if hierarchical logistic regression is used.",,,Article,Final,,Scopus,2-s2.0-6344270066
Puhan G.; Moses T.P.; Grant M.C.; McHale F.,"Puhan, Gautam (12445769400); Moses, Timothy P. (12243516500); Grant, Mary C. (30267579700); McHale, Frederick (6603471703)",12445769400; 12243516500; 30267579700; 6603471703,Small-sample equating using a single-group nearly equivalent test (SiGNET) design,2009,Journal of Educational Measurement,46,3,,344,362,18,11,10.1111/j.1745-3984.2009.00085.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749086194&doi=10.1111%2fj.1745-3984.2009.00085.x&partnerID=40&md5=0743cf972e43dcfb5c3813545241a1eb,"A single-group (SG) equating with nearly equivalent test forms (SiGNET) design was developed by Grant to equate small-volume tests. Under this design, the scored items for the operational form are divided into testlets or mini tests. An additional testlet is created but not scored for the first form. If the scored testlets are testlets 1-6 and the unscored testlet is testlet 7, then the first form is composed of testlets 1-6 and the second form is composed of testlets 2-7. The seven testlets are administered as a single administered form, and when a sufficient number of examinees have taken the administered form, the second form (testlets 2-7) is equated to the first form (testlets 1-6) using an SG equating design. As evident, this design facilitates the use of an SG equating and allows for the accumulation of data, both of which may reduce equating error. This study compared equatings under the SiGNET and common-item equating designs and found lower equating error for the SiGNET design in very small sample size conditions (e.g., N = 10). © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-69749086194
Kupermintz H.,"Kupermintz, Haggai (6602984493)",6602984493,On the reliability of categorically scored examinations,2004,Journal of Educational Measurement,41,3,,193,204,11,2,10.1111/j.1745-3984.2004.tb01162.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6344270068&doi=10.1111%2fj.1745-3984.2004.tb01162.x&partnerID=40&md5=abe18ca2eb2f7ed986d136bac70ade4a,A decision-theoretic approach to the question of reliability in categorically scored examinations is explored. The concepts of true scores and errors are discussed as they deviate from conventional psychometric definitions and measurement error in categorical scores is cast in terms of misclassifications. A reliability measure based on proportional reduction in loss (PRL) is then presented and exemplified with data from a large-scale assessment. The link between the PRL approach and the classical conception of reliability is discussed. Some design considerations for reliability studies are also discussed.,,,Article,Final,,Scopus,2-s2.0-6344270068
Bolt D.M.; Gierl M.J.,"Bolt, Daniel M. (7006174434); Gierl, Mark J. (6701316189)",7006174434; 6701316189,Testing features of graphical DIF: Application of a regression correction to three nonparametric statistical tests,2006,Journal of Educational Measurement,43,4,,313,333,20,17,10.1111/j.1745-3984.2006.00019.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845466950&doi=10.1111%2fj.1745-3984.2006.00019.x&partnerID=40&md5=1413dbc0753a5cc41d86ca9663bca95e,"Inspection of differential item functioning (DIF) in translated test items can be informed by graphical comparisons of item response functions (IRFs) across translated forms. Due to the many forms of DIF that can emerge in such analyses, it is important to develop statistical tests that can confirm various characteristics of DIF when present. Traditional nonparametric tests of DIF (Mantel-Haenszel, SIBTEST) are not designed to test for the presence of nonuniform or local DIF, while common probability difference (P-DIF) tests (e.g., SIBTEST) do not optimize power in testing for uniform DIF, and thus may be less useful in the context of graphical DIF analyses. In this article, modifications of three alternative nonparametric statistical tests for DIF, Fisher's χ 2 test, Cochran's Z test, and Goodman's U test (Marascuilo & Slaughter, 1981), are investigated for these purposes. A simulation study demonstrates the effectiveness of a regression correction procedure in improving the statistical performance of the tests when using an internal test score as the matching criterion. Simulation power and real data analyses demonstrate the unique information provided by these alternative methods compared to SIBTEST and Mantel-Haenszel in confirming various forms of DIF in translated tests.",,,Article,Final,,Scopus,2-s2.0-33845466950
Moses T.; Yang W.-L.; Wilson C.,"Moses, Tim (12243516500); Yang, Wen-Ling (7407758617); Wilson, Christine (55477609500)",12243516500; 7407758617; 55477609500,Using kernel equating to assess item order effects on test scores,2007,Journal of Educational Measurement,44,2,,157,178,21,16,10.1111/j.1745-3984.2007.00032.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247884310&doi=10.1111%2fj.1745-3984.2007.00032.x&partnerID=40&md5=d0df2aad0fe839d0a26c33146f7d05ce,"This study explored the use of kernel equating for integrating and extending two procedures proposed for assessing item order effects in test forms that have been administered to randomly equivalent groups. When these procedures are used together, they can provide complementary information about the extent to which item order effects impact test scores, in overall score distributions and also at specific test scores. In addition to detecting item order effects, the integrated procedures also suggest the equating function that most adequately adjusts the scores to mitigate the effects. To demonstrate, the statistical equivalences of alternate versions of two large-volume advanced placement exams were assessed. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34247884310
Dodeen H.,"Dodeen, Hamzeh (6507994960)",6507994960,The relationship between item parameters and item fit,2004,Journal of Educational Measurement,41,3,,261,270,9,9,10.1111/j.1745-3984.2004.tb01165.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6344256833&doi=10.1111%2fj.1745-3984.2004.tb01165.x&partnerID=40&md5=148c25dde390d2e882b746f787f4e087,"The effect of item parameters (discrimination, difficulty, and level of guessing) on the item fit statistic was investigated using simulated dichotomous data. Nine tests were simulated using 1, 000 persons, 50 items, three levels of item discrimination, three levels of item difficulty, and three levels of guessing. The item fit was estimated using two fit statistics: the likelihood ratio statistic (ξB2), and the standardized residuals (SRs). All the item parameters were simulated to be normally distributed. Results showed that the levels of item discrimination and guessing affected the item-fit values. As the level of item discrimination or guessing increased, item-fit values increased and more items misfit the model. The level of item difficulty did not affect the item-fit statistic.",,,Article,Final,,Scopus,2-s2.0-6344256833
Wang W.-C.; Wilson M.; Shih C.-L.,"Wang, Wen-Chung (7501757876); Wilson, Mark (55547134983); Shih, Ching-Lin (36070940400)",7501757876; 55547134983; 36070940400,Modeling randomness in judging rating scales with a random-effects rating scale model,2006,Journal of Educational Measurement,43,4,,335,353,18,25,10.1111/j.1745-3984.2006.00020.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845383286&doi=10.1111%2fj.1745-3984.2006.00020.x&partnerID=40&md5=41d11267bf8a346e0e182b50f678798b,"This study presents the random-effects rating scale model (RE-RSM) which takes into account randomness in the thresholds over persons by treating them as random-effects and adding a random variable for each threshold in the rating scale model (RSM) (Andrich, 1978). The RE-RSM turns out to be a special case of the multidimensional random coefficients multinomial logit model (MRCMLM) (Adams, Wilson, & Wang, 1997) so that the estimation procedures for the MRCMLM can be directly applied. The results of the simulation indicated that when the data were generated from the RSM, using the RSM and the RE-RSM to fit the data made little difference: both resulting in accurate parameter recovery. When the data were generated from the RE-RSM, using the RE-RSM to fit the data resulted in unbiased estimates, whereas using the RSM resulted in biased estimates, large fit statistics for the thresholds, and inflated test reliability. An empirical example of 10 items with four-point rating scales was illustrated in which four models were compared: the RSM, the RE-RSM, the partial credit model (Masters, 1982), and the constrained random-effects partial credit model. In this real data set, the need for a random-effects formulation becomes clear.",,,Article,Final,,Scopus,2-s2.0-33845383286
Kang T.; Chen T.T.,"Kang, Taehoon (16480692700); Chen, Troy T. (25722919000)",16480692700; 25722919000,Performance of the generalized S-X2 item fit index for polytomous IRT models,2008,Journal of Educational Measurement,45,4,,391,406,15,123,10.1111/j.1745-3984.2008.00071.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049129930&doi=10.1111%2fj.1745-3984.2008.00071.x&partnerID=40&md5=faa8d8f1a06fb05fce453507a8a18479,"Orlando and Thissen's S-X2 item fit index has performed better than traditional item fit statistics such as Yen's Q1 and McKinley and Mill's G2 for dichotomous item response theory (IRT) models. This study extends the utility of S-X2 to polytomous IRT models, including the generalized partial credit model, partial credit model, and rating scale model. The performance of the generalized S-X2 in assessing item model fit was studied in terms of empirical Type I error rates and power and compared to G2. The results suggest that the generalized S-X 2 is promising for polytomous items in educational and psychological testing programs. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-57049129930
Sinharay S.; Holland P.W.,"Sinharay, Sandip (6602980064); Holland, Paul W. (7202792268)",6602980064; 7202792268,Is it necessary to make anchor tests mini-versions of the tests being equated or can some restrictions be relaxed?,2007,Journal of Educational Measurement,44,3,,249,275,26,43,10.1111/j.1745-3984.2007.00037.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547968456&doi=10.1111%2fj.1745-3984.2007.00037.x&partnerID=40&md5=fd3d26b5cf4fe896bed53b5f9fcd4abe,"It is a widely held belief that anchor tests should be miniature versions (i.e., minitests), with respect to content and statistical characteristics, of the tests being equated. This article examines the foundations for this belief regarding statistical characteristics. It examines the requirement of statistical representativeness of anchor tests that are content representative. The equating performance of several types of anchor tests, including those having statistical characteristics that differ from those of the tests being equated, is examined through several simulation studies and a real data example. Anchor tests with a spread of item difficulties less than that of a total test seem to perform as well as a minitest with respect to equating bias and equating standard error. Hence, the results demonstrate that requiring an anchor test to mimic the statistical characteristics of the total test may be too restrictive and need not be optimal. As a side benefit, this article also provides a comparison of the equating performance of post-stratification equating and chain equipercentile equating. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34547968456
Cui Z.; Kolen M.J.,"Cui, Zhongmin (24173237800); Kolen, Michael J. (6603925839)",24173237800; 6603925839,Evaluation of two new Smoothing methods in equating: The cubic B-spline presmoothing method and the direct presmoothing method,2009,Journal of Educational Measurement,46,2,,135,158,23,14,10.1111/j.1745-3984.2009.00074.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749165904&doi=10.1111%2fj.1745-3984.2009.00074.x&partnerID=40&md5=895bd9944e41c6d07cd5d1fd1b2b9cd9,"This article considers two new smoothing methods in equipercentile equating, the cubic B-spline presmoothing method and the direct presmoothing method. Using a simulation study, these two methods are compared with established methods, the beta-4 method, the polynomial loglinear method, and the cubic spline postsmoothing method, under three sample sizes (300, 1,000, and 3,000) and for three test content areas (ITBS Maps and Diagrams, ITBS Reference and Materials, and ITBS Capitalization). Ten thousand random samples were simulated from population distributions, and the standard error, bias, and RMSE statistics were calculated. The cubic B-spline presmoothing method performed well in reducing total error of equating, whereas the direct presmoothing method appeared to need some modification for it to be as accurate as other smoothing methods. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-66749165904
Bolt D.,"Bolt, Daniel (7006174434)",7006174434,The present and future of IRT-based cognitive diagnostic models (ICDMs) and related methods,2007,Journal of Educational Measurement,44,4,,377,383,6,16,10.1111/j.1745-3984.2007.00045.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36349020862&doi=10.1111%2fj.1745-3984.2007.00045.x&partnerID=40&md5=e4f0b2881b91cbb574630c37767a5875,"As the goals of educational assessment evolve from the strictly evaluative to the diagnostically useful, so also evolve the statistical methods used to build, validate, and interpret educational tests. The methods discussed in this special issue all approach diagnosis in an item response theory (IRT) related way, with models that are parameterized at the item level and that extract information from individual item responses. Clearly, their most distinguishing feature is their more complex, multidimensional representation of examinee proficiency. This representation can be built directly into an item response model (as seen in most clearly in Almond, DiBello, Moulder, & Zapata-Rivera, 2007; Henson, Templin, & Douglas, 2007; Roussos, Templin, & Henson, 2007; Stout, 2007) or else it can provide a framework for interpreting (residual) pattems in item responses (as is seen in Gierl, 2007). The complexity of the proficiency space introduces corresponding complexities into the statistical modeling and score reporting aspects of diagnosis. A high level of expert judgment is needed in formulating appropriate models. One of the primary challenges in implementing IRT-based cognitively diagnostic model (ICDMs) requires determining which aspects of the modeling process should be constrained through expert judgment and which can and should be informed by observed item response data. The vast array of psychometric models now available for diagnosis and the different ways they handle these complexities (e.g., how many levels for each skill, how do skills interact, how does skill mastery translate to item performance, etc.) make model selection a central issue. At the same time, it can be challenging to compare models according to goodness of fit due to the many other aspects within each model that must be informed by experts (e.g., entries of the item-by-skill Q-
matrix, structure of the proficiency space, etc). Data-driven model re-specification is often messy. Collectively, the papers presented in this Special Issue provide a comprehensive overview of the state of the art in IRT-based diagnosis. While all emphasize a common end-goal of examinee diagnosis, the process by which this is achieved and the balance of data-driven and expert-driven decision making used along the way also introduce important differences.",,,Article,Final,,Scopus,2-s2.0-36349020862
Cui Y.; Leighton J.P.,"Cui, Ying (35208098100); Leighton, Jacqueline P. (7101772605)",35208098100; 7101772605,The hierarchy consistency index: Evaluating person fit for cognitive diagnostic assessment,2009,Journal of Educational Measurement,46,4,,429,449,20,35,10.1111/j.1745-3984.2009.00091.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549163147&doi=10.1111%2fj.1745-3984.2009.00091.x&partnerID=40&md5=1b132581aeeafae52545af487867f74a,"In this article, we introduce a person-fit statistic called the hierarchy consistency index (HCI) to help detect misfitting item response vectors for tests developed and analyzed based on a cognitive model. The HCI ranges from -1.0 to 1.0, with values close to -1.0 indicating that students respond unexpectedly or differently from the responses expected under a given cognitive model. A simulation study was conducted to evaluate the power of the HCI in detecting different types of misfitting item response vectors. Simulation results revealed that the detection rate of the HCI was a function of type of misfit, item discriminating power, and test length. The best detection rates were achieved when the HCI was applied to tests that consisted of a large number of highly discriminating items. In addition, whether a misfitting item response vector can be correctly identified depends, to a large degree, on the number of misfits of the item response vector relative to the cognitive model. When misfitting response behavior only affects a small number of item responses, the resulting item response vector will not be substantially different from the expectations under the cognitive model and consequently may not be statistically identified as misfitting. As an item response vector deviates further from the model expectations, misfits are more easily identified and consequently higher detection rates of the HCI are expected. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-71549163147
Davis S.L.; Buckendahl C.W.; Plake B.S.,"Davis, Susan L. (59158245200); Buckendahl, Chad W. (6507243182); Plake, Barbara S. (6603689848)",59158245200; 6507243182; 6603689848,When adaptation is not an option: An application of multilingual standard setting,2008,Journal of Educational Measurement,45,3,,287,304,17,1,10.1111/j.1745-3984.2008.00065.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849142510&doi=10.1111%2fj.1745-3984.2008.00065.x&partnerID=40&md5=0a9c33eea884d2639c7b990e1eba1f26,"As an alternative to adaptation, tests may also be developed simultaneously in multiple languages. Although the items on such tests could vary substantially, scores from these tests may be used to make the same types of decisions about different groups of examinees. The ability to make such decisions is contingent upon setting performance standards for each exam that allow for comparable interpretations of test results. This article describes a standard setting process used for a multilingual high school literacy assessment constructed under these conditions. This methodology was designed to address the specific challenges presented by this testing program including maintaining equivalent expectations for performance across different student populations. The validity evidence collected to support the methodology and results is discussed along with recommendations for future practice. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-50849142510
Moses T.; Holland P.W.,"Moses, Tim (12243516500); Holland, Paul W. (7202792268)",12243516500; 7202792268,Selection strategies for univariate loglinear smoothing models and their effect on equating function accuracy,2009,Journal of Educational Measurement,46,2,,159,176,17,13,10.1111/j.1745-3984.2009.00075.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749105146&doi=10.1111%2fj.1745-3984.2009.00075.x&partnerID=40&md5=5c7494ba898008e31ae6ea6777909475,"In this study, we compared 12 statistical strategies proposed for selecting loglinear models for smoothing univariate test score distributions and for enhancing the stability of equipercentile equating functions. The major focus was on evaluating the effects of the selection strategies on equating function accuracy. Selection strategies' influence on the estimation of cumulative test score distributions was also assessed. The results of this simulation study differentiate the selection strategies and define the situations where their use has the most important implications for equating function accuracy. The recommended strategy for estimating test score distributions and for equating is AIC minimization. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-66749105146
Kim D.-I.; Brennan R.; Kolen M.,"Kim, Dong-In (55947016900); Brennan, Robert (34975092300); Kolen, Michael (6603925839)",55947016900; 34975092300; 6603925839,A comparison of IRT equating and beta 4 equating,2005,Journal of Educational Measurement,42,1,,77,99,22,4,10.1111/j.0022-0655.2005.00005.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17244375664&doi=10.1111%2fj.0022-0655.2005.00005.x&partnerID=40&md5=75573ec7f43d9f6817fe0d936e7d5181,"Four equating methods (3PL true score equating, 3PL observed score equating, beta 4 true score equating, and beta 4 observed score equating) were compared using four equating criteria: first-order equity (FOE), second-order equity (SOE), conditional-mean-squared-error (CMSE) difference, and the equipercentile equating property. True score equating more closely achieved estimated FOE than observed score equating when the true score distribution was estimated using the psychometric model that was used in the equating. Observed score equating more closely achieved estimated SOE, estimated CMSE difference, and the equipercentile equating property than true score equating. Among the four equating methods, 3PL observed score equating most closely achieved estimated SOE and had the smallest estimated CMSE difference, and beta 4 observed score equating was the method that most closely met the equipercentile equating property.",,,Article,Final,,Scopus,2-s2.0-17244375664
Sinharay S.; Lu Y.,"Sinharay, Sandip (6602980064); Lu, Ying (55506484200)",6602980064; 55506484200,A further look at the correlation between item parameters and item fit statistics,2008,Journal of Educational Measurement,45,1,,1,15,14,13,10.1111/j.1745-3984.2007.00049.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549109644&doi=10.1111%2fj.1745-3984.2007.00049.x&partnerID=40&md5=8b5c364e001cbb88895bb47a996c604f,"Dodeen (2004) studied the correlation between the item parameters of the three-parameter logistic model and two item fit statistics, and found some linear relationships (e.g., a positive correlation between item discrimination parameters and item fit statistics) that have the potential for influencing the work of practitioners who employ item response theory. This article examines the same type of linear relationships as studied by Dodeen. However, this article adds to the literature by employing item fit statistics not considered by Dodeen, which have been recently suggested and whose Type I error rates have been demonstrated to be generally close to the nominal level. Detailed simulations show that if one uses certain of the recently suggested item fit statistics, there is no need to worry about any linear relationships between the item parameters and item fit statistics. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-40549109644
Culpepper S.A.; Davenport E.C.,"Culpepper, Steven A. (35095902600); Davenport, Ernest C. (7006557222)",35095902600; 7006557222,Assessing differential prediction of college grades by race/ethnicity with a multilevel model,2009,Journal of Educational Measurement,46,2,,220,242,22,22,10.1111/j.1745-3984.2009.00079.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749129650&doi=10.1111%2fj.1745-3984.2009.00079.x&partnerID=40&md5=dfe52792d704cf0392f207cd580e110d,"Previous research notes the importance of understanding racial/ethnic differential prediction of college grades across multiple institutions. Institutional variation in selection indices is especially important given some states' laws governing public institutions' admissions decisions. This paper employed multilevel moderated multiple regression to study the variation of selection indices across 30 institutions and the accuracy of selection indices in predicting college grades for students of different racial/ethnic backgrounds. Several benefits of multilevel models for cross-institutional differential prediction studies were described and include: controlling for institutional differences in range restriction, providing reliability estimates of least squares estimates, and adjusting criterion scores for differences in coursework difficulty. The findings from this study provide evidence of institutional variation in selection indices, which challenges current laws aimed at standardizing them. Specifically, there was evidence that the predictor slope coefficients varied across institutions, in addition to the estimates that measured intercept differences for African and Asian American students. Across universities, the results mirrored previous findings: high school grade point average (GPA) differentially predicted grades for African Americans, SAT verbal scores differentially predict grades for Asian Americans, and SAT math scores were better predictors of Asian Americans' grades. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-66749129650
DeCarlo L.T.,"DeCarlo, Lawrence T. (6701664430)",6701664430,A model of rater behavior in essay grading based on signal detection theory,2005,Journal of Educational Measurement,42,1,,53,76,23,50,10.1111/j.0022-0655.2005.00004.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17244372792&doi=10.1111%2fj.0022-0655.2005.00004.x&partnerID=40&md5=6f5ebcf3ae1af14e1ed95a34e25fb80b,"An approach to essay grading based on signal detection theory (SDT) is presented. SDT offers a basis for understanding rater behavior with respect to the scoring of construct responses, in that it provides a theory of psychological processes underlying the raters' behavior. The approach also provides measures of the precision of the raters and the accuracy of classifications. An application of latent class SDT to essay grading is detailed, and similarities to and differences from item response theory (IRT) are noted. The validity and utility of classifications obtained from the SDT model and scores obtained from IRT models are compared. Validity coefficients were found to be about equal in magnitude across SDT and IRT models. Results from a simulation study of a 5-class SDT model with eight raters are also presented.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-17244372792
Chen S.-Y.; Ankenmann R.D.,"Chen, Shu-Ying (8344121900); Ankenmann, Robert D. (6603355304)",8344121900; 6603355304,Effects of practical constraints on item selection rules at the early stages of computerized adaptive testing,2004,Journal of Educational Measurement,41,2,,149,174,25,33,10.1111/j.1745-3984.2004.tb01112.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544308544&doi=10.1111%2fj.1745-3984.2004.tb01112.x&partnerID=40&md5=6a04dc2fee5c89bb908c030f0aa37c61,"The purpose of this study was to compare the effects of four item selection rules-(1) Fisher information (F), (2) Fisher information with a posterior distribution (FP), (3) Kullback-Leibler information with a posterior distribution (KP), and (4) completely randomized item selection (RN)-with respect to the precision of trait estimation and the extent of item usage at the early stages of computerized adaptive testing. The comparison of the four item selection rules was carried out under three conditions: (1) using only the item information function as the item selection criterion; (2) using both the item information function and content balancing; and (3) using the item information function, content balancing, and item exposure control. When test length was less than 10 items, FP and KP tended to outperform F at extreme trait levels in Condition 1. However, in more realistic settings, it could not be concluded that FP and KP outperformed F, especially when item exposure control was imposed. When test length was greater than 10 items, the three nonrandom item selection procedures performed similarly no matter what the condition was, while F had slightly higher item usage.",,,Article,Final,,Scopus,2-s2.0-4544308544
Harik P.; Clauser B.E.; Grabovsky I.; Nungester R.J.; Swanson D.; Nandakumar R.,"Harik, Polina (6506596303); Clauser, Brian E. (7003595460); Grabovsky, Irina (23495981300); Nungester, Ronald J. (6602127510); Swanson, Dave (7201859959); Nandakumar, Ratna (7005999386)",6506596303; 7003595460; 23495981300; 6602127510; 7201859959; 7005999386,An examination of rater drift within a generalizability theory framework,2009,Journal of Educational Measurement,46,1,,43,58,15,45,10.1111/j.1745-3984.2009.01068.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349170677&doi=10.1111%2fj.1745-3984.2009.01068.x&partnerID=40&md5=7fab36414fb5bea972c901aec021845f,"The present study examined the long-term usefulness of estimated parameters used to adjust the scores from a performance assessment to account for differences in rater stringency. Ratings from four components of the USMLE® Step 2 Clinical Skills Examination data were analyzed. A generalizability-theory framework was used to examine the extent to which rater-related sources of error could be eliminated through statistical adjustment. Particular attention was given to the stability of these estimated parameters over time. The results suggest that rater stringency estimates obtained at a point in time and then used to adjust ratings over a period of months may substantially decrease in usefulness. In some cases, over several months, the use of these adjustments may become counterproductive. Additionally, it is hypothesized that the rate of deterioration in the usefulness of estimated parameters may be a function of the characteristics of the scale. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-61349170677
Gierl M.J.,"Gierl, Mark J. (6701316189)",6701316189,Making diagnostic inferences about cognitive attributes using the rule-space model and attribute hierarchy method,2007,Journal of Educational Measurement,44,4,,325,340,15,40,10.1111/j.1745-3984.2007.00042.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36349011408&doi=10.1111%2fj.1745-3984.2007.00042.x&partnerID=40&md5=7dd367b30fbe219fa2143e3f64a8424a,"The purpose of this paper is to describe the logic and identify key assumptions associated with making cognitive inferences using two attribute-based psychometric methods. The first method is Kikumi Tatsuoka's rule-space model. This model provides a strong point of reference for studying the nature of diagnostic inferences because it is important in the evolution of skills diagnostic testing and it is well documented. The second method is a new procedure called the attribute hierarchy method that was developed from the rule-space approach. Although the attribute hierarchy method shares many commonalities with rule space, it represents an extension by including an attribute hierarchy that serves as an explicit cognitive model of task performance designed to link psychometric practices with contemporary cognitive theories. In this paper, we describe and compare these two attribute-based psychometric methods and identify new directions for research and practice in skills diagnostic testing. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-36349011408
Roussos L.A.; Ozbek O.Y.,"Roussos, Louis A. (6603805095); Ozbek, Ozlem Yesim (57197139195)",6603805095; 57197139195,Formulation of the DETECT population parameter and evaluation of DETECT estimator bias,2006,Journal of Educational Measurement,43,3,,215,243,28,24,10.1111/j.1745-3984.2006.00014.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747328157&doi=10.1111%2fj.1745-3984.2006.00014.x&partnerID=40&md5=9f0c4d60d60b49073e5aa7cae290c8e9,"The development of the DETECT procedure marked an important advancement in nonparametric dimensionality analysis. DETECT is the first nonparametric technique to estimate the number of dimensions in a data set, estimate an effect size for multidimensionality, and identify which dimension is predominantly measured by each item. The efficacy of DETECT critically depends on accurate, minimally biased estimation of the expected conditional covariances of all the item pairs. However, the amount of bias in the DETECT estimator has been studied only in a few simulated unidimensional data sets. This is because the value of the DETECT population parameter is known to be zero for this case and has been unknown for cases when multidimensionality is present. In this article, integral formulas for the DETECT population parameter are derived for the most commonly used parametric multidimensional item response theory model, the Reckase and McKinley model. These formulas are then used to evaluate the bias in DETECT by positing a multidimensional model, simulating data from the model using a very large sample size (to eliminate random error), calculating the large-sample DETECT statistic, and finally calculating the DETECT population parameter to compare with the large-sample statistic. A wide variety of two- and three-dimensional models, including both simple structure and approximate simple structure, were investigated. The results indicated that DETECT does exhibit statistical bias in the large-sample estimation of the item-pair conditional covariances; but, for the simulated tests that had 20 or more items, the bias was small enough to result in the large-sample DETECT almost always correctly partitioning the items and the DETECT effect size estimator exhibiting negligible bias.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-33747328157
Meijer R.R.,"Meijer, Rob R. (7006574564)",7006574564,Using patterns of summed scores in paper-and-pencil tests and computer-adaptive tests to detect misfitting item score patterns,2004,Journal of Educational Measurement,41,2,,119,136,17,5,10.1111/j.1745-3984.2004.tb01110.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544222857&doi=10.1111%2fj.1745-3984.2004.tb01110.x&partnerID=40&md5=711b20584a4bc179f15f34207cd34247,"Two new methods have been proposed to determine unexpected sum scores on subtests (testlets) both for paper-and-pencil tests and computer adaptive tests. A method based on a conservative bound using the hypergeometric distribution, denoted p, was compared with a method where the probability for each score combination was calculated using a highest density region (HDR). Furthermore, these methods were compared with the standardized log-likelihood statistic with and without a correction for the estimated latent trait value (denoted as l z* and lz, respectively). Data were simulated on the basis of the one-parameter logistic model, and both parametric and nonparametric logistic regression was used to obtain estimates of the latent trait. Results showed that it is important to take the trait level into account when comparing subtest scores. In a nonparametric item response theory (IRT) context, on adapted version of the HDR method was a powerful alterative to ρ. In a parametric IRT context, results showed that lz* had the highest power when the data were simulated conditionally on the estimated latent trait level.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-4544222857
Camilli G.; Prowker A.; Dossey J.A.; Lindquist M.M.; Chiu T.-W.; Vargas S.; De La Torre J.,"Camilli, Gregory (7003383989); Prowker, Adam (16317319100); Dossey, John A. (25723103600); Lindquist, Mary M. (25723456400); Chiu, Ting-Wei (36730426100); Vargas, Sadako (22942732800); De La Torre, Jimmy (22940257400)",7003383989; 16317319100; 25723103600; 25723456400; 36730426100; 22942732800; 22940257400,Summarizing item difficulty variation with parcel scores,2008,Journal of Educational Measurement,45,4,,363,389,26,1,10.1111/j.1745-3984.2008.00070.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049115513&doi=10.1111%2fj.1745-3984.2008.00070.x&partnerID=40&md5=ef7dc161f4f7d377ca2622819a2e8dbe,"A new method for analyzing differential item functioning is proposed to investigate the relative strengths and weaknesses of multiple groups of examinees. Accordingly, the notion of a conditional measure of difference between two groups (Reference and Focal) is generalized to a conditional variance. The objective of this article is to present and illustrate a strategy for aggregating results across sets of similar items that exhibit item difficulty variation. Logically, this aggregation strategy is related to the idea of DIF amplification, but estimation is ultimately carried out in the framework of a confirmatory multidimensional Rasch model. Grade 4 data from the 2000 National Assessment of Educational Progress are used to illustrate the technique. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-57049115513
Schulz E.M.; Lee W.-C.; Mullen K.,"Schulz, E. Matthew (57213119158); Lee, Won-Chan (57203094500); Mullen, Ken (8326568500)",57213119158; 57203094500; 8326568500,A domain-level approach to describing growth in achievement,2005,Journal of Educational Measurement,42,1,,1,26,25,13,10.1111/j.0022-0655.2005.00002.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17244377395&doi=10.1111%2fj.0022-0655.2005.00002.x&partnerID=40&md5=4e967b1b30fc2b7cb10e6d5954880de2,"Descriptions of growth in educational achievement often rely on the notion that higher-level students can do whatever lower-level students can do, plus at least one more thing. This article presents a method of supporting such descriptions using the data of a subject-area achievement test. Multiple content domains with an expected order of difficulty were defined within the Grade 8 National Assessment of Educational Progress (NAEP) in mathematics. Teachers were able to reliably classify items into the domains by content. Using expected percentage correct scores on the domains, it was possible to describe each achievement level boundary (Basic, Proficient, and Advanced) on the NAEP scale by patterns of skill that include both mastery and non-mastery, and to show that higher achievement levels are associated with mastery of more skills. We conclude that general achievement tests like NAEP can be used to provide criterion-referenced descriptions of growth in achievement as a sequential mastery of skills.",,,Article,Final,,Scopus,2-s2.0-17244377395
Livingston S.A.; Kim S.,"Livingston, Samuel A. (35864363200); Kim, Sooyeon (56668293600)",35864363200; 56668293600,The circle-arc method for equating in small samples,2009,Journal of Educational Measurement,46,3,,330,343,13,40,10.1111/j.1745-3984.2009.00084.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749103636&doi=10.1111%2fj.1745-3984.2009.00084.x&partnerID=40&md5=013beeb98a299b9002dfebeaeda64717,"This article suggests a method for estimating a test-score equating relationship from small samples of test takers. The method does not require the estimated equating transformation to be linear. Instead, it constrains the estimated equating curve to pass through two pre-specified end points and a middle point determined from the data. In a resampling study with two test forms that differed substantially in difficulty, the proposed method compared favorably with other equating methods, especially for equating scores below the 10th percentile and above the 90th percentile. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-69749103636
Van Der Linden W.J.,"Van Der Linden, Wim J. (55409657500)",55409657500,Conceptual issues in response-time modeling,2009,Journal of Educational Measurement,46,3,,247,272,25,182,10.1111/j.1745-3984.2009.00080.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749124856&doi=10.1111%2fj.1745-3984.2009.00080.x&partnerID=40&md5=943d934ea8af173ab38f77f3a9a2cd2f,"Two different traditions of response-time (RT) modeling are reviewed: the tradition of distinct models for RTs and responses, and the tradition of model integration in which RTs are incorporated in response models or the other way around. Several conceptual issues underlying both traditions are made explicit and analyzed for their consequences. We then propose a hierarchical modeling framework consistent with the first tradition but with the integration of their parameter structures as a second level of modeling. Two examples of the framework are presented. Also, a fundamental equation is derived which relates the RTs on test items to the speed of the test taker and the time intensity of the items. The equation serves as the core of the RT model in the framework. Finally, empirical applications of the framework demonstrating its practical value are reviewed. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-69749124856
Huitzing H.A.,"Huitzing, Hiddo A. (6507680549)",6507680549,An interactive method to solve infeasibility in linear programming test assembling models,2004,Journal of Educational Measurement,41,2,,175,192,17,2,10.1111/j.1745-3984.2004.tb01113.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544292250&doi=10.1111%2fj.1745-3984.2004.tb01113.x&partnerID=40&md5=6288b634a14ae56f08a42418d71e006a,"In optimal assembly of tests from item banks, linear programming (LP) models have proved to be very useful. Assembly by hand has become nearly impossible, but these LP techniques are able to find the best solutions, given the demands and needs of the test to be assembled and the specifics of the item bank from which it is assembled. However, sometimes even LP techniques do not offer an acceptable solution to the test assembler. Infeasibility occurs when the demands are contradictory. These contradictions may be rather complex, especially when stated in terms of LP models. Techniques are described that can solve these infeasibility problems in different manners. The objectives are twofold. First, the assembler is given a helping hand to identify the bottlenecks in the specifications of the LP model. Second, a solution is forced, such that the test assembler is always presented a test as close as possible to the original specifications. These objectives should be realizable both automatically and interactively with the test assembler.",,,Article,Final,,Scopus,2-s2.0-4544292250
Sinharay S.,"Sinharay, Sandip (6602980064)",6602980064,Assessing fit of unidimensional item response theory models using a bayesian approach,2005,Journal of Educational Measurement,42,4,,375,394,19,95,10.1111/j.1745-3984.2005.00021.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-30344469096&doi=10.1111%2fj.1745-3984.2005.00021.x&partnerID=40&md5=7684a605d7266a71fe1b5e0e9334c1bf,"Even though Bayesian estimation has recently become quite popular in item response theory (IRT), there is a lack of works on model checking from a Bayesian perspective. This paper applies the posterior predictive model checking (PPMC) method (Guttman, 1967; Rubin, 1984), a popular Bayesian model checking tool, to a number of real applications of unidimensional IRT models. The applications demonstrate how to exploit the flexibility of the posterior predictive checks to meet the need of the researcher. This paper also examines practical consequences of misfit, an area often ignored in educational measurement literature while assessing model fit.",,,Article,Final,,Scopus,2-s2.0-30344469096
Liu J.; Cahn M.F.; Dorans N.J.,"Liu, Jinghua (35210488600); Cahn, Miriam F. (13604701800); Dorans, Neil J. (6602289148)",35210488600; 13604701800; 6602289148,An application of score equity assessment: Invariance of linkage of new SAT® to old SAT across gender groups,2006,Journal of Educational Measurement,43,2,,113,129,16,15,10.1111/j.1745-3984.2006.00008.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646693173&doi=10.1111%2fj.1745-3984.2006.00008.x&partnerID=40&md5=14b8a1cd9857601f3288ca1a25c817ef,"The College Board's SAT® data are used to illustrate how the score equity assessment (SEA) can help inform the program about equatability. SEA is used to examine whether the content change(s) to the revised new SAT result in differential linking functions across gender groups. Results of population sensitivity analyses are reported on the linkage of the new SAT critical reading (CR) prototype to an old SAT verbal (OV). Based on the criteria used in this study, population invariance was achieved with respect to gender groups.",,,Article,Final,,Scopus,2-s2.0-33646693173
Klockars A.J.; Lee Y.,"Klockars, Alan J. (6603819320); Lee, Yoonsun (37013561000)",6603819320; 37013561000,Simulated tests of differential item functioning using SIBTEST with and without impact,2008,Journal of Educational Measurement,45,3,,271,285,14,7,10.1111/j.1745-3984.2008.00064.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849138462&doi=10.1111%2fj.1745-3984.2008.00064.x&partnerID=40&md5=60d8c1bcce0d409f841ed0705dbea083,"Monte Carlo simulations with 20,000 replications are reported to estimate the probability of rejecting the null hypothesis regarding DIF using SIBTEST when there is DIF present and/or when impact is present due to differences on the primary dimension to be measured. Sample sizes are varied from 250 to 2000 and test lengths from 10 to 40 items. Results generally support previous findings for Type I error rates and power. Impact is inversely related to test length. The combination of DIF and impact, with the focal group having lower ability on both the primary and secondary dimensions, results in impact partially masking DIF so that items biased toward the reference group are less likely to be detected. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-50849138462
Petridou A.; Williams J.,"Petridou, Alexandra (35219315800); Williams, Julian (22936006200)",35219315800; 22936006200,Accounting for aberrant test response patterns using multilevel models,2007,Journal of Educational Measurement,44,3,,227,247,20,22,10.1111/j.1745-3984.2007.00036.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547968639&doi=10.1111%2fj.1745-3984.2007.00036.x&partnerID=40&md5=fc2812f6e4524acf650c58d197c77d72,"Hypotheses about aberrant test-response behavior and hence invalid person-measurement have hitherto included factors like ability, gender, language, test-anxiety, and motivation, but these have not previously been collectively investigated with real data, or with multilevel models. This study analyzes the effect of these factors on person aberrance using a real mathematics assessment data set under the framework of a two-level (person and classroom) hierarchical model. The results suggest that higher-scoring pupils, and, to a lesser extent, second-language learners are significantly more often aberrant. But more importantly, we find that the classroom makes a significant contribution to person aberrance and conclude that studies that investigate the sources of person aberrance with real data should model the classroom as well as individual levels. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34547968639
Van Nijlen D.; Janssen R.,"Van Nijlen, Daniël (23974009400); Janssen, Rianne (7202677512)",23974009400; 7202677512,Modeling judgments in the Angoff and contrasting-groups method of standard setting,2008,Journal of Educational Measurement,45,1,,45,63,18,15,10.1111/j.1745-3984.2007.00051.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549129432&doi=10.1111%2fj.1745-3984.2007.00051.x&partnerID=40&md5=11829713ae2a7514295421ef51b72f92,"Essential for the validity of the judgments in a standard-setting study is that they follow the implicit task assumptions. In the Angoff method, judgments are assumed to be inversely related to the difficulty of the items; contrasting-groups judgments are assumed to be positively related to the ability of the students. In the present study, judgments from both procedures were modeled with a random-effects probit regression model. The Angoff judgments showed a weaker link with the position of the items on the latent scale than the contrasting-groups judgments with the position of the students. Hence, in the specific context of the study, the contrasting-groups judgments were more aligned with the underlying assumptions of the method than the Angoff judgments. © 2007 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-40549129432
Kim S.-H.; Cohen A.S.; Alagoz C.; Kim S.,"Kim, Seock-Ho (7601601135); Cohen, Allan S. (55465451100); Alagoz, Cigdem (16300913900); Kim, Sukwoo (16301487400)",7601601135; 55465451100; 16300913900; 16301487400,DIF detection and effect size measures for polytomously scored items,2007,Journal of Educational Measurement,44,2,,93,116,23,66,10.1111/j.1745-3984.2007.00029.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247850754&doi=10.1111%2fj.1745-3984.2007.00029.x&partnerID=40&md5=010bd66b163dabeef827efbedadf0816,"Data from a large-scale performance assessment (N = 105,731) were analyzed with five differential item functioning (DIF) detection methods for polytomous items to examine the congruence among the DIF detection methods. Two different versions of the item response theory (IRT) model-based likelihood ratio test, the logistic regression likelihood ratio test, the Mantel test, and the generalized Mantel-Haenszel test were compared. Results indicated some agreement among the five DIF detection methods. Because statistical power is a function of the sample size, the DIF detection results from extremely large data sets are not practically useful. As alternatives to the DIF detection methods, four IRT model-based indices of standardized impact and four observed-score indices of standardized impact for polytomous items were obtained and compared with the R2 measures of logistic regression. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-34247850754
Huitzing H.A.; Veldkamp B.P.; Verschoor A.J.,"Huitzing, Hiddo A. (6507680549); Veldkamp, Bernard P. (6602896542); Verschoor, Angela J. (8726269200)",6507680549; 6602896542; 8726269200,Infeasibility in automated test assembly models: A comparison study of different methods,2005,Journal of Educational Measurement,42,3,,223,243,20,15,10.1111/j.1745-3984.2005.00012.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644494260&doi=10.1111%2fj.1745-3984.2005.00012.x&partnerID=40&md5=0c51c15abac7a47eb8eabc4aa73e03b1,"Several techniques exist to automatically put together a test meeting a number of specifications. In an item bank, the items are stored with their characteristics. A test is constructed by selecting a set of items that fulfills the specifications set by the test assembler. Test assembly problems are often formulated in terms of a model consisting of restrictions and an objective to be maximized or minimized. A problem arises when it is impossible to construct a test from the item pool that meets all specifications, that is, when the model is not feasible. Several methods exist to handle these infeasibility problems. In this article, test assembly models resulting from two practical testing programs were reconstructed to be infeasible. These models were analyzed using methods that forced a solution (Goal Programming, Multiple-Goal Programming, Greedy Heuristic), that analyzed the causes (Relaxed and Ordered Deletion Algorithm (RODA), Integer Randomized Deletion Algorithm (IRDA), Set Covering (SC), and Item Sampling), or that analyzed the causes and used this information to force a solution (Irreducible Infeasible Set-Solver). Specialized methods such as the IRDA and the Irreducible Infeasible Set-Solver performed best. Recommendations about the use of different methods are given.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-24644494260
Puhan G.; Moses T.P.; Yu L.; Dorans N.J.,"Puhan, Gautam (12445769400); Moses, Timothy P. (12243516500); Yu, Lei (7404164827); Dorans, Neil J. (6602289148)",12445769400; 12243516500; 7404164827; 6602289148,Using log-linear smoothing to improve small-sample DIF estimation,2009,Journal of Educational Measurement,46,1,,59,83,24,5,10.1111/j.1745-3984.2009.01069.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349134835&doi=10.1111%2fj.1745-3984.2009.01069.x&partnerID=40&md5=a1e3eb7ee424925f5d84aa116887b2ab,"This study examined the extent to which log-linear smoothing could improve the accuracy of differential item functioning (DIF) estimates in small samples of examinees. Examinee responses from a certification test were analyzed using White examinees in the reference group and African American examinees in the focal group. Using a simulation approach, separate DIF estimates for seven small-sample-size conditions were obtained using unsmoothed (U) and smoothed (S) score distributions. These small sample U and S DIF estimates were compared to a criterion (i.e., DIF estimates obtained using the unsmoothed total data) to assess their degree of variability (random error) and accuracy (bias). Results indicate that for most studied items smoothing the raw score distributions reduced random error and bias of the DIF estimates, especially in the small-sample-size conditions. Implications of these results for operational testing programs are discussed. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-61349134835
Finch H.,"Finch, Holmes (12767650300)",12767650300,Estimation of item response theory parameters in the presence of missing data,2008,Journal of Educational Measurement,45,3,,225,245,20,70,10.1111/j.1745-3984.2008.00062.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849128776&doi=10.1111%2fj.1745-3984.2008.00062.x&partnerID=40&md5=e687a8412c0409cf652340b2f7388dcf,"Missing data are a common problem in a variety of measurement settings, including responses to items on both cognitive and affective assessments. Researchers have shown that such missing data may create problems in the estimation of item difficulty parameters in the Item Response Theory (IRT) context, particularly if they are ignored. At the same time, a number of data imputation methods have been developed outside of the IRT framework and been shown to be effective tools for dealing with missing data. The current study takes several of these methods that have been found to be useful in other contexts and investigates their performance with IRT data that contain missing values. Through a simulation study, it is shown that these methods exhibit varying degrees of effectiveness in terms of imputing data that in turn produce accurate sample estimates of item difficulty and discrimination parameters. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-50849128776
Kim S.; Feldt L.S.,"Kim, Seonghoon (23060961000); Feldt, Leonard S. (7003911114)",23060961000; 7003911114,A comparison of tests for equality of two or more independent alpha coefficients,2008,Journal of Educational Measurement,45,2,,179,193,14,61,10.1111/j.1745-3984.2008.00059.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949103783&doi=10.1111%2fj.1745-3984.2008.00059.x&partnerID=40&md5=5e9b2f328d9ffa34b993a301a194adff,"This article extends the Bonett (2003a) approach to testing the equality of alpha coefficients from two independent samples to the case of m ≥ 2 independent samples. The extended Fisher-Bonett test and its competitor, the Hakstian-Whalen (1976) test, are illustrated with numerical examples of both hypothesis testing and power calculation. Computer simulations are used to compare the performance of the two tests and the Feldt (1969) test (for m = 2) in terms of power and Type I error control. It is shown that the Fisher-Bonett test is just as effective as its competitors in controlling Type I error, is comparable to them in power, and is equally robust against heterogeneity of error variance. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-43949103783
Clauser B.E.; Harik P.; Margolis M.J.,"Clauser, Brian E. (7003595460); Harik, Polina (6506596303); Margolis, Melissa J. (7101753505)",7003595460; 6506596303; 7101753505,A multivariate generalizability analysis of data from a performance assessment of physicians' clinical skills,2006,Journal of Educational Measurement,43,3,,173,191,18,31,10.1111/j.1745-3984.2006.00012.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747277764&doi=10.1111%2fj.1745-3984.2006.00012.x&partnerID=40&md5=cca39507527a4ec80ecabb331a9c4b1c,"Although multivariate generalizability theory was developed more than 30 years ago, little published research utilizing this framework exists and most of what does exist examines tests built from tables of specifications. In this context, it is assumed that the universe scores from levels of the fixed multivariate facet will be correlated, but the error terms will be uncorrelated because subscores result from mutually exclusive sets of test items. This paper reports on an application in which multiple subscores are derived from each task completed by the examinee. In this context, both universe scores and errors may be correlated across levels of the fixed multi-variate facet. The data described come from the United States Medical Licensing Examination® Step 2 Clinical Skills Examination. In this test, each examinee interacts with a series of standardized patients and each interaction results in four component scores. The paper focuses on the application of multivariate generalizability theory in this context and on the practical interpretation of the resulting estimated variance and covariance components.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-33747277764
Wise S.L.; Demars C.E.,"Wise, Steven L. (7202622563); Demars, Christine E. (7003447419)",7202622563; 7003447419,An application of item response time: The effort-moderated IRT model,2006,Journal of Educational Measurement,43,1,,19,38,19,202,10.1111/j.1745-3984.2006.00002.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645127618&doi=10.1111%2fj.1745-3984.2006.00002.x&partnerID=40&md5=187d922e26b690d5f5ce82841ab04985,"The validity of inferences based on achievement test scores is dependent on the amount of effort that examinees put forth while taking the test. With low-stakes tests, for which this problem is particularly prevalent, there is a consequent need for psychometric models that can take into account differing levels of examinee effort. This article introduces the effort-moderated IRT model, which incorporates item response time into proficiency estimation and item parameter estimation. In two studies of the effort-moderated model when rapid guessing (i.e., reflecting low examinee effort) was present, one based on real data and the other on simulated data, the effort-moderated model performed better than the standard 3PL model. Specifically, it was found that the effort-moderated model (a) showed better model fit, (b) yielded more accurate item parameter estimates, (c) more accurately estimated test information, and (d) yielded proficiency estimates with higher convergent validity. © Copyright 2006 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-33645127618
Stone C.A.; Zhang B.,"Stone, Clement A. (7201720784); Zhang, Bo (55619300516)",7201720784; 55619300516,Assessing Goodness of Fit of Item Response Theory Models: A Comparison of Traditional and Alternative Procedures,2003,Journal of Educational Measurement,40,4,,331,352,21,103,10.1111/j.1745-3984.2003.tb01150.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842714348&doi=10.1111%2fj.1745-3984.2003.tb01150.x&partnerID=40&md5=80a71c0bd30e2da3b2a9872fc01fefe7,"Testing the goodness of fit of item response theory (IRT) models is relevant to validating IRT models, and new procedures have been proposed. These alternatives compare observed and expected response frequencies conditional on observed total scores, and use posterior probabilities for responses across θ levels rather than cross-classifying examinees using point estimates of θ and score responses. This research compared these alternatives with regard to their methods, properties (Type I error rates and empirical power), available research, and practical issues (computational demands, treatment of missing data, effects of sample size and sparse data, and available computer programs). Different advantages and disadvantages related to these characteristics are discussed, A simulation study provided additional information about empirical power and Type I error rates.",,,Article,Final,,Scopus,2-s2.0-1842714348
Allen N.L.; Holland P.W.; Thayer D.T.,"Allen, Nancy L. (7202441192); Holland, Paul W. (7202792268); Thayer, Dorothy T. (7006657345)",7202441192; 7202792268; 7006657345,Measuring the benefits of examinee-selected questions,2005,Journal of Educational Measurement,42,1,,27,51,24,17,10.1111/j.0022-0655.2005.00003.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17244367063&doi=10.1111%2fj.0022-0655.2005.00003.x&partnerID=40&md5=9d3e7acee163f1dfd047d38273b341ec,"Allowing students to choose the question(s) that they will answer from among several possible alternatives is often viewed as a mechanism for increasing fairness in certain types of assessments. The fairness of optional topic choice is not a universally accepted fact, however, and various studies have been done to assess this question. We examine an important class of experiments that we call CI-A, ""choose one, answer all,"" designs, and point out an important problem that they face. We suggest two analytical methods that can be used to circumvent this problem. We illustrate our ideas using the data from Bridgeman et al. (1997). Our reanalysis of these data show: (a) that differential topic difficulty exists in real choice data, (b) that it affects naïve analyses of such data and masks the effects, positive or negative, of examinee choice, (c) that in this study there is a measurable and positive effect of examinee choice that follows predicted patterns in most but not all cases, (d) that the beneficial strength of examinee choice varies from case to case, and (e) that while the benefits of choice in terms of average points scored on the essays are usually positive, there is a substantial amount of variation around these averages and it is not uncommon for ""incorrect"" choices to be associated with higher test performance.",,,Article,Final,,Scopus,2-s2.0-17244367063
De La Torre J.; Deng W.,"De La Torre, Jimmy (22940257400); Deng, Weiling (36175373300)",22940257400; 36175373300,Improving person-fit assessment by correcting the ability estimate and its reference distribution,2008,Journal of Educational Measurement,45,2,,159,177,18,46,10.1111/j.1745-3984.2008.00058.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949138622&doi=10.1111%2fj.1745-3984.2008.00058.x&partnerID=40&md5=c71819479ea2a25355e8a9dff53dabc3,"The standardized log-likelihood of a response vector (lz) is a popular IRT-based person-fit test statistic for identifying model-misfitting response patterns. Traditional use of lz is overly conservative in detecting aberrance due to its incorrect assumption regarding its theoretical null distribution. This study proposes a method for improving the accuracy of person-fit analysis using lz which takes into account test unreliability when estimating the ability and constructs the distribution for each lz through resampling methods. The Type I error and power (or detection rate) of the proposed method were examined at different test lengths, ability levels, and nominal α levels along with other methods, and power to detect three types of aberrance - cheating, lack of motivation, and speeding - was considered. Results indicate that the proposed method is a viable and promising approach. It has Type I error rates close to the nominal value for most ability levels and reasonably good power. © 2008 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-43949138622
Finkelman M.; Kim W.; Roussos L.A.,"Finkelman, Matthew (24558635500); Kim, Wonsuk (55387130600); Roussos, Louis A. (6603805095)",24558635500; 55387130600; 6603805095,Automated test assembly for cognitive diagnosis models using a genetic algorithm,2009,Journal of Educational Measurement,46,3,,273,292,19,24,10.1111/j.1745-3984.2009.00081.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69749099893&doi=10.1111%2fj.1745-3984.2009.00081.x&partnerID=40&md5=398d35117374a6ec0380272e01894684,"Much recent psychometric literature has focused on cognitive diagnosis models (CDMs), a promising class of instruments used to measure the strengths and weaknesses of examinees. This article introduces a genetic algorithm to perform automated test assembly alongside CDMs. The algorithm is flexible in that it can be applied whether the goal is to minimize the average number of classification errors, minimize the maximum error rate across all attributes being measured, hit a target set of error rates, or optimize any other prescribed objective function. Under multiple simulation conditions, the algorithm compared favorably with a standard method of automated test assembly, successfully finding solutions that were appropriate for each stated goal. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-69749099893
Myford C.M.; Wolfe E.W.,"Myford, Carol M. (6602409531); Wolfe, Edward W. (7007156715)",6602409531; 7007156715,Monitoring rater performance over time: A framework for detecting differential accuracy and differential scale category use,2009,Journal of Educational Measurement,46,4,,371,389,18,70,10.1111/j.1745-3984.2009.00088.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549124344&doi=10.1111%2fj.1745-3984.2009.00088.x&partnerID=40&md5=eb883f73b05f50d2d6680be95f00cf0e,"In this study, we describe a framework for monitoring rater performance over time. We present several statistical indices to identify raters whose standards drift and explain how to use those indices operationally. To illustrate the use of the framework, we analyzed rating data from the 2002 Advanced Placement English Literature and Composition examination, employing a multifaceted Rasch approach to determine whether raters exhibited evidence of two types of differential rater functioning over time (i.e., changes in levels of accuracy or scale category use). Some raters showed statistically significant changes in their levels of accuracy as the scoring progressed, while other raters displayed evidence of differential scale category use over time. © 2009 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-71549124344
Van Der Linden W.J.; Breithaupt K.; Chuah S.C.; Zhang Y.,"Van Der Linden, Wim J. (55409657500); Breithaupt, Krista (6701408129); Chuah, Siang Chee (14049683500); Zhang, Yanwei (16302400900)",55409657500; 6701408129; 14049683500; 16302400900,Detecting differential speededness in multistage testing,2007,Journal of Educational Measurement,44,2,,117,130,13,54,10.1111/j.1745-3984.2007.00030.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247871269&doi=10.1111%2fj.1745-3984.2007.00030.x&partnerID=40&md5=f2f158efb5febb48589fbbdb637e7b52,"A potential undesirable effect of multistage testing is differential speededness, which happens if some of the test takers run out of time because they receive subtests with items that are more time intensive than others. This article shows how a probabilistic response-time model can be used for estimating differences in time intensities and speed between subtests and test takers and detecting differential speededness. An empirical data set for a multistage test in the computerized CPA Exam was used to demonstrate the procedures. Although the more difficult subtests appeared to have items that were more time intensive than the easier subtests, an analysis of the residual response times did not reveal any significant differential speededness because the time limit appeared to be appropriate. In a separate analysis, within each of the subtests, we found minor but consistent patterns of residual times that are believed to be due to a warm-up effect, that is, use of more time on the initial items than they actually need. © 2007 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-34247871269
Stout W.,"Stout, William (7005099761)",7005099761,Skills diagnosis using IRT-based continuous latent trait models,2007,Journal of Educational Measurement,44,4,,313,324,11,28,10.1111/j.1745-3984.2007.00041.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348968337&doi=10.1111%2fj.1745-3984.2007.00041.x&partnerID=40&md5=05e5dbcf2002a3d724c286d04bc25bb0,"This article summarizes the continuous latent trait IRT approach to skills diagnosis as particularized by a representative variety of continuous latent trait models using item response functions (IRFs). First, several basic IRT-based continuous latent trait approaches are presented in some detail. Then a brief summary of estimation, model checking, and assessment scoring aspects are discussed. Finally, the University of California at Berkeley multidimensional Rasch-model-grounded SEPUP middle school science-focused embedded assessment project is briefly described as one significant illustrative application. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-36348968337
Gierl M.J.; Zheng Y.; Cui Y.,"Gierl, Mark J. (6701316189); Zheng, Yinggan (58380318100); Cui, Ying (35208098100)",6701316189; 58380318100; 35208098100,Using the attribute hierarchy method to identify and interpret cognitive skills that produce group differences,2008,Journal of Educational Measurement,45,1,,65,89,24,17,10.1111/j.1745-3984.2007.00052.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549110655&doi=10.1111%2fj.1745-3984.2007.00052.x&partnerID=40&md5=3e9a7adef538fdf48d24c3f67fe1838f,"The purpose of this study is to describe how the attribute hierarchy method (AHM) can be used to evaluate differential group performance at the cognitive attribute level. The AHM is a psychometric method for classifying examinees' test item responses into a set of attribute-mastery patterns associated with different components in a cognitive model of task performance. Attribute probabilities, computed using a neural network, can be estimated on each attribute for each examinee thereby providing specific information about the examinee's attribute-mastery level. These probabilities can also be compared across groups. We describe a four-step procedure for estimating and interpreting group differences using the AHM. We also provide an example using student response data from a sample of algebra items on the SAT to illustrate our pattern recognition approach for studying group differences. © 2007 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-40549110655
Leighton J.P.; Gierl M.J.; Hunka S.M.,"Leighton, Jacqueline P. (7101772605); Gierl, Mark J. (6701316189); Hunka, Stephen M. (6603073833)",7101772605; 6701316189; 6603073833,The attribute hierarchy method for cognitive assessment: A variation on Tatsuoka's rule-space approach,2004,Journal of Educational Measurement,41,3,,205,237,32,276,10.1111/j.1745-3984.2004.tb01163.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-6344237380&doi=10.1111%2fj.1745-3984.2004.tb01163.x&partnerID=40&md5=5465e7e3932408f2597edcc2f59394b6,"A cognitive item response theory model called the attribute hierarchy method (AHM) is introduced and illustrated. This method represents a variation of Tatsuoka's rule-space approach. The AHM is designed explicitly to link cognitive theory and psychometric practice to facilitate the development and analyses of educational and psychological tests. The following are described: cognitive properties of the AHM; psychometric properties of the AHM, as well as a demonstration of how the AHM differs from Tatsuoka's rule-space approach; and application of the AHM to the domain of syllogistic reasoning to illustrate how this approach can be used to evaluate the cognitive competencies required in a higher-level thinking task. Future directions for research are also outlined.",,,Article,Final,,Scopus,2-s2.0-6344237380
Walker C.M.; Beretvas S.N.,"Walker, Cindy M. (36874825300); Beretvas, S. Natasha (6603091715)",36874825300; 6603091715,Comparing Multidimensional and Unidimensional Proficiency Classifications: Multidimensional IRT as a Diagnostic Aid,2003,Journal of Educational Measurement,40,3,,255,275,20,49,10.1111/j.1745-3984.2003.tb01107.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347592506&doi=10.1111%2fj.1745-3984.2003.tb01107.x&partnerID=40&md5=1cc4e7ad7ff15c78cf603ba07022e0ea,"This research examined the effect of scoring items thought to be multidimensional using a unidimensional model and demonstrated the use of multidimensional item response theory (MIRT) as a diagnostic tool. Using real data from a large-scale mathematics test, previously shown to function differentially in favor of proficient writers, the difference in proficiency classifications was explored when a two-versus one-dimensional confirmatory model was fit. The estimate of ability obtained when using the unidimensional model was considered to represent general mathematical ability. Under the two-dimensional model, one of the two dimensions was also considered to represent general mathematical ability. The second dimension was considered to represent the ability to communicate in mathematics. The resulting pattern of mismatched proficiency classifications suggested that examinees found to have less mathematics communication ability were more likely to be placed in a lower general mathematics proficiency classification under the unidimensional than multidimensional model. Results and implications are discussed.",,,Article,Final,,Scopus,2-s2.0-0347592506
Lee G.,"Lee, Guemin (7404851825)",7404851825,A comparison of methods of estimating conditional standard errors of measurement for testlet-based test scores using simulation techniques,2000,Journal of Educational Measurement,37,2,,91,112,21,10,10.1111/j.1745-3984.2000.tb01078.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034196662&doi=10.1111%2fj.1745-3984.2000.tb01078.x&partnerID=40&md5=372b330c90bf6bf65947b32db4ee9588,"The primary purpose of this study was to investigate the appropriateness and implication of incorporating a testlet definition into the estimation of procedures of the conditional standard error of measurement (SEM) for tests composed of testlets. Another purpose was to investigate the bias in estimates of the conditional SEM when using item-based methods instead of testlet-based methods. Several item-based and testlet-based estimation methods were proposed and compared. In general, item-based estimation methods underestimated the conditional SEM for tests composed for testlets, and the magnitude of this negative bias increased as the degree of conditional dependence among items within testlets increased. However, an item-based method using a generalizability theory model provided good estimates of the conditional SEM under mild violation of the assumptions for measurement modeling. Under moderate or somewhat severe violation, testlet-based methods with item response models provided good estimates.",,,Article,Final,,Scopus,2-s2.0-0034196662
Lee W.-C.; Brennan R.L.; Kolen M.J.,"Lee, Won-Chan (57203094500); Brennan, Robert L. (34975092300); Kolen, Michael J. (6603925839)",57203094500; 34975092300; 6603925839,Estimators of conditional scale-score standard errors of measurement: A simulation study,2000,Journal of Educational Measurement,37,1,,1,20,19,21,10.1111/j.1745-3984.2000.tb01073.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034145591&doi=10.1111%2fj.1745-3984.2000.tb01073.x&partnerID=40&md5=c0fd002e0e618e2f70363f7cdf12f35b,"This paper describes four procedures previously developed for estimating conditional standard errors of measurement for scale scores: the IRT procedure (Kolen, Zeng, & Hanson, 1996), the binomial procedure (Brennan & Lee, 1999), the compound binomial procedure (Brennan & Lee, 1999), and the Feldt-Qualls procedure (1998). These four procedures are based on different underlying assumptions. The IRT procedure is based on the unidimensional IRT model assumptions. The binomial and compound binomial procedures employ, as the distribution of errors, the binomial model and compound binomial model, respectively. By contrast, the Feldt-Qualls procedure does not depend on a particular psychometric model, and it simply translates any estimated conditional raw-score SEM to a conditional scale-score SEM. These procedures are compared in a simulation study, which involves two-dimensional data sets. The presence of two category dimensions reflects a violation of the IRT unidimensionality assumption. The relative accuracy of these procedures for estimating conditional scale-score standard errors of measurement is evaluated under various circumstances. The effects of three different types of transformations of raw scores are investigated including developmental standard scores, grade equivalents, and percentile ranks. All the procedures discussed appear viable. A general recommendation is made that test users select a procedure based on various factors such as the type of scale score of concern, characteristics of the test, assumptions involved in the estimation procedure, and feasibility and practicability of the estimation procedure.",,,Article,Final,,Scopus,2-s2.0-0034145591
Wollack J.A.,"Wollack, James A. (6701334965)",6701334965,Comparison of Answer Copying Indices with Real Data,2003,Journal of Educational Measurement,40,3,,189,205,16,24,10.1111/j.1745-3984.2003.tb01104.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347592505&doi=10.1111%2fj.1745-3984.2003.tb01104.x&partnerID=40&md5=28c9bb796fcfaceca6d693a25492ca68,"This study investigated the Type I error rate and power of four copying indices, K-index (Holland, 1996), Scrutiny! (Assessment Systems Corporation, 1993), g2 (Frary, Tideman, & Watts, 1977), and ω (Wollack, 1997) using real test data from 20,000 examinees over a 2-year period. The data were divided into three different test lengths (20, 40, and 80 items) and nine different sample sizes (ranging from 50 to 20,000). Four different amounts of answer copying were simulated (10%, 20%, 30%, and 40% of the items) within each condition. The ω index demonstrated the best Type I error control and power in all conditions and at all a. levels. Scrutiny! and the K-index were uniformly conservative, and both had poor power to detect true copiers at the small α levels typically used in answer copying detection, whereas g 2 was generally too liberal, particularly at small a levels. Some comments on the proper uses of copying indices are provided.",,,Article,Final,,Scopus,2-s2.0-0347592505
Brennan R.L.; Yin P.; Kane M.T.,"Brennan, Robert L. (34975092300); Yin, Ping (7103219369); Kane, Michael T. (36088969800)",34975092300; 7103219369; 36088969800,Methodology for Examining the Reliability of Group Mean Difference Scores,2003,Journal of Educational Measurement,40,3,,207,230,23,11,10.1111/j.1745-3984.2003.tb01105.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347592504&doi=10.1111%2fj.1745-3984.2003.tb01105.x&partnerID=40&md5=13ed3ff25594e38a02dbe3f8a5ed30d9,"This article treats various procedures for examining the reliability of group mean difference scores, with particular emphasis on procedures from univariate and multivariate generalizability theory. Attention is given to both traditional norm-referenced perspectives on reliability as well as criterion-referenced perspectives that focus on error-tolerance ratios and functions of them. The procedures discussed are illustrated using three cohorts of data for third- and fourth-grade students in Iowa who took the Iowa Tests of Basic Skills in recent years. For these data, estimates of reliability for norm-referenced decisions tend to be relatively low. By contrast, for criterion-referenced decisions, estimates of reliability-like coefficients based on error-tolerance ratios tend to be noticeably larger.",,,Article,Final,,Scopus,2-s2.0-0347592504
Katz I.R.; Bennett R.E.; Berger A.E.,"Katz, Irvin R. (7101824537); Bennett, Randy Elliot (7402440584); Berger, Aliza E. (7402970291)",7101824537; 7402440584; 7402970291,Effects of response format on difficulty of SAT-mathematics items: It's not the strategy,2000,Journal of Educational Measurement,37,1,,39,57,18,46,10.1111/j.1745-3984.2000.tb01075.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034336333&doi=10.1111%2fj.1745-3984.2000.tb01075.x&partnerID=40&md5=014e007c35aaed191a882740cccc32b9,"Problem-solving strategy is frequently cited as mediating the effects of response format (multiple-choice, constructed response) on item difficulty, yet there are few direct investigations of examinee solution procedures. Fifty-five high school students solved parallel constructed response and multiple-choice items that differed only in the presence of response options. Student performance was videotaped to assess solution strategies. Strategies were categorized as ""traditional"" - those associated with constructed response problem solving (e.g., writing and solving algebraic equations) - or ""nontraditional"" - those associated with multiple-choice problem solving (e.g., estimating a potential solution). Surprisingly, participants sometimes adopted nontraditional strategies to solve constructed response items. Furthermore, differences in difficulty between response formats did not correspond to differences in strategy choice: some items showed a format effect on strategy but no effect on difficulty; other items showed the reverse. We interpret these results in light of the relative comprehension challenges posed by the two groups of items.",,,Article,Final,,Scopus,2-s2.0-0034336333
Rodriguez M.C.,"Rodriguez, Michael C. (7404259511)",7404259511,Construct equivalence of multiple-choice and constructed-response items: A random effects synthesis of correlations,2003,Journal of Educational Measurement,40,2,,163,184,21,92,10.1111/j.1745-3984.2003.tb01102.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037697027&doi=10.1111%2fj.1745-3984.2003.tb01102.x&partnerID=40&md5=1e60367ce4a52c2ea12a64587bf021e7,"A thorough search of the literature was conducted to locate empirical studies investigating the trait or construct equivalence of multiple-choice (MC) and conslructed-response (CR) items. Of the 67 studies identified, 29 studies included 56 correlations between items in both formats. These 56 correlations were corrected for attenuation and synthesized to establish evidence for a common estimate of correlation (true-score correlations). The 56 disattenuated correlations were highly heterogeneous. A search for moderators to explain this variation uncovered the role of the design characteristics of test items used in the studies. When items are constructed in both formats using the same stem (stem equivalent), the mean correlation between the two formats approaches unity and is significantly higher than when using non-stem-equivalent items (particularly when using essay-type items). Construct equivalence, in part, appears to be a function of the item design method or the item writer's intent.",,,Article,Final,,Scopus,2-s2.0-0037697027
Chen S.-Y.; Ankenmann R.D.; Spray J.A.,"Chen, Shu-Ying (8344121900); Ankenmann, Robert D. (6603355304); Spray, Judith A. (7006830106)",8344121900; 6603355304; 7006830106,The relationship between item exposure and test overlap in computerized adaptive testing,2003,Journal of Educational Measurement,40,2,,129,145,16,57,10.1111/j.1745-3984.2003.tb01100.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037697029&doi=10.1111%2fj.1745-3984.2003.tb01100.x&partnerID=40&md5=c646c804f1f94f4eec95777e26d15d2b,"The purpose of this article is to present an analytical derivation for the mathematical form of an average between-test overlap index as a function of the item exposure index, for fixed-length computerized adaptive tests (CATs). This algebraic relationship is used to investigate the simultaneous control of item exposure at both the item and test levels. The results indicate that, in fixed-length CATs, control of the average between-test overlap is achieved via the mean and variance of the item exposure rates of the items that constitute the CAT item pool. The mean of the item exposure rates is easily manipulated. Control over the variance of the item exposure rates can be achieved via the maximum item exposure rate (rmax. Therefore, item exposure control methods which implement a specification of rmax (e.g., Sympson & Hetter, 1985) provide the most direct control at both the item and test levels.",,,Article,Final,,Scopus,2-s2.0-0037697029
Vispoel W.P.; Clough S.J.; Bleiler T.; Hendrickson A.B.; Ihrig D.,"Vispoel, Walter P. (6603886282); Clough, Sara J. (7005232963); Bleiler, Timothy (6603213880); Hendrickson, Amy B. (7006207129); Ihrig, Damien (6603415174)",6603886282; 7005232963; 6603213880; 7006207129; 6603415174,Can examinees use judgments of item difficulty to improve proficiency estimates on computerized adaptive vocabulary tests?,2002,Journal of Educational Measurement,39,4,,311,330,19,3,10.1111/j.1745-3984.2002.tb01145.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036902269&doi=10.1111%2fj.1745-3984.2002.tb01145.x&partnerID=40&md5=641db08ff2c28806b43b35d999b9bcd9,"Recent simulation studies indicate that there are occasions when examinees can use judgments of relative item difficulty to obtain positively biased proficiency estimates on computerized adaptive tests (CATs) that permit item review and answer change, Our purpose in the study reported here was to evaluate examinees' success in using these strategies while taking CATs in a live testing setting. We taught examinees two item difficulty judgment strategies designed to increase proficiency estimates. Examinees who were taught each strategy and examinees who were taught neither strategy were assigned at random to complete vocabulary CATs under conditions in which review was allowed after completing all items and when review was allowed only within successive blocks of items. We found that proficiency estimate changes following review were significantly higher in the regular review conditions than in the strategy conditions. Failure to obtain systematically higher scores in the strategy conditions was due in large part to errors examinees made in judging the relative difficulty of CAT items.",,,Article,Final,,Scopus,2-s2.0-0036902269
DeMars C.E.,"DeMars, Christine E. (7003447419)",7003447419,Detecting multidimensionality due to curricular differences,2003,Journal of Educational Measurement,40,1,,29,51,22,7,10.1111/j.1745-3984.2003.tb01095.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037354197&doi=10.1111%2fj.1745-3984.2003.tb01095.x&partnerID=40&md5=8ea466fa609605656b9d8ff6a13f6f77,"Data were generated to simulate multidimensionality resulting from including two or four subtopics on a test. Each item was dependent on an ability trait due to instruction and learning, which was the same across all items, as well as an ability trait unique to the subtopic of the test (such as biology on a general science test). The eigenvalues of the item correlation matrix and Yen's Q3 were not greatly influenced by multidimensionality under conditions where the responses of a large proportion of students shared the influence of common instruction across subtopics. In contrast, Stout's T procedure was effective at detecting this type of multidimensionality, unless the subtopic abilities were correlated.",,,Article,Final,,Scopus,2-s2.0-0037354197
Chang S.-W.; Ansley T.N.,"Chang, Shun-Wen (7405607162); Ansley, Timothy N. (6505980402)",7405607162; 6505980402,A comparative study of item exposure control methods in computerized adaptive testing,2003,Journal of Educational Measurement,40,1,,71,103,32,18,10.1111/j.1745-3984.2003.tb01097.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037354198&doi=10.1111%2fj.1745-3984.2003.tb01097.x&partnerID=40&md5=a5fd96d0d973db69195c2afe55361707,"This study compared the properties of five methods of item exposure control within the purview of estimating examinees' abilities in a computerized adaptive testing (CAT) context. Each exposure control algorithm was incorporated into the item selection procedure and the adaptive testing progressed based on the CAT design established for this study. The merits and shortcomings of these strategies were considered under different item pool sizes and different desired maximum exposure rates and were evaluated in light of the observed maximum exposure rates, the test overlap rates, and the conditional standard errors of measurement. Each method had its advantages and disadvantages, but no one possessed all of the desired characteristics. There was a clear and logical trade-off between item exposure control and measurement precision. The Stocking and Lewis conditional multinomial procedure and, to a slightly lesser extent, the Davey and Parshall method seemed to be the most promising considering all of the factors that this study addressed.",,,Article,Final,,Scopus,2-s2.0-0037354198
Bolt D.M.; Cohen A.S.; Wollack J.A.,"Bolt, Daniel M. (7006174434); Cohen, Allan S. (55465451100); Wollack, James A. (6701334965)",7006174434; 55465451100; 6701334965,Item parameter estimation under conditions of test speededness: Application of a mixture Rasch model with ordinal constraints,2002,Journal of Educational Measurement,39,4,,331,348,17,140,10.1111/j.1745-3984.2002.tb01146.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036905692&doi=10.1111%2fj.1745-3984.2002.tb01146.x&partnerID=40&md5=46a33f5df849a0ce3e5f211b6f778e7b,"When tests are administered under fixed time constraints, test performances can be affected by speededness. Among other consequences, speededness can result in inaccurate parameter estimates in item response theory (IRT) models, especially for items located near the end of tests (Oshima, 1994). This article presents an IRT strategy for reducing contamination in item difficulty estimates due to speededness. Ordinal constraints are applied to a mixture Rasch model (Rost, 1990) so as to distinguish two latent classes of examinees: (a) a ""speeded"" class, comprised of examinees that had insufficient time to adequately answer end-of-test items, and (b) a ""nonspeeded"" class, comprised of examinees that had sufficient time to answer all items. The parameter estimates obtained for end-of-test items in the nonspeeded class are shown to more accurately approximate their difficulties when the items are administered at earlier locations on a different form of the test. A mixture model can also be used to estimate the class memberships of individual examinees. In this way, it can be determined whether membership in the speeded class is associated with other student characteristics. Results are reported for gender and ethnicity.",,,Article,Final,,Scopus,2-s2.0-0036905692
Meijer R.R.,"Meijer, Rob R. (7006574564)",7006574564,Outlier detection in high-stakes certification testing,2002,Journal of Educational Measurement,39,3,,219,233,14,38,10.1111/j.1745-3984.2002.tb01175.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036727301&doi=10.1111%2fj.1745-3984.2002.tb01175.x&partnerID=40&md5=3fdab9b6d4f288a5bd32333b33c169ab,"Recent developments of person-fit analysis in computerized adaptive testing (CAT) are discussed. Methods from statistical process control are presented that have been proposed to classify an item score pattern as fitting or misfitting the underlying item response theory model in CAT. Most person-fit research in CAT is restricted to simulated data. In this study, empirical data from a certification test were used. Alternatives are discussed to generate norms so that bounds can be determined to classify an item score pattern as fitting or misfitting. Using bounds determined from a sample of a high-stakes certification test, the empirical analysis showed that different types of misfit can be distinguished. Further applications using statistical process control methods to detect misfitting item score patterns are discussed.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0036727301
Zenisky A.L.; Hambleton R.K.; Sireci S.G.,"Zenisky, April L. (56308805700); Hambleton, Ronald K. (7006242264); Sireci, Stephen G. (6701491909)",56308805700; 7006242264; 6701491909,Identification and evaluation of local item dependencies in the medical college admissions test,2002,Journal of Educational Measurement,39,4,,291,309,18,42,10.1111/j.1745-3984.2002.tb01144.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036902508&doi=10.1111%2fj.1745-3984.2002.tb01144.x&partnerID=40&md5=9c484a3ed4bfe2e71a2acdf5a0ddcd67,"Measurement specialists routinely assume examinee responses to test items are independent of one another. However, previous research has shown that many contemporary tests contain item dependencies and not accounting for these dependencies leads to misleading estimates of item, test, and ability parameters. The goals of the study were (a) to review methods for detecting local item dependence (LID), (b) to discuss the use of testlets to account for LID in context-dependent item sets, (c) to apply LID detection methods and testlet-based item calibrations to data from a large-scale, high-stakes admissions test, and (d) to evaluate the results with respect to test score reliability and examinee proficiency estimation. Item dependencies were found in the test and these were due to test speededness or context dependence (related to passage structure). Also, the results highlight that steps taken to correct for the presence of LID and obtain less biased reliability estimates may impact on the estimation of examinee proficiency. The practical effects of the presence of LID on passage-based tests are discussed, as are issues regarding how to calibrate context-dependent item sets using item response theory.",,,Article,Final,,Scopus,2-s2.0-0036902508
Walker C.M.; Beretvas S.N.,"Walker, Cindy M. (36874825300); Beretvas, S. Natasha (6603091715)",36874825300; 6603091715,An empirical investigation demonstrating the multidimensional DIF paradigm: A cognitive explanation for DIF,2001,Journal of Educational Measurement,38,2,,147,163,16,39,10.1111/j.1745-3984.2001.tb01120.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035536110&doi=10.1111%2fj.1745-3984.2001.tb01120.x&partnerID=40&md5=ffa79035ca99af3c9187a40e21d5266b,"Differential Item Functioning (DIF) is traditionally used to identify different item performance patterns between intact groups, most commonly involving race or sex comparisons. This study advocates expanding the utility of DIF as a step in construct validation. Rather than grouping examinees based on cultural differences, the reference and focal groups are chosen from two extremes along a distinct cognitive dimension that is hypothesized to supplement the dominant latent trait being measured. Specifically, this study investigates DIF between proficient and non-proficient fourth- and seventh-grade writers on open-ended mathematics test items that require students to communicate about mathematics. It is suggested that the occurrence of DIF in this situation actually enhances, rather than detracts from, the construct validity of the test because, according to the National Council of Teachers of Mathematics (NCTM), mathematical communication is an important component of mathematical ability, the dominant construct being assessed. However, the presence of DIF influences the validity of inferences that can be made from test scores and suggests that two scores should be reported, one for general mathematical ability and one for mathematical communication. The fact that currently only one test score is reported, a simple composite of scores on multiple-choice and open-ended items, may lead to incorrect decisions being made about examinees.",,,Article,Final,,Scopus,2-s2.0-0035536110
Buckendahl C.W.; Smith R.W.; Impara J.C.; Plake B.S.,"Buckendahl, Chad W. (6507243182); Smith, Russell W. (56142065400); Impara, James C. (6602233011); Plake, Barbara S. (6603689848)",6507243182; 56142065400; 6602233011; 6603689848,A comparison of Angoff and Bookmark standard setting methods,2002,Journal of Educational Measurement,39,3,,253,263,10,50,10.1111/j.1745-3984.2002.tb01177.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036729647&doi=10.1111%2fj.1745-3984.2002.tb01177.x&partnerID=40&md5=a3eed69f155df6d62492331da0dc1f24,"This article presents a comparison of simplified variations on two prevalent methods, Angoff and Bookmark, for setting cut scores on educational assessments. The comparison is presented through an application with a Grade 7 Mathematics Assessment in a midwestern school district. Training and operational methods and procedures for each method are described in detail along with comparative results for the application. An alternative item ordering strategy for the Bookmark method that may increase its usability is also introduced. Although the Angoff method is more widely used, the Bookmark method has some promising features, specifically in educational settings. Teachers are able to focus on the expected performance of the ""barely proficient"" student without the additional challenge of estimating absolute item difficulty.",,,Article,Final,,Scopus,2-s2.0-0036729647
Bielinski J.; Davison M.L.,"Bielinski, John (6603827737); Davison, Mark L. (7101919029)",6603827737; 7101919029,A sex difference by item difficulty interaction in multiple-choice mathematics items administered to national probability samples,2001,Journal of Educational Measurement,38,1,,51,77,26,18,10.1111/j.1745-3984.2001.tb01116.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035532030&doi=10.1111%2fj.1745-3984.2001.tb01116.x&partnerID=40&md5=01de7ba45512729cf1699a69fa88434d,"A 1998 study by Bielinski and Davison reported a sex difference by item difficulty interaction in which easy items tended to be easier for females than males, and hard items tended to be harder for females than males. To extend their research to nationally representative samples of students, this study used math achievement data from the 1992 NAEP, the TIMSS, and the NELS: 88. The data included students in grades 4, 8, 10, and 12. The interaction was assessed by correlating the item difficulty difference (bmale - bfemale) with item difficulty computed on the combined male/female sample. Using only the multiple-choice mathematics items, the predicted negative correlation was found for all eight populations and was significant in five. An argument is made that this phenomenon may help explain the greater variability in math achievement among males as compared to females and the emergence of higher performance of males in late adolescence.",,,Article,Final,,Scopus,2-s2.0-0035532030
Stocking M.L.; Lawrence I.; Feigenbaum M.; Jirele T.; Lewis C.; Van Essen T.,"Stocking, Martha L. (7006846128); Lawrence, Ida (7006099084); Feigenbaum, Miriam (6602462562); Jirele, Thomas (6507030084); Lewis, Charles (54389552600); Van Essen, Thomas (6602408475)",7006846128; 7006099084; 6602462562; 6507030084; 54389552600; 6602408475,An empirical investigation of impact moderation in test construction,2002,Journal of Educational Measurement,39,3,,235,252,17,1,10.1111/j.1745-3984.2002.tb01176.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036727265&doi=10.1111%2fj.1745-3984.2002.tb01176.x&partnerID=40&md5=df9acc7698cf34cb26c9dfaeef2c5482,"This investigation constructed four different kinds of test sections using three methods of test assembly that incorporate the goals of simultaneous moderation of three kinds of impact - gender impact, African-American impact, and Hispanic-American impact. The test sections were administered undetectably to random samples from the appropriate population. The results were evaluated by comparison of the characteristics of moderated sections with those of parallel operational sections. Almost all methods of test assembly produced either moderation of impact in the appropriate direction or no change in impact. Taking impact into account in test assembly tended to lower reliability slightly, raise concurrent validity slightly, and maintain the construct measured by the parallel operational section. It also reduced the relative efficiency for test takers in the middle score range while increasing efficiency for those with more extreme scores.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0036727265
Ma X.,"Ma, Xin (41762025300)",41762025300,Stability of school academic performance across subject areas,2001,Journal of Educational Measurement,38,1,,1,18,17,24,10.1111/j.1745-3984.2001.tb01114.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035532031&doi=10.1111%2fj.1745-3984.2001.tb01114.x&partnerID=40&md5=851971b8e45dd85dd7483a031f8c3fad,"Recent studies on the stability of school academic performance across school subject areas show two weaknesses: a lack of research attention to elementary schools and a lack of adequate statistical adjustments for school characteristics. With data describing elementary students (N = 6,883 students in Grade 6 in 148 schools) from the New Brunswick School Climate Study (NBSCS), the current study examined correlates of academic performance across mathematics, science, reading, and writing among students and among schools, using a multivariate multilevel model with statistical adjustments for student characteristics and school context and climate characteristics. Results indicated that (a) students were differentially successful in different subject areas, (b) schools were differentially effective in different subject areas, and (c) the differential success was more obvious among students than among schools. Findings of this study call for a new type of school programs that aim to ensure that students progress equally in different subject areas and for stronger school policies that systematically coordinate classroom or department practices.",,,Article,Final,,Scopus,2-s2.0-0035532031
Willingham W.W.; Pollack J.M.; Lewis C.,"Willingham, Warren W. (56633362200); Pollack, Judith M. (7101673359); Lewis, Charles (54389552600)",56633362200; 7101673359; 54389552600,Grades and test scores: Accounting for observed differences,2002,Journal of Educational Measurement,39,1,,1,37,36,168,10.1111/j.1745-3984.2002.tb01133.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005478&doi=10.1111%2fj.1745-3984.2002.tb01133.x&partnerID=40&md5=1a44bd696ebd4b3b4ac9b82658f39033,"Why do grades and test scores often differ? A framework of possible differences is proposed in this article. An approximation of the framework was tested with data on 8,454 high school seniors from the National Education Longitudinal Study. Individual and group differences in grade versus test performance were substantially reduced by focusing the two measures on similar academic subjects, correcting for grading variations and unreliability, and adding teacher ratings and other information about students. Concurrent prediction of high school average was thus increased from 0.62 to 0.90; differential prediction in eight subgroups was reduced to 0.02 letter-grades. Grading variation was a major source of discrepancy between grades and test scores. Other major sources were teacher ratings and Scholastic Engagement, a promising organizing principle for understanding student achievement. Engagement was defined by three types of observable behavior: employing school skills, demonstrating initiative, and avoiding competing activities. While groups varied in average achievement, group performance was generally similar on grades and tests. Major factors in achievement were similarly constituted and similarly related from group to group. Differences between grades and tests give these measures complementary strengths in high-stakes assessment. If artifactual differences between the two measures are not corrected, common statistical estimates of validity and fairness are unduly conservative.",,,Article,Final,,Scopus,2-s2.0-0036005478
Attali Y.; Fraenkel T.,"Attali, Yigal (7801686463); Fraenkel, Tamar (6506719617)",7801686463; 6506719617,The point-biserial as a discrimination index for distractors in multiple-choice items: Deficiencies in usage and an alternative,2000,Journal of Educational Measurement,37,1,,77,86,9,26,10.1111/j.1745-3984.2000.tb01077.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034336335&doi=10.1111%2fj.1745-3984.2000.tb01077.x&partnerID=40&md5=b7a679b35ef4b8f1d7dc92ffca7a64cc,We show that using the point-biserial as a discrimination index for distractors by differentiating between examinees who chose the distractor and examinees who did not choose the distractor is theoretically wrong and may lead to an incorrect rejection of items. We propose an alternative usage and present empirical evidence for its suitability.,,,Article,Final,,Scopus,2-s2.0-0034336335
Attali Y.; Bar-Hillel M.,"Attali, Yigal (7801686463); Bar-Hillel, Maya (6602143279)",7801686463; 6602143279,Guess where: The position of correct answers in multiple-choice test items as a psychometric variable,2003,Journal of Educational Measurement,40,2,,109,128,19,93,10.1111/j.1745-3984.2003.tb01099.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038373158&doi=10.1111%2fj.1745-3984.2003.tb01099.x&partnerID=40&md5=6234a41d003f4c283b7ed785dd5ad17a,"In this article, the authors show that test makers and test takers have a strong and systematic tendency for hiding correct answers - or, respectively, for seeking them - in middle positions. In single, isolated questions, both prefer middle positions to extreme ones in a ratio of up to 3 or 4 to 1. Because test makers routinely, deliberately, and excessively balance the answer key of operational tests, middle bias almost, though not quite, disappears in those keys. Examinees taking real tests also produce answer sequences that are more balanced than their single question tendencies but less balanced than the correct key. In a typical four-choice test, about 55% of erroneous answers are in the two central positions. The authors show that this bias is large enough to have real psychometric consequences, as questions with middle correct answers are easier and less discriminating than questions with extreme correct answers, a fact of which some implications are explored.",,,Article,Final,,Scopus,2-s2.0-0038373158
Kane M.,"Kane, Michael (36088969800)",36088969800,Inferences about variance components and reliability-generalizability coefficients in the absence of random sampling,2002,Journal of Educational Measurement,39,2,,165,181,16,20,10.1111/j.1745-3984.2002.tb01141.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036014613&doi=10.1111%2fj.1745-3984.2002.tb01141.x&partnerID=40&md5=a3e68e476ea35d7ab70e89dadcb832ce,"Generalizability theory (G theory) employs random-effects ANOVA to estimate the variance components included in generalizability coefficients, standard errors, and other indices of precision. The ANOVA models depend on random sampling assumptions, and the variance-component estimates are likely to be sensitive to violations of these assumptions. Yet, generalizability studies do not typically sample randomly. This kind of inconsistency between assumptions in statistical models and actual data collection procedures is not uncommon in science, but it does raise fundamental questions about the substantive inferences based on the statistical analyses. This article reviews criticisms of sampling assumptions in G theory (and in reliability theory) and examines the feasibility of using representative sampling, stratification, homogeneity assumptions, and replications to address these criticisms.",,,Article,Final,,Scopus,2-s2.0-0036014613
Lee G.,"Lee, Guemin (7404851825)",7404851825,The influence of several factors on reliability for complex reading comprehension tests,2002,Journal of Educational Measurement,39,2,,149,164,15,6,10.1111/j.1745-3984.2002.tb01140.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036014617&doi=10.1111%2fj.1745-3984.2002.tb01140.x&partnerID=40&md5=1f4965e200d7c445a10a898bcabfb9b6,"The purpose of this study was to investigate the effects of items, passages, contents, themes, and types of passages on the reliability and standard errors of measurement for complex reading comprehension tests. Seven different generalizability theory models were used in the analyses. Results indicated that generalizability coefficients estimated using multivariate models incorporating content strata and types of passages were similar in size to reliability estimates based upon a model that did not include these factors. In contrast, incorporating passages and themes within univariate generalizability theory models produced non-negligible differences in the reliability estimates. This suggested that passages and themes be taken into account when evaluating the reliability of test scores for complex reading comprehension tests.",,,Article,Final,,Scopus,2-s2.0-0036014617
Yu F.; Nandakumar R.,"Yu, Feng (37032391300); Nandakumar, Ratna (7005999386)",37032391300; 7005999386,Poly-detect for quantifying the degree of multidimensionality of item response data,2001,Journal of Educational Measurement,38,2,,99,120,21,7,10.1111/j.1745-3984.2001.tb01118.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035536106&doi=10.1111%2fj.1745-3984.2001.tb01118.x&partnerID=40&md5=da0efb5f2f3a30a9611c9aa0df3f027c,"This study investigated the validity of the Poly-Detect procedure to determine the approximate simple dimensionality structure and to quantify the degree of multidimensionality present in polytomous data through simulation and real data analysis. Both unidimensional and two-dimensional data were generated in the simulation study for the complete and the BIB data. The simulation results showed that the dimensionality structures were similar between the corresponding complete data and the BIB data as identified by the Poly-Detect procedure. Also, the simulation results indicated that the number of response categories of items did not affect the performance of the Poly-Detect procedure in quantifying the degree of multidimensionality of polytomous data. Based on the simulation results, a subjective scale of the degree of multidimensionality of a data set was developed. Application of the Poly-Detect procedure on the 1992 NAEP eighth-grade reading data indicated that the degree of multidimensionality of the entire 1992 NAEP eighth-grade reading item pool was moderate at most.",,,Article,Final,,Scopus,2-s2.0-0035536106
Ban J.-E.; Hanson B.A.; Wang T.; Yi Q.; Harris D.J.,"Ban, Jae-Chun (7006808582); Hanson, Bradley A. (7102036381); Wang, Tianyou (55709761000); Yi, Qing (23092639000); Harris, Deborah J. (7403921256)",7006808582; 7102036381; 55709761000; 23092639000; 7403921256,A comparative study of on-line pretest item-calibration/scaling methods in computerized adaptive testing,2001,Journal of Educational Measurement,38,3,,191,212,21,44,10.1111/j.1745-3984.2001.tb01123.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035438301&doi=10.1111%2fj.1745-3984.2001.tb01123.x&partnerID=40&md5=0862090bd8d2ed709502dd752c4e0e9b,"The purpose of this study was to compare and evaluate five on-line pretest item-calibration/scaling methods in computerized adaptive testing (CAT): marginal maximum likelihood estimate with one EM cycle (OEM), marginal maximum likelihood estimate with multiple EM cycles (MEM), Stocking's Method A, Stocking's Method B, and BILOG/Prior. The five methods were evaluated in terms of item-parameter recovery, using three different sample sizes (300, 1000 and 3000). The MEM method appeared to be the best choice among these, because it produced the smallest parameter-estimation errors for all sample size conditions. MEM and OEM are mathematically similar, although the OEM method produced larger errors. MEM also was preferable to OEM, unless the amount of time involved in iterative computation is a concern. Stocking's Method B also worked very well, but it required anchor items that either would increase test lengths or require larger sample sizes depending on test administration design. Until more appropriate ways of handling sparse data are devised, the BILOG/Prior method may not be a reasonable choice for small sample sizes. Stocking's Method A had the largest weighted total error, as well as a theoretical weakness (i.e., treating estimated ability as true ability); thus, there appeared to be little reason to use it.",,,Article,Final,,Scopus,2-s2.0-0035438301
Oshima T.C.; Davey T.C.; Lee K.,"Oshima, T.C. (55672664200); Davey, Tim C. (36785089100); Lee, K. (59044643500)",55672664200; 36785089100; 59044643500,Multidimensional linking: Four practical approaches,2000,Journal of Educational Measurement,37,4,,357,373,16,16,10.1111/j.1745-3984.2000.tb01092.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034337090&doi=10.1111%2fj.1745-3984.2000.tb01092.x&partnerID=40&md5=20e6cca556c3cc7eeb739211b4c95e15,"This paper introduces and evaluates four practical multidimensional linking procedures that are based on the theoretical framework recently proposed by Davey, Oshima, and Lee (1996): (a) the Direct method, (b) the Equated Function method, (c) the Test Characteristic Function (TCF) method, and (d) the Item Characteristic Function (ICF) method. The evaluation was conducted using simulated data. As anticipated, the competing procedures yielded different linking parameter estimates. The TCF and ICF methods were found to be more stable and recovered the true linking parameters better than the other two methods. Furthermore, all procedures were found to be acceptable under almost any of the minimization criteria and offered dramatic improvement over not linking at all. It is recommended that the choice of a linking procedure should depend on the purpose of linking.",,,Article,Final,,Scopus,2-s2.0-0034337090
De Ayala R.J.; Plake B.S.; Impara J.C.,"De Ayala, R.J. (6602251247); Plake, Barbara S. (6603689848); Impara, James C. (6602233011)",6602251247; 6603689848; 6602233011,The impact of omitted responses on the accuracy of ability estimation in item response theory,2001,Journal of Educational Measurement,38,3,,213,234,21,51,10.1111/j.1745-3984.2001.tb01124.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035438302&doi=10.1111%2fj.1745-3984.2001.tb01124.x&partnerID=40&md5=2a992b53b69211f7fd987b2367c41132,"Practitioners typically face situations in which examinees have not responded to all test items. This study investigated the effect on an examinee's ability estimate when an examinee is presented an item, has ample time to answer, but decides not to respond to the item. Three approaches to ability estimation (biweight estimation, expected a posteriori, and maximum likelihood estimation) were examined. A Monte Carlo study was performed and the effect of different levels of omissions on the simulee's ability estimates was determined. Results showed that the worst estimation occurred when omits were treated as incorrect. In contrast, substitution of 0.5 for omitted responses resulted in ability estimates that were almost as accurate as those using complete data. Implications for practitioners are discussed.",,,Article,Final,,Scopus,2-s2.0-0035438302
Gallagher A.; Bridgeman B.; Cahalan C.,"Gallagher, Ann (17834461300); Bridgeman, Brent (7005526936); Cahalan, Cara (6602212205)",17834461300; 7005526936; 6602212205,The effect of computer-based tests on racial-ethnic and gender groups,2002,Journal of Educational Measurement,39,2,,133,147,14,37,10.1111/j.1745-3984.2002.tb01139.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036014610&doi=10.1111%2fj.1745-3984.2002.tb01139.x&partnerID=40&md5=c2a6d7b386fb2112666f494fa9631277,"In this study data were examined from several national testing programs to determine whether the change from paper-based administration to computer-based tests (CBTs) influences group differences in performance. Performances by gender, racial, and ethnic groups on the Graduate Record Examination General Test, Graduate Management Admissions Test, SAT I: Reasoning Test, and Praxis: Professional Assessment for Beginning Teachers, were analyzed to determine whether the shift in testing format from paper-and-pencil tests to CBTs posed a disadvantage to any of these subgroups, beyond that already identified for paper-based test. Although all differences were quite small, some consistent patterns were found for some racial-ethnic and gender groups. African-American examinees and, to a lesser degree, Hispanic examinees appear to benefit from the CBT format. On some tests, female examinees' performance was relatively lower on the CBT version.",,,Article,Final,,Scopus,2-s2.0-0036014610
Bolt D.M.,"Bolt, Daniel M. (7006174434)",7006174434,A SIBTEST approach to testing DIF hypotheses using experimentally designed test items,2000,Journal of Educational Measurement,37,4,,307,327,20,17,10.1111/j.1745-3984.2000.tb01089.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034337095&doi=10.1111%2fj.1745-3984.2000.tb01089.x&partnerID=40&md5=c9be332437f0ad3232adc6c0fd9f27cd,"This paper considers a modification of the DIF procedure SIBTEST for investigating the causes of differential item functioning (DIF). One way in which factors believed to be responsible for DIF can be investigated is by systematically manipulating them across multiple versions of an item using a randomized DIF study (Schmitt, Holland, & Dorans, 1993). In this paper, it is shown that the additivity of the index used for testing DIF in SIBTEST motivates a new extension of the method for statistically testing the effects of DIF factors. Because an important consideration is whether or not a studied DIF factor is consistent in its effects across items, a methodology for testing item × factor interactions is also presented. Using data from the mathematical sections of the Scholastic Assessment Test (SAT), the effects of two potential DIP factors - item formal (multiple-choice versus open-ended) and problem type (abstract versus concrete) - are investigated for gender. Results suggest a small but statistically significant and consistent effect of item format (favoring males for multiple-choice items) across items, and a larger but less consistent effect due to problem type.",,,Article,Final,,Scopus,2-s2.0-0034337095
Hoskens M.; Wilson M.,"Hoskens, Machteld (6603649497); Wilson, Mark (55547134983)",6603649497; 55547134983,Real-time feedback on rater drift in constructed-response items: An example from the golden state examination,2001,Journal of Educational Measurement,38,2,,121,145,24,44,10.1111/j.1745-3984.2001.tb01119.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035536108&doi=10.1111%2fj.1745-3984.2001.tb01119.x&partnerID=40&md5=a79bf606a33f33449a4808fa9464d0fe,"In this study, patterns of variation in severities of a group of raters over time or so-called ""rater drift"" was examined when raters scored an essay written under examination conditions. At the same time feedback was given to rater leaders (called ""table leaders"") who then interpreted the feedback and reported to the raters. Rater severities in five successive periods were estimated using a modified linear logistic test model (LLTM, Fischer, 1973) approach. It was found that the raters did indeed drift towards the mean, but a planned comparision of the feedback with a control condition was not successful; it was believed that this was due to contamination at the table leader level. A series of models was also estimated designed to detect other types of rater effects beyond severity: a tendency to use extreme scores, and tendency to prefer certain categories. The models for these effects were found to be showing significant improvement in fit, implying that these effects were indeed present, although they were difficult to detect in relatively short time periods.",,,Article,Final,,Scopus,2-s2.0-0035536108
Kamata A.,"Kamata, Akihito (7003700792)",7003700792,Item analysis by the hierarchical generalized linear model,2001,Journal of Educational Measurement,38,1,,79,93,14,180,10.1111/j.1745-3984.2001.tb01117.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035532029&doi=10.1111%2fj.1745-3984.2001.tb01117.x&partnerID=40&md5=3cac96d48184c6d9639db8191e38a9f3,"The hierarchical generalized linear model (HGLM) is presented as an explicit, two-level formulation of a multilevel item response model. In this paper, it is shown that the HGLM is equivalent to the Rasch model and that, characteristic of the HGLM, person ability can be expressed in the form of random effects rather than parameters. The two-level item analysis model is presented as a latent regression model with person-characteristic variables. Furthermore, it is shown that the two-level HGLM model can be extended to a three-level latent regression model that permits investigation of the variation of students' performance across groups, such as is found in classrooms and schools, and of the interactive effect of person-and group-characteristic variables.",,,Article,Final,,Scopus,2-s2.0-0035532029
Yi Q.; Wang T.; Ban J.-C.,"Yi, Qing (23092639000); Wang, Tianyou (55709761000); Ban, Jae-Chun (7006808582)",23092639000; 55709761000; 7006808582,Effects of scale transformation and test-termination rule on the precision of ability estimation in computerized adaptive testing,2001,Journal of Educational Measurement,38,3,,267,292,25,11,10.1111/j.1745-3984.2001.tb01127.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035438305&doi=10.1111%2fj.1745-3984.2001.tb01127.x&partnerID=40&md5=71fd2b2698ec884f7c306e45faf31ad8,"Error indices (bias, standard error of estimation, and root mean squared error) obtained on different measurement scales under different test-termination rules in computerized adaptive testing (CAT) were examined. Four ability estimation methods (maximum likelihood estimation, weighted likelihood estimation, expected a posterior, and maximum a posterior), three measurement scales (θ, number-correct score, and ACT score), and three test-termination rules (fixed length, fixed standard error, and target information) were studied for a real and a generated item pool. The findings indicated that the amount and direction of bias, standard error of estimation, and root mean squared error obtained under different ability estimation methods were influenced both by scale transformations and by test-termination rules in a CAT environment. The implications of these effects for testing programs are discussed.",,,Article,Final,,Scopus,2-s2.0-0035438305
Kane M.T.,"Kane, Michael T. (36088969800)",36088969800,Current concerns in validity theory,2001,Journal of Educational Measurement,38,4,,319,342,23,466,10.1111/j.1745-3984.2001.tb01130.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035561276&doi=10.1111%2fj.1745-3984.2001.tb01130.x&partnerID=40&md5=7a7d497b8a6a8a44fdc34e58ab97c8ba,"We are at the end of the first century of work on models of educational and psychological measurement and into a new millennium. This certainly seems like an appropriate time for looking backward and looking forward in assessment. Furthermore, a new edition of the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 1999) has been published, and the previous editions of the Standards have served as benchmarks in the development of measurement theory. This backward glance will be just that, a glance. After a brief historical review focusing mainly on construct validity, the current state of validity theory will be summarized, with an emphasis on the role of arguments in validation. Then how an argument-based approach might be applied will be examined in regards to two issues in validity theory: the distinction between performance-based and theory-based interpretations, and the role of consequences in validation.",,,Article,Final,,Scopus,2-s2.0-0035561276
Pitkin A.K.; Vispoel W.P.,"Pitkin, Angela K. (6701745953); Vispoel, Walter P. (6603886282)",6701745953; 6603886282,Differences between self-adapted and computerized adaptive tests: A meta-analysis,2001,Journal of Educational Measurement,38,3,,235,247,12,13,10.1111/j.1745-3984.2001.tb01125.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035438303&doi=10.1111%2fj.1745-3984.2001.tb01125.x&partnerID=40&md5=09d17a9a867081ea45bac40ee77d2384,"Self-adapted testing has been described as a variation of computerized adaptive testing that reduces test anxiety and thereby enhances test performance. The purpose of this study was to gain a better understanding of these proposed effects of self-adapted tests (SATs); meta-analysis procedures were used to estimate differences between SATs and computerized adaptive tests (CATs) in proficiency estimates and post-test anxiety levels across studies in which these two types of tests have been compared. After controlling for measurement error, the results showed that SATs yielded proficiency estimates that were 0.12 standard deviation units higher and post-test anxiety levels that were 0.19 standard deviation units lower than those yielded by CATs. We speculate about possible reasons for these differences and discuss advantages and disadvantages of using SATs in operational settings.",,,Article,Final,,Scopus,2-s2.0-0035438303
Hau K.-T.; Chang H.-H.,"Hau, Kit-Tai (7006812724); Chang, Hua-Hua (7407524642)",7006812724; 7407524642,Item selection in computerized adaptive testing: Should more discriminating be used first?,2001,Journal of Educational Measurement,38,3,,249,266,17,34,10.1111/j.1745-3984.2001.tb01126.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035438304&doi=10.1111%2fj.1745-3984.2001.tb01126.x&partnerID=40&md5=470f15673cdd1622f4238eeaae750fa2,"During computerized adaptive testing (CAT), items are selected continuously according to the test-taker's estimated ability. The traditional method of attaining the highest efficiency in ability estimation is to select items of maximum Fisher information at the currently estimated ability. Test security has become a problem because high-discrimination items are more likely to be selected and become overexposed. So, there seems to be a tradeoff between high efficiency in ability estimations and balanced usage of items. This series of four studies with simulated data addressed the dilemma by focusing on the notion of whether more or less discriminating items should be used first in CAT. The first study demonstrated that the common maximum information method with Sympson and Hetter (1985) control resulted in the use of more discriminating items first. The remaining studies showed that using items in the reverse order (i.e., less discriminating items first), as described in Chang and Ying's (1999) stratified method had potential advantages: (a) a more balanced item usage and (b) a relatively stable resultant item pool structure with easy and inexpensive management: This stratified method may have ability-estimation efficiency better than or close to that of other methods, particularly for operational item pools when retired items cannot be totally replenished with similar highly discriminating items. It is argued that the judicious selection of items, as in the stratified method, is a more active control of item exposure, which can successfully even out the usage of all items.",,,Article,Final,,Scopus,2-s2.0-0035438304
Clauser B.E.; Swanson D.B.; Harik P.,"Clauser, Brian E. (7003595460); Swanson, David B. (7201859959); Harik, Polina (6506596303)",7003595460; 7201859959; 6506596303,Multivariate generalizability analysis of the impact of training and examinee performance information on judgments made in an Angoff-style standard-setting procedure,2002,Journal of Educational Measurement,39,4,,269,290,21,31,10.1111/j.1745-3984.2002.tb01143.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036903844&doi=10.1111%2fj.1745-3984.2002.tb01143.x&partnerID=40&md5=bb9871dd26873679f3c328a1b4a34599,"Cut scores, estimated using the Angoff procedure, are routinely used to make high-stakes classification decisions based on examinee scores. Precision is necessary in estimation of cut scores because of the importance of these decisions. Although much has been written about how these procedures should be implemented, there is relatively little literature providing empirical support for specific approaches to providing training and feedback to standard-setting judges. This article presents a multivariate generalizability analysis designed to examine the impact of training and feedback on various sources of error in estimation of cut scores for a standard-setting procedure in which multiple independent groups completed the judgments. The results indicate that after training, there was little improvement in the ability of judges to rank order items by difficulty but there was a substantial improvement in inter-judge consistency in centering ratings. The results also show a substantial group effect. Consistent with this result, the direction of change for the estimated cut score was shown to be group dependent.",,,Article,Final,,Scopus,2-s2.0-0036903844
Gierl M.J.; Khaliq S.N.,"Gierl, Mark J. (6701316189); Khaliq, Shameem Nyla (7004599110)",6701316189; 7004599110,Identifying sources of differential item and bundle functioning on translated achievement tests: A confirmatory analysis,2001,Journal of Educational Measurement,38,2,,164,187,23,91,10.1111/j.1745-3984.2001.tb01121.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035373504&doi=10.1111%2fj.1745-3984.2001.tb01121.x&partnerID=40&md5=7fe967e9d2eaadc948e91e7c7d1006cc,"Increasingly, tests are being translated and adapted into different languages. Differential item functioning (DIF) analyses are often used to identify non-equivalent items across language groups. However, few studies have focused on understanding why some translated items produce DIF. The purpose of the current study is to identify sources of differential item and bundle functioning on translated achievement tests using substantive and statistical analyses. A substantive analysis of existing DIF items was conducted by an 11-member committee of testing specialists. In their review, four sources of translation DIF were identified. Two certified translators used these four sources to categorize a new set of DIF items from Grade 6 and 9 Mathematics and Social Studies Achievement Tests. Each item was associated with a specific source of translation DIF and each item was anticipated to favor a specific group of examinees. Then, a statistical analysis was conducted on the items in each category using SIBTEST. The translators sorted the mathematics DIF items into three sources, and they correctly predicted the group that would he favored for seven of the eight items or bundles of items across two grade levels. The translators sorted the social studies DIF items into four sources, and they correctly predicted the group that would be favored for eight of the 13 items or bundles of items across two grade levels. The majority of items in mathematics and social studies were associated with differences in the words, expressions, or sentence structure of items that are not inherent to the language and/or culture. By combining substantive and statistical DIF analyses, researchers can study the sources of DIF and create a body of confirmed DIF hypotheses that may be used to develop guidelines and test construction principles for reducing DIF on translated tests.",,,Article,Final,,Scopus,2-s2.0-0035373504
Vispoel W.P.; Hendrickson A.B.; Bleiler T.,"Vispoel, Walter P. (6603886282); Hendrickson, Amy B. (7006207129); Bleiler, Timothy (6603213880)",6603886282; 7006207129; 6603213880,Limiting answer review and change on computerized adaptive vocabulary tests: Psychometric and attitudinal results,2000,Journal of Educational Measurement,37,1,,21,38,17,22,10.1111/j.1745-3984.2000.tb01074.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034146413&doi=10.1111%2fj.1745-3984.2000.tb01074.x&partnerID=40&md5=cf6b0e76c8431fefc79c3f16bdef2794,"Previous simulation studies of computerized adaptive tests (CATs) have revealed that the validity and precision of proficiency estimates can be maintained when review opportunities are limited to items within successive blocks. Our purpose in this study was to evaluate the effectiveness of CATs with such restricted review options in a live testing setting. Vocabulary CATs were compared under four conditions: (a) no item review allowed, (b) review allowed only within successive 5-item blocks, (c) review allowed only within successive 10-item blocks, and (d) review allowed only after answering all 40 items. Results revealed no trustworthy differences among conditions in vocabulary proficiency estimates, measurement error, or testing time. Within each review condition, ability estimates and number correct scores increased slightly after review, more answers were changed from wrong to right than from right to wrong, most examinees who changed answers improved proficiency estimates by doing so, and nearly all examinees indicated that they had an adequate opportunity to review their previous answers. These results suggest that restricting review opportunities on CATs may provide a viable way to satisfy examinee desires, maintain validity and measurement precision, and keep testing time at acceptable levels.",,,Article,Final,,Scopus,2-s2.0-0034146413
Caruso J.C.; Witkiewitz K.,"Caruso, John C. (7101691430); Witkiewitz, Katie (8773671600)",7101691430; 8773671600,Increasing the reliability of ability-achievement difference scores: An example using the Kaufman assessment battery for children,2002,Journal of Educational Measurement,39,1,,39,58,19,4,10.1111/j.1745-3984.2002.tb01134.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005464&doi=10.1111%2fj.1745-3984.2002.tb01134.x&partnerID=40&md5=54cd1c619a744966c932ae0c823cb292,"In this study, we focused on increasing the reliability of ability-achievement difference scores using the Kaufman Assessment Battery for Children (KABC) as an example. Ability-achievement difference scores are often used as indicators of learning disabilities, but when they are derived from traditional equally weighted ability and achievement scores, they have suboptimal psychometric properties because of the high correlations between the scores. As an alternative to equally weighted difference scores, we examined an orthogonal reliable component analysis (RCA) solution and an oblique principal component analysis (PCA) solution for the standardization sample of the KABC (among 5- to 12-year-olds). The components were easily identifiable as the simultaneous processing, sequential processing, and achievement constructs assessed by the KABC. As judged via the score intercorrelations, all three types of scores had adequate convergent validity, while the orthogonal RCA scores had superior discriminant validity, followed by the oblique PCA scores. Differences between the orthogonal RCA scores were more reliable than differences between the oblique PCA scores, which were in turn more reliable than differences between the traditional equally weighted scores. The increased reliability with which the KABC differences are assessed with the orthogonal RCA method has important practical implications, including narrower confidence intervals around difference scores used in individual administrations of the KABC.",,,Article,Final,,Scopus,2-s2.0-0036005464
Clauser B.E.; Harik P.; Clyman S.G.,"Clauser, Brian E. (7003595460); Harik, Polina (6506596303); Clyman, Stephen G. (6603827946)",7003595460; 6506596303; 6603827946,The generalizability of scores for a performance assessment scored with a computer-automated scoring system,2000,Journal of Educational Measurement,37,3,,245,261,16,29,10.1111/j.1745-3984.2000.tb01085.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034257275&doi=10.1111%2fj.1745-3984.2000.tb01085.x&partnerID=40&md5=f78cf09671c70b0d191dcbbc8478a787,"When performance assessments are delivered and scored by computer, the costs of scoring may be substantially lower than those of scoring the same assessment based on expert review of the individual performances. Computerized scoring algorithms also ensure that the scoring rules are implemented precisely and uniformly. Such computerized algorithms represent an effort to encode the scoring policies of experts. This raises the question, would a different group of experts have produced a meaningfully different algorithm? The research reported in this paper uses generalizability theory to assess the impact of using independent, randomly equivalent groups of experts to develop the scoring algorithms for a set of computer-simulation tasks designed to measure physicians' patient management skills. The results suggest that the impact of this ""expert group"" effect may be significant but that it can be controlled with appropriate test development strategies. The appendix presents multivariate generalizability analysis to examine the stability of the assessed proficiency across scores representing the scoring policies of different groups of experts.",,,Article,Final,,Scopus,2-s2.0-0034257275
Spray J.A.; Huang C.-Y.,"Spray, Judith A. (7006830106); Huang, Chi-Yu (7406887458)",7006830106; 7406887458,Obtaining test blueprint weights from job analysis surveys,2000,Journal of Educational Measurement,37,3,,187,201,14,14,10.1111/j.1745-3984.2000.tb01082.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034257276&doi=10.1111%2fj.1745-3984.2000.tb01082.x&partnerID=40&md5=a8054115ebb86e218c1241663171081f,"A method for combining multiple scale responses from job or task surveys based on a hierarchical ranking scheme is presented. A rationale for placing the resulting ordinal information onto an interval scale of measurement using the Rasch Rating Scale Model is also provided. After a simple linear transformation, the item or task parameter estimates can be used to obtain item weights to be used in constructing test blueprints. Prior weights can then be used to modify the item weights after data collection, based either on content balancing requirements or Bayesian prior content weights from SMEs (subject matter experts). Finally, a method is suggested to link two or more surveys, again using the Rasch Rating Scale Model and the computer program, Bigsteps, when it is desirable to shorten the length of the typical job or task survey.",,,Article,Final,,Scopus,2-s2.0-0034257276
Sykes R.C.; Yen W.M.,"Sykes, Robert C. (7101991229); Yen, Wendy M. (7102684621)",7101991229; 7102684621,The scaling of mixed-item-format tests with the one-parameter and two-parameter partial credit models,2000,Journal of Educational Measurement,37,3,,221,244,23,10,10.1111/j.1745-3984.2000.tb01084.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034257274&doi=10.1111%2fj.1745-3984.2000.tb01084.x&partnerID=40&md5=07fff436778026289ebce41a00716a7d,"Item response theory scalings were conducted for six tests with mixed item formats. These tests differed in their proportions of constructed response (c.r.) and multiple choice (m.c.) items and in overall difficulty. The scalings included those based on scores for the c.r. items that had maintained the number of levels as the item rubrics, either produced from single ratings or multiple ratings that were averaged and rounded to the nearest integer, as well as scalings for a single form of c.r. items obtained by summing multiple ratings. A one-parameter (1PPC) or two-parameter (2PPC) partial credit model was used for the c.r. items and the one-parameter logistic (1PL) or three-parameter logistic (3PL) model for the m.c. items. Item fit was substantially worse with the combination 1PL/1PPC model than the 3PL/2PPC model due to the former's restrictive assumptions that there would be no guessing on the m.c. items and equal item discrimination across items and item types. The presence of varying item discriminations resulted in the 1PL/1PPC model producing estimates of item information that could be spuriously inflated for c.r. items that had three or more score levels. Information for some items with summed ratings were usually overestimated by 300% or more for the 1PL/1PPC model. These inflated information values resulted in under-estimated standard errors of ability estimates. The constraints posed by the restricted model suggests limitations on the testing contexts in which the 1PL/1PPC model can be accurately applied.",,,Article,Final,,Scopus,2-s2.0-0034257274
Sotaridona L.S.; Meijer R.R.,"Sotaridona, Leonardo S. (6506883239); Meijer, Rob R. (7006574564)",6506883239; 7006574564,Statistical properties of the K-index for detecting answer copying,2002,Journal of Educational Measurement,39,2,,115,132,17,40,10.1111/j.1745-3984.2002.tb01138.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036014603&doi=10.1111%2fj.1745-3984.2002.tb01138.x&partnerID=40&md5=485db2d6fd45df35077ffe6784303a9d,"We investigated the statistical properties of the K-index (Holland, 1996) that can be used to detect copying behavior on a test. A simulation study was conducted to investigate the applicability of the K-index for small, medium, and large datasets. Furthermore, the Type I error rate and the detection rate of this index were compared with the copying index, ω (Wollack, 1997). Several approximations were used to calculate the K-index. Results showed that all approximations were able to hold the Type I error rates below the nominal level. Results further showed that using co resulted in higher detection rates than the K-indices for small and medium sample sizes (100 and 500 simulees).",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0036014603
Plake B.S.; Impara J.C.; Irwin P.M.,"Plake, Barbara S. (6603689848); Impara, James C. (6602233011); Irwin, Patrick M. (7101824713)",6603689848; 6602233011; 7101824713,Consistency of Angoff-based predictions of item performance: Evidence of technical quality of results from the angoff standard setting method,2000,Journal of Educational Measurement,37,4,,347,355,8,9,10.1111/j.1745-3984.2000.tb01091.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034337088&doi=10.1111%2fj.1745-3984.2000.tb01091.x&partnerID=40&md5=7d48032d3ee0122cfed6fa4fa257a3eb,"Judgmental standard-setting methods, such as the Angoff (1971) method, use item performance estimates as the basis for determining the minimum passing score (MPS).Therefore, the accuracy of these item performance estimates is crucial to the validity of the resulting MPS. Recent researchers (Shepard, 1995: Impara & Plake, 1998; National Research Council, 1999) have called into question the ability of judges to make accurate item performance estimates for target subgroups of candidates, such as minimally competent candidates. The purpose of this study was to examine the intra-and inter-rater consistency of item performance estimates from an Angoff standard setting. Results provide evidence that item perforperformance estimates were consistent within and across panels within and across years. Factors that might have influenced this high degree of reliability in the item performance estimates in a standard setting study are discussed.",,,Article,Final,,Scopus,2-s2.0-0034337088
Cohen J.; Snow S.,"Cohen, Jon (55475147800); Snow, Stephanie (7006271942)",55475147800; 7006271942,Impact of changing difficulty on inferences from the national assessment of educational progress,2002,Journal of Educational Measurement,39,2,,91,114,23,0,10.1111/j.1745-3984.2002.tb01137.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036014602&doi=10.1111%2fj.1745-3984.2002.tb01137.x&partnerID=40&md5=f162f319d314108a2f73747da260e913,"The U.S. Department of Education measures student achievement through the National Assessment of Educational Progress (NAEP). NAEP estimates of population proficiency quantiles are based on a Bayesian multiple-imputation procedure. This article shows (a) that the resulting estimates depend directly on the mix of item difficulties on the test, and (b) the difficulty of items on the NAEP mathematics exam has increased over time. Does the increasing difficulty of the exam lead to observable changes in student performance over time? This study compared the simulated performance of 1990 examinees on the easier 1990 exam and the more difficult 1996 exam. No significant differences were found. While our results instill confidence that these changes have not impacted the NAEP trend line, our findings are both data-specific and limited in scope, and NAEP should carefully evaluate future adjustments to the test in this manner.",,,Article,Final,,Scopus,2-s2.0-0036014602
Wainer H.; Sheehan K.M.; Wang X.,"Wainer, Howard (7006218234); Sheehan, Kathleen M. (7005727437); Wang, Xiaohui (57219369196)",7006218234; 7005727437; 57219369196,Some paths toward making praxis scores more useful,2000,Journal of Educational Measurement,37,2,,113,140,27,39,10.1111/j.1745-3984.2000.tb01079.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034195155&doi=10.1111%2fj.1745-3984.2000.tb01079.x&partnerID=40&md5=fa26469def5bf842983e111094b0ade9,"In this study we describe an analytic method for aiding in the generation of subscales that characterize the deep structure of tests. In addition we also derive a procedure for estimating scores for these scales that are much more statistically stable than subscores computed solely from the items that are contained on that scale. These scores achieve their stability through augmentation with information from other related information on the test. These methods were used to complement each other on a data set obtained from a Praxis administration. We found that the deep structure of the test yielded ten subscales and that, because the test was essentially unidimensional, ten subscores could be computed, all with very high reliability. This result was contrasted with the calculation of six traditional subscales based on surface features of the items. These subscales also yielded augmented subscores of high reliability.",,,Article,Final,,Scopus,2-s2.0-0034195155
Dorans N.J.; Holland P.W.,"Dorans, Neil J. (6602289148); Holland, Paul W. (7202792268)",6602289148; 7202792268,Population invariance and the equatability of tests: Basic theory and the linear case,2000,Journal of Educational Measurement,37,4,,281,306,25,189,10.1111/j.1745-3984.2000.tb01088.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034337086&doi=10.1111%2fj.1745-3984.2000.tb01088.x&partnerID=40&md5=2acce36c4bfa3db4d4850ec5d34d0a90,"How does the fact that two tests should not be equated manifest itself? This paper addresses this question through the study of the degree to which equating functions fail to exhibit population invariance across subpopulations. Equating functions are supposed to be population invariant by definition. But, when two tests are not equatable, it is possible that the linking functions, used to connect the scores of one to the scores of the other, are not invariant across different populations of examinees. While no acceptable equating function is ever completely population invariant, in the situations where equating is usually performed we believe that the dependence of the equating function on the population used to compute it is usually small enough to be ignored. We introduce two root-mean-square difference measures of the degree to which the functions used to link two tests computed on different subpopulations differ from the linking function computed for the whole population. We also introduce the system of ""parallel-linear"" linking functions for multiple subpopulations and show that, for this system, our measure of population invariance can be computed easily from the standardized mean differences between the scores of the subpopulations on the two tests. For the parallel-linear case, we develop a correlation-based upper bound on our measure that holds for all systems of subpopulations. We illustrate these ideas using data from the SAT I and from a concordance study of several combinations of ACT and SAT I scores. In the appendices, we give some theoretical results bearing on the other equating ""requirements"" of ""same construct,"" ""same reliability"" and one aspect of Lord's concept of equity.",,,Article,Final,,Scopus,2-s2.0-0034337086
Congdon P.J.; McQueen J.,"Congdon, Peter J. (36875104000); McQueen, Joy (7006882913)",36875104000; 7006882913,The stability of rater severity in large-scale assessment programs,2000,Journal of Educational Measurement,37,2,,163,178,15,83,10.1111/j.1745-3984.2000.tb01081.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034195156&doi=10.1111%2fj.1745-3984.2000.tb01081.x&partnerID=40&md5=b7b71b62891c0895272278412bd26e25,"The purpose of this study was to investigate the stability of rater severity over an extended rating period. Multifaceted Rasch analysis was applied to ratings of 16 raters on writing performances of 8,285 elementary school students. Each performance was rated by two trained raters over a period of seven rating days. Performances rated on the first day were re-rated at the end of the rating period. Statistically significant differences between raters were found within each day and in all days combined. Daily estimates of the relative severity of individual raters were found to differ significantly from single, on-average estimates for the whole rating period. For 10 raters, severity estimates on the last day were significantly different from estimates on the first day. These findings cast doubt on the practice of using a single calibration of rater severity as the basis for adjustment of person measures.",,,Article,Final,,Scopus,2-s2.0-0034195156
Jodoin M.G.,"Jodoin, Michael G. (6603491163)",6603491163,Measurement efficiency of innovative item formats in computer-based testing,2003,Journal of Educational Measurement,40,1,,1,15,14,49,10.1111/j.1745-3984.2003.tb01093.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037357320&doi=10.1111%2fj.1745-3984.2003.tb01093.x&partnerID=40&md5=acee0176470faa5f5162e07bc9dc3b5c,"The psychometric literature provides little empirical evaluation of examinee test data to assess essential psychometric properties of innovative items. In this study, examinee responses to conventional (e.g., multiple choice) and innovative item formats in a computer-based testing program were analyzed for IRT information with the three-parameter and graded response models. The innovative item types considered in this study provided more information across all levels of ability than multiple-choice items. In addition, accurate timing data captured via computer administration were analyzed to consider the relative efficiency of the multiple choice and innovative item types. As with previous research, multiple-choice items provide more information per unit time. Implications for balancing policy, psychometric, and pragmatic factors in selecting item formats are also discussed.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-0037357320
Sotaridona L.S.; Meijer R.R.,"Sotaridona, Leonardo S. (6506883239); Meijer, Rob R. (7006574564)",6506883239; 7006574564,Two new statistics to detect answer copying,2003,Journal of Educational Measurement,40,1,,53,69,16,36,10.1111/j.1745-3984.2003.tb01096.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037354099&doi=10.1111%2fj.1745-3984.2003.tb01096.x&partnerID=40&md5=76a49e1efd8cda47266389e568cb9817,"Two new indices to detect answer copying on a multiple-choice test - S1 and S2 - were proposed. The S1 index is similar to the K index (Holland, 1996) and the K̄2 index (Sotaridona & Meijer, 2002) but the distribution of the number of matching incorrect answers of the source and the copier is modeled by the Poisson distribution instead of the binomial distribution to improve the detection rate of K and K̄2. The S2 index was proposed to overcome a limitation of the K and K̄2 index, namely, their insensitiveness to correct answers copying. The S2 index incorporates the matching correct answers in addition to the matching incorrect answers. A simulation study was conducted to investigate the usefulness of S1 and S2 for 40- and 80-item tests, 100 and 500 sample sizes, and 10%, 20%, 30%, and 40% answer copying. The Type I errors and detection rates of S1 and S2 were compared with those of the K̄2 and the ω copying index (Wollack, 1997). Results showed that all four indices were able to maintain their Type I errors, with S1 and K̄2 being slightly conservative compared to S2 and ω. Furthermore, S1 had higher detection rates than K̄2. The S2 index showed a significant improvement in detection rate compared to K and K̄2.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0037354099
Wang T.; Kolen M.J.; Harris D.J.,"Wang, Tianyou (55709761000); Kolen, Michael J. (6603925839); Harris, Deborah J. (7403921256)",55709761000; 6603925839; 7403921256,Psychometric properties of scale scores and performance levels for performance assessments using polytomous IRT,2000,Journal of Educational Measurement,37,2,,141,162,21,30,10.1111/j.1745-3984.2000.tb01080.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034195157&doi=10.1111%2fj.1745-3984.2000.tb01080.x&partnerID=40&md5=83f747d56b3e23d82123a473d1276056,"With a focus on performance assessments, this paper describes procedures for calculating conditional standard error of measurement (CSEM) and reliability of scale scores and classification consistency of performance levels. Scale scores that are transformations of total raw scores are the focus of these procedures, although other types of raw scores are considered as well. Polytomous IRT models provide the psychometric foundation for the procedures that are described. The procedures are applied using test data from ACT's Work Keys Writing Assessment to demonstrate their usefulness. Two polytomous IRT models were compared, as were two different procedures for calculating scores. One simulation study was done using one of the models to evaluate the accuracy of the proposed procedures. The results suggest that the procedures provide quite stable estimates and have the potential to be useful in a variety of performance assessment situations.",,,Article,Final,,Scopus,2-s2.0-0034195157
Tate R.,"Tate, Richard (7102471183)",7102471183,Performance of a proposed method for the linking of mixed format tests with constructed response and multiple choice items,2000,Journal of Educational Measurement,37,4,,329,346,17,24,10.1111/j.1745-3984.2000.tb01090.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034337084&doi=10.1111%2fj.1745-3984.2000.tb01090.x&partnerID=40&md5=9fbe7f1e2e920bc6c893a34a5c66251d,"The error associated with a proposed linking method for tests consisting of both constructed response and multiple choice items was investigated in a simulation study. Study factors that were varied included the relative proportion of constructed response items in the test, the size of the year-to-year change in the ability metric, the number of anchor items, the number of linking papers to be reassessed, and the presence of guessing. The results supported the use of the proposed linking method. In addition, simulations were used to illustrate possible linking bias resulting from (a) the use of the traditional linking method and (b) the use of only multiple choice anchor items in the presence of test multidimensionality.",,,Article,Final,,Scopus,2-s2.0-0034337084
Wang N.,"Wang, Ning (7404340600)",7404340600,Use of the Rasch IRT Model in Standard Setting: An Item-mapping Method,2003,Journal of Educational Measurement,40,3,,231,253,22,47,10.1111/j.1745-3984.2003.tb01106.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346331551&doi=10.1111%2fj.1745-3984.2003.tb01106.x&partnerID=40&md5=c846ca2c1c2c4d12ace2852ac219388c,"This article provides both logical and empirical evidence to justify the use of an item-mapping method for establishing passing scores for multiple-choice licensure and certification examinations. After describing the item-mapping standard-setting process, the rationale and theoretical basis for this method are discussed, and the similarities and differences between the item-mapping and the Bookmark methods are also provided. Empirical evidence supporting use of the item-mapping method is provided by comparing results from four standard-setting studies for diverse licensure and certification examinations. The four cut score studies were conducted using both the item-mapping and the Angoff methods. Rating data from the four standard-setting studies, using each of the two methods, were analyzed using item-by-rater random effects generalizability and dependability studies to examine which method yielded higher inter-judge consistency. Results indicated that the item-mapping method produced higher inter-judge consistency and achieved greater rater agreement than the Angoff method.",,,Article,Final,,Scopus,2-s2.0-0346331551
Lee G.; Fitzpatrick A.R.,"Lee, Guemin (7404851825); Fitzpatrick, Anne R. (7004620831)",7404851825; 7004620831,The effects of a student sampling plan on estimates of the standard errors for student passing rates,2003,Journal of Educational Measurement,40,1,,17,28,11,1,10.1111/j.1745-3984.2003.tb01094.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037356391&doi=10.1111%2fj.1745-3984.2003.tb01094.x&partnerID=40&md5=4bd96e9b0f70e731c8ec15c8200b953d,"Examined in this study were three procedures for estimating the standard errors of school passing rates using a generalizability theory model. Also examined was how these procedures behaved for student samples that differed in size. The procedures differed in terms of their assumptions about the populations from which students were sampled, and it was found that student sample size generally had a notable effect on the size of the standard error estimates they produced. Also the three procedures produced markedly different standard error estimates when student sample size was small.",,,Article,Final,,Scopus,2-s2.0-0037356391
Smits N.; Mellenbergh G.J.; Vorst H.C.M.,"Smits, Niels (14219522200); Mellenbergh, Gideon J. (7003739438); Vorst, Harrie C.M. (6602890202)",14219522200; 7003739438; 6602890202,Alternative missing data techniques to grade point average: Imputing unavailable grades,2002,Journal of Educational Measurement,39,3,,187,206,19,21,10.1111/j.1745-3984.2002.tb01173.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036727283&doi=10.1111%2fj.1745-3984.2002.tb01173.x&partnerID=40&md5=028c1f91f91736558ed395e220f99835,"In this article, grade point average (GPA) is considered a missing data technique for unavailable grades in school grade records. In Study 1, theoretical and empirical differences between GPA and seven alternative missing grade techniques were considered. These seven techniques are subject mean substitution, corrected subject mean, subject correlation substitution, regression imputation, expectation maximization algorithm imputation and two multiple imputation methods - stochastic regression imputation and data augmentation. The missing grade techniques differ greatly. Data augmentation and stochastic regression imputation appear to be superior as missing grade techniques. In Study 2, the completed grade records (observed and imputed values) were used in two prediction analyses of academic achievement. One analysis was based on unweighed grades, the other on weighed grades. In both analyses, alternative missing grade methods produced better and more consistent predictions. It is concluded that some alternative missing grade methods are superior to GPA.",,,Article,Final,,Scopus,2-s2.0-0036727283
Ban J.-C.; Hanson B.A.; Yi Q.; Harris D.J.,"Ban, Jae-Chun (7006808582); Hanson, Bradley A. (7102036381); Yi, Qing (23092639000); Harris, Deborah J. (7403921256)",7006808582; 7102036381; 23092639000; 7403921256,Data sparseness and on-line pretest item calibration-scaling methods in CAT,2002,Journal of Educational Measurement,39,3,,207,218,11,14,10.1111/j.1745-3984.2002.tb01174.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036727284&doi=10.1111%2fj.1745-3984.2002.tb01174.x&partnerID=40&md5=5897050256de3ae53a5773cb60c9420a,"The purpose of this study was to compare and evaluate three on-line pretest item calibration-scaling methods (the marginal maximum likelihood estimate with one expectation maximization [EM] cycle [OEM] method, the marginal maximum likelihood estimate with multiple EM cycles [MEM] method, and Stocking's Method B) in terms of item parameter recovery when the item responses to the pretest items in the pool are sparse. Simulations of computerized adaptive tests were used to evaluate the results yielded by the three methods. The MEM method produced the smallest average total error in parameter estimation, and the OEM method yielded the largest total error.",,,Article,Final,,Scopus,2-s2.0-0036727284
Embretson S.; Gorin J.,"Embretson, Susan (6603914093); Gorin, Joanna (9249493900)",6603914093; 9249493900,Improving construct validity with cognitive psychology principles,2001,Journal of Educational Measurement,38,4,,343,368,25,135,10.1111/j.1745-3984.2001.tb01131.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035563921&doi=10.1111%2fj.1745-3984.2001.tb01131.x&partnerID=40&md5=5164d1821bb56ae9f9a17d1a6d953255,"Cognitive psychology principles have been heralded as possibly central to construct validity. In this paper, testing practices are examined in three stages: (a) the past, in which the traditional testing research paradigm left little role for cognitive psychology principles, (b) the present, in which testing research is enhanced by cognitive psychology principles, and (c) the future, for which we predict that cognitive psychology's potential will be fully realized through item design. An extended example of item design by cognitive theory is given to illustrate the principles. A spatial ability test that consists of an object assembly task highlights how cognitive design principles can lead to item generation.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0035563921
Dorans N.J.,"Dorans, Neil J. (6602289148)",6602289148,Recentering and realigning the SAT score distributions: How and why,2002,Journal of Educational Measurement,39,1,,59,84,25,33,10.1111/j.1745-3984.2002.tb01135.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036005453&doi=10.1111%2fj.1745-3984.2002.tb01135.x&partnerID=40&md5=d83ef4c3318326c3700566208f370c7c,"The process employed to produce the conversions that take scores from the original SAT scales to recentered scales, in which reference group scores are centered near the midpoint of the score-reporting range, is laid out. For the purposes of this article, SAT Verbal and SAT Mathematical scores were placed on recentered scales, which have reporting ranges of 920 to 980, means of 950, and standard deviations of 11. (The 920-to-980 scale is used in this article to highlight the distinction between it and the old 200-to-800 scale. In actuality, recentered scores were reported on a 200-to-800 scale.) Recentering was accomplished via a linear transformation of normally distributed scores that were obtained from a continuized, smoothed frequency distribution of original SAT scores that were originally on augmented two-digit scales (i.e., discrete scores rounded to either 0 or 5 in the third decimal place). These discrete scores were obtained for all students in the 1990 Reference Group using 35 different editions of the SAT spanning October 1988 to June 1990. The performance of this 1990 Reference Group on the original and recentered scales is described. The effects of recentering on scores of individuals and the 1990 Reference Group are also examined. Finally, recentering did not occur solely on the basis of its technical merit. Issues associated with converting recentering from a possibility into a reality are discussed.",,,Article,Final,,Scopus,2-s2.0-0036005453
Wainer H.; Wang X.,"Wainer, Howard (7006218234); Wang, Xiaohui (57219369196)",7006218234; 57219369196,Using a new statistical model for testlets to score TOEFL,2000,Journal of Educational Measurement,37,3,,203,220,17,78,10.1111/j.1745-3984.2000.tb01083.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034258099&doi=10.1111%2fj.1745-3984.2000.tb01083.x&partnerID=40&md5=c7fb9b6361c737825cee2637aae43bfa,"Standard item response theory (IRT) models fit to examination responses ignore the fact that sets of items (testlets) often are matched with a single common stimulus (e.g., a reading comprehension passage). In this setting, all items given to an examinee are unlikely to be conditionally independent (given examinee proficiency). Models that assume conditional independence will overestimate the precision with which examinee proficiency is measured. Overstatement of precision may lead to inaccurate inferences as well as prematurely ended examinations in which the stopping rule is based on the estimated standard error of examinee proficiency (e.g., an adaptive test). The standard three parameter IRT model was modified to include an additional random effect for items nested within the same testlet (Wainer, Bradlow, & Du, 2000). This parameter, γ, characterizes the amount of local dependence in a testlet. We fit 86 TOEFL testlets (50 reading comprehension and 36 listening comprehension) with the new model, and obtained a value for the variance of γ for each testlet. We compared the standard parameters (discrimination (a), difficulty (b) and guessing (c)) with what is obtained through traditional modeling. We found that difficulties were well estimated either way, but estimates of both a and c were biased if conditional independence is incorrectly assumed. Of greater import, we found that test information was substantially over-estimated when conditional independence was incorrectly assumed.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-0034258099
Scrams D.J.; McLeod L.D.,"Scrams, David J. (6507515074); McLeod, Lori D. (7006673109)",6507515074; 7006673109,An expected response function approach to graphical differential item functioning,2000,Journal of Educational Measurement,37,3,,263,280,17,4,10.1111/j.1745-3984.2000.tb01086.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034258423&doi=10.1111%2fj.1745-3984.2000.tb01086.x&partnerID=40&md5=e396b4e10a6d78f64945eb5477f528f6,"In this paper a new approach to graphical differential item functioning (DIF) is offered. The methodology is based on a sampling-theory approach to expected response functions (Lewis, 1985; Mislevy, Wingersky, & Sheehan, 1994). Essentially, error in item calibrations is modeled explicitly, and repeated samples are taken from the posterior distributions of the item parameters. Sampled parameter values are used to estimate the posterior distribution of the difference in item characteristic curves (ICCs) for two groups. A point-wise expectation is taken as an estimate of the true difference between the ICCs, and the sampled-difference functions indicate uncertainty in the estimate. The approach is applied to a set of pretest items, and the results are compared to traditional Mantel-Haenszel DIF statistics. The expected-response-function approach is contrasted with Pashley's (1992) graphical DIF approach.",,,Article,Final,,Scopus,2-s2.0-0034258423
Bassiri D.; Schulz E.M.,"Bassiri, Dina (7801375114); Schulz, E. Matthew (57213119158)",7801375114; 57213119158,Constructing a universal scale of high school course difficulty,2003,Journal of Educational Measurement,40,2,,147,161,14,12,10.1111/j.1745-3984.2003.tb01101.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038711437&doi=10.1111%2fj.1745-3984.2003.tb01101.x&partnerID=40&md5=4ec584d3f68c4bfb5846373140c92686,"This study examined the usefulness of applying the Rasch rating scale model (Andrich, 1978) to high school grade data. ACT Assessment test scores (English, Mathematics, Reading, and Science Reasoning) were used as ""common items"" to adjust for different grading standards in individual high school courses both within and across schools. This scaling approach yielded an ACT Assessment-adjusted high school grade point average (AA-HSGPA) on a common scale across high schools and cohorts within a large public university. AA-HSGPA was a better predictor of first-year college grade point average (CGPA) than the regular high school grade point average. The best model for predicting CGPA included both the ACT composite score and AA-HSGPA.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0038711437
Penfield R.D.; Algina J.,"Penfield, Randall D. (6601923478); Algina, James (7003768166)",6601923478; 7003768166,Applying the Liu-Agresti Estimator of the Cumulative Common Odds Ratio to DIF Detection in Polytomous Items,2003,Journal of Educational Measurement,40,4,,353,370,17,48,10.1111/j.1745-3984.2003.tb01151.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1842714346&doi=10.1111%2fj.1745-3984.2003.tb01151.x&partnerID=40&md5=7a8c78189087e594e03f4c8b458a81dd,"Liu and Agresti (1996) proposed a Mantel and Haenszel-type (1959) estimator of a common odds ratio for several 2 × J tables, where the J columns are ordinal levels of a response variable. This article applies the Liu-Agresti estimator to the case of assessing differential item functioning (DIF) in items having an ordinal response variable. A simulation study was conducted to investigate the accuracy of the Liu-Agresti estimator in relation to other statistical DIF detection procedures. The results of the simulation study indicate that the Liu-Agresti estimator is a viable alternative to other DIF detection statistics.",,,Article,Final,,Scopus,2-s2.0-1842714346
Cole N.S.; Zieky M.J.,"Cole, Nancy S. (38961260400); Zieky, Michael J. (6507036142)",38961260400; 6507036142,The new faces of fairness,2001,Journal of Educational Measurement,38,4,,369,382,13,58,10.1111/j.1745-3984.2001.tb01132.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035562588&doi=10.1111%2fj.1745-3984.2001.tb01132.x&partnerID=40&md5=2597e83eb07ff44e0af08813b9e94eec,"For most of the 20th century, measurement professionals paid little interest to item and test fairness. A confluence of events in the late 1960s and early 1970s led to an intense interest in fairness issues among measurement professionals. In spite of more than 30 years of effort, there is still no generally accepted definition of fairness with respect to testing and no measure that can prove or disprove the fairness of a test. To advance the fairness of tests, measurement professionals must pay more attention to reducing group differences at the design stage of test development, to providing all examinees an opportunity to demonstrate their knowledge and skills, to deterring test misuse, and to accommodating differences among individuals.",,,Article,Final,,Scopus,2-s2.0-0035562588
Stone C.A.,"Stone, Clement A. (7201720784)",7201720784,Monte Carlo based null distribution for an alternative goodness-of-fit test statistic in IRT models,2000,Journal of Educational Measurement,37,1,,58,75,17,63,10.1111/j.1745-3984.2000.tb01076.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034336345&doi=10.1111%2fj.1745-3984.2000.tb01076.x&partnerID=40&md5=6d30fe6f9e60c561746b867e52f723f8,"Assessing the correspondence between model predictions and observed data is a recommended procedure for justifying the application of an IRT model. However, with shorter tests, current goodness-of-fit procedures that assume precise point estimates of ability, are inappropriate. The present paper describes a goodness-of-fit statistic that considers the imprecision with which ability is estimated and involves constructing item fit tables based on each examinee's posterior distribution of ability, given the likelihood of their response pattern and an assumed marginal ability distribution. However, the posterior expectations that are computed are dependent and the distribution of the goodness-of-fit statistic is unknown. The present paper also describes a Monte Carlo resampling procedure that can be used to assess the significance of the fit statistic and compares this method with a previously used method. The results indicate that the method described herein is an effective and reasonably simple procedure for assessing the validity of applying IRT models when ability estimates are imprecise.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-0034336345
Cicmanec K.M.,"Cicmanec, Karen Mauck (6507610958)",6507610958,Standards-based scoring and traditional grading practices,2001,Journal of Educational Measurement,38,2,,188,190,2,0,10.1111/j.1745-3984.2001.tb01122.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035373522&doi=10.1111%2fj.1745-3984.2001.tb01122.x&partnerID=40&md5=b11a847b14d56d9593703b7d56b6f316,"Reviews the book 'Standards-Based Scoring and Traditional Grading Practices,' edited by E. Trumbull and B. Farr.",,,Article,Final,,Scopus,2-s2.0-0035373522
