Authors,Title,Year,Volume,Abstract,Title_spell,Abstract_spell,A_tokens,T_tokens,A_tokens_join,T_tokens_join,lda_0,lda_1,lda_2,lda_3,lda_4,nmf_0,nmf_1,nmf_2,nmf_3,nmf_4
Domingue B.W.; Kanopka K.; Stenhaug B.; Soland J.; Kuhfeld M.; Wise S.; Piech C.,Variation in Respondent Speed and its Implications: Evidence from an Adaptive Testing Scenario,2021,58,"The more frequent collection of response time data is leading to an increased need for an understanding of how such data can be included in measurement models. Models for response time have been advanced, but relatively limited large-scale empirical investigations have been conducted. We take advantage of a large data set from the adaptive NWEA MAP Growth Reading Assessment to shed light on emergent features of response time behavior. We identify two behaviors in particular. The first, response acceleration, is a reduction in response time for responses that occur later in the assessment. We note that such reductions are heterogeneous as a function of estimated ability (lower ability estimates are associated with larger increases in acceleration) and that reductions in response time lead to lower accuracy relative to expectation for lower ability students. The second is within-person variation in the association between time usage and accuracy. Idiosyncratic within-person changes in response time have inconsistent implications for accuracy; in some cases additional response time predicts higher accuracy but in other cases additional response time predicts declines in accuracy. These findings have implications for models that incorporate response time and accuracy. Our approach may be useful in other studies of adaptive testingÂ data. Â© 2021 by the National Council on Measurement in Education",Variation in Respondent Speed and its Implications: Evidence from an Adaptive Testing Scenario,"The more frequent collection of response time data is leading to an increased need for an understanding of how such data can be included in measurement models. Models for response time have been advanced, but relatively limited large-scale empirical investigations have been conducted. We take advantage of a large data set from the adaptive NWEA MAP Growth Reading Assessment to shed light on emergent features of response time behavior. We identify two behaviors in particular. The first, response acceleration, is a reduction in response time for responses that occur later in the assessment. We note that such reductions are heterogeneous as a function of estimated ability (lower ability estimates are associated with larger increases in acceleration) and that reductions in response time lead to lower accuracy relative to expectation for lower ability students. The second is within-person variation in the association between time usage and accuracy. Idiosyncratic within-person changes in response time have inconsistent implications for accuracy; in some cases additional response time predicts higher accuracy but in other cases additional response time predicts declines in accuracy. These findings have implications for models that incorporate response time and accuracy. Our approach may be useful in other studies of adaptive testingÂ data. Â© 2021 by the National Council on Measurement in Education","['frequent', 'collection', 'response', 'time', 'datum', 'lead', 'increase', 'need', 'understanding', 'datum', 'include', 'Models', 'response', 'time', 'advanced', 'relatively', 'limited', 'largescale', 'empirical', 'investigation', 'conduct', 'advantage', 'large', 'datum', 'set', 'adaptive', 'NWEA', 'MAP', 'Growth', 'Reading', 'Assessment', 'shed', 'light', 'emergent', 'feature', 'response', 'time', 'behavior', 'identify', 'behavior', 'particular', 'response', 'acceleration', 'reduction', 'response', 'time', 'response', 'occur', 'later', 'assessment', 'note', 'reduction', 'heterogeneous', 'function', 'estimate', 'ability', 'low', 'ability', 'estimate', 'associate', 'large', 'increase', 'acceleration', 'reduction', 'response', 'time', 'lead', 'low', 'accuracy', 'relative', 'expectation', 'low', 'ability', 'student', 'second', 'withinperson', 'variation', 'association', 'time', 'usage', 'accuracy', 'Idiosyncratic', 'withinperson', 'change', 'response', 'time', 'inconsistent', 'implication', 'accuracy', 'case', 'additional', 'response', 'time', 'predict', 'high', 'accuracy', 'case', 'additional', 'response', 'time', 'predict', 'decline', 'accuracy', 'finding', 'implication', 'incorporate', 'response', 'time', 'accuracy', 'approach', 'useful', 'study', 'adaptive', 'testing', 'datum', 'Â©', '2021', 'National', 'Council']","['Variation', 'Respondent', 'Speed', 'Implications', 'Evidence', 'Adaptive', 'Testing', 'Scenario']",frequent collection response time datum lead increase need understanding datum include Models response time advanced relatively limited largescale empirical investigation conduct advantage large datum set adaptive NWEA MAP Growth Reading Assessment shed light emergent feature response time behavior identify behavior particular response acceleration reduction response time response occur later assessment note reduction heterogeneous function estimate ability low ability estimate associate large increase acceleration reduction response time lead low accuracy relative expectation low ability student second withinperson variation association time usage accuracy Idiosyncratic withinperson change response time inconsistent implication accuracy case additional response time predict high accuracy case additional response time predict decline accuracy finding implication incorporate response time accuracy approach useful study adaptive testing datum Â© 2021 National Council,Variation Respondent Speed Implications Evidence Adaptive Testing Scenario,0.025943603,0.025783576,0.025723045,0.896793637,0.025756139,0.135986086,0,0,0,0
Baldwin P.; Yaneva V.; Mee J.; Clauser B.E.; Ha L.A.,Using Natural Language Processing to Predict Item Response Times and Improve Test Construction,2021,58,"In this article, it is shown how item text can be represented by (a) 113 features quantifying the text's linguistic characteristics, (b) 16 measures of the extent to which an information-retrieval-based automatic question-answering system finds an item challenging, and (c) through dense word representations (word embeddings). Using a random forests algorithm, these data then are used to train a prediction model for item response times and predicted response times then are used to assemble test forms. Using empirical data from the United States Medical Licensing Examination, we show that timing demands are more consistent across these specially assembled forms than across forms comprising randomly-selected items. Because an exam's timing conditions affect examinee performance, this result has implications for exam fairness whenever examinees are compared with each other or against a common standard. Â© 2020 by the National Council on Measurement in Education",Using Natural Language Processing to Predict Item Response Times and Improve Test Construction,"In this article, it is shown how item text can be represented by (a) 113 features quantifying the text's linguistic characteristics, (b) 16 measures of the extent to which an information-retrieval-based automatic question-answering system finds an item challenging, and (c) through dense word representations (word embeddings). Using a random forests algorithm, these data then are used to train a prediction model for item response times and predicted response times then are used to assemble test forms. Using empirical data from the United States Medical Licensing Examination, we show that timing demands are more consistent across these specially assembled forms than across forms comprising randomly-selected items. Because an exam's timing conditions affect examinee performance, this result has implications for exam fairness whenever examinees are compared with each other or against a common standard. Â© 2020 by the National Council on Measurement in Education","['article', 'item', 'text', 'represent', '113', 'feature', 'quantify', 'text', 'linguistic', 'characteristic', 'b', '16', 'measure', 'extent', 'informationretrievalbased', 'automatic', 'questionanswering', 'system', 'find', 'item', 'challenge', 'c', 'dense', 'word', 'representation', 'word', 'embedding', 'random', 'forest', 'algorithm', 'datum', 'train', 'prediction', 'item', 'response', 'time', 'predict', 'response', 'time', 'assemble', 'test', 'form', 'empirical', 'datum', 'United', 'States', 'Medical', 'Licensing', 'Examination', 'timing', 'demand', 'consistent', 'specially', 'assembled', 'form', 'form', 'comprise', 'randomlyselecte', 'item', 'exam', 'time', 'condition', 'affect', 'examinee', 'performance', 'result', 'implication', 'exam', 'fairness', 'examinee', 'compare', 'common', 'standard', 'Â©', '2020', 'National', 'Council']","['Natural', 'Language', 'Processing', 'Predict', 'Item', 'Response', 'Times', 'improve', 'Test', 'construction']",article item text represent 113 feature quantify text linguistic characteristic b 16 measure extent informationretrievalbased automatic questionanswering system find item challenge c dense word representation word embedding random forest algorithm datum train prediction item response time predict response time assemble test form empirical datum United States Medical Licensing Examination timing demand consistent specially assembled form form comprise randomlyselecte item exam time condition affect examinee performance result implication exam fairness examinee compare common standard Â© 2020 National Council,Natural Language Processing Predict Item Response Times improve Test construction,0.901621024,0.024447939,0.024391448,0.024916329,0.02462326,0.07139442,0.021041987,0.000659956,0.006151385,0
Lim H.; Choe E.M.; Han K.T.,A Residual-Based Differential Item Functioning Detection Framework in Item Response Theory,2022,59,"Differential item functioning (DIF) of test items should be evaluated using practical methods that can produce accurate and useful results. Among a plethora of DIF detection techniques, we introduce the new Residual DIF (RDIF) framework, which stands out for its accessibility without sacrificing efficacy. This framework consists of three item response theory (IRT) residual statistics: (Formula presented.), (Formula presented.), and (Formula presented.). We conducted a simulation study with a 40-item test to assess the performance of RDIF in comparison with the Mantel-Haenszel, logistic regression, and IRT-based likelihood ratio test methods. Even when analyzing small sample sizes, the results revealed (Formula presented.) to be the most robust DIF detection statistic with strict control of Type I error across all simulated conditions when paired with the purification procedure. Also, (Formula presented.) and (Formula presented.) proved to be powerful indicators of uniform and nonuniform DIF, respectively. Therefore, (Formula presented.) should serve as the primary flagging criterion, whereas (Formula presented.) and (Formula presented.) best serve as indicators of DIF type. An empirical DIF study also showed that the RDIF framework could perform satisfactorily with real data from a large-scale assessment. Overall, the RDIF framework demonstrated its potential as a new standard for IRT-based DIF detection methodology. Â© 2022 by the National Council on Measurement in Education.",A Residual-Based Differential Item Functioning Detection Framework in Item Response Theory,"Differential item functioning (DIF) of test items should be evaluated using practical methods that can produce accurate and useful results. Among a plethora of DIF detection techniques, we introduce the new Residual DIF (RDIF) framework, which stands out for its accessibility without sacrificing efficacy. This framework consists of three item response theory (IRT) residual statistics: (Formula presented.), (Formula presented.), and (Formula presented.). We conducted a simulation study with a 40-item test to assess the performance of RDIF in comparison with the Mantel-Haenszel, logistic regression, and IRT-based likelihood ratio test methods. Even when analyzing small sample sizes, the results revealed (Formula presented.) to be the most robust DIF detection statistic with strict control of Type I error across all simulated conditions when paired with the purification procedure. Also, (Formula presented.) and (Formula presented.) proved to be powerful indicators of uniform and nonuniform DIF, respectively. Therefore, (Formula presented.) should serve as the primary flagging criterion, whereas (Formula presented.) and (Formula presented.) best serve as indicators of DIF type. An empirical DIF study also showed that the RDIF framework could perform satisfactorily with real data from a large-scale assessment. Overall, the RDIF framework demonstrated its potential as a new standard for IRT-based DIF detection methodology. Â© 2022 by the National Council on Measurement in Education.","['differential', 'item', 'function', 'DIF', 'test', 'item', 'evaluate', 'practical', 'method', 'produce', 'accurate', 'useful', 'result', 'plethora', 'DIF', 'detection', 'technique', 'introduce', 'new', 'residual', 'DIF', 'RDIF', 'framework', 'stand', 'accessibility', 'sacrifice', 'efficacy', 'framework', 'consist', 'item', 'response', 'theory', 'IRT', 'residual', 'statistic', 'Formula', 'present', 'Formula', 'present', 'Formula', 'present', 'conduct', 'simulation', 'study', '40item', 'test', 'assess', 'performance', 'RDIF', 'comparison', 'MantelHaenszel', 'logistic', 'regression', 'irtbase', 'likelihood', 'ratio', 'test', 'method', 'analyze', 'small', 'sample', 'size', 'result', 'reveal', 'Formula', 'present', 'robust', 'DIF', 'detection', 'statistic', 'strict', 'control', 'Type', 'I', 'error', 'simulate', 'condition', 'pair', 'purification', 'procedure', 'Formula', 'present', 'Formula', 'present', 'prove', 'powerful', 'indicator', 'uniform', 'nonuniform', 'DIF', 'respectively', 'Therefore', 'Formula', 'present', 'serve', 'primary', 'flag', 'criterion', 'Formula', 'present', 'Formula', 'present', 'good', 'serve', 'indicator', 'DIF', 'type', 'empirical', 'dif', 'study', 'rdif', 'framework', 'perform', 'satisfactorily', 'real', 'datum', 'largescale', 'assessment', 'Overall', 'rdif', 'framework', 'demonstrate', 'potential', 'new', 'standard', 'irtbased', 'DIF', 'detection', 'methodology', 'Â©', '2022', 'National', 'Council']","['ResidualBased', 'Differential', 'Item', 'Functioning', 'Detection', 'Framework', 'Item', 'Response', 'Theory']",differential item function DIF test item evaluate practical method produce accurate useful result plethora DIF detection technique introduce new residual DIF RDIF framework stand accessibility sacrifice efficacy framework consist item response theory IRT residual statistic Formula present Formula present Formula present conduct simulation study 40item test assess performance RDIF comparison MantelHaenszel logistic regression irtbase likelihood ratio test method analyze small sample size result reveal Formula present robust DIF detection statistic strict control Type I error simulate condition pair purification procedure Formula present Formula present prove powerful indicator uniform nonuniform DIF respectively Therefore Formula present serve primary flag criterion Formula present Formula present good serve indicator DIF type empirical dif study rdif framework perform satisfactorily real datum largescale assessment Overall rdif framework demonstrate potential new standard irtbased DIF detection methodology Â© 2022 National Council,ResidualBased Differential Item Functioning Detection Framework Item Response Theory,0.028358491,0.887330145,0.027761108,0.028489743,0.028060513,0,0,0,0,0.260377325
Liao X.; Bolt D.M.; Kim J.-S.,Curvilinearity in the Reference Composite and Practical Implications for Measurement,2024,,"Item difficulty and dimensionality often correlate, implying that unidimensional IRT approximations to multidimensional data (i.e., reference composites) can take a curvilinear form in the multidimensional space. Although this issue has been previously discussed in the context of vertical scaling applications, we illustrate how such a phenomenon can also easily occur within individual tests. Measures of reading proficiency, for example, often use different task types within a single assessment, a feature that may not only lead to multidimensionality, but also an association between item difficulty and dimensionality. Using a latent regression strategy, we demonstrate through simulations and empirical analysis how associations between dimensionality and difficulty yield a nonlinear reference composite where the weights of the underlying dimensions change across the scale continuum according to the difficulties of the items associated with the dimensions. We further show how this form of curvilinearity produces systematic forms of misspecification in traditional unidimensional IRT models (e.g., 2PL) and can be better accommodated by models such as monotone-polynomial or asymmetric IRT models. Simulations and a real-data example from the Early Childhood Longitudinal Study?”Kindergarten are provided for demonstration. Some implications for measurement modeling and for understanding the effects of 2PL misspecification on measurement metrics are discussed. Â© 2024 The Author(s). Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Curvilinearity in the Reference Composite and Practical Implications for Measurement,"Item difficulty and dimensionality often correlate, implying that unidimensional IRT approximations to multidimensional data (i.e., reference composites) can take a curvilinear form in the multidimensional space. Although this issue has been previously discussed in the context of vertical scaling applications, we illustrate how such a phenomenon can also easily occur within individual tests. Measures of reading proficiency, for example, often use different task types within a single assessment, a feature that may not only lead to multidimensionality, but also an association between item difficulty and dimensionality. Using a latent regression strategy, we demonstrate through simulations and empirical analysis how associations between dimensionality and difficulty yield a nonlinear reference composite where the weights of the underlying dimensions change across the scale continuum according to the difficulties of the items associated with the dimensions. We further show how this form of curvilinearity produces systematic forms of misspecification in traditional unidimensional IRT models (e.g., 2PL) and can be better accommodated by models such as monotone-polynomial or asymmetric IRT models. Simulations and a real-data example from the Early Childhood Longitudinal Study?”Kindergarten are provided for demonstration. Some implications for measurement modeling and for understanding the effects of 2PL misspecification on measurement metrics are discussed. Â© 2024 The Author(s). Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['item', 'difficulty', 'dimensionality', 'correlate', 'imply', 'unidimensional', 'IRT', 'approximation', 'multidimensional', 'datum', 'ie', 'reference', 'composite', 'curvilinear', 'form', 'multidimensional', 'space', 'issue', 'previously', 'discuss', 'context', 'vertical', 'scaling', 'application', 'illustrate', 'phenomenon', 'easily', 'occur', 'individual', 'test', 'measure', 'read', 'proficiency', 'example', 'different', 'task', 'type', 'single', 'assessment', 'feature', 'lead', 'multidimensionality', 'association', 'item', 'difficulty', 'dimensionality', 'latent', 'regression', 'strategy', 'demonstrate', 'simulation', 'empirical', 'analysis', 'association', 'dimensionality', 'difficulty', 'yield', 'nonlinear', 'reference', 'composite', 'weight', 'underlie', 'dimension', 'change', 'scale', 'continuum', 'accord', 'difficulty', 'item', 'associate', 'dimension', 'far', 'form', 'curvilinearity', 'produce', 'systematic', 'form', 'misspecification', 'traditional', 'unidimensional', 'IRT', 'eg', '2PL', 'accommodate', 'monotonepolynomial', 'asymmetric', 'IRT', 'simulation', 'realdata', 'example', 'Early', 'Childhood', 'Longitudinal', 'Study', '??, 'Kindergarten', 'provide', 'demonstration', 'implication', 'modeling', 'understand', 'effect', '2pl', 'misspecification', 'metric', 'discuss', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['curvilinearity', 'Reference', 'Composite', 'Practical', 'Implications']",item difficulty dimensionality correlate imply unidimensional IRT approximation multidimensional datum ie reference composite curvilinear form multidimensional space issue previously discuss context vertical scaling application illustrate phenomenon easily occur individual test measure read proficiency example different task type single assessment feature lead multidimensionality association item difficulty dimensionality latent regression strategy demonstrate simulation empirical analysis association dimensionality difficulty yield nonlinear reference composite weight underlie dimension change scale continuum accord difficulty item associate dimension far form curvilinearity produce systematic form misspecification traditional unidimensional IRT eg 2PL accommodate monotonepolynomial asymmetric IRT simulation realdata example Early Childhood Longitudinal Study ??Kindergarten provide demonstration implication modeling understand effect 2pl misspecification metric discuss Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,curvilinearity Reference Composite Practical Implications,0.021765086,0.913284321,0.021236798,0.022148516,0.02156528,0.026887013,0.055507243,0,0.017955013,0.021148824
Kim S.Y.; Lee W.-C.,Several Variations of Simple-Structure MIRT Equating,2023,60,"The current study proposed several variants of simple-structure multidimensional item response theory equating procedures. Four distinct sets of data were used to demonstrate feasibility of proposed equating methods for two different equating designs: a random groups design and a common-item nonequivalent groups design. Findings indicated some notable differences between the multidimensional and unidimensional approaches when data exhibited evidence for multidimensionality. In addition, some of the proposed methods were successful in providing equating results for both section-level and composite-level scores, which has not been achieved by most of the existing methodologies. The traditional method of using a set of quadrature points and weights for equating turned out to be computationally intensive, particularly for the data with higher dimensions. The study suggested an alternative way of using the Monte-Carlo approach for such data. This study also proposed a simple-structure true-score equating procedure that does not rely on a multivariate observed-score distribution. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Several Variations of Simple-Structure MIRT Equating,"The current study proposed several variants of simple-structure multidimensional item response theory equating procedures. Four distinct sets of data were used to demonstrate feasibility of proposed equating methods for two different equating designs: a random groups design and a common-item nonequivalent groups design. Findings indicated some notable differences between the multidimensional and unidimensional approaches when data exhibited evidence for multidimensionality. In addition, some of the proposed methods were successful in providing equating results for both section-level and composite-level scores, which has not been achieved by most of the existing methodologies. The traditional method of using a set of quadrature points and weights for equating turned out to be computationally intensive, particularly for the data with higher dimensions. The study suggested an alternative way of using the Monte-Carlo approach for such data. This study also proposed a simple-structure true-score equating procedure that does not rely on a multivariate observed-score distribution. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['current', 'study', 'propose', 'variant', 'simplestructure', 'multidimensional', 'item', 'response', 'theory', 'equate', 'procedure', 'distinct', 'set', 'datum', 'demonstrate', 'feasibility', 'propose', 'equating', 'method', 'different', 'equating', 'design', 'random', 'group', 'design', 'commonitem', 'nonequivalent', 'group', 'design', 'Findings', 'indicate', 'notable', 'difference', 'multidimensional', 'unidimensional', 'approach', 'datum', 'exhibit', 'evidence', 'multidimensionality', 'addition', 'propose', 'method', 'successful', 'provide', 'equate', 'result', 'sectionlevel', 'compositelevel', 'score', 'achieve', 'exist', 'methodology', 'traditional', 'method', 'set', 'quadrature', 'point', 'weight', 'equating', 'turn', 'computationally', 'intensive', 'particularly', 'datum', 'high', 'dimension', 'study', 'suggest', 'alternative', 'way', 'MonteCarlo', 'approach', 'datum', 'study', 'propose', 'simplestructure', 'truescore', 'equate', 'procedure', 'rely', 'multivariate', 'observedscore', 'distribution', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['Variations', 'SimpleStructure', 'MIRT', 'Equating']",current study propose variant simplestructure multidimensional item response theory equate procedure distinct set datum demonstrate feasibility propose equating method different equating design random group design commonitem nonequivalent group design Findings indicate notable difference multidimensional unidimensional approach datum exhibit evidence multidimensionality addition propose method successful provide equate result sectionlevel compositelevel score achieve exist methodology traditional method set quadrature point weight equating turn computationally intensive particularly datum high dimension study suggest alternative way MonteCarlo approach datum study propose simplestructure truescore equate procedure rely multivariate observedscore distribution Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,Variations SimpleStructure MIRT Equating,0.904628293,0.02378367,0.023830207,0.023959087,0.023798743,0.01795512,0.036686189,0.139303158,0.002809728,0
Li D.,Assessing the Impact of Equating Error on Group Means and Group Mean Differences,2022,59,"Equating error is usually small relative to the magnitude of measurement error, but it could be one of the major sources of error contributing to mean scores of large groups in educational measurement, such as the year-to-year state mean score fluctuations. Though testing programs may routinely calculate the standard error of equating (SEE), the commonly used summary statistics of SEE are not direct quantifications of the impact of equating error on group means. This article proposed summary statistics that directly quantify the impact of equating error on group means or group mean differences and provided empirical and analytical methods to estimate these statistics. Examples based on empirical data were used to illustrate practical applications of these statistics. Â© 2022 by the National Council on Measurement in Education.",Assessing the Impact of Equating Error on Group Means and Group Mean Differences,"Equating error is usually small relative to the magnitude of measurement error, but it could be one of the major sources of error contributing to mean scores of large groups in educational measurement, such as the year-to-year state mean score fluctuations. Though testing programs may routinely calculate the standard error of equating (SEE), the commonly used summary statistics of SEE are not direct quantifications of the impact of equating error on group means. This article proposed summary statistics that directly quantify the impact of equating error on group means or group mean differences and provided empirical and analytical methods to estimate these statistics. Examples based on empirical data were used to illustrate practical applications of these statistics. Â© 2022 by the National Council on Measurement in Education.","['equating', 'error', 'usually', 'small', 'relative', 'magnitude', 'error', 'major', 'source', 'error', 'contribute', 'mean', 'score', 'large', 'group', 'educational', 'yeartoyear', 'state', 'mean', 'score', 'fluctuation', 'testing', 'program', 'routinely', 'calculate', 'standard', 'error', 'equate', 'commonly', 'summary', 'statistic', 'SEE', 'direct', 'quantification', 'impact', 'equate', 'error', 'group', 'mean', 'article', 'propose', 'summary', 'statistic', 'directly', 'quantify', 'impact', 'equate', 'error', 'group', 'mean', 'group', 'mean', 'difference', 'provide', 'empirical', 'analytical', 'method', 'estimate', 'statistic', 'Examples', 'base', 'empirical', 'datum', 'illustrate', 'practical', 'application', 'statistic', 'Â©', '2022', 'National', 'Council']","['assess', 'Impact', 'Equating', 'Error', 'Group', 'Means', 'Group', 'Mean', 'difference']",equating error usually small relative magnitude error major source error contribute mean score large group educational yeartoyear state mean score fluctuation testing program routinely calculate standard error equate commonly summary statistic SEE direct quantification impact equate error group mean article propose summary statistic directly quantify impact equate error group mean group mean difference provide empirical analytical method estimate statistic Examples base empirical datum illustrate practical application statistic Â© 2022 National Council,assess Impact Equating Error Group Means Group Mean difference,0.873136156,0.031477989,0.031559288,0.032094565,0.031732003,0,0,0.220120819,0,0
Wise S.L.; Kuhfeld M.R.,Using Retest Data to Evaluate and Improve Effort-Moderated Scoring,2021,58,"There has been a growing research interest in the identification and management of disengaged test taking, which poses a validity threat that is particularly prevalent with low-stakes tests. This study investigated effort-moderated (E-M) scoring, in which item responses classified as rapid guesses are identified and excluded from scoring. Using achievement test data composed of test takers who were quickly retested and showed differential degrees of disengagement, three basic findings emerged. First, standard E-M scoring accounted for roughly one-third of the score distortion due to differential disengagement. Second, a modified E-M scoring method that used more liberal time thresholds performed better?”accounting for two-thirds or more of the distortion. Finally, the inability of E-M scoring to account for all of the score distortion suggests the additional presence of nonrapid item responses that reflect less-than-full engagement by some test takers. Â© 2020 by the National Council on Measurement in Education",Using Retest Data to Evaluate and Improve Effort-Moderated Scoring,"There has been a growing research interest in the identification and management of disengaged test taking, which poses a validity threat that is particularly prevalent with low-stakes tests. This study investigated effort-moderated (E-M) scoring, in which item responses classified as rapid guesses are identified and excluded from scoring. Using achievement test data composed of test takers who were quickly retested and showed differential degrees of disengagement, three basic findings emerged. First, standard E-M scoring accounted for roughly one-third of the score distortion due to differential disengagement. Second, a modified E-M scoring method that used more liberal time thresholds performed better?”accounting for two-thirds or more of the distortion. Finally, the inability of E-M scoring to account for all of the score distortion suggests the additional presence of nonrapid item responses that reflect less-than-full engagement by some test takers. Â© 2020 by the National Council on Measurement in Education","['grow', 'research', 'interest', 'identification', 'management', 'disengaged', 'test', 'taking', 'pose', 'validity', 'threat', 'particularly', 'prevalent', 'lowstake', 'test', 'study', 'investigate', 'effortmoderated', 'EM', 'scoring', 'item', 'response', 'classify', 'rapid', 'guess', 'identify', 'exclude', 'score', 'achievement', 'test', 'datum', 'compose', 'test', 'taker', 'quickly', 'reteste', 'differential', 'degree', 'disengagement', 'basic', 'finding', 'emerge', 'First', 'standard', 'EM', 'scoring', 'account', 'roughly', 'onethird', 'score', 'distortion', 'differential', 'disengagement', 'Second', 'modified', 'EM', 'scoring', 'method', 'liberal', 'time', 'threshold', 'perform', '??, 'account', 'twothird', 'distortion', 'finally', 'inability', 'EM', 'scoring', 'account', 'score', 'distortion', 'suggest', 'additional', 'presence', 'nonrapid', 'item', 'response', 'reflect', 'lessthanfull', 'engagement', 'test', 'taker', 'Â©', '2020', 'National', 'Council']","['Retest', 'Data', 'evaluate', 'improve', 'EffortModerated', 'Scoring']",grow research interest identification management disengaged test taking pose validity threat particularly prevalent lowstake test study investigate effortmoderated EM scoring item response classify rapid guess identify exclude score achievement test datum compose test taker quickly reteste differential degree disengagement basic finding emerge First standard EM scoring account roughly onethird score distortion differential disengagement Second modified EM scoring method liberal time threshold perform ??account twothird distortion finally inability EM scoring account score distortion suggest additional presence nonrapid item response reflect lessthanfull engagement test taker Â© 2020 National Council,Retest Data evaluate improve EffortModerated Scoring,0.025753917,0.02560328,0.896542789,0.026201768,0.025898245,0.019957535,0.046078058,0,0.049438626,0.000734101
Setzer J.C.; Cheng Y.; Liu C.,Classification Accuracy and Consistency of Compensatory Composite Test Scores,2023,60,"Test scores are often used to make decisions about examinees, such as in licensure and certification testing, as well as in many educational contexts. In some cases, these decisions are based upon compensatory scores, such as those from multiple sections or components of an exam. Classification accuracy and classification consistency are two psychometric characteristics of test scores that are often reported when decisions are based on those scores, and several techniques currently exist for estimating both accuracy and consistency. However, research on classification accuracy and consistency on compensatory test scores is scarce. This study demonstrates two techniques that can be used to estimate classification accuracy and consistency when test scores are used in a compensatory manner. First, a simulation study demonstrates that both methods provide very similar results under the studied conditions. Second, we demonstrate how the two methods could be used with a high-stakes licensure exam. Â© 2023 by the National Council on Measurement in Education.",Classification Accuracy and Consistency of Compensatory Composite Test Scores,"Test scores are often used to make decisions about examinees, such as in licensure and certification testing, as well as in many educational contexts. In some cases, these decisions are based upon compensatory scores, such as those from multiple sections or components of an exam. Classification accuracy and classification consistency are two psychometric characteristics of test scores that are often reported when decisions are based on those scores, and several techniques currently exist for estimating both accuracy and consistency. However, research on classification accuracy and consistency on compensatory test scores is scarce. This study demonstrates two techniques that can be used to estimate classification accuracy and consistency when test scores are used in a compensatory manner. First, a simulation study demonstrates that both methods provide very similar results under the studied conditions. Second, we demonstrate how the two methods could be used with a high-stakes licensure exam. Â© 2023 by the National Council on Measurement in Education.","['test', 'score', 'decision', 'examinee', 'licensure', 'certification', 'testing', 'educational', 'context', 'case', 'decision', 'base', 'compensatory', 'score', 'multiple', 'section', 'component', 'exam', 'Classification', 'accuracy', 'classification', 'consistency', 'psychometric', 'characteristic', 'test', 'score', 'report', 'decision', 'base', 'score', 'technique', 'currently', 'exist', 'estimate', 'accuracy', 'consistency', 'research', 'classification', 'accuracy', 'consistency', 'compensatory', 'test', 'score', 'scarce', 'study', 'demonstrate', 'technique', 'estimate', 'classification', 'accuracy', 'consistency', 'test', 'score', 'compensatory', 'manner', 'simulation', 'study', 'demonstrate', 'method', 'provide', 'similar', 'result', 'study', 'condition', 'Second', 'demonstrate', 'method', 'highstake', 'licensure', 'exam', 'Â©', '2023', 'National', 'Council']","['Classification', 'Accuracy', 'Consistency', 'Compensatory', 'Composite', 'Test', 'Scores']",test score decision examinee licensure certification testing educational context case decision base compensatory score multiple section component exam Classification accuracy classification consistency psychometric characteristic test score report decision base score technique currently exist estimate accuracy consistency research classification accuracy consistency compensatory test score scarce study demonstrate technique estimate classification accuracy consistency test score compensatory manner simulation study demonstrate method provide similar result study condition Second demonstrate method highstake licensure exam Â© 2023 National Council,Classification Accuracy Consistency Compensatory Composite Test Scores,0.032777803,0.869322027,0.032700421,0.032832777,0.032366973,0,0.161383438,0,0,0
Goldhammer F.; Kroehne U.; Hahnel C.; Naumann J.; De Boeck P.,Does Timed Testing Affect the Interpretation of Efficiency Scores??”A GLMM Analysis of Reading Components,2024,,"The efficiency of cognitive component skills is typically assessed with speeded performance tests. Interpreting only effective ability or effective speed as efficiency may be challenging because of the within-person dependency between both variables (speed-ability tradeoff, SAT). The present study measures efficiency as effective ability conditional on speed by controlling speed experimentally. Item-level time limits control the stimulus presentation time and the time window for responding (timed condition). The overall goal was to examine the construct validity of effective ability scores obtained from untimed and timed condition by comparing the effects of theory-based item properties on item difficulty. If such effects exist, the scores reflect how well the test-takers were able to cope with the theory-based requirements. A German subsample from PISA 2012 completed two reading component skills tasks (i.e., word recognition and semantic integration) with and without item-level time limits. Overall, the included linguistic item properties showed stronger effects on item difficulty in the timed than the untimed condition. In the semantic integration task, item properties explained the time required in the untimed condition. The results suggest that effective ability scores in the timed condition better reflect how well test-takers were able to cope with the theoretically relevant task demands. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Does Timed Testing Affect the Interpretation of Efficiency Scores??”A GLMM Analysis of Reading Components,"The efficiency of cognitive component skills is typically assessed with speeded performance tests. Interpreting only effective ability or effective speed as efficiency may be challenging because of the within-person dependency between both variables (speed-ability tradeoff, SAT). The present study measures efficiency as effective ability conditional on speed by controlling speed experimentally. Item-level time limits control the stimulus presentation time and the time window for responding (timed condition). The overall goal was to examine the construct validity of effective ability scores obtained from untimed and timed condition by comparing the effects of theory-based item properties on item difficulty. If such effects exist, the scores reflect how well the test-takers were able to cope with the theory-based requirements. A German subsample from PISA 2012 completed two reading component skills tasks (i.e., word recognition and semantic integration) with and without item-level time limits. Overall, the included linguistic item properties showed stronger effects on item difficulty in the timed than the untimed condition. In the semantic integration task, item properties explained the time required in the untimed condition. The results suggest that effective ability scores in the timed condition better reflect how well test-takers were able to cope with the theoretically relevant task demands. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['efficiency', 'cognitive', 'component', 'skill', 'typically', 'assess', 'speeded', 'performance', 'test', 'interpret', 'effective', 'ability', 'effective', 'speed', 'efficiency', 'challenge', 'withinperson', 'dependency', 'variable', 'speedability', 'tradeoff', 'SAT', 'present', 'study', 'measure', 'efficiency', 'effective', 'ability', 'conditional', 'speed', 'control', 'speed', 'experimentally', 'itemlevel', 'time', 'limit', 'control', 'stimulus', 'presentation', 'time', 'time', 'window', 'respond', 'time', 'condition', 'overall', 'goal', 'examine', 'construct', 'validity', 'effective', 'ability', 'score', 'obtain', 'untimed', 'time', 'condition', 'compare', 'effect', 'theorybase', 'item', 'property', 'item', 'difficulty', 'effect', 'exist', 'score', 'reflect', 'testtaker', 'able', 'cope', 'theorybase', 'requirement', 'german', 'subsample', 'PISA', '2012', 'complete', 'read', 'component', 'skill', 'task', 'ie', 'word', 'recognition', 'semantic', 'integration', 'itemlevel', 'time', 'limit', 'overall', 'include', 'linguistic', 'item', 'property', 'strong', 'effect', 'item', 'difficulty', 'time', 'untimed', 'condition', 'semantic', 'integration', 'task', 'item', 'property', 'explain', 'time', 'require', 'untimed', 'condition', 'result', 'suggest', 'effective', 'ability', 'score', 'time', 'condition', 'reflect', 'testtaker', 'able', 'cope', 'theoretically', 'relevant', 'task', 'demand', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['Timed', 'Testing', 'Affect', 'Interpretation', 'Efficiency', 'Scores', '??, 'GLMM', 'analysis', 'Reading', 'Components']",efficiency cognitive component skill typically assess speeded performance test interpret effective ability effective speed efficiency challenge withinperson dependency variable speedability tradeoff SAT present study measure efficiency effective ability conditional speed control speed experimentally itemlevel time limit control stimulus presentation time time window respond time condition overall goal examine construct validity effective ability score obtain untimed time condition compare effect theorybase item property item difficulty effect exist score reflect testtaker able cope theorybase requirement german subsample PISA 2012 complete read component skill task ie word recognition semantic integration itemlevel time limit overall include linguistic item property strong effect item difficulty time untimed condition semantic integration task item property explain time require untimed condition result suggest effective ability score time condition reflect testtaker able cope theoretically relevant task demand Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,Timed Testing Affect Interpretation Efficiency Scores ??GLMM analysis Reading Components,0.024166867,0.023487719,0.023437647,0.024013966,0.904893802,0.101964339,0,0,0.006947627,0
Madison M.J.; Wind S.A.; Maas L.; Yamaguchi K.; Haab S.,A One-Parameter Diagnostic Classification Model with Familiar Measurement Properties,2024,,"Diagnostic classification models (DCMs) are psychometric models designed to classify examinees according to their proficiency or nonproficiency of specified latent characteristics. These models are well suited for providing diagnostic and actionable feedback to support intermediate and formative assessment efforts. Several DCMs have been developed and applied in different settings. This study examines a DCM with functional form similar to the 1-parameter logistic item response theory model. Using data from a large-scale mathematics education research study, we demonstrate and prove that the proposed DCM has measurement properties akin to the Rasch and one-parameter logistic item response theory models, including sum score sufficiency, item-free and person-free measurement, and invariant item and person ordering. We introduce some potential applications for this model, and discuss the implications and limitations of these developments, as well as directions for future research. Â© 2024 by the National Council on Measurement in Education.",A One-Parameter Diagnostic Classification Model with Familiar Measurement Properties,"Diagnostic classification models (DCMs) are psychometric models designed to classify examinees according to their proficiency or nonproficiency of specified latent characteristics. These models are well suited for providing diagnostic and actionable feedback to support intermediate and formative assessment efforts. Several DCMs have been developed and applied in different settings. This study examines a DCM with functional form similar to the 1-parameter logistic item response theory model. Using data from a large-scale mathematics education research study, we demonstrate and prove that the proposed DCM has measurement properties akin to the Rasch and one-parameter logistic item response theory models, including sum score sufficiency, item-free and person-free measurement, and invariant item and person ordering. We introduce some potential applications for this model, and discuss the implications and limitations of these developments, as well as directions for future research. Â© 2024 by the National Council on Measurement in Education.","['diagnostic', 'classification', 'dcm', 'psychometric', 'design', 'classify', 'examinee', 'accord', 'proficiency', 'nonproficiency', 'specify', 'latent', 'characteristic', 'suited', 'provide', 'diagnostic', 'actionable', 'feedback', 'support', 'intermediate', 'formative', 'assessment', 'effort', 'dcm', 'develop', 'apply', 'different', 'setting', 'study', 'examine', 'dcm', 'functional', 'form', 'similar', '1parameter', 'logistic', 'item', 'response', 'theory', 'datum', 'largescale', 'mathematics', 'research', 'study', 'demonstrate', 'prove', 'propose', 'DCM', 'property', 'akin', 'Rasch', 'oneparameter', 'logistic', 'item', 'response', 'theory', 'include', 'sum', 'score', 'sufficiency', 'itemfree', 'personfree', 'invariant', 'item', 'person', 'order', 'introduce', 'potential', 'application', 'discuss', 'implication', 'limitation', 'development', 'direction', 'future', 'research', 'Â©', '2024', 'National', 'Council']","['OneParameter', 'Diagnostic', 'Classification', 'Familiar', 'property']",diagnostic classification dcm psychometric design classify examinee accord proficiency nonproficiency specify latent characteristic suited provide diagnostic actionable feedback support intermediate formative assessment effort dcm develop apply different setting study examine dcm functional form similar 1parameter logistic item response theory datum largescale mathematics research study demonstrate prove propose DCM property akin Rasch oneparameter logistic item response theory include sum score sufficiency itemfree personfree invariant item person order introduce potential application discuss implication limitation development direction future research Â© 2024 National Council,OneParameter Diagnostic Classification Familiar property,0.900694275,0.024864198,0.024784589,0.024890986,0.024765952,0.036320378,0.041474327,0,0.038503234,0.004530668
Gerasimova D.,Argument-Based Approach to Validity: Developing a Living Document and Incorporating Preregistration,2024,61,"I propose two practical advances to the argument-based approach to validity: developing a living document and incorporating preregistration. First, I present a potential structure for the living document that includes an up-to-date summary of the validity argument. As the validation process may span across multiple studies, the living document allows future users of the instrument to access the entire validity argument in one place. Second, I describe how preregistration can be incorporated in the argument-based approach. Specifically, I distinguish between two types of preregistration: preregistration of the argument and preregistration of validation studies. Preregistration of the argument is a single preregistration that is specified for the entire validation process. Here, the developer specifies interpretations, uses, and claims before collecting validity evidence. Preregistration of a validation study refers to preregistering a single validation study that aims to evaluate a set of claims. Here, the developer describes study components (e.g., research design, data collection, data analysis, etc.), before collecting data. Both preregistration types have the potential to reduce the risk of bias (e.g., hindsight and confirmation biases), as well as to allow others to evaluate the risk of bias and, hence, calibrate confidence, in the developer's evaluation of the validity argument. Â© 2024 by the National Council on Measurement in Education.",Argument-Based Approach to Validity: Developing a Living Document and Incorporating Preregistration,"I propose two practical advances to the argument-based approach to validity: developing a living document and incorporating preregistration. First, I present a potential structure for the living document that includes an up-to-date summary of the validity argument. As the validation process may span across multiple studies, the living document allows future users of the instrument to access the entire validity argument in one place. Second, I describe how preregistration can be incorporated in the argument-based approach. Specifically, I distinguish between two types of preregistration: preregistration of the argument and preregistration of validation studies. Preregistration of the argument is a single preregistration that is specified for the entire validation process. Here, the developer specifies interpretations, uses, and claims before collecting validity evidence. Preregistration of a validation study refers to preregistering a single validation study that aims to evaluate a set of claims. Here, the developer describes study components (e.g., research design, data collection, data analysis, etc.), before collecting data. Both preregistration types have the potential to reduce the risk of bias (e.g., hindsight and confirmation biases), as well as to allow others to evaluate the risk of bias and, hence, calibrate confidence, in the developer's evaluation of the validity argument. Â© 2024 by the National Council on Measurement in Education.","['I', 'propose', 'practical', 'advance', 'argumentbased', 'approach', 'validity', 'develop', 'live', 'document', 'incorporate', 'preregistration', 'First', 'I', 'present', 'potential', 'structure', 'living', 'document', 'include', 'uptodate', 'summary', 'validity', 'argument', 'validation', 'process', 'span', 'multiple', 'study', 'live', 'document', 'allow', 'future', 'user', 'instrument', 'access', 'entire', 'validity', 'argument', 'place', 'second', 'I', 'describe', 'preregistration', 'incorporate', 'argumentbased', 'approach', 'specifically', 'I', 'distinguish', 'type', 'preregistration', 'preregistration', 'argument', 'preregistration', 'validation', 'study', 'Preregistration', 'argument', 'single', 'preregistration', 'specify', 'entire', 'validation', 'process', 'developer', 'specifie', 'interpretation', 'claim', 'collect', 'validity', 'evidence', 'Preregistration', 'validation', 'study', 'refer', 'preregistere', 'single', 'validation', 'study', 'aim', 'evaluate', 'set', 'claim', 'developer', 'describe', 'study', 'component', 'eg', 'research', 'design', 'datum', 'collection', 'datum', 'analysis', 'etc', 'collect', 'datum', 'preregistration', 'type', 'potential', 'reduce', 'risk', 'bias', 'eg', 'hindsight', 'confirmation', 'bias', 'allow', 'evaluate', 'risk', 'bias', 'calibrate', 'confidence', 'developer', 'evaluation', 'validity', 'argument', 'Â©', '2024', 'National', 'Council']","['ArgumentBased', 'Approach', 'Validity', 'develop', 'Living', 'Document', 'incorporate', 'Preregistration']",I propose practical advance argumentbased approach validity develop live document incorporate preregistration First I present potential structure living document include uptodate summary validity argument validation process span multiple study live document allow future user instrument access entire validity argument place second I describe preregistration incorporate argumentbased approach specifically I distinguish type preregistration preregistration argument preregistration validation study Preregistration argument single preregistration specify entire validation process developer specifie interpretation claim collect validity evidence Preregistration validation study refer preregistere single validation study aim evaluate set claim developer describe study component eg research design datum collection datum analysis etc collect datum preregistration type potential reduce risk bias eg hindsight confirmation bias allow evaluate risk bias calibrate confidence developer evaluation validity argument Â© 2024 National Council,ArgumentBased Approach Validity develop Living Document incorporate Preregistration,0.030309525,0.029235246,0.880915191,0.030050422,0.029489616,0.001837573,0,0.001088754,0.070891922,0.02340127
Fu J.; Tan X.; Kyllonen P.C.,Information Functions of Rank-2PL Models for Forced-Choice Questionnaires,2024,61,"This paper presents the item and test information functions of the Rank two-parameter logistic models (Rank-2PLM) for items with two (pair) and three (triplet) statements in forced-choice questionnaires. The Rank-2PLM model for pairs is the MUPP-2PLM (Multi-Unidimensional Pairwise Preference) and, for triplets, is the Triplet-2PLM. Fisher's information and directional information are described, and the test information for Maximum Likelihood (ML), Maximum A Posterior (MAP), and Expected A Posterior (EAP) trait score estimates is distinguished. Expected item/test information indexes at various levels are proposed and plotted to provide diagnostic information on items and tests. The expected test information indexes for EAP scores may be difficult to compute due to a typical test's vast number of item response patterns. The relationships of item/test information with discrimination parameters of statements, standard error, and reliability estimates of trait score estimates are discussed and demonstrated using real data. Practical suggestions for checking the various expected item/test information indexes and plots are provided. Â© 2023 by the National Council on Measurement in Education.",Information Functions of Rank-2PL Models for Forced-Choice Questionnaires,"This paper presents the item and test information functions of the Rank two-parameter logistic models (Rank-2PLM) for items with two (pair) and three (triplet) statements in forced-choice questionnaires. The Rank-2PLM model for pairs is the MUPP-2PLM (Multi-Unidimensional Pairwise Preference) and, for triplets, is the Triplet-2PLM. Fisher's information and directional information are described, and the test information for Maximum Likelihood (ML), Maximum A Posterior (MAP), and Expected A Posterior (EAP) trait score estimates is distinguished. Expected item/test information indexes at various levels are proposed and plotted to provide diagnostic information on items and tests. The expected test information indexes for EAP scores may be difficult to compute due to a typical test's vast number of item response patterns. The relationships of item/test information with discrimination parameters of statements, standard error, and reliability estimates of trait score estimates are discussed and demonstrated using real data. Practical suggestions for checking the various expected item/test information indexes and plots are provided. Â© 2023 by the National Council on Measurement in Education.","['paper', 'present', 'item', 'test', 'information', 'function', 'Rank', 'twoparameter', 'logistic', 'Rank2PLM', 'item', 'pair', 'triplet', 'statement', 'forcedchoice', 'questionnaire', 'rank2plm', 'pair', 'MUPP2PLM', 'MultiUnidimensional', 'Pairwise', 'Preference', 'triplet', 'Triplet2PLM', 'fisher', 'information', 'directional', 'information', 'describe', 'test', 'information', 'Maximum', 'Likelihood', 'ML', 'Maximum', 'A', 'Posterior', 'MAP', 'expect', 'Posterior', 'EAP', 'trait', 'score', 'estimate', 'distinguish', 'expect', 'itemtest', 'information', 'index', 'level', 'propose', 'plot', 'provide', 'diagnostic', 'information', 'item', 'test', 'expected', 'test', 'information', 'index', 'EAP', 'score', 'difficult', 'compute', 'typical', 'test', 'vast', 'number', 'item', 'response', 'pattern', 'relationship', 'itemtest', 'information', 'discrimination', 'parameter', 'statement', 'standard', 'error', 'reliability', 'estimate', 'trait', 'score', 'estimate', 'discuss', 'demonstrate', 'real', 'datum', 'practical', 'suggestion', 'check', 'expect', 'itemt', 'information', 'index', 'plot', 'provide', 'Â©', '2023', 'National', 'Council']","['Information', 'Functions', 'Rank2PL', 'Models', 'ForcedChoice', 'Questionnaires']",paper present item test information function Rank twoparameter logistic Rank2PLM item pair triplet statement forcedchoice questionnaire rank2plm pair MUPP2PLM MultiUnidimensional Pairwise Preference triplet Triplet2PLM fisher information directional information describe test information Maximum Likelihood ML Maximum A Posterior MAP expect Posterior EAP trait score estimate distinguish expect itemtest information index level propose plot provide diagnostic information item test expected test information index EAP score difficult compute typical test vast number item response pattern relationship itemtest information discrimination parameter statement standard error reliability estimate trait score estimate discuss demonstrate real datum practical suggestion check expect itemt information index plot provide Â© 2023 National Council,Information Functions Rank2PL Models ForcedChoice Questionnaires,0.893827972,0.02626245,0.026345801,0.026859259,0.026704518,0.01760171,0.081558794,0.007688879,0,0.016280968
Castellano K.E.; McCaffrey D.F.; Lockwood J.R.,An Exploration of an Improved Aggregate Student Growth Measure Using Data from Two States,2023,60,"The simple average of student growth scores is often used in accountability systems, but it can be problematic for decision making. When computed using a small/moderate number of students, it can be sensitive to the sample, resulting in inaccurate representations of growth of the students, low year-to-year stability, and inequities for low-incidence groups. An alternative designed to address these issues is to use an Empirical Best Linear Prediction (EBLP), which is a weighted average of growth score data from other years and/or subjects. We apply both approaches to two statewide datasets to answer empirical questions about their performance. The EBLP outperforms the simple average in accuracy and cross-year stability with the exception that accuracy was not necessarily improved for very large districts in one of the states. In such exceptions, we show a beneficial alternative may be to use a hybrid approach in which very large districts receive the simple average and all others receive the EBLP. We find that adding more growth score data to the computation of the EBLP can improve accuracy, but not necessarily for larger schools/districts. We review key decision points in aggregate growth reporting and in specifying an EBLP weighted average in practice. Â© 2023 by the National Council on Measurement in Education.",An Exploration of an Improved Aggregate Student Growth Measure Using Data from Two States,"The simple average of student growth scores is often used in accountability systems, but it can be problematic for decision making. When computed using a small/moderate number of students, it can be sensitive to the sample, resulting in inaccurate representations of growth of the students, low year-to-year stability, and inequities for low-incidence groups. An alternative designed to address these issues is to use an Empirical Best Linear Prediction (EBLP), which is a weighted average of growth score data from other years and/or subjects. We apply both approaches to two statewide datasets to answer empirical questions about their performance. The EBLP outperforms the simple average in accuracy and cross-year stability with the exception that accuracy was not necessarily improved for very large districts in one of the states. In such exceptions, we show a beneficial alternative may be to use a hybrid approach in which very large districts receive the simple average and all others receive the EBLP. We find that adding more growth score data to the computation of the EBLP can improve accuracy, but not necessarily for larger schools/districts. We review key decision points in aggregate growth reporting and in specifying an EBLP weighted average in practice. Â© 2023 by the National Council on Measurement in Education.","['simple', 'average', 'student', 'growth', 'score', 'accountability', 'system', 'problematic', 'decision', 'compute', 'smallmoderate', 'number', 'student', 'sensitive', 'sample', 'result', 'inaccurate', 'representation', 'growth', 'student', 'low', 'yeartoyear', 'stability', 'inequity', 'lowincidence', 'group', 'alternative', 'design', 'address', 'issue', 'Empirical', 'Best', 'Linear', 'Prediction', 'EBLP', 'weighted', 'average', 'growth', 'score', 'datum', 'year', 'andor', 'subject', 'apply', 'approach', 'statewide', 'dataset', 'answer', 'empirical', 'question', 'performance', 'EBLP', 'outperform', 'simple', 'average', 'accuracy', 'crossyear', 'stability', 'exception', 'accuracy', 'necessarily', 'improve', 'large', 'district', 'state', 'exception', 'beneficial', 'alternative', 'hybrid', 'approach', 'large', 'district', 'receive', 'simple', 'average', 'receive', 'EBLP', 'find', 'add', 'growth', 'score', 'datum', 'computation', 'EBLP', 'improve', 'accuracy', 'necessarily', 'large', 'schoolsdistrict', 'review', 'key', 'decision', 'point', 'aggregate', 'growth', 'reporting', 'specify', 'EBLP', 'weight', 'average', 'practice', 'Â©', '2023', 'National', 'Council']","['Exploration', 'improved', 'Aggregate', 'Student', 'Growth', 'measure', 'Data', 'state']",simple average student growth score accountability system problematic decision compute smallmoderate number student sensitive sample result inaccurate representation growth student low yeartoyear stability inequity lowincidence group alternative design address issue Empirical Best Linear Prediction EBLP weighted average growth score datum year andor subject apply approach statewide dataset answer empirical question performance EBLP outperform simple average accuracy crossyear stability exception accuracy necessarily improve large district state exception beneficial alternative hybrid approach large district receive simple average receive EBLP find add growth score datum computation EBLP improve accuracy necessarily large schoolsdistrict review key decision point aggregate growth reporting specify EBLP weight average practice Â© 2023 National Council,Exploration improved Aggregate Student Growth measure Data state,0.02657019,0.026259927,0.026405337,0.894166977,0.026597569,0,0.078241912,0.007254902,0.016348111,0
Baldwin P.; Clauser B.E.,Historical Perspectives on Score Comparability Issues Raised by Innovations in Testing,2022,59,"While score comparability across test forms typically relies on common (or randomly equivalent) examinees or items, innovations in item formats, test delivery, and efforts to extend the range of score interpretation may require a special data collection before examinees or items can be used in this way?”or may be incompatible with common examinee or item designs altogether. When comparisons are necessary under these nonroutine conditions, forms still must be connected by something and this article focuses on these form-invariant connective somethings. A conceptual framework for thinking about the problem of score comparability in this way is given followed by a description of three classes of connectives. Examples from the history of innovations in testing are given for each class. Â© 2022 by the National Council on Measurement in Education.",Historical Perspectives on Score Comparability Issues Raised by Innovations in Testing,"While score comparability across test forms typically relies on common (or randomly equivalent) examinees or items, innovations in item formats, test delivery, and efforts to extend the range of score interpretation may require a special data collection before examinees or items can be used in this way?”or may be incompatible with common examinee or item designs altogether. When comparisons are necessary under these nonroutine conditions, forms still must be connected by something and this article focuses on these form-invariant connective somethings. A conceptual framework for thinking about the problem of score comparability in this way is given followed by a description of three classes of connectives. Examples from the history of innovations in testing are given for each class. Â© 2022 by the National Council on Measurement in Education.","['score', 'comparability', 'test', 'form', 'typically', 'rely', 'common', 'randomly', 'equivalent', 'examinee', 'item', 'innovation', 'item', 'format', 'test', 'delivery', 'effort', 'extend', 'range', 'score', 'interpretation', 'require', 'special', 'datum', 'collection', 'examinee', 'item', 'way', '??, 'incompatible', 'common', 'examinee', 'item', 'design', 'altogether', 'comparison', 'necessary', 'nonroutine', 'condition', 'form', 'connect', 'article', 'focus', 'forminvariant', 'connective', 'conceptual', 'framework', 'think', 'problem', 'score', 'comparability', 'way', 'follow', 'description', 'class', 'connective', 'example', 'history', 'innovation', 'testing', 'class', 'Â©', '2022', 'National', 'Council']","['historical', 'Perspectives', 'Score', 'Comparability', 'issue', 'raise', 'innovation', 'testing']",score comparability test form typically rely common randomly equivalent examinee item innovation item format test delivery effort extend range score interpretation require special datum collection examinee item way ??incompatible common examinee item design altogether comparison necessary nonroutine condition form connect article focus forminvariant connective conceptual framework think problem score comparability way follow description class connective example history innovation testing class Â© 2022 National Council,historical Perspectives Score Comparability issue raise innovation testing,0.027358267,0.89059541,0.027183193,0.027553989,0.02730914,0.005960916,0.075625197,0,0.037628589,0
Huggins-Manley C.; Raborn A.W.; Jones P.K.; Myers T.,A Nonparametric Composite Group DIF Index for Focal Groups Stemming from Multicategorical Variables,2024,,"The purpose of this study is to develop a nonparametric DIF method that (a) compares focal groups directly to the composite group that will be used to develop the reported test score scale, and (b) allows practitioners to explore for DIF related to focal groups stemming from multicategorical variables that constitute a small proportion of the overall testing population. We propose the nonparametric root expected proportion squared difference (REPSD) index that evaluates the statistical significance of composite group DIF for relatively small focal groups stemming from multicategorical focal variables, with decisions of statistical significance based on quasi-exact p values obtained from Monte Carlo permutations of the DIF statistic under the null distribution. We conduct a simulation to evaluate conditions under which the index produces acceptable Type I error and power rates, as well as an application to a school district assessment. Practitioners can calculate the REPSD index in a freely available package we created in the R environment. Â© 2024 by the National Council on Measurement in Education.",A Nonparametric Composite Group DIF Index for Focal Groups Stemming from Multicategorical Variables,"The purpose of this study is to develop a nonparametric DIF method that (a) compares focal groups directly to the composite group that will be used to develop the reported test score scale, and (b) allows practitioners to explore for DIF related to focal groups stemming from multicategorical variables that constitute a small proportion of the overall testing population. We propose the nonparametric root expected proportion squared difference (REPSD) index that evaluates the statistical significance of composite group DIF for relatively small focal groups stemming from multicategorical focal variables, with decisions of statistical significance based on quasi-exact p values obtained from Monte Carlo permutations of the DIF statistic under the null distribution. We conduct a simulation to evaluate conditions under which the index produces acceptable Type I error and power rates, as well as an application to a school district assessment. Practitioners can calculate the REPSD index in a freely available package we created in the R environment. Â© 2024 by the National Council on Measurement in Education.","['purpose', 'study', 'develop', 'nonparametric', 'DIF', 'method', 'compare', 'focal', 'group', 'directly', 'composite', 'group', 'develop', 'report', 'test', 'score', 'scale', 'b', 'allow', 'practitioner', 'explore', 'DIF', 'relate', 'focal', 'group', 'stem', 'multicategorical', 'variable', 'constitute', 'small', 'proportion', 'overall', 'testing', 'population', 'propose', 'nonparametric', 'root', 'expect', 'proportion', 'square', 'difference', 'repsd', 'index', 'evaluate', 'statistical', 'significance', 'composite', 'group', 'DIF', 'relatively', 'small', 'focal', 'group', 'stem', 'multicategorical', 'focal', 'variable', 'decision', 'statistical', 'significance', 'base', 'quasiexact', 'p', 'value', 'obtain', 'Monte', 'Carlo', 'permutation', 'DIF', 'statistic', 'null', 'distribution', 'conduct', 'simulation', 'evaluate', 'condition', 'index', 'produce', 'acceptable', 'Type', 'I', 'error', 'power', 'rate', 'application', 'school', 'district', 'assessment', 'practitioner', 'calculate', 'repsd', 'index', 'freely', 'available', 'package', 'create', 'r', 'environment', 'Â©', '2024', 'National', 'Council']","['Nonparametric', 'Composite', 'Group', 'DIF', 'Index', 'Focal', 'Groups', 'Stemming', 'multicategorical', 'Variables']",purpose study develop nonparametric DIF method compare focal group directly composite group develop report test score scale b allow practitioner explore DIF relate focal group stem multicategorical variable constitute small proportion overall testing population propose nonparametric root expect proportion square difference repsd index evaluate statistical significance composite group DIF relatively small focal group stem multicategorical focal variable decision statistical significance base quasiexact p value obtain Monte Carlo permutation DIF statistic null distribution conduct simulation evaluate condition index produce acceptable Type I error power rate application school district assessment practitioner calculate repsd index freely available package create r environment Â© 2024 National Council,Nonparametric Composite Group DIF Index Focal Groups Stemming multicategorical Variables,0.024988451,0.024467675,0.024538185,0.901359724,0.024645965,0,0,0.016119742,0.00552557,0.171118274
Deribo T.; Kroehne U.; Goldhammer F.,Model-Based Treatment of Rapid Guessing,2021,58,"The increased availability of time-related information as a result of computer-based assessment has enabled new ways to measure test-taking engagement. One of these ways is to distinguish between solution and rapid guessing behavior. Prior research has recommended response-level filtering to deal with rapid guessing. Response-level filtering can lead to parameter bias if rapid guessing depends on the measured trait or (un-)observed covariates. Therefore, a model based on Mislevy and Wu (1996) was applied to investigate the assumption of ignorable missing data underlying response-level filtering. The model allowed us to investigate different approaches to treating response-level filtered responses in a single framework through model parameterization. The study found that lower-ability test-takers tend to rapidly guess more frequently and are more likely to be unable to solve an item they guessed on, indicating a violation of the assumption of ignorable missing data underlying response-level filtering. Further ability estimation seemed sensitive to different approaches to treating response-level filtered responses. Moreover, model-based approaches exhibited better model fit and higher convergent validity evidence compared to more naÃ¯ve treatments of rapid guessing. The results illustrate the need to thoroughly investigate the assumptions underlying specific treatments of rapid guessing as well as the need for robust methods. Â© 2021 by the National Council on Measurement in Education",,"The increased availability of time-related information as a result of computer-based assessment has enabled new ways to measure test-taking engagement. One of these ways is to distinguish between solution and rapid guessing behavior. Prior research has recommended response-level filtering to deal with rapid guessing. Response-level filtering can lead to parameter bias if rapid guessing depends on the measured trait or (un-)observed covariates. Therefore, a model based on Mislevy and Wu (1996) was applied to investigate the assumption of ignorable missing data underlying response-level filtering. The model allowed us to investigate different approaches to treating response-level filtered responses in a single framework through model parameterization. The study found that lower-ability test-takers tend to rapidly guess more frequently and are more likely to be unable to solve an item they guessed on, indicating a violation of the assumption of ignorable missing data underlying response-level filtering. Further ability estimation seemed sensitive to different approaches to treating response-level filtered responses. Moreover, model-based approaches exhibited better model fit and higher convergent validity evidence compared to more naÃ¯ve treatments of rapid guessing. The results illustrate the need to thoroughly investigate the assumptions underlying specific treatments of rapid guessing as well as the need for robust methods. Â© 2021 by the National Council on Measurement in Education","['increase', 'availability', 'timerelate', 'information', 'result', 'computerbase', 'assessment', 'enable', 'new', 'way', 'measure', 'testtaking', 'engagement', 'way', 'distinguish', 'solution', 'rapid', 'guess', 'behavior', 'Prior', 'research', 'recommend', 'responselevel', 'filtering', 'deal', 'rapid', 'guess', 'Responselevel', 'filtering', 'lead', 'parameter', 'bias', 'rapid', 'guessing', 'depend', 'measured', 'trait', 'unobserved', 'covariate', 'base', 'Mislevy', 'Wu', '1996', 'apply', 'investigate', 'assumption', 'ignorable', 'miss', 'datum', 'underlie', 'responselevel', 'filtering', 'allow', 'investigate', 'different', 'approach', 'treat', 'responselevel', 'filter', 'response', 'single', 'framework', 'parameterization', 'study', 'find', 'lowerability', 'testtaker', 'tend', 'rapidly', 'guess', 'frequently', 'likely', 'unable', 'solve', 'item', 'guess', 'indicate', 'violation', 'assumption', 'ignorable', 'miss', 'datum', 'underlie', 'responselevel', 'filter', 'ability', 'estimation', 'sensitive', 'different', 'approach', 'treat', 'responselevel', 'filter', 'response', 'modelbase', 'approach', 'exhibit', 'fit', 'high', 'convergent', 'validity', 'evidence', 'compare', 'naÃ¯ve', 'treatment', 'rapid', 'guess', 'result', 'illustrate', 'need', 'thoroughly', 'investigate', 'assumption', 'underlie', 'specific', 'treatment', 'rapid', 'guessing', 'need', 'robust', 'method', 'Â©', '2021', 'National', 'Council']",,increase availability timerelate information result computerbase assessment enable new way measure testtaking engagement way distinguish solution rapid guess behavior Prior research recommend responselevel filtering deal rapid guess Responselevel filtering lead parameter bias rapid guessing depend measured trait unobserved covariate base Mislevy Wu 1996 apply investigate assumption ignorable miss datum underlie responselevel filtering allow investigate different approach treat responselevel filter response single framework parameterization study find lowerability testtaker tend rapidly guess frequently likely unable solve item guess indicate violation assumption ignorable miss datum underlie responselevel filter ability estimation sensitive different approach treat responselevel filter response modelbase approach exhibit fit high convergent validity evidence compare naÃ¯ve treatment rapid guess result illustrate need thoroughly investigate assumption underlie specific treatment rapid guessing need robust method Â© 2021 National Council,,0.025540018,0.025161832,0.025196957,0.898677211,0.025423982,0.03353192,0.010896284,0.003990838,0.026445643,0.007699871
Yuan L.; Huang Y.; Li S.; Chen P.,Online Calibration in Multidimensional Computerized Adaptive Testing with Polytomously Scored Items,2023,60,"Online calibration is a key technology for item calibration in computerized adaptive testing (CAT) and has been widely used in various forms of CAT, including unidimensional CAT, multidimensional CAT (MCAT), CAT with polytomously scored items, and cognitive diagnostic CAT. However, as multidimensional and polytomous assessment data become more common, only a few published reports focus on online calibration in MCAT with polytomously scored items (P-MCAT). Therefore, standing on the shoulders of the existing online calibration methods/designs, this study proposes four new P-MCAT online calibration methods and two new P-MCAT online calibration designs and conducts two simulation studies to evaluate their performance under varying conditions (i.e., different calibration sample sizes and correlations between dimensions). Results show that all of the newly proposed methods can accurately recover item parameters, and the adaptive designs outperform the random design in most cases. In the end, this paper provides practical guidance based on simulation results. Â© 2022 by the National Council on Measurement in Education.",Online Calibration in Multidimensional Computerized Adaptive Testing with Polytomously Scored Items,"Online calibration is a key technology for item calibration in computerized adaptive testing (CAT) and has been widely used in various forms of CAT, including unidimensional CAT, multidimensional CAT (MCAT), CAT with polytomously scored items, and cognitive diagnostic CAT. However, as multidimensional and polytomous assessment data become more common, only a few published reports focus on online calibration in MCAT with polytomously scored items (P-MCAT). Therefore, standing on the shoulders of the existing online calibration methods/designs, this study proposes four new P-MCAT online calibration methods and two new P-MCAT online calibration designs and conducts two simulation studies to evaluate their performance under varying conditions (i.e., different calibration sample sizes and correlations between dimensions). Results show that all of the newly proposed methods can accurately recover item parameters, and the adaptive designs outperform the random design in most cases. In the end, this paper provides practical guidance based on simulation results. Â© 2022 by the National Council on Measurement in Education.","['online', 'calibration', 'key', 'technology', 'item', 'calibration', 'computerized', 'adaptive', 'testing', 'CAT', 'widely', 'form', 'CAT', 'include', 'unidimensional', 'CAT', 'multidimensional', 'CAT', 'MCAT', 'CAT', 'polytomously', 'score', 'item', 'cognitive', 'diagnostic', 'CAT', 'multidimensional', 'polytomous', 'assessment', 'datum', 'common', 'publish', 'report', 'focus', 'online', 'calibration', 'MCAT', 'polytomously', 'score', 'item', 'PMCAT', 'stand', 'shoulder', 'exist', 'online', 'calibration', 'methodsdesign', 'study', 'propose', 'new', 'PMCAT', 'online', 'calibration', 'method', 'new', 'PMCAT', 'online', 'calibration', 'design', 'conduct', 'simulation', 'study', 'evaluate', 'performance', 'vary', 'condition', 'ie', 'different', 'calibration', 'sample', 'size', 'correlation', 'dimension', 'result', 'newly', 'propose', 'method', 'accurately', 'recover', 'item', 'parameter', 'adaptive', 'design', 'outperform', 'random', 'design', 'case', 'end', 'paper', 'provide', 'practical', 'guidance', 'base', 'simulation', 'result', 'Â©', '2022', 'National', 'Council']","['Online', 'Calibration', 'Multidimensional', 'Computerized', 'Adaptive', 'Testing', 'Polytomously', 'Scored', 'item']",online calibration key technology item calibration computerized adaptive testing CAT widely form CAT include unidimensional CAT multidimensional CAT MCAT CAT polytomously score item cognitive diagnostic CAT multidimensional polytomous assessment datum common publish report focus online calibration MCAT polytomously score item PMCAT stand shoulder exist online calibration methodsdesign study propose new PMCAT online calibration method new PMCAT online calibration design conduct simulation study evaluate performance vary condition ie different calibration sample size correlation dimension result newly propose method accurately recover item parameter adaptive design outperform random design case end paper provide practical guidance base simulation result Â© 2022 National Council,Online Calibration Multidimensional Computerized Adaptive Testing Polytomously Scored item,0.881463205,0.029562615,0.029494774,0.029832766,0.02964664,0,0.141079364,0.002080897,0,0.003024906
Gong T.; Shuai L.; Mislevy R.J.,Sociocognitive Processes and Item Response Models: A Didactic Example,2024,61,"The usual interpretation of the person and task variables in between-persons measurement models such as item response theory (IRT) is as attributes of persons and tasks, respectively. They can be viewed instead as ensemble descriptors of patterns of interactions among persons and situations that arise from sociocognitive complex adaptive system (CASs). This view offers insights for interpreting and using between-persons measurement models and connecting with sociocognitive research. In this article, we use data generated from an agent-based model to illustrate relations between ?œsocial??and ?œcognitive??features of a simple underlying CAS and the variables of an IRT model fit to resulting data. We note how the ideas connect to explanatory item response modeling and briefly comment on implications for score interpretations and uses in practice. Â© 2023 by the National Council on Measurement in Education.",Sociocognitive Processes and Item Response Models: A Didactic Example,"The usual interpretation of the person and task variables in between-persons measurement models such as item response theory (IRT) is as attributes of persons and tasks, respectively. They can be viewed instead as ensemble descriptors of patterns of interactions among persons and situations that arise from sociocognitive complex adaptive system (CASs). This view offers insights for interpreting and using between-persons measurement models and connecting with sociocognitive research. In this article, we use data generated from an agent-based model to illustrate relations between ?œsocial??and ?œcognitive??features of a simple underlying CAS and the variables of an IRT model fit to resulting data. We note how the ideas connect to explanatory item response modeling and briefly comment on implications for score interpretations and uses in practice. Â© 2023 by the National Council on Measurement in Education.","['usual', 'interpretation', 'person', 'task', 'variable', 'betweenperson', 'item', 'response', 'theory', 'IRT', 'attribute', 'person', 'task', 'respectively', 'view', 'instead', 'ensemble', 'descriptor', 'pattern', 'interaction', 'person', 'situation', 'arise', 'sociocognitive', 'complex', 'adaptive', 'system', 'cas', 'view', 'offer', 'insight', 'interpreting', 'betweenperson', 'connect', 'sociocognitive', 'research', 'article', 'datum', 'generate', 'agentbase', 'illustrate', 'relation', '""', 'social', '""', '""', 'cognitive', '""', 'feature', 'simple', 'underlie', 'CAS', 'variable', 'IRT', 'fit', 'result', 'datum', 'note', 'idea', 'connect', 'explanatory', 'item', 'response', 'modeling', 'briefly', 'comment', 'implication', 'score', 'interpretation', 'practice', 'Â©', '2023', 'National', 'Council']","['Sociocognitive', 'Processes', 'Item', 'Response', 'Models', 'Didactic', 'example']","usual interpretation person task variable betweenperson item response theory IRT attribute person task respectively view instead ensemble descriptor pattern interaction person situation arise sociocognitive complex adaptive system cas view offer insight interpreting betweenperson connect sociocognitive research article datum generate agentbase illustrate relation "" social "" "" cognitive "" feature simple underlie CAS variable IRT fit result datum note idea connect explanatory item response modeling briefly comment implication score interpretation practice Â© 2023 National Council",Sociocognitive Processes Item Response Models Didactic example,0.026781263,0.026657121,0.026613589,0.893299039,0.026648988,0.035671588,0.008385615,0,0.039844084,0
Park S.; Kim K.Y.; Lee W.-C.,Estimating Classification Accuracy and Consistency Indices for Multiple Measures with the Simple Structure MIRT Model,2023,60,"Multiple measures, such as multiple content domains or multiple types of performance, are used in various testing programs to classify examinees for screening or selection. Despite the popular usages of multiple measures, there is little research on classification consistency and accuracy of multiple measures. Accordingly, this study introduces an approach to estimate classification consistency and accuracy indices for multiple measures under four possible decision rules: (1) complementary, (2) conjunctive, (3) compensatory, and (4) pairwise combinations of the three. The current study uses the IRT-recursive-based approach with the simple-structure multidimensional IRT model (SS-MIRT) to estimate the classification consistency and accuracy for multiple measures. Theoretical formulations of the four decision rules with a binary decision (Pass/Fail) are presented. The estimation procedures are illustrated using an empirical data example based on SS-MIRT. In addition, this study applies the estimation procedures to the unidimensional IRT (UIRT) context, considering that UIRT is practically used more. This application shows that the proposed procedure of classification consistency and accuracy could be used with a UIRT model for individual measures as an alternative method of SS-MIRT. Â© 2022 by the National Council on Measurement in Education.",Estimating Classification Accuracy and Consistency Indices for Multiple Measures with the Simple Structure MIRT Model,"Multiple measures, such as multiple content domains or multiple types of performance, are used in various testing programs to classify examinees for screening or selection. Despite the popular usages of multiple measures, there is little research on classification consistency and accuracy of multiple measures. Accordingly, this study introduces an approach to estimate classification consistency and accuracy indices for multiple measures under four possible decision rules: (1) complementary, (2) conjunctive, (3) compensatory, and (4) pairwise combinations of the three. The current study uses the IRT-recursive-based approach with the simple-structure multidimensional IRT model (SS-MIRT) to estimate the classification consistency and accuracy for multiple measures. Theoretical formulations of the four decision rules with a binary decision (Pass/Fail) are presented. The estimation procedures are illustrated using an empirical data example based on SS-MIRT. In addition, this study applies the estimation procedures to the unidimensional IRT (UIRT) context, considering that UIRT is practically used more. This application shows that the proposed procedure of classification consistency and accuracy could be used with a UIRT model for individual measures as an alternative method of SS-MIRT. Â© 2022 by the National Council on Measurement in Education.","['multiple', 'measure', 'multiple', 'content', 'domain', 'multiple', 'type', 'performance', 'testing', 'program', 'classify', 'examinee', 'screening', 'selection', 'despite', 'popular', 'usage', 'multiple', 'measure', 'little', 'research', 'classification', 'consistency', 'accuracy', 'multiple', 'measure', 'accordingly', 'study', 'introduce', 'approach', 'estimate', 'classification', 'consistency', 'accuracy', 'index', 'multiple', 'measure', 'possible', 'decision', 'rule', '1', 'complementary', '2', 'conjunctive', '3', 'compensatory', '4', 'pairwise', 'combination', 'current', 'study', 'irtrecursivebased', 'approach', 'simplestructure', 'multidimensional', 'IRT', 'SSMIRT', 'estimate', 'classification', 'consistency', 'accuracy', 'multiple', 'measure', 'theoretical', 'formulation', 'decision', 'rule', 'binary', 'decision', 'PassFail', 'present', 'estimation', 'procedure', 'illustrate', 'empirical', 'data', 'example', 'base', 'SSMIRT', 'addition', 'study', 'apply', 'estimation', 'procedure', 'unidimensional', 'IRT', 'uirt', 'context', 'consider', 'UIRT', 'practically', 'application', 'propose', 'procedure', 'classification', 'consistency', 'accuracy', 'uirt', 'individual', 'measure', 'alternative', 'method', 'SSMIRT', 'Â©', '2022', 'National', 'Council']","['estimate', 'Classification', 'Accuracy', 'Consistency', 'Indices', 'Multiple', 'Measures', 'Simple', 'structure', 'MIRT']",multiple measure multiple content domain multiple type performance testing program classify examinee screening selection despite popular usage multiple measure little research classification consistency accuracy multiple measure accordingly study introduce approach estimate classification consistency accuracy index multiple measure possible decision rule 1 complementary 2 conjunctive 3 compensatory 4 pairwise combination current study irtrecursivebased approach simplestructure multidimensional IRT SSMIRT estimate classification consistency accuracy multiple measure theoretical formulation decision rule binary decision PassFail present estimation procedure illustrate empirical data example base SSMIRT addition study apply estimation procedure unidimensional IRT uirt context consider UIRT practically application propose procedure classification consistency accuracy uirt individual measure alternative method SSMIRT Â© 2022 National Council,estimate Classification Accuracy Consistency Indices Multiple Measures Simple structure MIRT,0.027077184,0.026518382,0.892408175,0.027343231,0.026653028,0,0.154686285,0,0,0
Lim H.; Choe E.M.,Detecting Differential Item Functioning in CAT Using IRT Residual DIF Approach,2023,60,"The residual differential item functioning (RDIF) detection framework was developed recently under a linear testing context. To explore the potential application of this framework to computerized adaptive testing (CAT), the present study investigated the utility of the RDIFR statistic both as an index for detecting uniform DIF of pretest items in CAT and as a direct measure of the effect size of uniform DIF. Extensive CAT simulations revealed RDIFR to have well-controlled Type I error and slightly higher power to detect uniform DIF compared with CATSIB, especially when pretest items were calibrated using fixed-item parameter calibration. Moreover, RDIFR accurately estimated the amount of uniform DIF irrespective of the presence of impact. Therefore, RDIFR demonstrates its potential as a useful tool for evaluating both the statistical and practical significance of uniform DIF in CAT. Â© 2023 by the National Council on Measurement in Education.",Detecting Differential Item Functioning in CAT Using IRT Residual DIF Approach,"The residual differential item functioning (RDIF) detection framework was developed recently under a linear testing context. To explore the potential application of this framework to computerized adaptive testing (CAT), the present study investigated the utility of the RDIFR statistic both as an index for detecting uniform DIF of pretest items in CAT and as a direct measure of the effect size of uniform DIF. Extensive CAT simulations revealed RDIFR to have well-controlled Type I error and slightly higher power to detect uniform DIF compared with CATSIB, especially when pretest items were calibrated using fixed-item parameter calibration. Moreover, RDIFR accurately estimated the amount of uniform DIF irrespective of the presence of impact. Therefore, RDIFR demonstrates its potential as a useful tool for evaluating both the statistical and practical significance of uniform DIF in CAT. Â© 2023 by the National Council on Measurement in Education.","['residual', 'differential', 'item', 'function', 'RDIF', 'detection', 'framework', 'develop', 'recently', 'linear', 'testing', 'context', 'explore', 'potential', 'application', 'framework', 'computerized', 'adaptive', 'testing', 'CAT', 'present', 'study', 'investigate', 'utility', 'RDIFR', 'statistic', 'index', 'detect', 'uniform', 'DIF', 'pret', 'item', 'CAT', 'direct', 'measure', 'effect', 'size', 'uniform', 'DIF', 'extensive', 'CAT', 'simulation', 'reveal', 'RDIFR', 'wellcontrolle', 'Type', 'I', 'error', 'slightly', 'high', 'power', 'detect', 'uniform', 'DIF', 'compare', 'CATSIB', 'especially', 'pret', 'item', 'calibrate', 'fixeditem', 'parameter', 'calibration', 'RDIFR', 'accurately', 'estimate', 'uniform', 'DIF', 'irrespective', 'presence', 'impact', 'Therefore', 'RDIFR', 'demonstrate', 'potential', 'useful', 'tool', 'evaluate', 'statistical', 'practical', 'significance', 'uniform', 'DIF', 'CAT', 'Â©', '2023', 'National', 'Council']","['detect', 'Differential', 'Item', 'Functioning', 'CAT', 'IRT', 'residual', 'DIF', 'approach']",residual differential item function RDIF detection framework develop recently linear testing context explore potential application framework computerized adaptive testing CAT present study investigate utility RDIFR statistic index detect uniform DIF pret item CAT direct measure effect size uniform DIF extensive CAT simulation reveal RDIFR wellcontrolle Type I error slightly high power detect uniform DIF compare CATSIB especially pret item calibrate fixeditem parameter calibration RDIFR accurately estimate uniform DIF irrespective presence impact Therefore RDIFR demonstrate potential useful tool evaluate statistical practical significance uniform DIF CAT Â© 2023 National Council,detect Differential Item Functioning CAT IRT residual DIF approach,0.029107738,0.028760728,0.028525792,0.884894298,0.028711444,0,0.007292495,0,0,0.222356661
Ranger J.; Kuhn J.-T.; Wolgast A.,Robust Estimation of Ability and Mental Speed Employing the Hierarchical Model for Responses and Response Times,2021,58,"Van der Linden's hierarchical model for responses and response times can be used in order to infer the ability and mental speed of test takers from their responses and response times in an educational test. A standard approach for this is maximum likelihood estimation. In real-world applications, the data of some test takers might be partly irregular, resulting from rapid guessing or item preknowledge. The maximum likelihood estimator is not robust against contamination with irregular data. In this article, we propose a robust estimator of ability and mental speed. The estimator consists of two steps. In the first step, the mental speed is estimated with the estimator of Gervini and Yohai that ignores outlying response times. In the second step, the ability is estimated with an M-estimator that down weights unusual responses given at unusual response times. This is achieved by combining the hard-rejection weights of Gervini and Yohai with the M-estimator suggested by Croux and Haesbroeck for the logistic regression model. The proposed estimator is consistent, almost as efficient as the maximum likelihood estimator in uncontaminated data and robust in contaminated data. The performance of the estimator is analyzed in a simulation study and an empirical example. Â© 2020 by the National Council on Measurement in Education",Robust Estimation of Ability and Mental Speed Employing the Hierarchical Model for Responses and Response Times,"Van der Linden's hierarchical model for responses and response times can be used in order to infer the ability and mental speed of test takers from their responses and response times in an educational test. A standard approach for this is maximum likelihood estimation. In real-world applications, the data of some test takers might be partly irregular, resulting from rapid guessing or item preknowledge. The maximum likelihood estimator is not robust against contamination with irregular data. In this article, we propose a robust estimator of ability and mental speed. The estimator consists of two steps. In the first step, the mental speed is estimated with the estimator of Gervini and Yohai that ignores outlying response times. In the second step, the ability is estimated with an M-estimator that down weights unusual responses given at unusual response times. This is achieved by combining the hard-rejection weights of Gervini and Yohai with the M-estimator suggested by Croux and Haesbroeck for the logistic regression model. The proposed estimator is consistent, almost as efficient as the maximum likelihood estimator in uncontaminated data and robust in contaminated data. The performance of the estimator is analyzed in a simulation study and an empirical example. Â© 2020 by the National Council on Measurement in Education","['Van', 'der', 'linden', 'hierarchical', 'response', 'response', 'time', 'order', 'infer', 'ability', 'mental', 'speed', 'test', 'taker', 'response', 'response', 'time', 'educational', 'test', 'standard', 'approach', 'maximum', 'likelihood', 'estimation', 'realworld', 'application', 'datum', 'test', 'taker', 'partly', 'irregular', 'result', 'rapid', 'guessing', 'item', 'preknowledge', 'maximum', 'likelihood', 'estimator', 'robust', 'contamination', 'irregular', 'datum', 'article', 'propose', 'robust', 'estimator', 'ability', 'mental', 'speed', 'estimator', 'consist', 'step', 'step', 'mental', 'speed', 'estimate', 'estimator', 'Gervini', 'Yohai', 'ignore', 'outlying', 'response', 'time', 'second', 'step', 'ability', 'estimate', 'Mestimator', 'weight', 'unusual', 'response', 'unusual', 'response', 'time', 'achieve', 'combine', 'hardrejection', 'weight', 'Gervini', 'Yohai', 'Mestimator', 'suggest', 'Croux', 'Haesbroeck', 'logistic', 'regression', 'propose', 'estimator', 'consistent', 'efficient', 'maximum', 'likelihood', 'estimator', 'uncontaminated', 'datum', 'robust', 'contaminated', 'datum', 'performance', 'estimator', 'analyze', 'simulation', 'study', 'empirical', 'example', 'Â©', '2020', 'National', 'Council']","['robust', 'Estimation', 'Ability', 'Mental', 'Speed', 'Employing', 'Hierarchical', 'Responses', 'Response', 'Times']",Van der linden hierarchical response response time order infer ability mental speed test taker response response time educational test standard approach maximum likelihood estimation realworld application datum test taker partly irregular result rapid guessing item preknowledge maximum likelihood estimator robust contamination irregular datum article propose robust estimator ability mental speed estimator consist step step mental speed estimate estimator Gervini Yohai ignore outlying response time second step ability estimate Mestimator weight unusual response unusual response time achieve combine hardrejection weight Gervini Yohai Mestimator suggest Croux Haesbroeck logistic regression propose estimator consistent efficient maximum likelihood estimator uncontaminated datum robust contaminated datum performance estimator analyze simulation study empirical example Â© 2020 National Council,robust Estimation Ability Mental Speed Employing Hierarchical Responses Response Times,0.027283027,0.027045571,0.02684268,0.027149018,0.891679704,0.101676946,0,0.003313327,0,0
Tijmstra J.; Bolsinova M.; Liaw Y.-L.; Rutkowski L.; Rutkowski D.,Sensitivity of the RMSD for Detecting Item-Level Misfit in Low-Performing Countries,2020,57,"Although the root-mean squared deviation (RMSD) is a popular statistical measure for evaluating country-specific item-level misfit (i.e., differential item functioning [DIF]) in international large-scale assessment, this paper shows that its sensitivity to detect misfit may depend strongly on the proficiency distribution of the considered countries. Specifically, items for which most respondents in a country have a very low (or high) probability of providing a correct answer will rarely be flagged by the RMSD as showing misfit, even if very strong DIF is present. With many international large-scale assessment initiatives moving toward covering a more heterogeneous group of countries, this raises issues for the ability of the RMSD to detect item-level misfit, especially in low-performing countries that are not well-aligned with the overall difficulty level of the test. This may put one at risk of incorrectly assuming measurement invariance to hold, and may also inflate estimated between-country difference in proficiency. The degree to which the RMSD is able to detect DIF in low-performing countries is studied using both an empirical example from PISA 2015 and a simulationÂ study. Â© 2019 by the National Council on Measurement in Education",Sensitivity of the RMSD for Detecting Item-Level Misfit in Low-Performing Countries,"Although the root-mean squared deviation (RMSD) is a popular statistical measure for evaluating country-specific item-level misfit (i.e., differential item functioning [DIF]) in international large-scale assessment, this paper shows that its sensitivity to detect misfit may depend strongly on the proficiency distribution of the considered countries. Specifically, items for which most respondents in a country have a very low (or high) probability of providing a correct answer will rarely be flagged by the RMSD as showing misfit, even if very strong DIF is present. With many international large-scale assessment initiatives moving toward covering a more heterogeneous group of countries, this raises issues for the ability of the RMSD to detect item-level misfit, especially in low-performing countries that are not well-aligned with the overall difficulty level of the test. This may put one at risk of incorrectly assuming measurement invariance to hold, and may also inflate estimated between-country difference in proficiency. The degree to which the RMSD is able to detect DIF in low-performing countries is studied using both an empirical example from PISA 2015 and a simulationÂ study. Â© 2019 by the National Council on Measurement in Education","['rootmean', 'squared', 'deviation', 'RMSD', 'popular', 'statistical', 'measure', 'evaluate', 'countryspecific', 'itemlevel', 'misfit', 'ie', 'differential', 'item', 'function', 'DIF', 'international', 'largescale', 'assessment', 'paper', 'sensitivity', 'detect', 'misfit', 'depend', 'strongly', 'proficiency', 'distribution', 'consider', 'country', 'specifically', 'item', 'respondent', 'country', 'low', 'high', 'probability', 'provide', 'correct', 'answer', 'rarely', 'flag', 'RMSD', 'misfit', 'strong', 'DIF', 'present', 'international', 'largescale', 'assessment', 'initiative', 'cover', 'heterogeneous', 'group', 'country', 'raise', 'issue', 'ability', 'RMSD', 'detect', 'itemlevel', 'misfit', 'especially', 'lowperforme', 'country', 'wellaligne', 'overall', 'difficulty', 'level', 'test', 'risk', 'incorrectly', 'assume', 'invariance', 'hold', 'inflate', 'estimated', 'betweencountry', 'difference', 'proficiency', 'degree', 'RMSD', 'able', 'detect', 'DIF', 'lowperforme', 'country', 'study', 'empirical', 'example', 'PISA', '2015', 'simulation', 'study', 'Â©', '2019', 'National', 'Council']","['sensitivity', 'RMSD', 'Detecting', 'ItemLevel', 'Misfit', 'LowPerforming', 'country']",rootmean squared deviation RMSD popular statistical measure evaluate countryspecific itemlevel misfit ie differential item function DIF international largescale assessment paper sensitivity detect misfit depend strongly proficiency distribution consider country specifically item respondent country low high probability provide correct answer rarely flag RMSD misfit strong DIF present international largescale assessment initiative cover heterogeneous group country raise issue ability RMSD detect itemlevel misfit especially lowperforme country wellaligne overall difficulty level test risk incorrectly assume invariance hold inflate estimated betweencountry difference proficiency degree RMSD able detect DIF lowperforme country study empirical example PISA 2015 simulation study Â© 2019 National Council,sensitivity RMSD Detecting ItemLevel Misfit LowPerforming country,0.894580915,0.026283517,0.026193821,0.026566566,0.026375181,0.014434343,0,0,0.00783971,0.129666691
Henninger M.,A Novel Partial Credit Extension Using Varying Thresholds to Account for Response Tendencies,2021,58,"Item Response Theory models with varying thresholds are essential tools to account for unknown types of response tendencies in rating data. However, in order to separate constructs to be measured and response tendencies, specific constraints have to be imposed on varying thresholds and their interrelations. In this article, a multidimensional extension of a Partial Credit Model using a sum-to-zero constraint for varying thresholds is proposed. The new model allows us to flexibly account for response tendencies and to model covariations between varying thresholds that are commonly found in empirical data. The model's ability to estimate different types of response tendencies under various data structures is shown in a simulation study. An illustrative multicountry analysis demonstrates that differences between respondents in terms of their response tendencies exist and can be captured by the new model. Furthermore, it is well suited to account for extreme and mid response styles, but also to accommodate unknown, previously unmodeled, response tendencies. Therewith, the sum-to-zero model can be considered a suitable candidate to examine the types response tendencies in rating data and to account for biases in construct measures due to responseÂ tendencies. Â© 2020 by the National Council on Measurement in Education",A Novel Partial Credit Extension Using Varying Thresholds to Account for Response Tendencies,"Item Response Theory models with varying thresholds are essential tools to account for unknown types of response tendencies in rating data. However, in order to separate constructs to be measured and response tendencies, specific constraints have to be imposed on varying thresholds and their interrelations. In this article, a multidimensional extension of a Partial Credit Model using a sum-to-zero constraint for varying thresholds is proposed. The new model allows us to flexibly account for response tendencies and to model covariations between varying thresholds that are commonly found in empirical data. The model's ability to estimate different types of response tendencies under various data structures is shown in a simulation study. An illustrative multicountry analysis demonstrates that differences between respondents in terms of their response tendencies exist and can be captured by the new model. Furthermore, it is well suited to account for extreme and mid response styles, but also to accommodate unknown, previously unmodeled, response tendencies. Therewith, the sum-to-zero model can be considered a suitable candidate to examine the types response tendencies in rating data and to account for biases in construct measures due to responseÂ tendencies. Â© 2020 by the National Council on Measurement in Education","['Item', 'Response', 'Theory', 'vary', 'threshold', 'essential', 'tool', 'account', 'unknown', 'type', 'response', 'tendency', 'rating', 'datum', 'order', 'separate', 'construct', 'measure', 'response', 'tendency', 'specific', 'constraint', 'impose', 'vary', 'threshold', 'interrelation', 'article', 'multidimensional', 'extension', 'Partial', 'Credit', 'sumtozero', 'constraint', 'vary', 'threshold', 'propose', 'new', 'allow', 'flexibly', 'account', 'response', 'tendency', 'covariation', 'vary', 'threshold', 'commonly', 'find', 'empirical', 'datum', 'ability', 'estimate', 'different', 'type', 'response', 'tendency', 'datum', 'structure', 'simulation', 'study', 'illustrative', 'multicountry', 'analysis', 'demonstrate', 'difference', 'respondent', 'term', 'response', 'tendency', 'exist', 'capture', 'new', 'furthermore', 'suited', 'account', 'extreme', 'mid', 'response', 'style', 'accommodate', 'unknown', 'previously', 'unmodeled', 'response', 'tendency', 'Therewith', 'sumtozero', 'consider', 'suitable', 'candidate', 'examine', 'type', 'response', 'tendency', 'rating', 'datum', 'account', 'bias', 'construct', 'measure', 'response', 'tendency', 'Â©', '2020', 'National', 'Council']","['Novel', 'Partial', 'Credit', 'Extension', 'vary', 'Thresholds', 'Account', 'Response', 'tendency']",Item Response Theory vary threshold essential tool account unknown type response tendency rating datum order separate construct measure response tendency specific constraint impose vary threshold interrelation article multidimensional extension Partial Credit sumtozero constraint vary threshold propose new allow flexibly account response tendency covariation vary threshold commonly find empirical datum ability estimate different type response tendency datum structure simulation study illustrative multicountry analysis demonstrate difference respondent term response tendency exist capture new furthermore suited account extreme mid response style accommodate unknown previously unmodeled response tendency Therewith sumtozero consider suitable candidate examine type response tendency rating datum account bias construct measure response tendency Â© 2020 National Council,Novel Partial Credit Extension vary Thresholds Account Response tendency,0.030152843,0.029611861,0.02955931,0.02998413,0.880691857,0.06316395,0,0,0.023202727,0
Chalmers R.P.,A Unified Comparison of IRT-Based Effect Sizes for DIF Investigations,2023,60,"Several marginal effect size (ES) statistics suitable for quantifying the magnitude of differential item functioning (DIF) have been proposed in the area of item response theory; for instance, the Differential Functioning of Items and Tests (DFIT) statistics, signed and unsigned item difference in the sample statistics (SIDS, UIDS, NSIDS, and NUIDS), the standardized indices of impact, and the differential response functioning (DRF) statistics. However, the relationship between these proposed statistics has not been fully discussed, particularly with respect to population parameter definitions and recovery performance across independent samples. To address these issues, this article provides a unified presentation of competing DIF ES definitions and estimators, and evaluates the recovery efficacy of these competing estimators using a set of Monte Carlo simulation experiments. Statistical and inferential properties of the estimators are discussed, as well as future areas of research in this model-based area of biasÂ quantification. Â© 2022 by the National Council on Measurement in Education.",A Unified Comparison of IRT-Based Effect Sizes for DIF Investigations,"Several marginal effect size (ES) statistics suitable for quantifying the magnitude of differential item functioning (DIF) have been proposed in the area of item response theory; for instance, the Differential Functioning of Items and Tests (DFIT) statistics, signed and unsigned item difference in the sample statistics (SIDS, UIDS, NSIDS, and NUIDS), the standardized indices of impact, and the differential response functioning (DRF) statistics. However, the relationship between these proposed statistics has not been fully discussed, particularly with respect to population parameter definitions and recovery performance across independent samples. To address these issues, this article provides a unified presentation of competing DIF ES definitions and estimators, and evaluates the recovery efficacy of these competing estimators using a set of Monte Carlo simulation experiments. Statistical and inferential properties of the estimators are discussed, as well as future areas of research in this model-based area of biasÂ quantification. Â© 2022 by the National Council on Measurement in Education.","['marginal', 'effect', 'size', 'ES', 'statistic', 'suitable', 'quantify', 'magnitude', 'differential', 'item', 'function', 'DIF', 'propose', 'area', 'item', 'response', 'theory', 'instance', 'Differential', 'Functioning', 'Items', 'Tests', 'dfit', 'statistic', 'sign', 'unsigned', 'item', 'difference', 'sample', 'statistic', 'SIDS', 'UIDS', 'NSIDS', 'NUIDS', 'standardized', 'index', 'impact', 'differential', 'response', 'function', 'drf', 'statistic', 'relationship', 'propose', 'statistic', 'fully', 'discuss', 'particularly', 'respect', 'population', 'parameter', 'definition', 'recovery', 'performance', 'independent', 'sample', 'address', 'issue', 'article', 'provide', 'unified', 'presentation', 'compete', 'DIF', 'ES', 'definition', 'estimator', 'evaluate', 'recovery', 'efficacy', 'compete', 'estimator', 'set', 'Monte', 'Carlo', 'simulation', 'experiment', 'statistical', 'inferential', 'property', 'estimator', 'discuss', 'future', 'area', 'research', 'modelbase', 'area', 'bias', 'quantification', 'Â©', '2022', 'National', 'Council']","['Unified', 'Comparison', 'IRTBased', 'Effect', 'Sizes', 'DIF', 'investigation']",marginal effect size ES statistic suitable quantify magnitude differential item function DIF propose area item response theory instance Differential Functioning Items Tests dfit statistic sign unsigned item difference sample statistic SIDS UIDS NSIDS NUIDS standardized index impact differential response function drf statistic relationship propose statistic fully discuss particularly respect population parameter definition recovery performance independent sample address issue article provide unified presentation compete DIF ES definition estimator evaluate recovery efficacy compete estimator set Monte Carlo simulation experiment statistical inferential property estimator discuss future area research modelbase area bias quantification Â© 2022 National Council,Unified Comparison IRTBased Effect Sizes DIF investigation,0.024803973,0.024427443,0.024414778,0.024881163,0.901472643,0.021210322,0,0.05712368,0,0.103602751
Lu J.; Wang C.,A Response Time Process Model for Not-Reached and Omitted Items,2020,57,"Item nonresponses are prevalent in standardized testing. They happen either when students fail to reach the end of a test due to a time limit or quitting, or when students choose to omit some items strategically. Oftentimes, item nonresponses are nonrandom, and hence, the missing data mechanism needs to be properly modeled. In this paper, we proposed to use an innovative item response time model as a cohesive missing data model to account for the two most common item nonresponses: not-reached items and omitted items. In particular, the new model builds on a behavior process interpretation: a person chooses to skip an item if the required effort exceeds the implicit time the person allocates to the item (Lee & Ying, 2015; Wolf, Smith, & Birnbaum, 1995), whereas a person fails to reach the end of the test due to lack of time. This assumption was verified by analyzing the 2015 PISA computer-based mathematics data. Simulation studies were conducted to further evaluate the performance of the proposed Bayesian estimation algorithm for the new model and to compare the new model with a recently proposed ?œspeed-accuracy + omission??model (Ulitzsch, von Davier, & Pohl, 2019). Results revealed that all model parameters could recover properly, and inadequately accounting for missing data caused biased item and person parameter estimates. Â© 2020 by the National Council on Measurement in Education",A Response Time Process Model for Not-Reached and Omitted Items,"Item nonresponses are prevalent in standardized testing. They happen either when students fail to reach the end of a test due to a time limit or quitting, or when students choose to omit some items strategically. Oftentimes, item nonresponses are nonrandom, and hence, the missing data mechanism needs to be properly modeled. In this paper, we proposed to use an innovative item response time model as a cohesive missing data model to account for the two most common item nonresponses: not-reached items and omitted items. In particular, the new model builds on a behavior process interpretation: a person chooses to skip an item if the required effort exceeds the implicit time the person allocates to the item (Lee & Ying, 2015; Wolf, Smith, & Birnbaum, 1995), whereas a person fails to reach the end of the test due to lack of time. This assumption was verified by analyzing the 2015 PISA computer-based mathematics data. Simulation studies were conducted to further evaluate the performance of the proposed Bayesian estimation algorithm for the new model and to compare the new model with a recently proposed ?œspeed-accuracy + omission??model (Ulitzsch, von Davier, & Pohl, 2019). Results revealed that all model parameters could recover properly, and inadequately accounting for missing data caused biased item and person parameter estimates. Â© 2020 by the National Council on Measurement in Education","['item', 'nonresponse', 'prevalent', 'standardized', 'testing', 'happen', 'student', 'fail', 'reach', 'end', 'test', 'time', 'limit', 'quitting', 'student', 'choose', 'omit', 'item', 'strategically', 'Oftentimes', 'item', 'nonresponse', 'nonrandom', 'miss', 'datum', 'mechanism', 'need', 'properly', 'paper', 'propose', 'innovative', 'item', 'response', 'time', 'cohesive', 'miss', 'datum', 'account', 'common', 'item', 'nonresponse', 'notreached', 'item', 'omit', 'item', 'particular', 'new', 'build', 'behavior', 'process', 'interpretation', 'person', 'choose', 'skip', 'item', 'require', 'effort', 'exceed', 'implicit', 'time', 'person', 'allocate', 'item', 'Lee', 'Ying', '2015', 'Wolf', 'Smith', 'Birnbaum', '1995', 'person', 'fail', 'reach', 'end', 'test', 'lack', 'time', 'assumption', 'verify', 'analyze', '2015', 'PISA', 'computerbase', 'mathematics', 'data', 'Simulation', 'study', 'conduct', 'far', 'evaluate', 'performance', 'propose', 'bayesian', 'estimation', 'algorithm', 'new', 'compare', 'new', 'recently', 'propose', '""', 'speedaccuracy', 'omission', '""', 'Ulitzsch', 'von', 'Davier', 'Pohl', '2019', 'result', 'reveal', 'parameter', 'recover', 'properly', 'inadequately', 'account', 'miss', 'datum', 'cause', 'biased', 'item', 'person', 'parameter', 'estimate', 'Â©', '2020', 'National', 'Council']","['Response', 'Time', 'Process', 'NotReached', 'Omitted', 'Items']","item nonresponse prevalent standardized testing happen student fail reach end test time limit quitting student choose omit item strategically Oftentimes item nonresponse nonrandom miss datum mechanism need properly paper propose innovative item response time cohesive miss datum account common item nonresponse notreached item omit item particular new build behavior process interpretation person choose skip item require effort exceed implicit time person allocate item Lee Ying 2015 Wolf Smith Birnbaum 1995 person fail reach end test lack time assumption verify analyze 2015 PISA computerbase mathematics data Simulation study conduct far evaluate performance propose bayesian estimation algorithm new compare new recently propose "" speedaccuracy omission "" Ulitzsch von Davier Pohl 2019 result reveal parameter recover properly inadequately account miss datum cause biased item person parameter estimate Â© 2020 National Council",Response Time Process NotReached Omitted Items,0.022249627,0.02196344,0.021916295,0.022178052,0.911692588,0.088144266,0.02161962,0,0,0
Liu J.; Becker K.,The Impact of Cheating on Score Comparability via Pool-Based IRT Pre-equating,2022,59,"For any testing programs that administer multiple forms across multiple years, maintaining score comparability via equating is essential. With continuous testing and high-stakes results, especially with less secure online administrations, testing programs must consider the potential for cheating on their exams. This study used empirical and simulated data to examine the impact of item exposure and prior knowledge on the estimation of item difficulty and test taker's ability via pool-based IRT preequating. Raw-to-theta transformations were derived from two groups of test takers with and without possible prior knowledge of exposed items, and these were compared to a criterion raw to theta transformation. Results indicated that item exposure has a large impact on item difficulty, not only altering the difficulty of exposed items, but also altering the difficulty of unexposed items. Item exposure makes test takers with prior knowledge appear more able. Further, theta estimation bias for test takers without prior knowledge increases when more test takers with possible prior knowledge are in the calibration population. Score inflation occurs for test takers with and without prior knowledge, especially for those with lower abilities. Â© 2022 by the National Council on Measurement in Education.",The Impact of Cheating on Score Comparability via Pool-Based IRT Pre-equating,"For any testing programs that administer multiple forms across multiple years, maintaining score comparability via equating is essential. With continuous testing and high-stakes results, especially with less secure online administrations, testing programs must consider the potential for cheating on their exams. This study used empirical and simulated data to examine the impact of item exposure and prior knowledge on the estimation of item difficulty and test taker's ability via pool-based IRT preequating. Raw-to-theta transformations were derived from two groups of test takers with and without possible prior knowledge of exposed items, and these were compared to a criterion raw to theta transformation. Results indicated that item exposure has a large impact on item difficulty, not only altering the difficulty of exposed items, but also altering the difficulty of unexposed items. Item exposure makes test takers with prior knowledge appear more able. Further, theta estimation bias for test takers without prior knowledge increases when more test takers with possible prior knowledge are in the calibration population. Score inflation occurs for test takers with and without prior knowledge, especially for those with lower abilities. Â© 2022 by the National Council on Measurement in Education.","['testing', 'program', 'administer', 'multiple', 'form', 'multiple', 'year', 'maintain', 'score', 'comparability', 'equate', 'essential', 'continuous', 'testing', 'highstake', 'result', 'especially', 'secure', 'online', 'administration', 'testing', 'program', 'consider', 'potential', 'cheat', 'exam', 'study', 'empirical', 'simulated', 'datum', 'examine', 'impact', 'item', 'exposure', 'prior', 'knowledge', 'estimation', 'item', 'difficulty', 'test', 'taker', 'ability', 'poolbase', 'IRT', 'preequate', 'Rawtotheta', 'transformation', 'derive', 'group', 'test', 'taker', 'possible', 'prior', 'knowledge', 'expose', 'item', 'compare', 'criterion', 'raw', 'theta', 'transformation', 'result', 'indicate', 'item', 'exposure', 'large', 'impact', 'item', 'difficulty', 'alter', 'difficulty', 'expose', 'item', 'alter', 'difficulty', 'unexposed', 'item', 'Item', 'exposure', 'test', 'taker', 'prior', 'knowledge', 'appear', 'able', 'theta', 'estimation', 'bias', 'test', 'taker', 'prior', 'knowledge', 'increase', 'test', 'taker', 'possible', 'prior', 'knowledge', 'calibration', 'population', 'Score', 'inflation', 'occur', 'test', 'taker', 'prior', 'knowledge', 'especially', 'low', 'ability', 'Â©', '2022', 'National', 'Council']","['Impact', 'cheating', 'Score', 'Comparability', 'PoolBased', 'IRT', 'preequate']",testing program administer multiple form multiple year maintain score comparability equate essential continuous testing highstake result especially secure online administration testing program consider potential cheat exam study empirical simulated datum examine impact item exposure prior knowledge estimation item difficulty test taker ability poolbase IRT preequate Rawtotheta transformation derive group test taker possible prior knowledge expose item compare criterion raw theta transformation result indicate item exposure large impact item difficulty alter difficulty expose item alter difficulty unexposed item Item exposure test taker prior knowledge appear able theta estimation bias test taker prior knowledge increase test taker possible prior knowledge calibration population Score inflation occur test taker prior knowledge especially low ability Â© 2022 National Council,Impact cheating Score Comparability PoolBased IRT preequate,0.029160007,0.027903517,0.02777581,0.02845998,0.886700685,0.011308436,0.12156597,0.000682136,0,0
Ma W.; Sorrel M.A.; Zhai X.; Ge Y.,A Dual-Purpose Model for Binary Data: Estimating Ability and Misconceptions,2024,61,"Most existing diagnostic models are developed to detect whether students have mastered a set of skills of interest, but few have focused on identifying what scientific misconceptions students possess. This article developed a general dual-purpose model for simultaneously estimating students' overall ability and the presence and absence of misconceptions. The expectation-maximization algorithm was developed to estimate the model parameters. A simulation study was conducted to evaluate to what extent the parameters can be accurately recovered under varied conditions. A set of real data in science education was also analyzed to examine the viability of the proposed model in practice. Â© 2024 by the National Council on Measurement in Education.",A Dual-Purpose Model for Binary Data: Estimating Ability and Misconceptions,"Most existing diagnostic models are developed to detect whether students have mastered a set of skills of interest, but few have focused on identifying what scientific misconceptions students possess. This article developed a general dual-purpose model for simultaneously estimating students' overall ability and the presence and absence of misconceptions. The expectation-maximization algorithm was developed to estimate the model parameters. A simulation study was conducted to evaluate to what extent the parameters can be accurately recovered under varied conditions. A set of real data in science education was also analyzed to examine the viability of the proposed model in practice. Â© 2024 by the National Council on Measurement in Education.","['exist', 'diagnostic', 'develop', 'detect', 'student', 'master', 'set', 'skill', 'interest', 'focus', 'identify', 'scientific', 'misconception', 'student', 'possess', 'article', 'develop', 'general', 'dualpurpose', 'simultaneously', 'estimate', 'student', 'overall', 'ability', 'presence', 'absence', 'misconception', 'expectationmaximization', 'algorithm', 'develop', 'estimate', 'parameter', 'simulation', 'study', 'conduct', 'evaluate', 'extent', 'parameter', 'accurately', 'recover', 'varied', 'condition', 'set', 'real', 'datum', 'science', 'analyze', 'examine', 'viability', 'propose', 'practice', 'Â©', '2024', 'National', 'Council']","['DualPurpose', 'Binary', 'Data', 'Estimating', 'Ability', 'misconception']",exist diagnostic develop detect student master set skill interest focus identify scientific misconception student possess article develop general dualpurpose simultaneously estimate student overall ability presence absence misconception expectationmaximization algorithm develop estimate parameter simulation study conduct evaluate extent parameter accurately recover varied condition set real datum science analyze examine viability propose practice Â© 2024 National Council,DualPurpose Binary Data Estimating Ability misconception,0.88810554,0.02785049,0.027869038,0.028146723,0.028028209,0.036897393,0.041371077,0.003074926,0.014873808,0
Wyse A.E.; McBride J.R.,A Framework for Measuring the Amount of Adaptation of Rasch-based Computerized Adaptive Tests,2021,58,A key consideration when giving any computerized adaptive test (CAT) is how much adaptation is present when the test is used in practice. This study introduces a new framework to measure the amount of adaptation of Rasch-based CATs based on looking at the differences between the selected item locations (Rasch item difficulty parameters) of the administered items and target item locations determined from provisional ability estimates at the start of each item. Several new indices based on this framework are introduced and compared to previously suggested measures of adaptation using simulated and real test data. Results from the simulation indicate that some previously suggested indices are not as sensitive to changes in item pool size and the use of constraints as the new indices and may not work as well under different item selection rules. The simulation study and real data example also illustrate the utility of using the new indices to measure adaptation at both a group and individual level. Discussion is provided on how one may use several of the indices to measure adaptation of Rasch-based CATs in practice. Â© 2020 by the National Council on Measurement in Education,A Framework for Measuring the Amount of Adaptation of Rasch-based Computerized Adaptive Tests,A key consideration when giving any computerized adaptive test (CAT) is how much adaptation is present when the test is used in practice. This study introduces a new framework to measure the amount of adaptation of Rasch-based CATs based on looking at the differences between the selected item locations (Rasch item difficulty parameters) of the administered items and target item locations determined from provisional ability estimates at the start of each item. Several new indices based on this framework are introduced and compared to previously suggested measures of adaptation using simulated and real test data. Results from the simulation indicate that some previously suggested indices are not as sensitive to changes in item pool size and the use of constraints as the new indices and may not work as well under different item selection rules. The simulation study and real data example also illustrate the utility of using the new indices to measure adaptation at both a group and individual level. Discussion is provided on how one may use several of the indices to measure adaptation of Rasch-based CATs in practice. Â© 2020 by the National Council on Measurement in Education,"['key', 'consideration', 'computerized', 'adaptive', 'test', 'CAT', 'adaptation', 'present', 'test', 'practice', 'study', 'introduce', 'new', 'framework', 'measure', 'adaptation', 'raschbased', 'cats', 'base', 'look', 'difference', 'select', 'item', 'location', 'Rasch', 'item', 'difficulty', 'parameter', 'administer', 'item', 'target', 'item', 'location', 'determine', 'provisional', 'ability', 'estimate', 'start', 'item', 'new', 'index', 'base', 'framework', 'introduce', 'compare', 'previously', 'suggest', 'measure', 'adaptation', 'simulated', 'real', 'test', 'datum', 'result', 'simulation', 'indicate', 'previously', 'suggest', 'index', 'sensitive', 'change', 'item', 'pool', 'size', 'constraint', 'new', 'index', 'work', 'different', 'item', 'selection', 'rule', 'simulation', 'study', 'real', 'datum', 'example', 'illustrate', 'utility', 'new', 'index', 'measure', 'adaptation', 'group', 'individual', 'level', 'Discussion', 'provide', 'index', 'measure', 'adaptation', 'raschbased', 'cats', 'practice', 'Â©', '2020', 'National', 'Council']","['Framework', 'measure', 'Amount', 'Adaptation', 'Raschbased', 'Computerized', 'Adaptive', 'test']",key consideration computerized adaptive test CAT adaptation present test practice study introduce new framework measure adaptation raschbased cats base look difference select item location Rasch item difficulty parameter administer item target item location determine provisional ability estimate start item new index base framework introduce compare previously suggest measure adaptation simulated real test datum result simulation indicate previously suggest index sensitive change item pool size constraint new index work different item selection rule simulation study real datum example illustrate utility new index measure adaptation group individual level Discussion provide index measure adaptation raschbased cats practice Â© 2020 National Council,Framework measure Amount Adaptation Raschbased Computerized Adaptive test,0.028950358,0.028461947,0.028529789,0.885275839,0.028782067,0.014570229,0.111349995,0,0,0.038550096
Huang S.; Chung S.; Falk C.F.,Modeling Response Styles in Cross-Classified Data Using a Cross-Classified Multidimensional Nominal Response Model,2024,,"In this study, we introduced a cross-classified multidimensional nominal response model (CC-MNRM) to account for various response styles (RS) in the presence of cross-classified data. The proposed model allows slopes to vary across items and can explore impacts of observed covariates on latent constructs. We applied a recently developed variant of the Metropolis-Hastings Robbins-Monro (MH-RM) algorithm to address the computational challenge of estimating the proposed model. To demonstrate our new approach, we analyzed empirical student evaluation of teaching (SET) data collected from a large public university with three models: a CC-MNRM with RS, a CC-MNRM with no RS, and a multilevel MNRM with RS. Results indicated that the three models led to different inferences regarding the observed covariates. Additionally, in the example, ignoring/incorporating RS led to changes in student substantive scores, while the instructor substantive scores were less impacted. Misspecifying the cross-classified data structure resulted in apparent changes on instructor scores. To further evaluate the proposed modeling approach, we conducted a preliminary simulation study and observed good parameter and score recovery. We concluded this study with discussions of limitations and future researchÂ directions. Â© 2024 by the National Council on Measurement in Education.",Modeling Response Styles in Cross-Classified Data Using a Cross-Classified Multidimensional Nominal Response Model,"In this study, we introduced a cross-classified multidimensional nominal response model (CC-MNRM) to account for various response styles (RS) in the presence of cross-classified data. The proposed model allows slopes to vary across items and can explore impacts of observed covariates on latent constructs. We applied a recently developed variant of the Metropolis-Hastings Robbins-Monro (MH-RM) algorithm to address the computational challenge of estimating the proposed model. To demonstrate our new approach, we analyzed empirical student evaluation of teaching (SET) data collected from a large public university with three models: a CC-MNRM with RS, a CC-MNRM with no RS, and a multilevel MNRM with RS. Results indicated that the three models led to different inferences regarding the observed covariates. Additionally, in the example, ignoring/incorporating RS led to changes in student substantive scores, while the instructor substantive scores were less impacted. Misspecifying the cross-classified data structure resulted in apparent changes on instructor scores. To further evaluate the proposed modeling approach, we conducted a preliminary simulation study and observed good parameter and score recovery. We concluded this study with discussions of limitations and future researchÂ directions. Â© 2024 by the National Council on Measurement in Education.","['study', 'introduce', 'crossclassified', 'multidimensional', 'nominal', 'response', 'CCMNRM', 'account', 'response', 'style', 'rs', 'presence', 'crossclassified', 'datum', 'propose', 'allow', 'slope', 'vary', 'item', 'explore', 'impact', 'observe', 'covariate', 'latent', 'construct', 'apply', 'recently', 'develop', 'variant', 'MetropolisHastings', 'RobbinsMonro', 'MHRM', 'algorithm', 'address', 'computational', 'challenge', 'estimate', 'propose', 'demonstrate', 'new', 'approach', 'analyze', 'empirical', 'student', 'evaluation', 'teach', 'SET', 'datum', 'collect', 'large', 'public', 'university', 'ccmnrm', 'RS', 'ccmnrm', 'rs', 'multilevel', 'mnrm', 'RS', 'result', 'indicate', 'lead', 'different', 'inference', 'regard', 'observe', 'covariate', 'additionally', 'example', 'ignoringincorporate', 'RS', 'lead', 'change', 'student', 'substantive', 'score', 'instructor', 'substantive', 'score', 'impact', 'misspecifye', 'crossclassified', 'data', 'structure', 'result', 'apparent', 'change', 'instructor', 'score', 'far', 'evaluate', 'propose', 'modeling', 'approach', 'conduct', 'preliminary', 'simulation', 'study', 'observe', 'good', 'parameter', 'score', 'recovery', 'conclude', 'study', 'discussion', 'limitation', 'future', 'research', 'direction', 'Â©', '2024', 'National', 'Council']","['Response', 'Styles', 'CrossClassified', 'Data', 'CrossClassified', 'Multidimensional', 'Nominal', 'Response']",study introduce crossclassified multidimensional nominal response CCMNRM account response style rs presence crossclassified datum propose allow slope vary item explore impact observe covariate latent construct apply recently develop variant MetropolisHastings RobbinsMonro MHRM algorithm address computational challenge estimate propose demonstrate new approach analyze empirical student evaluation teach SET datum collect large public university ccmnrm RS ccmnrm rs multilevel mnrm RS result indicate lead different inference regard observe covariate additionally example ignoringincorporate RS lead change student substantive score instructor substantive score impact misspecifye crossclassified data structure result apparent change instructor score far evaluate propose modeling approach conduct preliminary simulation study observe good parameter score recovery conclude study discussion limitation future research direction Â© 2024 National Council,Response Styles CrossClassified Data CrossClassified Multidimensional Nominal Response,0.60617716,0.023510133,0.322297647,0.024155878,0.023859182,0.037311341,0.028721407,0.004858793,0.041892057,0.003250761
Qiao X.; Jiao H.; He Q.,"Multiple-Group Joint Modeling of Item Responses, Response Times, and Action Counts with the Conway-Maxwell-Poisson Distribution",2023,60,"Multiple group modeling is one of the methods to address the measurement noninvariance issue. Traditional studies on multiple group modeling have mainly focused on item responses. In computer-based assessments, joint modeling of response times and action counts with item responses helps estimate the latent speed and action levels in addition to latent ability. These two new data sources can also be used to further address the measurement noninvariance issue. One challenge, however, is to correctly model action counts which can be underdispersed, overdispersed, or equidispersed in real data sets. To address this, we adopted the Conway-Maxwell-Poisson distribution that accounts for different types of dispersion in action counts and incorporated it in the multiple group joint modeling of item responses, response times, and action counts. Bayesian Markov Chain Monte Carlo method was used for model parameter estimation. To illustrate an application of the proposed model, an empirical data analysis was conducted using the Programme for International Student Assessment (PISA) 2015 collaborative problem-solving items where potential measurement noninvariance issue existed between gender groups. Results indicated that Conway-Maxwell-Poisson model yielded better model fit than alternative count data models such as negative binomial and Poisson models. In addition, response times and action counts provided further information on performance differences between groups. Â© 2022 by the National Council on Measurement in Education.","Multiple-Group Joint Modeling of Item Responses, Response Times, and Action Counts with the Conway-Maxwell-Poisson Distribution","Multiple group modeling is one of the methods to address the measurement noninvariance issue. Traditional studies on multiple group modeling have mainly focused on item responses. In computer-based assessments, joint modeling of response times and action counts with item responses helps estimate the latent speed and action levels in addition to latent ability. These two new data sources can also be used to further address the measurement noninvariance issue. One challenge, however, is to correctly model action counts which can be underdispersed, overdispersed, or equidispersed in real data sets. To address this, we adopted the Conway-Maxwell-Poisson distribution that accounts for different types of dispersion in action counts and incorporated it in the multiple group joint modeling of item responses, response times, and action counts. Bayesian Markov Chain Monte Carlo method was used for model parameter estimation. To illustrate an application of the proposed model, an empirical data analysis was conducted using the Programme for International Student Assessment (PISA) 2015 collaborative problem-solving items where potential measurement noninvariance issue existed between gender groups. Results indicated that Conway-Maxwell-Poisson model yielded better model fit than alternative count data models such as negative binomial and Poisson models. In addition, response times and action counts provided further information on performance differences between groups. Â© 2022 by the National Council on Measurement in Education.","['multiple', 'group', 'modeling', 'method', 'address', 'noninvariance', 'issue', 'traditional', 'study', 'multiple', 'group', 'modeling', 'mainly', 'focus', 'item', 'response', 'computerbase', 'assessment', 'joint', 'modeling', 'response', 'time', 'action', 'count', 'item', 'response', 'help', 'estimate', 'latent', 'speed', 'action', 'level', 'addition', 'latent', 'ability', 'new', 'datum', 'source', 'far', 'address', 'noninvariance', 'issue', 'challenge', 'correctly', 'action', 'count', 'underdisperse', 'overdispersed', 'equidisperse', 'real', 'datum', 'set', 'address', 'adopt', 'ConwayMaxwellPoisson', 'distribution', 'account', 'different', 'type', 'dispersion', 'action', 'count', 'incorporate', 'multiple', 'group', 'joint', 'modeling', 'item', 'response', 'response', 'time', 'action', 'count', 'Bayesian', 'Markov', 'Chain', 'Monte', 'Carlo', 'method', 'parameter', 'estimation', 'illustrate', 'application', 'propose', 'empirical', 'datum', 'analysis', 'conduct', 'Programme', 'International', 'Student', 'Assessment', 'PISA', '2015', 'collaborative', 'problemsolving', 'item', 'potential', 'noninvariance', 'issue', 'exist', 'gender', 'group', 'result', 'indicate', 'ConwayMaxwellPoisson', 'yield', 'fit', 'alternative', 'count', 'datum', 'negative', 'binomial', 'Poisson', 'addition', 'response', 'time', 'action', 'count', 'provide', 'information', 'performance', 'difference', 'group', 'Â©', '2022', 'National', 'Council']","['multiplegroup', 'Joint', 'modeling', 'Item', 'Responses', 'Response', 'Times', 'Action', 'Counts', 'ConwayMaxwellPoisson', 'Distribution']",multiple group modeling method address noninvariance issue traditional study multiple group modeling mainly focus item response computerbase assessment joint modeling response time action count item response help estimate latent speed action level addition latent ability new datum source far address noninvariance issue challenge correctly action count underdisperse overdispersed equidisperse real datum set address adopt ConwayMaxwellPoisson distribution account different type dispersion action count incorporate multiple group joint modeling item response response time action count Bayesian Markov Chain Monte Carlo method parameter estimation illustrate application propose empirical datum analysis conduct Programme International Student Assessment PISA 2015 collaborative problemsolving item potential noninvariance issue exist gender group result indicate ConwayMaxwellPoisson yield fit alternative count datum negative binomial Poisson addition response time action count provide information performance difference group Â© 2022 National Council,multiplegroup Joint modeling Item Responses Response Times Action Counts ConwayMaxwellPoisson Distribution,0.893064718,0.026754147,0.026661936,0.026766945,0.026752255,0.085356774,0.011323678,0.014978987,0.004458279,0
Jones P.; Tong Y.; Liu J.; Borglum J.; Primoli V.,Score Comparability between Online Proctored and In-Person Credentialing Exams,2022,59,"This article studied two methods to detect mode effects in two credentialing exams. In Study 1, we used a ?œmodal scale comparison approach,??where the same pool of items was calibrated separately, without transformation, within two TC cohorts (TC1 and TC2) and one OP cohort (OP1) matched on their pool-based scale score distributions. The calibrations from all three groups were used to score the TC2 cohort, designated the validation sample. The TC1 item parameters and TC1-based thetas and pass rates were more like the native TC2 values than the OP1-based values, indicating mode effects, but the score and pass/fail decision differences were small. In Study 2, we used a ?œcross-modal repeater approach??in which test takers who failed their first attempt in one modality took the test again in either the same or different modality. The two pairs of repeater groups (TC ??TC: TC ??OP, and OP ??OP: OP ??TC) were matched exactly on their first attempt scores. Results showed increased pass rate and greater score variability in all conditions involving OP, with mode effects noticeable in both the TC ??OP condition and less-strongly in the OP ??TC condition. Limitations of the study and implications for exam developers were discussed. Â© 2022 by the National Council on Measurement in Education.",Score Comparability between Online Proctored and In-Person Credentialing Exams,"This article studied two methods to detect mode effects in two credentialing exams. In Study 1, we used a ?œmodal scale comparison approach,??where the same pool of items was calibrated separately, without transformation, within two TC cohorts (TC1 and TC2) and one OP cohort (OP1) matched on their pool-based scale score distributions. The calibrations from all three groups were used to score the TC2 cohort, designated the validation sample. The TC1 item parameters and TC1-based thetas and pass rates were more like the native TC2 values than the OP1-based values, indicating mode effects, but the score and pass/fail decision differences were small. In Study 2, we used a ?œcross-modal repeater approach??in which test takers who failed their first attempt in one modality took the test again in either the same or different modality. The two pairs of repeater groups (TC ??TC: TC ??OP, and OP ??OP: OP ??TC) were matched exactly on their first attempt scores. Results showed increased pass rate and greater score variability in all conditions involving OP, with mode effects noticeable in both the TC ??OP condition and less-strongly in the OP ??TC condition. Limitations of the study and implications for exam developers were discussed. Â© 2022 by the National Council on Measurement in Education.","['article', 'study', 'method', 'detect', 'mode', 'effect', 'credentialing', 'exam', 'study', '1', '""', 'modal', 'scale', 'comparison', 'approach', '""', 'pool', 'item', 'calibrate', 'separately', 'transformation', 'tc', 'cohort', 'TC1', 'TC2', 'op', 'cohort', 'OP1', 'match', 'poolbase', 'scale', 'score', 'distribution', 'calibration', 'group', 'score', 'TC2', 'cohort', 'designate', 'validation', 'sample', 'TC1', 'item', 'parameter', 'tc1base', 'theta', 'pass', 'rate', 'like', 'native', 'TC2', 'value', 'op1base', 'value', 'indicate', 'mode', 'effect', 'score', 'passfail', 'decision', 'difference', 'small', 'Study', '2', '""', 'crossmodal', 'repeater', 'approach', '""', 'test', 'taker', 'fail', 'attempt', 'modality', 'test', 'different', 'modality', 'pair', 'repeater', 'group', 'tc', '??, 'TC', 'tc', '??, 'op', 'op', '??, 'op', 'op', '??, 'tc', 'match', 'exactly', 'attempt', 'score', 'result', 'increase', 'pass', 'rate', 'great', 'score', 'variability', 'condition', 'involve', 'op', 'mode', 'effect', 'noticeable', 'tc', '??, 'op', 'condition', 'lessstrongly', 'op', '??, 'tc', 'condition', 'Limitations', 'study', 'implication', 'exam', 'developer', 'discuss', 'Â©', '2022', 'National', 'Council']","['score', 'Comparability', 'Online', 'Proctored', 'InPerson', 'Credentialing', 'Exams']","article study method detect mode effect credentialing exam study 1 "" modal scale comparison approach "" pool item calibrate separately transformation tc cohort TC1 TC2 op cohort OP1 match poolbase scale score distribution calibration group score TC2 cohort designate validation sample TC1 item parameter tc1base theta pass rate like native TC2 value op1base value indicate mode effect score passfail decision difference small Study 2 "" crossmodal repeater approach "" test taker fail attempt modality test different modality pair repeater group tc ??TC tc ??op op ??op op ??tc match exactly attempt score result increase pass rate great score variability condition involve op mode effect noticeable tc ??op condition lessstrongly op ??tc condition Limitations study implication exam developer discuss Â© 2022 National Council",score Comparability Online Proctored InPerson Credentialing Exams,0.88181718,0.029354634,0.029769125,0.029554271,0.02950479,0.001962428,0.042892707,0.012403059,0.011868614,0.012158399
Chen C.-W.; Andersson B.; Zhu J.,A Factor Mixture Model for Item Responses and Certainty of Response Indices to Identify Student Knowledge Profiles,2023,60,"The certainty of response index (CRI) measures respondents' confidence level when answering an item. In conjunction with the answers to the items, previous studies have used descriptive statistics and arbitrary thresholds to identify student knowledge profiles with the CRIs. Whereas this approach overlooked the measurement error of the observed item responses and indices, we address this by proposing a factor mixture model that integrates a latent class model to detect student subgroups and a measurement model to control for student ability and confidence level. Applying the model to 773 seventh graders' responses to an algebra test, where some items were related to new material that had not been taught in class, we found two subgroups: (1) students who had high confidence in answering items involving the new material; and (2) students who had low confidence in answering items involving the new material but higher general self-confidence than the first group. We regressed the posterior probability of the group membership on gender, prior achievement, and preview behavior and found preview behavior a significant factor associated with the membership. Finally, we discussed the implications of the current study for teaching practices and futureÂ research. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",A Factor Mixture Model for Item Responses and Certainty of Response Indices to Identify Student Knowledge Profiles,"The certainty of response index (CRI) measures respondents' confidence level when answering an item. In conjunction with the answers to the items, previous studies have used descriptive statistics and arbitrary thresholds to identify student knowledge profiles with the CRIs. Whereas this approach overlooked the measurement error of the observed item responses and indices, we address this by proposing a factor mixture model that integrates a latent class model to detect student subgroups and a measurement model to control for student ability and confidence level. Applying the model to 773 seventh graders' responses to an algebra test, where some items were related to new material that had not been taught in class, we found two subgroups: (1) students who had high confidence in answering items involving the new material; and (2) students who had low confidence in answering items involving the new material but higher general self-confidence than the first group. We regressed the posterior probability of the group membership on gender, prior achievement, and preview behavior and found preview behavior a significant factor associated with the membership. Finally, we discussed the implications of the current study for teaching practices and futureÂ research. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['certainty', 'response', 'index', 'CRI', 'measure', 'respondent', 'confidence', 'level', 'answer', 'item', 'conjunction', 'answer', 'item', 'previous', 'study', 'descriptive', 'statistic', 'arbitrary', 'threshold', 'identify', 'student', 'knowledge', 'profile', 'cri', 'approach', 'overlook', 'error', 'observed', 'item', 'response', 'index', 'address', 'propose', 'factor', 'mixture', 'integrate', 'latent', 'class', 'detect', 'student', 'subgroup', 'control', 'student', 'ability', 'confidence', 'level', 'apply', '773', 'seventh', 'grader', 'response', 'algebra', 'test', 'item', 'relate', 'new', 'material', 'teach', 'class', 'find', 'subgroup', '1', 'student', 'high', 'confidence', 'answer', 'item', 'involve', 'new', 'material', '2', 'student', 'low', 'confidence', 'answer', 'item', 'involve', 'new', 'material', 'high', 'general', 'selfconfidence', 'group', 'regress', 'posterior', 'probability', 'group', 'membership', 'gender', 'prior', 'achievement', 'preview', 'behavior', 'find', 'preview', 'behavior', 'significant', 'factor', 'associate', 'membership', 'finally', 'discuss', 'implication', 'current', 'study', 'teach', 'practice', 'future', 'research', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['Factor', 'Mixture', 'Item', 'Responses', 'Certainty', 'Response', 'Indices', 'identify', 'Student', 'Knowledge', 'Profiles']",certainty response index CRI measure respondent confidence level answer item conjunction answer item previous study descriptive statistic arbitrary threshold identify student knowledge profile cri approach overlook error observed item response index address propose factor mixture integrate latent class detect student subgroup control student ability confidence level apply 773 seventh grader response algebra test item relate new material teach class find subgroup 1 student high confidence answer item involve new material 2 student low confidence answer item involve new material high general selfconfidence group regress posterior probability group membership gender prior achievement preview behavior find preview behavior significant factor associate membership finally discuss implication current study teach practice future research Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,Factor Mixture Item Responses Certainty Response Indices identify Student Knowledge Profiles,0.023967122,0.023109086,0.023006476,0.023858117,0.906059199,0.068845241,0.021493106,0.024883774,0,0.008698695
Becker B.; Weirich S.; Goldhammer F.; Debeer D.,Controlling the Speededness of Assembled Test Forms: A Generalization to the Three-Parameter Lognormal Response Time Model,2023,60,"When designing or modifying a test, an important challenge is controlling its speededness. To achieve this, vanÂ der Linden (2011a, 2011b) proposed using a lognormal response time model, more specifically the two-parameter lognormal model, and automated test assembly (ATA) via mixed integer linear programming. However, this approach has a severe limitation, in that the two-parameter lognormal model lacks a slope parameter. This means that the model assumes that all items are equally speed sensitive. From a conceptual perspective, this assumption seems very restrictive. Furthermore, various other empirical studies and new data analyses performed by us show that this assumption almost never holds in practice. To overcome this shortcoming, we bring together the already frequently used three-parameter lognormal model for response times, which contains a slope parameter, and the ATA approach for controlling speededness by van der Linden. Using multiple empirically based illustrations, the proposed extension is illustrated, including complete and documented R code. Both the original van der Linden approach and our newly proposed approach are available to practitioners in the freely available R package eatATA. Â© 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Controlling the Speededness of Assembled Test Forms: A Generalization to the Three-Parameter Lognormal Response Time Model,"When designing or modifying a test, an important challenge is controlling its speededness. To achieve this, vanÂ der Linden (2011a, 2011b) proposed using a lognormal response time model, more specifically the two-parameter lognormal model, and automated test assembly (ATA) via mixed integer linear programming. However, this approach has a severe limitation, in that the two-parameter lognormal model lacks a slope parameter. This means that the model assumes that all items are equally speed sensitive. From a conceptual perspective, this assumption seems very restrictive. Furthermore, various other empirical studies and new data analyses performed by us show that this assumption almost never holds in practice. To overcome this shortcoming, we bring together the already frequently used three-parameter lognormal model for response times, which contains a slope parameter, and the ATA approach for controlling speededness by van der Linden. Using multiple empirically based illustrations, the proposed extension is illustrated, including complete and documented R code. Both the original van der Linden approach and our newly proposed approach are available to practitioners in the freely available R package eatATA. Â© 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['design', 'modify', 'test', 'important', 'challenge', 'control', 'speededness', 'achieve', 'van', 'der', 'Linden', '2011a', '2011b', 'propose', 'lognormal', 'response', 'time', 'specifically', 'twoparameter', 'lognormal', 'automate', 'test', 'assembly', 'ATA', 'mixed', 'integer', 'linear', 'programming', 'approach', 'severe', 'limitation', 'twoparameter', 'lognormal', 'lack', 'slope', 'parameter', 'mean', 'assume', 'item', 'equally', 'speed', 'sensitive', 'conceptual', 'perspective', 'assumption', 'restrictive', 'furthermore', 'empirical', 'study', 'new', 'datum', 'analysis', 'perform', 'assumption', 'hold', 'practice', 'overcome', 'shortcoming', 'bring', 'frequently', 'threeparameter', 'lognormal', 'response', 'time', 'contain', 'slope', 'parameter', 'ATA', 'approach', 'control', 'speededness', 'van', 'der', 'Linden', 'multiple', 'empirically', 'base', 'illustration', 'propose', 'extension', 'illustrate', 'include', 'complete', 'document', 'r', 'code', 'original', 'van', 'der', 'Linden', 'approach', 'newly', 'propose', 'approach', 'available', 'practitioner', 'freely', 'available', 'r', 'package', 'eatATA', 'Â©', '2023', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['control', 'speededness', 'Assembled', 'Test', 'Forms', 'generalization', 'ThreeParameter', 'Lognormal', 'Response', 'Time']",design modify test important challenge control speededness achieve van der Linden 2011a 2011b propose lognormal response time specifically twoparameter lognormal automate test assembly ATA mixed integer linear programming approach severe limitation twoparameter lognormal lack slope parameter mean assume item equally speed sensitive conceptual perspective assumption restrictive furthermore empirical study new datum analysis perform assumption hold practice overcome shortcoming bring frequently threeparameter lognormal response time contain slope parameter ATA approach control speededness van der Linden multiple empirically base illustration propose extension illustrate include complete document r code original van der Linden approach newly propose approach available practitioner freely available r package eatATA Â© 2023 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,control speededness Assembled Test Forms generalization ThreeParameter Lognormal Response Time,0.909423787,0.022574754,0.022543479,0.022868868,0.022589113,0.065049945,0.014426612,0.005762336,0.007440514,0
DeCarlo L.T.; Zhou X.,A Latent Class Signal Detection Model for Rater Scoring with Ordered Perceptual Distributions,2021,58,"In signal detection rater models for constructed response (CR) scoring, it is assumed that raters discriminate equally well between different latent classes defined by the scoring rubric. An extended model that relaxes this assumption is introduced; the model recognizes that a rater may not discriminate equally well between some of the scoring classes. The extension recognizes a different type of rater effect and is shown to offer useful tests and diagnostic plots of the equal discrimination assumption, along with ways to assess rater accuracy and various rater effects. The approach is illustrated with an application to a large-scale language test. Â© 2020 by the National Council on Measurement in Education",A Latent Class Signal Detection Model for Rater Scoring with Ordered Perceptual Distributions,"In signal detection rater models for constructed response (CR) scoring, it is assumed that raters discriminate equally well between different latent classes defined by the scoring rubric. An extended model that relaxes this assumption is introduced; the model recognizes that a rater may not discriminate equally well between some of the scoring classes. The extension recognizes a different type of rater effect and is shown to offer useful tests and diagnostic plots of the equal discrimination assumption, along with ways to assess rater accuracy and various rater effects. The approach is illustrated with an application to a large-scale language test. Â© 2020 by the National Council on Measurement in Education","['signal', 'detection', 'rater', 'construct', 'response', 'CR', 'scoring', 'assume', 'rater', 'discriminate', 'equally', 'different', 'latent', 'class', 'define', 'scoring', 'rubric', 'extended', 'relax', 'assumption', 'introduce', 'recognize', 'rater', 'discriminate', 'equally', 'scoring', 'class', 'extension', 'recognize', 'different', 'type', 'rater', 'effect', 'offer', 'useful', 'test', 'diagnostic', 'plot', 'equal', 'discrimination', 'assumption', 'way', 'assess', 'rater', 'accuracy', 'rater', 'effect', 'approach', 'illustrate', 'application', 'largescale', 'language', 'test', 'Â©', '2020', 'National', 'Council']","['Latent', 'Class', 'Signal', 'Detection', 'Rater', 'Scoring', 'Ordered', 'Perceptual', 'Distributions']",signal detection rater construct response CR scoring assume rater discriminate equally different latent class define scoring rubric extended relax assumption introduce recognize rater discriminate equally scoring class extension recognize different type rater effect offer useful test diagnostic plot equal discrimination assumption way assess rater accuracy rater effect approach illustrate application largescale language test Â© 2020 National Council,Latent Class Signal Detection Rater Scoring Ordered Perceptual Distributions,0.034774442,0.034142862,0.034019016,0.034669084,0.862394597,0,0,0,0.158910356,0
Hurtz G.M.; Mucino R.,Expanding the Lognormal Response Time Model Using Profile Similarity Metrics to Improve the Detection of Anomalous Testing Behavior,2024,,"The Lognormal Response Time (LNRT) model measures the speed of test-takers relative to the normative time demands of items on a test. The resulting speed parameters and model residuals are often analyzed for evidence of anomalous test-taking behavior associated with fast and poorly fitting response time patterns. Extending this model, we demonstrate the connection between the existing LNRT model parameters and the ?œlevel??component of profile similarity, and we define two new parameters for the LNRT model representing profile ?œdispersion??and ?œshape.??We show that while the LNRT model measures level (speed), profile dispersion and shape are conflated in model residuals, and that distinguishing them provides meaningful and useful parameters for identifying anomalous testing behavior. Results from data in a situation where many test-takers gained preknowledge of test items revealed that profile shape, not currently measured in the LNRT model, was the most sensitive response time index to the abnormal test-taking behavior patterns. Results strongly support expanding the LNRT model to measure not only each test-taker's level of speed, but also the dispersion and shape of their response time profiles. Â© 2024 by the National Council on Measurement in Education.",Expanding the Lognormal Response Time Model Using Profile Similarity Metrics to Improve the Detection of Anomalous Testing Behavior,"The Lognormal Response Time (LNRT) model measures the speed of test-takers relative to the normative time demands of items on a test. The resulting speed parameters and model residuals are often analyzed for evidence of anomalous test-taking behavior associated with fast and poorly fitting response time patterns. Extending this model, we demonstrate the connection between the existing LNRT model parameters and the ?œlevel??component of profile similarity, and we define two new parameters for the LNRT model representing profile ?œdispersion??and ?œshape.??We show that while the LNRT model measures level (speed), profile dispersion and shape are conflated in model residuals, and that distinguishing them provides meaningful and useful parameters for identifying anomalous testing behavior. Results from data in a situation where many test-takers gained preknowledge of test items revealed that profile shape, not currently measured in the LNRT model, was the most sensitive response time index to the abnormal test-taking behavior patterns. Results strongly support expanding the LNRT model to measure not only each test-taker's level of speed, but also the dispersion and shape of their response time profiles. Â© 2024 by the National Council on Measurement in Education.","['Lognormal', 'Response', 'Time', 'LNRT', 'measure', 'speed', 'testtaker', 'relative', 'normative', 'time', 'demand', 'item', 'test', 'result', 'speed', 'parameter', 'residual', 'analyze', 'evidence', 'anomalous', 'testtaking', 'behavior', 'associate', 'fast', 'poorly', 'fitting', 'response', 'time', 'pattern', 'extend', 'demonstrate', 'connection', 'exist', 'LNRT', 'parameter', '""', 'level', '""', 'component', 'profile', 'similarity', 'define', 'new', 'parameter', 'LNRT', 'represent', 'profile', '""', 'dispersion', '""', '""', 'shape', '""', 'LNRT', 'measure', 'level', 'speed', 'profile', 'dispersion', 'shape', 'conflate', 'residual', 'distinguish', 'provide', 'meaningful', 'useful', 'parameter', 'identify', 'anomalous', 'testing', 'behavior', 'result', 'datum', 'situation', 'testtaker', 'gain', 'preknowledge', 'test', 'item', 'reveal', 'profile', 'shape', 'currently', 'measure', 'LNRT', 'sensitive', 'response', 'time', 'index', 'abnormal', 'testtaking', 'behavior', 'pattern', 'result', 'strongly', 'support', 'expand', 'LNRT', 'measure', 'testtaker', 'level', 'speed', 'dispersion', 'shape', 'response', 'time', 'profile', 'Â©', '2024', 'National', 'Council']","['expand', 'Lognormal', 'Response', 'Time', 'Using', 'Profile', 'Similarity', 'Metrics', 'improve', 'Detection', 'Anomalous', 'Testing', 'Behavior']","Lognormal Response Time LNRT measure speed testtaker relative normative time demand item test result speed parameter residual analyze evidence anomalous testtaking behavior associate fast poorly fitting response time pattern extend demonstrate connection exist LNRT parameter "" level "" component profile similarity define new parameter LNRT represent profile "" dispersion "" "" shape "" LNRT measure level speed profile dispersion shape conflate residual distinguish provide meaningful useful parameter identify anomalous testing behavior result datum situation testtaker gain preknowledge test item reveal profile shape currently measure LNRT sensitive response time index abnormal testtaking behavior pattern result strongly support expand LNRT measure testtaker level speed dispersion shape response time profile Â© 2024 National Council",expand Lognormal Response Time Using Profile Similarity Metrics improve Detection Anomalous Testing Behavior,0.882157937,0.029201292,0.029183348,0.030055213,0.02940221,0.091162441,0,0,0,0
Pan Y.; Wollack J.A.,An Unsupervised-Learning-Based Approach to Compromised Items Detection,2021,58,"As technologies have been improved, item preknowledge has become a common concern in the test security area. The present study proposes an unsupervised-learning-based approach to detect compromised items. The unsupervised-learning-based compromised item detection approach contains three steps: (1) classify responses of each examinee as either normal or aberrant based on both the item response and the response time; (2) use a recursive algorithm to cluster examinees into groups based on their response similarity; (3) identify the group with strongest preknowledge signal and report questionable items as compromised. Results show that under the conditions studied, provided the amount of preknowledge is not overwhelming and aberrance effect is at least moderate, the approach controls the false-negative rate at a relatively low level and the false-positive rate at an extremely low level. Â© 2021 by the National Council on Measurement in Education",An Unsupervised-Learning-Based Approach to Compromised Items Detection,"As technologies have been improved, item preknowledge has become a common concern in the test security area. The present study proposes an unsupervised-learning-based approach to detect compromised items. The unsupervised-learning-based compromised item detection approach contains three steps: (1) classify responses of each examinee as either normal or aberrant based on both the item response and the response time; (2) use a recursive algorithm to cluster examinees into groups based on their response similarity; (3) identify the group with strongest preknowledge signal and report questionable items as compromised. Results show that under the conditions studied, provided the amount of preknowledge is not overwhelming and aberrance effect is at least moderate, the approach controls the false-negative rate at a relatively low level and the false-positive rate at an extremely low level. Â© 2021 by the National Council on Measurement in Education","['technology', 'improve', 'item', 'preknowledge', 'common', 'concern', 'test', 'security', 'area', 'present', 'study', 'propose', 'unsupervisedlearningbased', 'approach', 'detect', 'compromise', 'item', 'unsupervisedlearningbased', 'compromise', 'item', 'detection', 'approach', 'contain', 'step', '1', 'classify', 'response', 'examinee', 'normal', 'aberrant', 'base', 'item', 'response', 'response', 'time', '2', 'recursive', 'algorithm', 'cluster', 'examinee', 'group', 'base', 'response', 'similarity', '3', 'identify', 'group', 'strong', 'preknowledge', 'signal', 'report', 'questionable', 'item', 'compromise', 'result', 'condition', 'study', 'provide', 'preknowledge', 'overwhelming', 'aberrance', 'effect', 'moderate', 'approach', 'control', 'falsenegative', 'rate', 'relatively', 'low', 'level', 'falsepositive', 'rate', 'extremely', 'low', 'level', 'Â©', '2021', 'National', 'Council']","['unsupervisedlearningbased', 'approach', 'Compromised', 'Items', 'Detection']",technology improve item preknowledge common concern test security area present study propose unsupervisedlearningbased approach detect compromise item unsupervisedlearningbased compromise item detection approach contain step 1 classify response examinee normal aberrant base item response response time 2 recursive algorithm cluster examinee group base response similarity 3 identify group strong preknowledge signal report questionable item compromise result condition study provide preknowledge overwhelming aberrance effect moderate approach control falsenegative rate relatively low level falsepositive rate extremely low level Â© 2021 National Council,unsupervisedlearningbased approach Compromised Items Detection,0.025767482,0.025630534,0.025452509,0.897341184,0.025808291,0.099639096,0.010886952,0,0,0.022907824
Mutak A.; Krause R.; Ulitzsch E.; Much S.; Ranger J.; Pohl S.,Modeling the Intraindividual Relation of Ability and Speed within a Test,2024,,"Understanding the intraindividual relation between an individual's speed and ability in testing scenarios is essential to assure a fair assessment. Different approaches exist for estimating this relationship, that either rely on specific study designs or on specific assumptions. This paper aims to add to the toolbox of approaches for estimating this relationship. We propose the intraindividual speed-ability-relation (ISAR) model, which relies on nonstationarity of speed and ability over the course of the test. The ISAR model explicitly models intraindividual change in ability and speed within a test and assesses the intraindividual relation of speed and ability by evaluating the relationship of both latent change variables. Model estimation is good, when there are interindividual differences in speed and ability changes in the data. In empirical data from PISA, we found that the intraindividual relationship between speed and ability is not universally negative for all individuals and varies across different competence domains and countries. We discuss possible explanations for thisÂ relationship. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Modeling the Intraindividual Relation of Ability and Speed within a Test,"Understanding the intraindividual relation between an individual's speed and ability in testing scenarios is essential to assure a fair assessment. Different approaches exist for estimating this relationship, that either rely on specific study designs or on specific assumptions. This paper aims to add to the toolbox of approaches for estimating this relationship. We propose the intraindividual speed-ability-relation (ISAR) model, which relies on nonstationarity of speed and ability over the course of the test. The ISAR model explicitly models intraindividual change in ability and speed within a test and assesses the intraindividual relation of speed and ability by evaluating the relationship of both latent change variables. Model estimation is good, when there are interindividual differences in speed and ability changes in the data. In empirical data from PISA, we found that the intraindividual relationship between speed and ability is not universally negative for all individuals and varies across different competence domains and countries. We discuss possible explanations for thisÂ relationship. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['understand', 'intraindividual', 'relation', 'individual', 'speed', 'ability', 'testing', 'scenario', 'essential', 'assure', 'fair', 'assessment', 'different', 'approach', 'exist', 'estimate', 'relationship', 'rely', 'specific', 'study', 'design', 'specific', 'assumption', 'paper', 'aim', 'add', 'toolbox', 'approach', 'estimate', 'relationship', 'propose', 'intraindividual', 'speedabilityrelation', 'ISAR', 'rely', 'nonstationarity', 'speed', 'ability', 'course', 'test', 'ISAR', 'explicitly', 'intraindividual', 'change', 'ability', 'speed', 'test', 'assess', 'intraindividual', 'relation', 'speed', 'ability', 'evaluate', 'relationship', 'latent', 'change', 'variable', 'estimation', 'good', 'interindividual', 'difference', 'speed', 'ability', 'change', 'datum', 'empirical', 'datum', 'PISA', 'find', 'intraindividual', 'relationship', 'speed', 'ability', 'universally', 'negative', 'individual', 'varie', 'different', 'competence', 'domain', 'country', 'discuss', 'possible', 'explanation', 'relationship', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['Intraindividual', 'Relation', 'ability', 'speed', 'test']",understand intraindividual relation individual speed ability testing scenario essential assure fair assessment different approach exist estimate relationship rely specific study design specific assumption paper aim add toolbox approach estimate relationship propose intraindividual speedabilityrelation ISAR rely nonstationarity speed ability course test ISAR explicitly intraindividual change ability speed test assess intraindividual relation speed ability evaluate relationship latent change variable estimation good interindividual difference speed ability change datum empirical datum PISA find intraindividual relationship speed ability universally negative individual varie different competence domain country discuss possible explanation relationship Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,Intraindividual Relation ability speed test,0.029391174,0.02848764,0.028444288,0.884866172,0.028810725,0.058938211,0.012998005,0.001490617,0.004148917,0
Gorney K.; Wollack J.A.,Generating Models for Item Preknowledge,2022,59,"Detection methods for item preknowledge are often evaluated in simulation studies where models are used to generate the data. To ensure the reliability of such methods, it is crucial that these models are able to accurately represent situations that are encountered in practice. The purpose of this article is to provide a critical analysis of common models that have been used to simulate preknowledge. Both response accuracy (RA) and response time (RT) models are considered. The justifications and supporting evidence for each model are evaluated using three real data sets, and the impact of generating model on detection power is examined in two simulationÂ studies. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,"Detection methods for item preknowledge are often evaluated in simulation studies where models are used to generate the data. To ensure the reliability of such methods, it is crucial that these models are able to accurately represent situations that are encountered in practice. The purpose of this article is to provide a critical analysis of common models that have been used to simulate preknowledge. Both response accuracy (RA) and response time (RT) models are considered. The justifications and supporting evidence for each model are evaluated using three real data sets, and the impact of generating model on detection power is examined in two simulationÂ studies. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['detection', 'method', 'item', 'preknowledge', 'evaluate', 'simulation', 'study', 'generate', 'datum', 'ensure', 'reliability', 'method', 'crucial', 'able', 'accurately', 'represent', 'situation', 'encounter', 'practice', 'purpose', 'article', 'provide', 'critical', 'analysis', 'common', 'simulate', 'preknowledge', 'response', 'accuracy', 'RA', 'response', 'time', 'RT', 'consider', 'justification', 'support', 'evidence', 'evaluate', 'real', 'datum', 'set', 'impact', 'generating', 'detection', 'power', 'examine', 'simulation', 'study', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']",,detection method item preknowledge evaluate simulation study generate datum ensure reliability method crucial able accurately represent situation encounter practice purpose article provide critical analysis common simulate preknowledge response accuracy RA response time RT consider justification support evidence evaluate real datum set impact generating detection power examine simulation study Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,,0.027003513,0.026486118,0.026440903,0.893424036,0.026645431,0.096273074,0.020062208,0.001971852,0.003474587,0.011676573
Hassan M.U.; Miller F.,Optimal Calibration of Items for Multidimensional Achievement Tests,2024,61,"Multidimensional achievement tests are recently gaining more importance in educational and psychological measurements. For example, multidimensional diagnostic tests can help students to determine which particular domain of knowledge they need to improve for better performance. To estimate the characteristics of candidate items (calibration) for future multidimensional achievement tests, we use optimal design theory. We generalize a previously developed exchange algorithm for optimal design computation to the multidimensional setting. We also develop an asymptotic theorem saying which item should be calibrated by examinees with extreme abilities. For several examples, we compute the optimal design numerically with the exchange algorithm. We see clear structures in these results and explain them using the asymptotic theorem. Moreover, we investigate the performance of the optimal design in a simulationÂ study. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Optimal Calibration of Items for Multidimensional Achievement Tests,"Multidimensional achievement tests are recently gaining more importance in educational and psychological measurements. For example, multidimensional diagnostic tests can help students to determine which particular domain of knowledge they need to improve for better performance. To estimate the characteristics of candidate items (calibration) for future multidimensional achievement tests, we use optimal design theory. We generalize a previously developed exchange algorithm for optimal design computation to the multidimensional setting. We also develop an asymptotic theorem saying which item should be calibrated by examinees with extreme abilities. For several examples, we compute the optimal design numerically with the exchange algorithm. We see clear structures in these results and explain them using the asymptotic theorem. Moreover, we investigate the performance of the optimal design in a simulationÂ study. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['multidimensional', 'achievement', 'test', 'recently', 'gain', 'importance', 'educational', 'psychological', 'example', 'multidimensional', 'diagnostic', 'test', 'help', 'student', 'determine', 'particular', 'domain', 'knowledge', 'need', 'improve', 'performance', 'estimate', 'characteristic', 'candidate', 'item', 'calibration', 'future', 'multidimensional', 'achievement', 'test', 'optimal', 'design', 'theory', 'generalize', 'previously', 'develop', 'exchange', 'algorithm', 'optimal', 'design', 'computation', 'multidimensional', 'setting', 'develop', 'asymptotic', 'theorem', 'item', 'calibrate', 'examinee', 'extreme', 'ability', 'example', 'compute', 'optimal', 'design', 'numerically', 'exchange', 'algorithm', 'clear', 'structure', 'result', 'explain', 'asymptotic', 'theorem', 'Moreover', 'investigate', 'performance', 'optimal', 'design', 'simulation', 'study', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['optimal', 'calibration', 'Items', 'Multidimensional', 'Achievement', 'test']",multidimensional achievement test recently gain importance educational psychological example multidimensional diagnostic test help student determine particular domain knowledge need improve performance estimate characteristic candidate item calibration future multidimensional achievement test optimal design theory generalize previously develop exchange algorithm optimal design computation multidimensional setting develop asymptotic theorem item calibrate examinee extreme ability example compute optimal design numerically exchange algorithm clear structure result explain asymptotic theorem Moreover investigate performance optimal design simulation study Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,optimal calibration Items Multidimensional Achievement test,0.892003352,0.026931786,0.026807614,0.02711397,0.027143278,0.017307163,0.098340524,0.005843395,0,0
Yavuz Temel G.,Detecting Multidimensional DIF in Polytomous Items with IRT Methods and Estimation Approaches,2024,61,"The purpose of this study was to investigate multidimensional DIF with a simple and nonsimple structure in the context of multidimensional Graded Response Model (MGRM). This study examined and compared the performance of the IRT-LR and Wald test using MML-EM and MHRM estimation approaches with different test factors and test structures in simulation studies and applying real data sets. When the test structure included two dimensions, the IRT-LR (MML-EM) generally performed better than the Wald test and provided higher power rates. If the test included three dimensions, the methods provided similar performance in DIF detection. In contrast to these results, when the number of dimensions in the test was four, MML-EM estimation completely lost precision in estimating the nonuniform DIF, even with large sample sizes. The Wald with MHRM estimation approaches outperformed the Wald test (MML-EM) and IRT-LR (MML-EM). The Wald test had higher power rate and acceptable type I error rates for nonuniform DIF with the MHRM estimation approach.The small and/or unbalanced sample sizes, small DIF magnitudes, unequal ability distributions between groups, number of dimensions, estimation methods and test structure were evaluated as important test factors for detecting multidimensional DIF. Â© 2023 by the National Council on Measurement in Education.",Detecting Multidimensional DIF in Polytomous Items with IRT Methods and Estimation Approaches,"The purpose of this study was to investigate multidimensional DIF with a simple and nonsimple structure in the context of multidimensional Graded Response Model (MGRM). This study examined and compared the performance of the IRT-LR and Wald test using MML-EM and MHRM estimation approaches with different test factors and test structures in simulation studies and applying real data sets. When the test structure included two dimensions, the IRT-LR (MML-EM) generally performed better than the Wald test and provided higher power rates. If the test included three dimensions, the methods provided similar performance in DIF detection. In contrast to these results, when the number of dimensions in the test was four, MML-EM estimation completely lost precision in estimating the nonuniform DIF, even with large sample sizes. The Wald with MHRM estimation approaches outperformed the Wald test (MML-EM) and IRT-LR (MML-EM). The Wald test had higher power rate and acceptable type I error rates for nonuniform DIF with the MHRM estimation approach.The small and/or unbalanced sample sizes, small DIF magnitudes, unequal ability distributions between groups, number of dimensions, estimation methods and test structure were evaluated as important test factors for detecting multidimensional DIF. Â© 2023 by the National Council on Measurement in Education.","['purpose', 'study', 'investigate', 'multidimensional', 'DIF', 'simple', 'nonsimple', 'structure', 'context', 'multidimensional', 'Graded', 'Response', 'MGRM', 'study', 'examine', 'compare', 'performance', 'IRTLR', 'Wald', 'test', 'MMLEM', 'MHRM', 'estimation', 'approach', 'different', 'test', 'factor', 'test', 'structure', 'simulation', 'study', 'apply', 'real', 'datum', 'set', 'test', 'structure', 'include', 'dimension', 'IRTLR', 'MMLEM', 'generally', 'perform', 'Wald', 'test', 'provide', 'high', 'power', 'rate', 'test', 'include', 'dimension', 'method', 'provide', 'similar', 'performance', 'dif', 'detection', 'contrast', 'result', 'number', 'dimension', 'test', 'MMLEM', 'estimation', 'completely', 'lose', 'precision', 'estimate', 'nonuniform', 'dif', 'large', 'sample', 'size', 'Wald', 'MHRM', 'estimation', 'approach', 'outperform', 'Wald', 'test', 'MMLEM', 'IRTLR', 'MMLEM', 'Wald', 'test', 'high', 'power', 'rate', 'acceptable', 'type', 'I', 'error', 'rate', 'nonuniform', 'DIF', 'MHRM', 'estimation', 'approachthe', 'small', 'andor', 'unbalanced', 'sample', 'size', 'small', 'DIF', 'magnitude', 'unequal', 'ability', 'distribution', 'group', 'number', 'dimension', 'estimation', 'method', 'test', 'structure', 'evaluate', 'important', 'test', 'factor', 'detect', 'multidimensional', 'dif', 'Â©', '2023', 'National', 'Council']","['detect', 'Multidimensional', 'DIF', 'Polytomous', 'Items', 'IRT', 'Methods', 'Estimation', 'Approaches']",purpose study investigate multidimensional DIF simple nonsimple structure context multidimensional Graded Response MGRM study examine compare performance IRTLR Wald test MMLEM MHRM estimation approach different test factor test structure simulation study apply real datum set test structure include dimension IRTLR MMLEM generally perform Wald test provide high power rate test include dimension method provide similar performance dif detection contrast result number dimension test MMLEM estimation completely lose precision estimate nonuniform dif large sample size Wald MHRM estimation approach outperform Wald test MMLEM IRTLR MMLEM Wald test high power rate acceptable type I error rate nonuniform DIF MHRM estimation approachthe small andor unbalanced sample size small DIF magnitude unequal ability distribution group number dimension estimation method test structure evaluate important test factor detect multidimensional dif Â© 2023 National Council,detect Multidimensional DIF Polytomous Items IRT Methods Estimation Approaches,0.889019528,0.0275795,0.02762072,0.028147292,0.02763296,0,0.034621679,0,0,0.187349005
Liu J.; Meng X.; Xu G.; Gao W.; Shi N.,MSAEM Estimation for Confirmatory Multidimensional Four-Parameter Normal Ogive Models,2024,61,"In this paper, we develop a mixed stochastic approximation expectation-maximization (MSAEM) algorithm coupled with a Gibbs sampler to compute the marginalized maximum a posteriori estimate (MMAPE) of a confirmatory multidimensional four-parameter normal ogive (M4PNO) model. The proposed MSAEM algorithm not only has the computational advantages of the stochastic approximation expectation-maximization (SAEM) algorithm for multidimensional data, but it also alleviates the potential instability caused by label-switching, and then improved the estimation accuracy. Simulation studies are conducted to illustrate the good performance of the proposed MSAEM method, where MSAEM consistently performs better than SAEM and some other existing methods in multidimensional item response theory. Moreover, the proposed method is applied to a real data set from the 2018 Programme for International Student Assessment (PISA) to demonstrate the usefulness of the 4PNO model as well as MSAEM inÂ practice. Â© 2023 by the National Council on Measurement in Education.",MSAEM Estimation for Confirmatory Multidimensional Four-Parameter Normal Ogive Models,"In this paper, we develop a mixed stochastic approximation expectation-maximization (MSAEM) algorithm coupled with a Gibbs sampler to compute the marginalized maximum a posteriori estimate (MMAPE) of a confirmatory multidimensional four-parameter normal ogive (M4PNO) model. The proposed MSAEM algorithm not only has the computational advantages of the stochastic approximation expectation-maximization (SAEM) algorithm for multidimensional data, but it also alleviates the potential instability caused by label-switching, and then improved the estimation accuracy. Simulation studies are conducted to illustrate the good performance of the proposed MSAEM method, where MSAEM consistently performs better than SAEM and some other existing methods in multidimensional item response theory. Moreover, the proposed method is applied to a real data set from the 2018 Programme for International Student Assessment (PISA) to demonstrate the usefulness of the 4PNO model as well as MSAEM inÂ practice. Â© 2023 by the National Council on Measurement in Education.","['paper', 'develop', 'mixed', 'stochastic', 'approximation', 'expectationmaximization', 'MSAEM', 'algorithm', 'couple', 'Gibbs', 'sampler', 'compute', 'marginalized', 'maximum', 'posteriori', 'estimate', 'MMAPE', 'confirmatory', 'multidimensional', 'fourparameter', 'normal', 'ogive', 'M4PNO', 'propose', 'MSAEM', 'algorithm', 'computational', 'advantage', 'stochastic', 'approximation', 'expectationmaximization', 'SAEM', 'algorithm', 'multidimensional', 'datum', 'alleviate', 'potential', 'instability', 'cause', 'labelswitche', 'improve', 'estimation', 'accuracy', 'Simulation', 'study', 'conduct', 'illustrate', 'good', 'performance', 'propose', 'MSAEM', 'method', 'MSAEM', 'consistently', 'perform', 'SAEM', 'exist', 'method', 'multidimensional', 'item', 'response', 'theory', 'propose', 'method', 'apply', 'real', 'datum', 'set', '2018', 'Programme', 'International', 'Student', 'Assessment', 'PISA', 'demonstrate', 'usefulness', '4pno', 'MSAEM', 'practice', 'Â©', '2023', 'National', 'Council']","['MSAEM', 'Estimation', 'Confirmatory', 'Multidimensional', 'FourParameter', 'Normal', 'Ogive', 'Models']",paper develop mixed stochastic approximation expectationmaximization MSAEM algorithm couple Gibbs sampler compute marginalized maximum posteriori estimate MMAPE confirmatory multidimensional fourparameter normal ogive M4PNO propose MSAEM algorithm computational advantage stochastic approximation expectationmaximization SAEM algorithm multidimensional datum alleviate potential instability cause labelswitche improve estimation accuracy Simulation study conduct illustrate good performance propose MSAEM method MSAEM consistently perform SAEM exist method multidimensional item response theory propose method apply real datum set 2018 Programme International Student Assessment PISA demonstrate usefulness 4pno MSAEM practice Â© 2023 National Council,MSAEM Estimation Confirmatory Multidimensional FourParameter Normal Ogive Models,0.02805734,0.027398078,0.027355017,0.027572474,0.889617091,0.012314007,0.081810743,0.002975613,0,0
Dorsey D.W.; Michaels H.R.,Validity Arguments Meet Artificial Intelligence in Innovative Educational Assessment: A Discussion and Look Forward,2022,59,"In this concluding article of the special issue, we provide an overall discussion and point to future emerging trends in AI that might shape our approach to validity and building validity arguments. Â© 2022 by the National Council on Measurement in Education.",Validity Arguments Meet Artificial Intelligence in Innovative Educational Assessment: A Discussion and Look Forward,"In this concluding article of the special issue, we provide an overall discussion and point to future emerging trends in AI that might shape our approach to validity and building validity arguments. Â© 2022 by the National Council on Measurement in Education.","['conclude', 'article', 'special', 'issue', 'provide', 'overall', 'discussion', 'point', 'future', 'emerge', 'trend', 'AI', 'shape', 'approach', 'validity', 'build', 'validity', 'argument', 'Â©', '2022', 'National', 'Council']","['Validity', 'Arguments', 'Meet', 'Artificial', 'Intelligence', 'Innovative', 'Educational', 'Assessment', 'A', 'discussion', 'look', 'Forward']",conclude article special issue provide overall discussion point future emerge trend AI shape approach validity build validity argument Â© 2022 National Council,Validity Arguments Meet Artificial Intelligence Innovative Educational Assessment A discussion look Forward,0.038649747,0.038426632,0.03843735,0.846054314,0.038431957,0,0,0,0.092900249,0
Kim R.Y.; Yoo Y.J.,Cognitive Diagnostic Multistage Testing by Partitioning Hierarchically Structured Attributes,2023,60,"In cognitive diagnostic models (CDMs), a set of fine-grained attributes is required to characterize complex problem solving and provide detailed diagnostic information about an examinee. However, it is challenging to ensure reliable estimation and control computational complexity when The test aims to identify the examinee's attribute profile in a large-scale map of attributes. To address this problem, this study proposes a cognitive diagnostic multistage testing by partitioning hierarchically structured attributes (CD-MST-PH) as a multistage testing for CDM. In CD-MST-PH, multiple testlets can be constructed based on separate attribute groups before testing occurs, which retains the advantages of multistage testing over fully adaptive testing or the on-the-fly approach. Moreover, testlets are offered sequentially and adaptively, thus improving test accuracy and efficiency. An item information measure is proposed to compute the discrimination power of an item for each attribute, and a module assembly method is presented to construct modules anchored at each separate attribute group. Several module selection indices for CD-MST-PH are also proposed by modifying the item selection indices used in cognitive diagnostic computerized adaptive testing. The results of simulation study show that CD-MST-PH can improve test accuracy and efficiency relative to the conventional test without adaptive stages. Â© 2022 by the National Council on Measurement in Education.",Cognitive Diagnostic Multistage Testing by Partitioning Hierarchically Structured Attributes,"In cognitive diagnostic models (CDMs), a set of fine-grained attributes is required to characterize complex problem solving and provide detailed diagnostic information about an examinee. However, it is challenging to ensure reliable estimation and control computational complexity when The test aims to identify the examinee's attribute profile in a large-scale map of attributes. To address this problem, this study proposes a cognitive diagnostic multistage testing by partitioning hierarchically structured attributes (CD-MST-PH) as a multistage testing for CDM. In CD-MST-PH, multiple testlets can be constructed based on separate attribute groups before testing occurs, which retains the advantages of multistage testing over fully adaptive testing or the on-the-fly approach. Moreover, testlets are offered sequentially and adaptively, thus improving test accuracy and efficiency. An item information measure is proposed to compute the discrimination power of an item for each attribute, and a module assembly method is presented to construct modules anchored at each separate attribute group. Several module selection indices for CD-MST-PH are also proposed by modifying the item selection indices used in cognitive diagnostic computerized adaptive testing. The results of simulation study show that CD-MST-PH can improve test accuracy and efficiency relative to the conventional test without adaptive stages. Â© 2022 by the National Council on Measurement in Education.","['cognitive', 'diagnostic', 'cdm', 'set', 'finegraine', 'attribute', 'require', 'characterize', 'complex', 'problem', 'solve', 'provide', 'detailed', 'diagnostic', 'information', 'examinee', 'challenge', 'ensure', 'reliable', 'estimation', 'control', 'computational', 'complexity', 'test', 'aim', 'identify', 'examinee', 'attribute', 'profile', 'largescale', 'map', 'attribute', 'address', 'problem', 'study', 'propose', 'cognitive', 'diagnostic', 'multistage', 'testing', 'partition', 'hierarchically', 'structured', 'attribute', 'CDMSTPH', 'multistage', 'testing', 'CDM', 'CDMSTPH', 'multiple', 'testlet', 'construct', 'base', 'separate', 'attribute', 'group', 'testing', 'occur', 'retain', 'advantage', 'multistage', 'testing', 'fully', 'adaptive', 'testing', 'onthefly', 'approach', 'testlet', 'offer', 'sequentially', 'adaptively', 'improve', 'test', 'accuracy', 'efficiency', 'item', 'information', 'measure', 'propose', 'compute', 'discrimination', 'power', 'item', 'attribute', 'module', 'assembly', 'method', 'present', 'construct', 'module', 'anchor', 'separate', 'attribute', 'group', 'module', 'selection', 'index', 'CDMSTPH', 'propose', 'modify', 'item', 'selection', 'index', 'cognitive', 'diagnostic', 'computerized', 'adaptive', 'testing', 'result', 'simulation', 'study', 'CDMSTPH', 'improve', 'test', 'accuracy', 'efficiency', 'relative', 'conventional', 'test', 'adaptive', 'stage', 'Â©', '2022', 'National', 'Council']","['cognitive', 'Diagnostic', 'Multistage', 'Testing', 'partition', 'hierarchically', 'Structured', 'Attributes']",cognitive diagnostic cdm set finegraine attribute require characterize complex problem solve provide detailed diagnostic information examinee challenge ensure reliable estimation control computational complexity test aim identify examinee attribute profile largescale map attribute address problem study propose cognitive diagnostic multistage testing partition hierarchically structured attribute CDMSTPH multistage testing CDM CDMSTPH multiple testlet construct base separate attribute group testing occur retain advantage multistage testing fully adaptive testing onthefly approach testlet offer sequentially adaptively improve test accuracy efficiency item information measure propose compute discrimination power item attribute module assembly method present construct module anchor separate attribute group module selection index CDMSTPH propose modify item selection index cognitive diagnostic computerized adaptive testing result simulation study CDMSTPH improve test accuracy efficiency relative conventional test adaptive stage Â© 2022 National Council,cognitive Diagnostic Multistage Testing partition hierarchically Structured Attributes,0.02564544,0.02515638,0.025116485,0.898739294,0.025342401,0.001302919,0.138521642,0,0,0
Bengs D.; Kroehne U.; Brefeld U.,Simultaneous Constrained Adaptive Item Selection for Group-Based Testing,2021,58,"By tailoring test forms to the test-taker's proficiency, Computerized Adaptive Testing (CAT) enables substantial increases in testing efficiency over fixed forms testing. When used for formative assessment, the alignment of task difficulty with proficiency increases the chance that teachers can derive useful feedback from assessment data. The application of CAT to formative assessment in the classroom, however, is hindered by the large number of different items used for the whole class; the required familiarization with a large number of test items puts a significant burden on teachers. An improved CAT procedure for group-based testing is presented, which uses simultaneous automated test assembly to impose a limit on the number of items used per group. The proposed linear model for simultaneous adaptive item selection allows for full adaptivity and the accommodation of constraints on test content. The effectiveness of the group-based CAT is demonstrated with real-world items in a simulated adaptive test of 3,000 groups of test-takers, under different assumptions on group composition. Results show that the group-based CAT maintained the efficiency of CAT, while a reduction in the number of used items by one half to two-thirds was achieved, depending on the within-group variance ofÂ proficiencies. Â© 2020 by the National Council on Measurement in Education",Simultaneous Constrained Adaptive Item Selection for Group-Based Testing,"By tailoring test forms to the test-taker's proficiency, Computerized Adaptive Testing (CAT) enables substantial increases in testing efficiency over fixed forms testing. When used for formative assessment, the alignment of task difficulty with proficiency increases the chance that teachers can derive useful feedback from assessment data. The application of CAT to formative assessment in the classroom, however, is hindered by the large number of different items used for the whole class; the required familiarization with a large number of test items puts a significant burden on teachers. An improved CAT procedure for group-based testing is presented, which uses simultaneous automated test assembly to impose a limit on the number of items used per group. The proposed linear model for simultaneous adaptive item selection allows for full adaptivity and the accommodation of constraints on test content. The effectiveness of the group-based CAT is demonstrated with real-world items in a simulated adaptive test of 3,000 groups of test-takers, under different assumptions on group composition. Results show that the group-based CAT maintained the efficiency of CAT, while a reduction in the number of used items by one half to two-thirds was achieved, depending on the within-group variance ofÂ proficiencies. Â© 2020 by the National Council on Measurement in Education","['tailor', 'test', 'form', 'testtaker', 'proficiency', 'Computerized', 'Adaptive', 'Testing', 'CAT', 'enable', 'substantial', 'increase', 'testing', 'efficiency', 'fix', 'form', 'testing', 'formative', 'assessment', 'alignment', 'task', 'difficulty', 'proficiency', 'increase', 'chance', 'teacher', 'derive', 'useful', 'feedback', 'assessment', 'datum', 'application', 'CAT', 'formative', 'assessment', 'classroom', 'hinder', 'large', 'number', 'different', 'item', 'class', 'require', 'familiarization', 'large', 'number', 'test', 'item', 'significant', 'burden', 'teacher', 'improved', 'CAT', 'procedure', 'groupbased', 'testing', 'present', 'simultaneous', 'automate', 'test', 'assembly', 'impose', 'limit', 'number', 'item', 'group', 'propose', 'linear', 'simultaneous', 'adaptive', 'item', 'selection', 'allow', 'adaptivity', 'accommodation', 'constraint', 'test', 'content', 'effectiveness', 'groupbased', 'CAT', 'demonstrate', 'realworld', 'item', 'simulated', 'adaptive', 'test', '3000', 'group', 'testtaker', 'different', 'assumption', 'group', 'composition', 'result', 'groupbased', 'CAT', 'maintain', 'efficiency', 'CAT', 'reduction', 'number', 'item', 'half', 'twothird', 'achieve', 'depend', 'withingroup', 'variance', 'proficiency', 'Â©', '2020', 'National', 'Council']","['simultaneous', 'Constrained', 'Adaptive', 'Item', 'Selection', 'GroupBased', 'Testing']",tailor test form testtaker proficiency Computerized Adaptive Testing CAT enable substantial increase testing efficiency fix form testing formative assessment alignment task difficulty proficiency increase chance teacher derive useful feedback assessment datum application CAT formative assessment classroom hinder large number different item class require familiarization large number test item significant burden teacher improved CAT procedure groupbased testing present simultaneous automate test assembly impose limit number item group propose linear simultaneous adaptive item selection allow adaptivity accommodation constraint test content effectiveness groupbased CAT demonstrate realworld item simulated adaptive test 3000 group testtaker different assumption group composition result groupbased CAT maintain efficiency CAT reduction number item half twothird achieve depend withingroup variance proficiency Â© 2020 National Council,simultaneous Constrained Adaptive Item Selection GroupBased Testing,0.900879059,0.024568282,0.024537812,0.025009231,0.025005615,0.002578285,0.119385602,0,0.011796956,0.015446634
Cho S.-J.; Goodwin A.; Naveiras M.; Salas J.,Differential and Functional Response Time Item Analysis: An Application to Understanding Paper versus Digital Reading Processes,2024,61,"Despite the growing interest in incorporating response time data into item response models, there has been a lack of research investigating how the effect of speed on the probability of a correct response varies across different groups (e.g., experimental conditions) for various items (i.e., differential response time item analysis). Furthermore, previous research has shown a complex relationship between response time and accuracy, necessitating a functional analysis to understand the patterns that manifest from this relationship. In this study, response time data are incorporated into an item response model for two purposes: (a) to examine how individuals' speed within an experimental condition affects their response accuracy on an item, and (b) to detect the differences in individuals' speed between conditions in the presence of within-condition effects. For these two purposes, by-variable smooth functions are employed to model differential and functional response time effects by experimental condition for each item. This model is illustrated using an empirical data set to describe the effect of individuals' speed on their reading comprehension ability in two experimental conditions of reading medium (paper vs. digital) by item. A simulation study showed that the recovery of parameters and by-variable smooth functions of response time was satisfactory, and that the type I error rate and power of the test for the by-variable smooth function of response time were acceptable in conditions similar to the empirical data set. In addition, the proposed method correctly identified the range of response time where between-condition differences in the effect of response time on the probability of a correct response wereÂ accurate. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Differential and Functional Response Time Item Analysis: An Application to Understanding Paper versus Digital Reading Processes,"Despite the growing interest in incorporating response time data into item response models, there has been a lack of research investigating how the effect of speed on the probability of a correct response varies across different groups (e.g., experimental conditions) for various items (i.e., differential response time item analysis). Furthermore, previous research has shown a complex relationship between response time and accuracy, necessitating a functional analysis to understand the patterns that manifest from this relationship. In this study, response time data are incorporated into an item response model for two purposes: (a) to examine how individuals' speed within an experimental condition affects their response accuracy on an item, and (b) to detect the differences in individuals' speed between conditions in the presence of within-condition effects. For these two purposes, by-variable smooth functions are employed to model differential and functional response time effects by experimental condition for each item. This model is illustrated using an empirical data set to describe the effect of individuals' speed on their reading comprehension ability in two experimental conditions of reading medium (paper vs. digital) by item. A simulation study showed that the recovery of parameters and by-variable smooth functions of response time was satisfactory, and that the type I error rate and power of the test for the by-variable smooth function of response time were acceptable in conditions similar to the empirical data set. In addition, the proposed method correctly identified the range of response time where between-condition differences in the effect of response time on the probability of a correct response wereÂ accurate. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['despite', 'grow', 'interest', 'incorporate', 'response', 'time', 'datum', 'item', 'response', 'lack', 'research', 'investigate', 'effect', 'speed', 'probability', 'correct', 'response', 'vary', 'different', 'group', 'eg', 'experimental', 'condition', 'item', 'ie', 'differential', 'response', 'time', 'item', 'analysis', 'furthermore', 'previous', 'research', 'complex', 'relationship', 'response', 'time', 'accuracy', 'necessitate', 'functional', 'analysis', 'understand', 'pattern', 'manifest', 'relationship', 'study', 'response', 'time', 'datum', 'incorporate', 'item', 'response', 'purpose', 'examine', 'individual', 'speed', 'experimental', 'condition', 'affect', 'response', 'accuracy', 'item', 'b', 'detect', 'difference', 'individual', 'speed', 'condition', 'presence', 'withincondition', 'effect', 'purpose', 'byvariable', 'smooth', 'function', 'employ', 'differential', 'functional', 'response', 'time', 'effect', 'experimental', 'condition', 'item', 'illustrate', 'empirical', 'datum', 'set', 'describe', 'effect', 'individual', 'speed', 'reading', 'comprehension', 'ability', 'experimental', 'condition', 'read', 'medium', 'paper', 'vs', 'digital', 'item', 'simulation', 'study', 'recovery', 'parameter', 'byvariable', 'smooth', 'function', 'response', 'time', 'satisfactory', 'type', 'I', 'error', 'rate', 'power', 'test', 'byvariable', 'smooth', 'function', 'response', 'time', 'acceptable', 'condition', 'similar', 'empirical', 'datum', 'set', 'addition', 'propose', 'method', 'correctly', 'identify', 'range', 'response', 'time', 'betweencondition', 'difference', 'effect', 'response', 'time', 'probability', 'correct', 'response', 'accurate', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['differential', 'Functional', 'Response', 'Time', 'Item', 'Analysis', 'application', 'Understanding', 'Paper', 'versus', 'Digital', 'Reading', 'Processes']",despite grow interest incorporate response time datum item response lack research investigate effect speed probability correct response vary different group eg experimental condition item ie differential response time item analysis furthermore previous research complex relationship response time accuracy necessitate functional analysis understand pattern manifest relationship study response time datum incorporate item response purpose examine individual speed experimental condition affect response accuracy item b detect difference individual speed condition presence withincondition effect purpose byvariable smooth function employ differential functional response time effect experimental condition item illustrate empirical datum set describe effect individual speed reading comprehension ability experimental condition read medium paper vs digital item simulation study recovery parameter byvariable smooth function response time satisfactory type I error rate power test byvariable smooth function response time acceptable condition similar empirical datum set addition propose method correctly identify range response time betweencondition difference effect response time probability correct response accurate Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,differential Functional Response Time Item Analysis application Understanding Paper versus Digital Reading Processes,0.024621431,0.024109547,0.024127848,0.902808158,0.024333017,0.159722603,0,0.002043535,0,0.002795584
Kasli M.; Zopluoglu C.; Toton S.L.,A Deterministic Gated Lognormal Response Time Model to Identify Examinees with Item Preknowledge,2023,60,"Response times (RTs) have recently attracted a significant amount of attention in the literature as they may provide meaningful information about item preknowledge. In this study, a new model, the Deterministic Gated Lognormal Response Time (DG-LNRT) model, is proposed to identify examinees with item preknowledge using RTs. The proposed model was applied to two different data sets and performance was assessed with false-positive rates, true-positive rates, and precision. The results were compared with another recently proposed Z-statistic. Follow-up simulation studies were also conducted to examine model performance in settings similar to the real data sets. The results indicate that the proposed model is viable and can help detect item preknowledge under certain conditions. However, its performance is highly dependent on the correct specification of the compromised items. Â© 2022 by the National Council on Measurement in Education.",A Deterministic Gated Lognormal Response Time Model to Identify Examinees with Item Preknowledge,"Response times (RTs) have recently attracted a significant amount of attention in the literature as they may provide meaningful information about item preknowledge. In this study, a new model, the Deterministic Gated Lognormal Response Time (DG-LNRT) model, is proposed to identify examinees with item preknowledge using RTs. The proposed model was applied to two different data sets and performance was assessed with false-positive rates, true-positive rates, and precision. The results were compared with another recently proposed Z-statistic. Follow-up simulation studies were also conducted to examine model performance in settings similar to the real data sets. The results indicate that the proposed model is viable and can help detect item preknowledge under certain conditions. However, its performance is highly dependent on the correct specification of the compromised items. Â© 2022 by the National Council on Measurement in Education.","['response', 'time', 'rt', 'recently', 'attract', 'significant', 'attention', 'literature', 'provide', 'meaningful', 'information', 'item', 'preknowledge', 'study', 'new', 'Deterministic', 'Gated', 'Lognormal', 'Response', 'Time', 'DGLNRT', 'propose', 'identify', 'examinee', 'item', 'preknowledge', 'rt', 'propose', 'apply', 'different', 'data', 'set', 'performance', 'assess', 'falsepositive', 'rate', 'truepositive', 'rate', 'precision', 'result', 'compare', 'recently', 'propose', 'Zstatistic', 'Followup', 'simulation', 'study', 'conduct', 'examine', 'performance', 'setting', 'similar', 'real', 'datum', 'set', 'result', 'indicate', 'propose', 'viable', 'help', 'detect', 'item', 'preknowledge', 'certain', 'condition', 'performance', 'highly', 'dependent', 'correct', 'specification', 'compromise', 'item', 'Â©', '2022', 'National', 'Council']","['Deterministic', 'Gated', 'Lognormal', 'Response', 'Time', 'identify', 'examinee', 'Item', 'Preknowledge']",response time rt recently attract significant attention literature provide meaningful information item preknowledge study new Deterministic Gated Lognormal Response Time DGLNRT propose identify examinee item preknowledge rt propose apply different data set performance assess falsepositive rate truepositive rate precision result compare recently propose Zstatistic Followup simulation study conduct examine performance setting similar real datum set result indicate propose viable help detect item preknowledge certain condition performance highly dependent correct specification compromise item Â© 2022 National Council,Deterministic Gated Lognormal Response Time identify examinee Item Preknowledge,0.026826658,0.026064614,0.026011313,0.026468485,0.894628929,0.118711695,0,0.00519124,0,0.001538775
Wolkowitz A.A.,A Computationally Simple Method for Estimating Decision Consistency,2021,58,"Decision consistency (DC) is the reliability of a classification decision based on a test score. In professional credentialing, the decision is often a high-stakes pass/fail decision. The current methods for estimating DC are computationally complex. The purpose of this research is to provide a computationally and conceptually simple method for estimating DC that produces results comparable to, and at times potentially better than, the widely used Livingston-Lewis method. Â© 2021 by the National Council on Measurement in Education",A Computationally Simple Method for Estimating Decision Consistency,"Decision consistency (DC) is the reliability of a classification decision based on a test score. In professional credentialing, the decision is often a high-stakes pass/fail decision. The current methods for estimating DC are computationally complex. The purpose of this research is to provide a computationally and conceptually simple method for estimating DC that produces results comparable to, and at times potentially better than, the widely used Livingston-Lewis method. Â© 2021 by the National Council on Measurement in Education","['decision', 'consistency', 'DC', 'reliability', 'classification', 'decision', 'base', 'test', 'score', 'professional', 'credentialing', 'decision', 'highstake', 'passfail', 'decision', 'current', 'method', 'estimate', 'DC', 'computationally', 'complex', 'purpose', 'research', 'provide', 'computationally', 'conceptually', 'simple', 'method', 'estimate', 'DC', 'produce', 'result', 'comparable', 'time', 'potentially', 'widely', 'LivingstonLewis', 'method', 'Â©', '2021', 'National', 'Council']","['computationally', 'simple', 'Method', 'estimate', 'decision', 'consistency']",decision consistency DC reliability classification decision base test score professional credentialing decision highstake passfail decision current method estimate DC computationally complex purpose research provide computationally conceptually simple method estimate DC produce result comparable time potentially widely LivingstonLewis method Â© 2021 National Council,computationally simple Method estimate decision consistency,0.036244761,0.036136632,0.854646649,0.036679697,0.03629226,0,0.108146821,0,0,0
Huggins-Manley A.C.; Booth B.M.; D'Mello S.K.,Toward Argument-Based Fairness with an Application to AI-Enhanced Educational Assessments,2022,59,"The field of educational measurement places validity and fairness as central concepts of assessment quality. Prior research has proposed embedding fairness arguments within argument-based validity processes, particularly when fairness is conceived as comparability in assessment properties across groups. However, we argue that a more flexible approach to fairness arguments that occurs outside of and complementary to validity arguments is required to address many of the views on fairness that a set of assessment stakeholders may hold. Accordingly, we focus this manuscript on two contributions: (a) introducing the argument-based fairness approach to complement argument-based validity for both traditional and artificial intelligence (AI)-enhanced assessments and (b) applying it in an illustrative AI assessment of perceived hireability in automated video interviews used to prescreen job candidates. We conclude with recommendations for further advancing argument-based fairness approaches. Â© 2022 by the National Council on Measurement in Education.",Toward Argument-Based Fairness with an Application to AI-Enhanced Educational Assessments,"The field of educational measurement places validity and fairness as central concepts of assessment quality. Prior research has proposed embedding fairness arguments within argument-based validity processes, particularly when fairness is conceived as comparability in assessment properties across groups. However, we argue that a more flexible approach to fairness arguments that occurs outside of and complementary to validity arguments is required to address many of the views on fairness that a set of assessment stakeholders may hold. Accordingly, we focus this manuscript on two contributions: (a) introducing the argument-based fairness approach to complement argument-based validity for both traditional and artificial intelligence (AI)-enhanced assessments and (b) applying it in an illustrative AI assessment of perceived hireability in automated video interviews used to prescreen job candidates. We conclude with recommendations for further advancing argument-based fairness approaches. Â© 2022 by the National Council on Measurement in Education.","['field', 'educational', 'place', 'validity', 'fairness', 'central', 'concept', 'assessment', 'quality', 'Prior', 'research', 'propose', 'embed', 'fairness', 'argument', 'argumentbased', 'validity', 'process', 'particularly', 'fairness', 'conceive', 'comparability', 'assessment', 'property', 'group', 'argue', 'flexible', 'approach', 'fairness', 'argument', 'occur', 'outside', 'complementary', 'validity', 'argument', 'require', 'address', 'view', 'fairness', 'set', 'assessment', 'stakeholder', 'hold', 'accordingly', 'focus', 'manuscript', 'contribution', 'introduce', 'argumentbased', 'fairness', 'approach', 'complement', 'argumentbase', 'validity', 'traditional', 'artificial', 'intelligence', 'AIenhanced', 'assessment', 'b', 'apply', 'illustrative', 'AI', 'assessment', 'perceive', 'hireability', 'automate', 'video', 'interview', 'prescreen', 'job', 'candidate', 'conclude', 'recommendation', 'far', 'advance', 'argumentbased', 'fairness', 'approach', 'Â©', '2022', 'National', 'Council']","['ArgumentBased', 'Fairness', 'application', 'AIEnhanced', 'Educational', 'assessment']",field educational place validity fairness central concept assessment quality Prior research propose embed fairness argument argumentbased validity process particularly fairness conceive comparability assessment property group argue flexible approach fairness argument occur outside complementary validity argument require address view fairness set assessment stakeholder hold accordingly focus manuscript contribution introduce argumentbased fairness approach complement argumentbase validity traditional artificial intelligence AIenhanced assessment b apply illustrative AI assessment perceive hireability automate video interview prescreen job candidate conclude recommendation far advance argumentbased fairness approach Â© 2022 National Council,ArgumentBased Fairness application AIEnhanced Educational assessment,0.028619749,0.028241431,0.028351592,0.886306263,0.028480964,0,0,0,0.125334425,0
Briggs D.C.,NCME Presidential Address 2022: Turning the Page to the Next Chapter of Educational Measurement,2022,59,"Objective
Given theoretical and methodological advances that propose hypothesis about change in one or multiple processes, analytical methods for longitudinal data have been developed that provide researchers with various options for analyzing change over time. In this paper, we revisit several latent growth curve models that may be considered to answer questions about repeated measures of continuous variables, which may be operationalised as time varying covariates or outcomes.

Study design and setting
To illustrate each of the models discussed and how to interpret parameter estimates, we present examples of each method discussed using cognitive and blood pressure measures from a longitudinal study of ageing, the OCTO Twin Study.

Result and Conclusion
Although statistical models are helpful tools to test theoretical hypotheses about the dynamics between multiple processes, the choice of model and its specification will influence results and conclusions made.

Keywords: latent growth model, time varying covariates, bivariate latent growth model, longitudinal models",NCME Presidential Address 2022: Turning the Page to the Next Chapter of Educational Measurement,"Objective
Given theoretical and methodological advances that propose hypothesis about change in one or multiple processes, analytical methods for longitudinal data have been developed that provide researchers with various options for analyzing change over time. In this paper, we revisit several latent growth curve models that may be considered to answer questions about repeated measures of continuous variables, which may be operationalised as time varying covariates or outcomes.

Study design and setting
To illustrate each of the models discussed and how to interpret parameter estimates, we present examples of each method discussed using cognitive and blood pressure measures from a longitudinal study of ageing, the OCTO Twin Study.

Result and Conclusion
Although statistical models are helpful tools to test theoretical hypotheses about the dynamics between multiple processes, the choice of model and its specification will influence results and conclusions made.

Keywords: latent growth model, time varying covariates, bivariate latent growth model, longitudinal models","['objective', 'theoretical', 'methodological', 'advance', 'propose', 'hypothesis', 'change', 'multiple', 'process', 'analytical', 'method', 'longitudinal', 'datum', 'develop', 'provide', 'researcher', 'option', 'analyze', 'change', 'time', 'paper', 'revisit', 'latent', 'growth', 'curve', 'consider', 'answer', 'question', 'repeat', 'measure', 'continuous', 'variable', 'operationalise', 'time', 'vary', 'covariate', 'outcome', 'Study', 'design', 'set', 'illustrate', 'discuss', 'interpret', 'parameter', 'estimate', 'present', 'example', 'method', 'discuss', 'cognitive', 'blood', 'pressure', 'measure', 'longitudinal', 'study', 'age', 'OCTO', 'Twin', 'Study', 'Result', 'conclusion', 'statistical', 'helpful', 'tool', 'test', 'theoretical', 'hypothesis', 'dynamic', 'multiple', 'process', 'choice', 'specification', 'influence', 'result', 'conclusion', 'Keywords', 'latent', 'growth', 'time', 'vary', 'covariate', 'bivariate', 'latent', 'growth', 'longitudinal']","['NCME', 'Presidential', 'Address', '2022', 'turn', 'Page', 'chapter', 'Educational']",objective theoretical methodological advance propose hypothesis change multiple process analytical method longitudinal datum develop provide researcher option analyze change time paper revisit latent growth curve consider answer question repeat measure continuous variable operationalise time vary covariate outcome Study design set illustrate discuss interpret parameter estimate present example method discuss cognitive blood pressure measure longitudinal study age OCTO Twin Study Result conclusion statistical helpful tool test theoretical hypothesis dynamic multiple process choice specification influence result conclusion Keywords latent growth time vary covariate bivariate latent growth longitudinal,NCME Presidential Address 2022 turn Page chapter Educational,0.025385122,0.025013974,0.024995017,0.025769324,0.898836563,0.037835067,0.063283015,0.003454227,0.003200858,0
Puhan G.; Kim S.,Score Comparability Issues with At-Home Testing and How to Address Them,2022,59,"As a result of the COVID-19 pandemic, at-home testing has become a popular delivery mode in many testing programs. When programs offer at-home testing to expand their service, the score comparability between test takers testing remotely and those testing in a test center is critical. This article summarizes statistical procedures that could be used to evaluate potential mode effects at both the item level and the total score levels. Using operational data from a licensure test, we also compared linking relationships between the test center and at-home testing groups to determine the reporting score conversion from a subpopulation invariance perspective. Â© 2022 by the National Council on Measurement in Education.",Score Comparability Issues with At-Home Testing and How to Address Them,"As a result of the COVID-19 pandemic, at-home testing has become a popular delivery mode in many testing programs. When programs offer at-home testing to expand their service, the score comparability between test takers testing remotely and those testing in a test center is critical. This article summarizes statistical procedures that could be used to evaluate potential mode effects at both the item level and the total score levels. Using operational data from a licensure test, we also compared linking relationships between the test center and at-home testing groups to determine the reporting score conversion from a subpopulation invariance perspective. Â© 2022 by the National Council on Measurement in Education.","['result', 'covid19', 'pandemic', 'athome', 'testing', 'popular', 'delivery', 'mode', 'testing', 'program', 'program', 'offer', 'athome', 'testing', 'expand', 'service', 'score', 'comparability', 'test', 'taker', 'test', 'remotely', 'testing', 'test', 'center', 'critical', 'article', 'summarize', 'statistical', 'procedure', 'evaluate', 'potential', 'mode', 'effect', 'item', 'level', 'total', 'score', 'level', 'operational', 'datum', 'licensure', 'test', 'compare', 'link', 'relationship', 'test', 'center', 'athome', 'testing', 'group', 'determine', 'reporting', 'score', 'conversion', 'subpopulation', 'invariance', 'perspective', 'Â©', '2022', 'National', 'Council']","['Score', 'Comparability', 'Issues', 'AtHome', 'Testing', 'address']",result covid19 pandemic athome testing popular delivery mode testing program program offer athome testing expand service score comparability test taker test remotely testing test center critical article summarize statistical procedure evaluate potential mode effect item level total score level operational datum licensure test compare link relationship test center athome testing group determine reporting score conversion subpopulation invariance perspective Â© 2022 National Council,Score Comparability Issues AtHome Testing address,0.029960091,0.029757462,0.029991768,0.880211504,0.030079175,0,0.123106936,0,0.006203281,0.004768975
van Laar S.; Braeken J.,Random Responders in the TIMSS 2015 Student Questionnaire: A Threat to Validity?,2022,59,"The low-stakes character of international large-scale educational assessments implies that a participating student might at times provide unrelated answers as if s/he was not even reading the items and choosing a response option randomly throughout. Depending on the severity of this invalid response behavior, interpretations of the assessment results are at risk of being invalidated. Not much is known about the prevalence nor impact of such random responders in the context of international large-scale educational assessments. Following a mixture item response theory (IRT) approach, an initial investigation of both issues is conducted for the Confidence in and Value of Mathematics/Science (VoM/VoS) scales in the Trends in International Mathematics and Science Study (TIMSS) 2015 student questionnaire. We end with a call to facilitate further mapping of invalid response behavior in this context by the inclusion of instructed response items and survey completion speed indicators in the assessments and a habit of sensitivity checks in all secondary dataÂ studies. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Random Responders in the TIMSS 2015 Student Questionnaire: A Threat to Validity?,"The low-stakes character of international large-scale educational assessments implies that a participating student might at times provide unrelated answers as if s/he was not even reading the items and choosing a response option randomly throughout. Depending on the severity of this invalid response behavior, interpretations of the assessment results are at risk of being invalidated. Not much is known about the prevalence nor impact of such random responders in the context of international large-scale educational assessments. Following a mixture item response theory (IRT) approach, an initial investigation of both issues is conducted for the Confidence in and Value of Mathematics/Science (VoM/VoS) scales in the Trends in International Mathematics and Science Study (TIMSS) 2015 student questionnaire. We end with a call to facilitate further mapping of invalid response behavior in this context by the inclusion of instructed response items and survey completion speed indicators in the assessments and a habit of sensitivity checks in all secondary dataÂ studies. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['lowstake', 'character', 'international', 'largescale', 'educational', 'assessment', 'imply', 'participate', 'student', 'time', 'provide', 'unrelated', 'answer', 'read', 'item', 'choose', 'response', 'option', 'randomly', 'depend', 'severity', 'invalid', 'response', 'behavior', 'interpretation', 'assessment', 'result', 'risk', 'invalidate', 'know', 'prevalence', 'impact', 'random', 'responder', 'context', 'international', 'largescale', 'educational', 'assessment', 'follow', 'mixture', 'item', 'response', 'theory', 'IRT', 'approach', 'initial', 'investigation', 'issue', 'conduct', 'Confidence', 'Value', 'MathematicsScience', 'VoMVoS', 'scale', 'Trends', 'International', 'Mathematics', 'Science', 'Study', 'TIMSS', '2015', 'student', 'questionnaire', 'end', 'facilitate', 'mapping', 'invalid', 'response', 'behavior', 'context', 'inclusion', 'instruct', 'response', 'item', 'survey', 'completion', 'speed', 'indicator', 'assessment', 'habit', 'sensitivity', 'check', 'secondary', 'datum', 'study', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['Random', 'Responders', 'TIMSS', '2015', 'Student', 'Questionnaire', 'threat', 'validity']",lowstake character international largescale educational assessment imply participate student time provide unrelated answer read item choose response option randomly depend severity invalid response behavior interpretation assessment result risk invalidate know prevalence impact random responder context international largescale educational assessment follow mixture item response theory IRT approach initial investigation issue conduct Confidence Value MathematicsScience VoMVoS scale Trends International Mathematics Science Study TIMSS 2015 student questionnaire end facilitate mapping invalid response behavior context inclusion instruct response item survey completion speed indicator assessment habit sensitivity check secondary datum study Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,Random Responders TIMSS 2015 Student Questionnaire threat validity,0.02254843,0.91087011,0.021940666,0.022464534,0.022176259,0.100569584,0,0,0.026642916,0
Han S.; Kang H.-A.,Online Monitoring of Test-Taking Behavior Based on Item Responses and Response Times,2023,60,"The study presents multivariate sequential monitoring procedures for examining test-taking behaviors online. The procedures monitor examinee's responses and response times and signal aberrancy as soon as significant change is identifieddetected in the test-taking behavior. The study in particular proposes three schemes to track different indicators of a test-taking mode?”the observable manifest variables, latent trait variables, and measurement likelihood. For each procedure, sequential sampling strategies are presented to implement online monitoring. Numerical experimentation based on simulated data suggests that the proposed procedures demonstrate adequate performance. The procedures identified examinees with aberrant behaviors with high detection power and timeliness, while maintaining error rates reasonably small. Experimental application to real data also suggested that the procedures have practical relevance to real assessments. Based on the observations from the experiential analysis, the study discusses implications and guidelines for practicalÂ use. Â© 2023 by the National Council on Measurement in Education.",Online Monitoring of Test-Taking Behavior Based on Item Responses and Response Times,"The study presents multivariate sequential monitoring procedures for examining test-taking behaviors online. The procedures monitor examinee's responses and response times and signal aberrancy as soon as significant change is identifieddetected in the test-taking behavior. The study in particular proposes three schemes to track different indicators of a test-taking mode?”the observable manifest variables, latent trait variables, and measurement likelihood. For each procedure, sequential sampling strategies are presented to implement online monitoring. Numerical experimentation based on simulated data suggests that the proposed procedures demonstrate adequate performance. The procedures identified examinees with aberrant behaviors with high detection power and timeliness, while maintaining error rates reasonably small. Experimental application to real data also suggested that the procedures have practical relevance to real assessments. Based on the observations from the experiential analysis, the study discusses implications and guidelines for practicalÂ use. Â© 2023 by the National Council on Measurement in Education.","['study', 'present', 'multivariate', 'sequential', 'monitoring', 'procedure', 'examine', 'testtake', 'behavior', 'online', 'procedure', 'monitor', 'examine', 'response', 'response', 'time', 'signal', 'aberrancy', 'soon', 'significant', 'change', 'identifieddetecte', 'testtaking', 'behavior', 'study', 'particular', 'propose', 'scheme', 'track', 'different', 'indicator', 'testtaking', 'mode', '??, 'observable', 'manifest', 'variable', 'latent', 'trait', 'variable', 'likelihood', 'procedure', 'sequential', 'sampling', 'strategy', 'present', 'implement', 'online', 'monitor', 'Numerical', 'experimentation', 'base', 'simulate', 'datum', 'suggest', 'propose', 'procedure', 'demonstrate', 'adequate', 'performance', 'procedure', 'identify', 'examinee', 'aberrant', 'behavior', 'high', 'detection', 'power', 'timeliness', 'maintain', 'error', 'rate', 'reasonably', 'small', 'experimental', 'application', 'real', 'datum', 'suggest', 'procedure', 'practical', 'relevance', 'real', 'assessment', 'base', 'observation', 'experiential', 'analysis', 'study', 'discuss', 'implication', 'guideline', 'practical', 'Â©', '2023', 'National', 'Council']","['Online', 'Monitoring', 'TestTaking', 'Behavior', 'base', 'Item', 'Responses', 'Response', 'Times']",study present multivariate sequential monitoring procedure examine testtake behavior online procedure monitor examine response response time signal aberrancy soon significant change identifieddetecte testtaking behavior study particular propose scheme track different indicator testtaking mode ??observable manifest variable latent trait variable likelihood procedure sequential sampling strategy present implement online monitor Numerical experimentation base simulate datum suggest propose procedure demonstrate adequate performance procedure identify examinee aberrant behavior high detection power timeliness maintain error rate reasonably small experimental application real datum suggest procedure practical relevance real assessment base observation experiential analysis study discuss implication guideline practical Â© 2023 National Council,Online Monitoring TestTaking Behavior base Item Responses Response Times,0.025321737,0.024644057,0.900536707,0.024708216,0.024789283,0.053520917,0.007295584,0.035562396,0.008070752,0.017105079
Bulut O.; Gorgun G.; Karamese H.,Incorporating Test-Taking Engagement into Multistage Adaptive Testing Design for Large-Scale Assessments,2023,,"The use of multistage adaptive testing (MST) has gradually increased in large-scale testing programs as MST achieves a balanced compromise between linear test design and item-level adaptive testing. MST works on the premise that each examinee gives their best effort when attempting the items, and their responses truly reflect what they know or can do. However, research shows that large-scale assessments may suffer from a lack of test-taking engagement, especially if they are low stakes. Examinees with low test-taking engagement are likely to show noneffortful responding (e.g., answering the items very rapidly without reading the item stem or response options). To alleviate the impact of noneffortful responses on the measurement accuracy of MST, test-taking engagement can be operationalized as a latent trait based on response times and incorporated into the on-the-fly module assembly procedure. To demonstrate the proposed approach, a Monte-Carlo simulation study was conducted based on item parameters from an international large-scale assessment. The results indicated that the on-the-fly module assembly considering both ability and test-taking engagement could minimize the impact of noneffortful responses, yielding more accurate ability estimates and classifications. Implications for practice and directions for future research were discussed. Â© 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Incorporating Test-Taking Engagement into Multistage Adaptive Testing Design for Large-Scale Assessments,"The use of multistage adaptive testing (MST) has gradually increased in large-scale testing programs as MST achieves a balanced compromise between linear test design and item-level adaptive testing. MST works on the premise that each examinee gives their best effort when attempting the items, and their responses truly reflect what they know or can do. However, research shows that large-scale assessments may suffer from a lack of test-taking engagement, especially if they are low stakes. Examinees with low test-taking engagement are likely to show noneffortful responding (e.g., answering the items very rapidly without reading the item stem or response options). To alleviate the impact of noneffortful responses on the measurement accuracy of MST, test-taking engagement can be operationalized as a latent trait based on response times and incorporated into the on-the-fly module assembly procedure. To demonstrate the proposed approach, a Monte-Carlo simulation study was conducted based on item parameters from an international large-scale assessment. The results indicated that the on-the-fly module assembly considering both ability and test-taking engagement could minimize the impact of noneffortful responses, yielding more accurate ability estimates and classifications. Implications for practice and directions for future research were discussed. Â© 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['multistage', 'adaptive', 'testing', 'MST', 'gradually', 'increase', 'largescale', 'testing', 'program', 'MST', 'achieve', 'balanced', 'compromise', 'linear', 'test', 'design', 'itemlevel', 'adaptive', 'testing', 'MST', 'work', 'premise', 'examinee', 'good', 'effort', 'attempt', 'item', 'response', 'truly', 'reflect', 'know', 'research', 'largescale', 'assessment', 'suffer', 'lack', 'testtaking', 'engagement', 'especially', 'low', 'stake', 'examine', 'low', 'testtaking', 'engagement', 'likely', 'noneffortful', 'respond', 'eg', 'answer', 'item', 'rapidly', 'read', 'item', 'stem', 'response', 'option', 'alleviate', 'impact', 'noneffortful', 'response', 'accuracy', 'MST', 'testtaking', 'engagement', 'operationalize', 'latent', 'trait', 'base', 'response', 'time', 'incorporate', 'onthefly', 'module', 'assembly', 'procedure', 'demonstrate', 'propose', 'approach', 'MonteCarlo', 'simulation', 'study', 'conduct', 'base', 'item', 'parameter', 'international', 'largescale', 'assessment', 'result', 'indicate', 'onthefly', 'module', 'assembly', 'consider', 'ability', 'testtaking', 'engagement', 'minimize', 'impact', 'noneffortful', 'response', 'yield', 'accurate', 'ability', 'estimate', 'classification', 'Implications', 'practice', 'direction', 'future', 'research', 'discuss', 'Â©', '2023', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['incorporate', 'TestTaking', 'Engagement', 'Multistage', 'Adaptive', 'Testing', 'Design', 'LargeScale', 'assessment']",multistage adaptive testing MST gradually increase largescale testing program MST achieve balanced compromise linear test design itemlevel adaptive testing MST work premise examinee good effort attempt item response truly reflect know research largescale assessment suffer lack testtaking engagement especially low stake examine low testtaking engagement likely noneffortful respond eg answer item rapidly read item stem response option alleviate impact noneffortful response accuracy MST testtaking engagement operationalize latent trait base response time incorporate onthefly module assembly procedure demonstrate propose approach MonteCarlo simulation study conduct base item parameter international largescale assessment result indicate onthefly module assembly consider ability testtaking engagement minimize impact noneffortful response yield accurate ability estimate classification Implications practice direction future research discuss Â© 2023 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,incorporate TestTaking Engagement Multistage Adaptive Testing Design LargeScale assessment,0.022591239,0.022208089,0.022152837,0.910702488,0.022345346,0.067141495,0.079568914,0,0.005634424,0
Moses T.,Linking and Comparability across Conditions of Measurement: Established Frameworks and Proposed Updates,2022,59,"One result of recent changes in testing is that previously established linking frameworks may not adequately address challenges in current linking situations. Test linking through equating, concordance, vertical scaling or battery scaling may not represent linkings for the scores of tests developed to measure constructs differently for different examinees, or tests that are administered in different modes and data collection designs. This article considers how previously proposed linking frameworks might be updated to address more recent testing situations. The first section summarizes the definitions and frameworks described in previous test linking discussions. Additional sections consider some sources of more disparate approaches to test development and administrations, as well as the implications of these for test linking. Possibilities for reflecting these features in an expanded test linking framework are proposed that encourage limited comparability, such as comparability that is restricted to subgroups or to the conditions of a linking study when a linking is produced, or within, but not across tests or test forms when an empirical linking based on examinee data is not produced. The implications of an updated framework of previously established linking approaches are further described in a final discussion. Â© 2022 by the National Council on Measurement in Education.",Linking and Comparability across Conditions of Measurement: Established Frameworks and Proposed Updates,"One result of recent changes in testing is that previously established linking frameworks may not adequately address challenges in current linking situations. Test linking through equating, concordance, vertical scaling or battery scaling may not represent linkings for the scores of tests developed to measure constructs differently for different examinees, or tests that are administered in different modes and data collection designs. This article considers how previously proposed linking frameworks might be updated to address more recent testing situations. The first section summarizes the definitions and frameworks described in previous test linking discussions. Additional sections consider some sources of more disparate approaches to test development and administrations, as well as the implications of these for test linking. Possibilities for reflecting these features in an expanded test linking framework are proposed that encourage limited comparability, such as comparability that is restricted to subgroups or to the conditions of a linking study when a linking is produced, or within, but not across tests or test forms when an empirical linking based on examinee data is not produced. The implications of an updated framework of previously established linking approaches are further described in a final discussion. Â© 2022 by the National Council on Measurement in Education.","['result', 'recent', 'change', 'testing', 'previously', 'establish', 'link', 'framework', 'adequately', 'address', 'challenge', 'current', 'linking', 'situation', 'test', 'link', 'equate', 'concordance', 'vertical', 'scaling', 'battery', 'scaling', 'represent', 'linking', 'score', 'test', 'develop', 'measure', 'construct', 'differently', 'different', 'examinee', 'test', 'administer', 'different', 'mode', 'datum', 'collection', 'design', 'article', 'consider', 'previously', 'propose', 'link', 'framework', 'update', 'address', 'recent', 'testing', 'situation', 'section', 'summarize', 'definition', 'framework', 'describe', 'previous', 'test', 'link', 'discussion', 'additional', 'section', 'consider', 'source', 'disparate', 'approach', 'test', 'development', 'administration', 'implication', 'test', 'link', 'Possibilities', 'reflect', 'feature', 'expand', 'test', 'link', 'framework', 'propose', 'encourage', 'limited', 'comparability', 'comparability', 'restrict', 'subgroup', 'condition', 'link', 'study', 'linking', 'produce', 'test', 'test', 'form', 'empirical', 'linking', 'base', 'examinee', 'datum', 'produce', 'implication', 'update', 'framework', 'previously', 'establish', 'linking', 'approach', 'far', 'describe', 'final', 'discussion', 'Â©', '2022', 'National', 'Council']","['linking', 'Comparability', 'Conditions', 'Established', 'Frameworks', 'Proposed', 'update']",result recent change testing previously establish link framework adequately address challenge current linking situation test link equate concordance vertical scaling battery scaling represent linking score test develop measure construct differently different examinee test administer different mode datum collection design article consider previously propose link framework update address recent testing situation section summarize definition framework describe previous test link discussion additional section consider source disparate approach test development administration implication test link Possibilities reflect feature expand test link framework propose encourage limited comparability comparability restrict subgroup condition link study linking produce test test form empirical linking base examinee datum produce implication update framework previously establish linking approach far describe final discussion Â© 2022 National Council,linking Comparability Conditions Established Frameworks Proposed update,0.029724371,0.02878724,0.028745549,0.029684707,0.883058134,0,0.125055922,0,0.009803641,0
Karatoprak Ersen R.; Lee W.-C.,Pretest Item Calibration in Computerized Multistage Adaptive Testing,2023,60,"The purpose of this study was to compare calibration and linking methods for placing pretest item parameter estimates on the item pool scale in a 1-3 computerized multistage adaptive testing design in terms of item parameter recovery. Two models were used: embedded-section, in which pretest items were administered within a separate module, and embedded-items, in which pretest items were distributed across operational modules. The calibration methods were separate calibration with linking (SC) and fixed calibration (FC) with three parallel approaches under each (FC-1 and SC-1; FC-2 and SC-2; and FC-3 and SC-3). The FC-1 and SC-1 used only operational items in the routing module to link pretest items. The FC-2 and SC-2 also used only operational items in the routing module for linking, but in addition, the operational items in second stage modules were freely estimated. The FC-3 and SC-3 used operational items in all modules to link pretest items. The third calibration approach (i.e., FC-3 and SC-3) yielded the best results. For all three approaches, SC outperformed FC in all study conditions which were module length, sample size and examinee distributions. Â© 2023 National Council on Measurement in Education.",Pretest Item Calibration in Computerized Multistage Adaptive Testing,"The purpose of this study was to compare calibration and linking methods for placing pretest item parameter estimates on the item pool scale in a 1-3 computerized multistage adaptive testing design in terms of item parameter recovery. Two models were used: embedded-section, in which pretest items were administered within a separate module, and embedded-items, in which pretest items were distributed across operational modules. The calibration methods were separate calibration with linking (SC) and fixed calibration (FC) with three parallel approaches under each (FC-1 and SC-1; FC-2 and SC-2; and FC-3 and SC-3). The FC-1 and SC-1 used only operational items in the routing module to link pretest items. The FC-2 and SC-2 also used only operational items in the routing module for linking, but in addition, the operational items in second stage modules were freely estimated. The FC-3 and SC-3 used operational items in all modules to link pretest items. The third calibration approach (i.e., FC-3 and SC-3) yielded the best results. For all three approaches, SC outperformed FC in all study conditions which were module length, sample size and examinee distributions. Â© 2023 National Council on Measurement in Education.","['purpose', 'study', 'compare', 'calibration', 'link', 'method', 'place', 'pret', 'item', 'parameter', 'estimate', 'item', 'pool', 'scale', '13', 'computerized', 'multistage', 'adaptive', 'testing', 'design', 'term', 'item', 'parameter', 'recovery', 'embeddedsection', 'pretest', 'item', 'administer', 'separate', 'module', 'embeddeditem', 'pretest', 'item', 'distribute', 'operational', 'module', 'calibration', 'method', 'separate', 'calibration', 'link', 'SC', 'fix', 'calibration', 'fc', 'parallel', 'approach', 'FC1', 'SC1', 'FC2', 'SC2', 'FC3', 'SC3', 'FC1', 'SC1', 'operational', 'item', 'route', 'module', 'link', 'pret', 'item', 'FC2', 'SC2', 'operational', 'item', 'routing', 'module', 'link', 'addition', 'operational', 'item', 'second', 'stage', 'module', 'freely', 'estimate', 'FC3', 'SC3', 'operational', 'item', 'module', 'link', 'pret', 'item', 'calibration', 'approach', 'ie', 'FC3', 'SC3', 'yield', 'good', 'result', 'approach', 'SC', 'outperform', 'fc', 'study', 'condition', 'module', 'length', 'sample', 'size', 'examinee', 'distribution', 'Â©', '2023', 'National', 'Council']","['Pretest', 'Item', 'Calibration', 'Computerized', 'Multistage', 'Adaptive', 'Testing']",purpose study compare calibration link method place pret item parameter estimate item pool scale 13 computerized multistage adaptive testing design term item parameter recovery embeddedsection pretest item administer separate module embeddeditem pretest item distribute operational module calibration method separate calibration link SC fix calibration fc parallel approach FC1 SC1 FC2 SC2 FC3 SC3 FC1 SC1 operational item route module link pret item FC2 SC2 operational item routing module link addition operational item second stage module freely estimate FC3 SC3 operational item module link pret item calibration approach ie FC3 SC3 yield good result approach SC outperform fc study condition module length sample size examinee distribution Â© 2023 National Council,Pretest Item Calibration Computerized Multistage Adaptive Testing,0.030759224,0.030128971,0.030120548,0.878621147,0.03037011,0,0.117456978,0,0,0
Joo S.-H.; Lee P.,Detecting Differential Item Functioning Using Posterior Predictive Model Checking: A Comparison of Discrepancy Statistics,2022,59,"This study proposes a new Bayesian differential item functioning (DIF) detection method using posterior predictive model checking (PPMC). Item fit measures including infit, outfit, observed score distribution (OSD), and Q1 were considered as discrepancy statistics for the PPMC DIF methods. The performance of the PPMC DIF method was evaluated via a Monte Carlo simulation manipulating sample size, DIF size, DIF type, DIF percentage, and subpopulation trait distribution. Parametric DIF methods, such as Lord's chi-square and Raju's area approaches, were also included in the simulation design in order to compare the performance of the proposed PPMC DIF methods to those previously existing. Based on Type I error and power analysis, we found that PPMC DIF methods showed better-controlled Type I error rates than the existing methods and comparable power to detect uniform DIF. The implications and recommendations for applied researchers are discussed. Â© 2022 by the National Council on Measurement in Education.",Detecting Differential Item Functioning Using Posterior Predictive Model Checking: A Comparison of Discrepancy Statistics,"This study proposes a new Bayesian differential item functioning (DIF) detection method using posterior predictive model checking (PPMC). Item fit measures including infit, outfit, observed score distribution (OSD), and Q1 were considered as discrepancy statistics for the PPMC DIF methods. The performance of the PPMC DIF method was evaluated via a Monte Carlo simulation manipulating sample size, DIF size, DIF type, DIF percentage, and subpopulation trait distribution. Parametric DIF methods, such as Lord's chi-square and Raju's area approaches, were also included in the simulation design in order to compare the performance of the proposed PPMC DIF methods to those previously existing. Based on Type I error and power analysis, we found that PPMC DIF methods showed better-controlled Type I error rates than the existing methods and comparable power to detect uniform DIF. The implications and recommendations for applied researchers are discussed. Â© 2022 by the National Council on Measurement in Education.","['study', 'propose', 'new', 'bayesian', 'differential', 'item', 'function', 'DIF', 'detection', 'method', 'posterior', 'predictive', 'check', 'PPMC', 'Item', 'fit', 'measure', 'include', 'infit', 'outfit', 'observe', 'score', 'distribution', 'OSD', 'Q1', 'consider', 'discrepancy', 'statistic', 'PPMC', 'dif', 'method', 'performance', 'PPMC', 'DIF', 'method', 'evaluate', 'Monte', 'Carlo', 'simulation', 'manipulate', 'sample', 'size', 'dif', 'size', 'dif', 'type', 'DIF', 'percentage', 'subpopulation', 'trait', 'distribution', 'Parametric', 'DIF', 'method', 'Lords', 'chisquare', 'Rajus', 'area', 'approach', 'include', 'simulation', 'design', 'order', 'compare', 'performance', 'propose', 'PPMC', 'dif', 'method', 'previously', 'exist', 'base', 'Type', 'I', 'error', 'power', 'analysis', 'find', 'PPMC', 'DIF', 'method', 'bettercontrolled', 'Type', 'I', 'error', 'rate', 'exist', 'method', 'comparable', 'power', 'detect', 'uniform', 'DIF', 'implication', 'recommendation', 'apply', 'researcher', 'discuss', 'Â©', '2022', 'National', 'Council']","['detect', 'Differential', 'Item', 'Functioning', 'Posterior', 'Predictive', 'Checking', 'Comparison', 'Discrepancy', 'Statistics']",study propose new bayesian differential item function DIF detection method posterior predictive check PPMC Item fit measure include infit outfit observe score distribution OSD Q1 consider discrepancy statistic PPMC dif method performance PPMC DIF method evaluate Monte Carlo simulation manipulate sample size dif size dif type DIF percentage subpopulation trait distribution Parametric DIF method Lords chisquare Rajus area approach include simulation design order compare performance propose PPMC dif method previously exist base Type I error power analysis find PPMC DIF method bettercontrolled Type I error rate exist method comparable power detect uniform DIF implication recommendation apply researcher discuss Â© 2022 National Council,detect Differential Item Functioning Posterior Predictive Checking Comparison Discrepancy Statistics,0.029696142,0.029208039,0.02927939,0.029519705,0.882296723,0,0,0.002554364,0,0.283022206
Thompson W.J.; Nash B.; Clark A.K.; Hoover J.C.,Using Simulated Retests to Estimate the Reliability of Diagnostic Assessment Systems,2023,60,"As diagnostic classification models become more widely used in large-scale operational assessments, we must give consideration to the methods for estimating and reporting reliability. Researchers must explore alternatives to traditional reliability methods that are consistent with the design, scoring, and reporting levels of diagnostic assessment systems. In this article, we describe and evaluate a method for simulating retests to summarize reliability evidence at multiple reporting levels. We evaluate how the performance of reliability estimates from simulated retests compares to other measures of classification consistency and accuracy for diagnostic assessments that have previously been described in the literature, but which limit the level at which reliability can be reported. Overall, the findings show that reliability estimates from simulated retests are an accurate measure of reliability and are consistent with other measures of reliability for diagnostic assessments. We then apply this method to real data from the Examination for the Certificate of Proficiency in English to demonstrate the method in practice and compare reliability estimates from observed data. Finally, we discuss implications for the field and possible next directions. Â© 2023 by the National Council on Measurement in Education.",Using Simulated Retests to Estimate the Reliability of Diagnostic Assessment Systems,"As diagnostic classification models become more widely used in large-scale operational assessments, we must give consideration to the methods for estimating and reporting reliability. Researchers must explore alternatives to traditional reliability methods that are consistent with the design, scoring, and reporting levels of diagnostic assessment systems. In this article, we describe and evaluate a method for simulating retests to summarize reliability evidence at multiple reporting levels. We evaluate how the performance of reliability estimates from simulated retests compares to other measures of classification consistency and accuracy for diagnostic assessments that have previously been described in the literature, but which limit the level at which reliability can be reported. Overall, the findings show that reliability estimates from simulated retests are an accurate measure of reliability and are consistent with other measures of reliability for diagnostic assessments. We then apply this method to real data from the Examination for the Certificate of Proficiency in English to demonstrate the method in practice and compare reliability estimates from observed data. Finally, we discuss implications for the field and possible next directions. Â© 2023 by the National Council on Measurement in Education.","['diagnostic', 'classification', 'widely', 'largescale', 'operational', 'assessment', 'consideration', 'method', 'estimate', 'reporting', 'reliability', 'Researchers', 'explore', 'alternative', 'traditional', 'reliability', 'method', 'consistent', 'design', 'scoring', 'reporting', 'level', 'diagnostic', 'assessment', 'system', 'article', 'describe', 'evaluate', 'method', 'simulate', 'retest', 'summarize', 'reliability', 'evidence', 'multiple', 'reporting', 'level', 'evaluate', 'performance', 'reliability', 'estimate', 'simulated', 'retest', 'compare', 'measure', 'classification', 'consistency', 'accuracy', 'diagnostic', 'assessment', 'previously', 'describe', 'literature', 'limit', 'level', 'reliability', 'report', 'overall', 'finding', 'reliability', 'estimate', 'simulated', 'retest', 'accurate', 'measure', 'reliability', 'consistent', 'measure', 'reliability', 'diagnostic', 'assessment', 'apply', 'method', 'real', 'datum', 'Examination', 'Certificate', 'Proficiency', 'English', 'demonstrate', 'method', 'practice', 'compare', 'reliability', 'estimate', 'observed', 'datum', 'finally', 'discuss', 'implication', 'field', 'possible', 'direction', 'Â©', '2023', 'National', 'Council']","['simulated', 'retest', 'estimate', 'Reliability', 'Diagnostic', 'Assessment', 'Systems']",diagnostic classification widely largescale operational assessment consideration method estimate reporting reliability Researchers explore alternative traditional reliability method consistent design scoring reporting level diagnostic assessment system article describe evaluate method simulate retest summarize reliability evidence multiple reporting level evaluate performance reliability estimate simulated retest compare measure classification consistency accuracy diagnostic assessment previously describe literature limit level reliability report overall finding reliability estimate simulated retest accurate measure reliability consistent measure reliability diagnostic assessment apply method real datum Examination Certificate Proficiency English demonstrate method practice compare reliability estimate observed datum finally discuss implication field possible direction Â© 2023 National Council,simulated retest estimate Reliability Diagnostic Assessment Systems,0.030586616,0.030025826,0.029991085,0.030449483,0.87894699,0,0.124522861,0,0.021591981,0
Kim H.J.; Lee W.-C.,Evaluation of Factors Affecting the Performance of the S?’X2 Item-Fit Index,2022,59,"Orlando and Thissen (2000) introduced the (Formula presented.) item-fit index for testing goodness-of-fit with dichotomous item response theory (IRT) models. This study considers and evaluates an alternative approach for computing (Formula presented.) values and other factors associated with collapsing tables of observed and expected numbers (OE tables), which can affect flagging items. Results suggest that collapsing OE tables requires careful consideration of a trade-off between power and empirical type I error rate. Concurrent collapsing of score categories would be preferred over separate collapsing for its procedural simplicity, minimal effect of choice of a minimum cell value on empirical type I error rates, and reasonable type I error rates even for the most sparse condition in the study. For separate collapsing, a smaller minimum cell value is recommended as OE tables possess more sparseness (e.g., longer test lengths and smaller sample sizes) if inflated type I error rates are more of a concern in detecting items for misfit based on the (Formula presented.) index. If it is more important to identify misfit items, the study results recommend using a larger minimum cell value forÂ collapsing. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Evaluation of Factors Affecting the Performance of the S?’X2 Item-Fit Index,"Orlando and Thissen (2000) introduced the (Formula presented.) item-fit index for testing goodness-of-fit with dichotomous item response theory (IRT) models. This study considers and evaluates an alternative approach for computing (Formula presented.) values and other factors associated with collapsing tables of observed and expected numbers (OE tables), which can affect flagging items. Results suggest that collapsing OE tables requires careful consideration of a trade-off between power and empirical type I error rate. Concurrent collapsing of score categories would be preferred over separate collapsing for its procedural simplicity, minimal effect of choice of a minimum cell value on empirical type I error rates, and reasonable type I error rates even for the most sparse condition in the study. For separate collapsing, a smaller minimum cell value is recommended as OE tables possess more sparseness (e.g., longer test lengths and smaller sample sizes) if inflated type I error rates are more of a concern in detecting items for misfit based on the (Formula presented.) index. If it is more important to identify misfit items, the study results recommend using a larger minimum cell value forÂ collapsing. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['Orlando', 'Thissen', '2000', 'introduce', 'Formula', 'present', 'itemfit', 'index', 'test', 'goodnessoffit', 'dichotomous', 'item', 'response', 'theory', 'IRT', 'study', 'consider', 'evaluate', 'alternative', 'approach', 'compute', 'Formula', 'present', 'value', 'factor', 'associate', 'collapse', 'table', 'observe', 'expect', 'number', 'oe', 'table', 'affect', 'flagging', 'item', 'result', 'suggest', 'collapse', 'OE', 'table', 'require', 'careful', 'consideration', 'tradeoff', 'power', 'empirical', 'type', 'I', 'error', 'rate', 'Concurrent', 'collapse', 'score', 'category', 'prefer', 'separate', 'collapse', 'procedural', 'simplicity', 'minimal', 'effect', 'choice', 'minimum', 'cell', 'value', 'empirical', 'type', 'I', 'error', 'rate', 'reasonable', 'type', 'I', 'error', 'rate', 'sparse', 'condition', 'study', 'separate', 'collapse', 'small', 'minimum', 'cell', 'value', 'recommend', 'OE', 'table', 'possess', 'sparseness', 'eg', 'long', 'test', 'length', 'small', 'sample', 'size', 'inflated', 'type', 'I', 'error', 'rate', 'concern', 'detect', 'item', 'misfit', 'base', 'Formula', 'present', 'index', 'important', 'identify', 'misfit', 'item', 'study', 'result', 'recommend', 'large', 'minimum', 'cell', 'value', 'collapse', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['evaluation', 'factor', 'affect', 'performance', 's?’x2', 'ItemFit', 'Index']",Orlando Thissen 2000 introduce Formula present itemfit index test goodnessoffit dichotomous item response theory IRT study consider evaluate alternative approach compute Formula present value factor associate collapse table observe expect number oe table affect flagging item result suggest collapse OE table require careful consideration tradeoff power empirical type I error rate Concurrent collapse score category prefer separate collapse procedural simplicity minimal effect choice minimum cell value empirical type I error rate reasonable type I error rate sparse condition study separate collapse small minimum cell value recommend OE table possess sparseness eg long test length small sample size inflated type I error rate concern detect item misfit base Formula present index important identify misfit item study result recommend large minimum cell value collapse Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,evaluation factor affect performance s?’x2 ItemFit Index,0.025172633,0.024774877,0.024745712,0.900486301,0.024820477,0.017444047,0,0.032378512,0,0.103405454
Jin K.-Y.; Siu W.-L.; Huang X.,Exploring the Impact of Random Guessing in Distractor Analysis,2022,59,"Multiple-choice (MC) items are widely used in educational tests. Distractor analysis, an important procedure for checking the utility of response options within an MC item, can be readily implemented in the framework of item response theory (IRT). Although random guessing is a popular behavior of test-takers when answering MC items, none of the existing IRT models for distractor analysis have considered the influence of random guessing in this process. In this article, we propose a new IRT model to distinguish the influence of random guessing from response option functioning. A brief simulation study was conducted to examine the parameter recovery of the proposed model. To demonstrate its effectiveness, the new model was applied to the mathematics tests of the Hong Kong Diploma of Secondary Education Examination (HKDSE) from 2015 to 2019. The results of empirical analyses suggest that the complexity of item contents is a key factor in inducing students??random guessing. The implications and applications of the new model to other testing situations are also discussed. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Exploring the Impact of Random Guessing in Distractor Analysis,"Multiple-choice (MC) items are widely used in educational tests. Distractor analysis, an important procedure for checking the utility of response options within an MC item, can be readily implemented in the framework of item response theory (IRT). Although random guessing is a popular behavior of test-takers when answering MC items, none of the existing IRT models for distractor analysis have considered the influence of random guessing in this process. In this article, we propose a new IRT model to distinguish the influence of random guessing from response option functioning. A brief simulation study was conducted to examine the parameter recovery of the proposed model. To demonstrate its effectiveness, the new model was applied to the mathematics tests of the Hong Kong Diploma of Secondary Education Examination (HKDSE) from 2015 to 2019. The results of empirical analyses suggest that the complexity of item contents is a key factor in inducing students??random guessing. The implications and applications of the new model to other testing situations are also discussed. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['Multiplechoice', 'MC', 'item', 'widely', 'educational', 'test', 'Distractor', 'analysis', 'important', 'procedure', 'check', 'utility', 'response', 'option', 'MC', 'item', 'readily', 'implement', 'framework', 'item', 'response', 'theory', 'IRT', 'random', 'guessing', 'popular', 'behavior', 'testtaker', 'answer', 'MC', 'item', 'exist', 'IRT', 'distractor', 'analysis', 'consider', 'influence', 'random', 'guessing', 'process', 'article', 'propose', 'new', 'IRT', 'distinguish', 'influence', 'random', 'guessing', 'response', 'option', 'function', 'brief', 'simulation', 'study', 'conduct', 'examine', 'parameter', 'recovery', 'propose', 'demonstrate', 'effectiveness', 'new', 'apply', 'mathematics', 'test', 'Hong', 'Kong', 'Diploma', 'Secondary', 'Examination', 'HKDSE', '2015', '2019', 'result', 'empirical', 'analysis', 'suggest', 'complexity', 'item', 'content', 'key', 'factor', 'induce', 'student', '??, 'random', 'guess', 'implication', 'application', 'new', 'testing', 'situation', 'discuss', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['explore', 'Impact', 'Random', 'Guessing', 'Distractor', 'Analysis']",Multiplechoice MC item widely educational test Distractor analysis important procedure check utility response option MC item readily implement framework item response theory IRT random guessing popular behavior testtaker answer MC item exist IRT distractor analysis consider influence random guessing process article propose new IRT distinguish influence random guessing response option function brief simulation study conduct examine parameter recovery propose demonstrate effectiveness new apply mathematics test Hong Kong Diploma Secondary Examination HKDSE 2015 2019 result empirical analysis suggest complexity item content key factor induce student ??random guess implication application new testing situation discuss Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,explore Impact Random Guessing Distractor Analysis,0.023727073,0.023327823,0.023272053,0.906281808,0.023391242,0.077861783,0.023917457,0.021410497,0,0.012233824
DeCarlo L.T.,On Joining a Signal Detection Choice Model with Response Time Models,2021,58,"In a signal detection theory (SDT) approach to multiple choice exams, examinees are viewed as choosing, for each item, the alternative that is perceived as being the most plausible, with perceived plausibility depending in part on whether or not an item is known. The SDT model is a process model and provides measures of item difficulty, item discrimination, and the relative plausibility of each alternative. It is shown how to incorporate information from response times into the model, which has potential benefits for estimation and also offers a way to study underlying processes. The SDT model is joined with a lognormal response time (RT) model in a manner similar to that used in hierarchical models. In addition, a mixture extension of the RT model is joined in a novel way with the SDT choice model, using the idea that the probability of ?œknowing??in the SDT model might be related to the probability of working in one of two speed states in the mixture RT model. A semiparametric version of the mixture RT model is also used to assess robustness. The fused SDT/RT models are examined with science items from the 2015 Program for International Student Assessment. Â© 2021 by the National Council on Measurement in Education",On Joining a Signal Detection Choice Model with Response Time Models,"In a signal detection theory (SDT) approach to multiple choice exams, examinees are viewed as choosing, for each item, the alternative that is perceived as being the most plausible, with perceived plausibility depending in part on whether or not an item is known. The SDT model is a process model and provides measures of item difficulty, item discrimination, and the relative plausibility of each alternative. It is shown how to incorporate information from response times into the model, which has potential benefits for estimation and also offers a way to study underlying processes. The SDT model is joined with a lognormal response time (RT) model in a manner similar to that used in hierarchical models. In addition, a mixture extension of the RT model is joined in a novel way with the SDT choice model, using the idea that the probability of ?œknowing??in the SDT model might be related to the probability of working in one of two speed states in the mixture RT model. A semiparametric version of the mixture RT model is also used to assess robustness. The fused SDT/RT models are examined with science items from the 2015 Program for International Student Assessment. Â© 2021 by the National Council on Measurement in Education","['signal', 'detection', 'theory', 'SDT', 'approach', 'multiple', 'choice', 'exam', 'examinee', 'view', 'choose', 'item', 'alternative', 'perceive', 'plausible', 'perceive', 'plausibility', 'depend', 'item', 'know', 'SDT', 'process', 'provide', 'measure', 'item', 'difficulty', 'item', 'discrimination', 'relative', 'plausibility', 'alternative', 'incorporate', 'information', 'response', 'time', 'potential', 'benefit', 'estimation', 'offer', 'way', 'study', 'underlie', 'process', 'SDT', 'join', 'lognormal', 'response', 'time', 'RT', 'manner', 'similar', 'hierarchical', 'addition', 'mixture', 'extension', 'RT', 'join', 'novel', 'way', 'SDT', 'choice', 'idea', 'probability', '""', 'know', '""', 'SDT', 'relate', 'probability', 'work', 'speed', 'state', 'mixture', 'RT', 'semiparametric', 'version', 'mixture', 'RT', 'assess', 'robustness', 'fused', 'sdtrt', 'examine', 'science', 'item', '2015', 'Program', 'International', 'Student', 'Assessment', 'Â©', '2021', 'National', 'Council']","['join', 'Signal', 'Detection', 'Choice', 'Response', 'Time', 'Models']","signal detection theory SDT approach multiple choice exam examinee view choose item alternative perceive plausible perceive plausibility depend item know SDT process provide measure item difficulty item discrimination relative plausibility alternative incorporate information response time potential benefit estimation offer way study underlie process SDT join lognormal response time RT manner similar hierarchical addition mixture extension RT join novel way SDT choice idea probability "" know "" SDT relate probability work speed state mixture RT semiparametric version mixture RT assess robustness fused sdtrt examine science item 2015 Program International Student Assessment Â© 2021 National Council",join Signal Detection Choice Response Time Models,0.026760297,0.026683008,0.026316402,0.893719774,0.02652052,0.083978327,0,0,0.004501908,0
van der Linden W.J.; Belov D.I.,A Statistical Test for the Detection of Item Compromise Combining Responses and Response Times,2023,60,"A test of item compromise is presented which combines the test takers' responses and response times (RTs) into a statistic defined as the number of correct responses on the item for test takers with RTs flagged as suspicious. The test has null and alternative distributions belonging to the well-known family of compound binomial distributions, is simple to calculate, and has results that are easy to interpret. It also demonstrated nearly perfect power for the detection of compromise with no more than 10 test takers with preknowledge of the more difficult and discriminating items in a set of empirical examples. For the easier and less discriminating items, the presence of some 20 test takers with preknowledge still sufficed. A test based on the reverse statistic of the total time by test takers with responses flagged as suspicious may seem a natural alternative but misses the property of a monotone likelihood ratio necessary to decide between a test that should be left or rightÂ sided. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",A Statistical Test for the Detection of Item Compromise Combining Responses and Response Times,"A test of item compromise is presented which combines the test takers' responses and response times (RTs) into a statistic defined as the number of correct responses on the item for test takers with RTs flagged as suspicious. The test has null and alternative distributions belonging to the well-known family of compound binomial distributions, is simple to calculate, and has results that are easy to interpret. It also demonstrated nearly perfect power for the detection of compromise with no more than 10 test takers with preknowledge of the more difficult and discriminating items in a set of empirical examples. For the easier and less discriminating items, the presence of some 20 test takers with preknowledge still sufficed. A test based on the reverse statistic of the total time by test takers with responses flagged as suspicious may seem a natural alternative but misses the property of a monotone likelihood ratio necessary to decide between a test that should be left or rightÂ sided. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['test', 'item', 'compromise', 'present', 'combine', 'test', 'taker', 'response', 'response', 'time', 'rt', 'statistic', 'define', 'number', 'correct', 'response', 'item', 'test', 'taker', 'rt', 'flag', 'suspicious', 'test', 'null', 'alternative', 'distribution', 'belong', 'wellknown', 'family', 'compound', 'binomial', 'distribution', 'simple', 'calculate', 'result', 'easy', 'interpret', 'demonstrate', 'nearly', 'perfect', 'power', 'detection', 'compromise', '10', 'test', 'taker', 'preknowledge', 'difficult', 'discriminate', 'item', 'set', 'empirical', 'example', 'easy', 'discriminating', 'item', 'presence', '20', 'test', 'taker', 'preknowledge', 'suffice', 'test', 'base', 'reverse', 'statistic', 'total', 'time', 'test', 'taker', 'response', 'flag', 'suspicious', 'natural', 'alternative', 'miss', 'property', 'monotone', 'likelihood', 'ratio', 'necessary', 'decide', 'test', 'leave', 'right', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['statistical', 'Test', 'Detection', 'Item', 'Compromise', 'Combining', 'Responses', 'Response', 'Times']",test item compromise present combine test taker response response time rt statistic define number correct response item test taker rt flag suspicious test null alternative distribution belong wellknown family compound binomial distribution simple calculate result easy interpret demonstrate nearly perfect power detection compromise 10 test taker preknowledge difficult discriminate item set empirical example easy discriminating item presence 20 test taker preknowledge suffice test base reverse statistic total time test taker response flag suspicious natural alternative miss property monotone likelihood ratio necessary decide test leave right Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,statistical Test Detection Item Compromise Combining Responses Response Times,0.026265438,0.025403896,0.896450702,0.026292936,0.025587027,0.08066162,0.028696977,0.004342162,0,0.018280926
Kwon T.Y.; Huggins-Manley A.C.; Templin J.; Zheng M.,Modeling Hierarchical Attribute Structures in Diagnostic Classification Models with Multiple Attempts,2024,61,"In classroom assessments, examinees can often answer test items multiple times, resulting in sequential multiple-attempt data. Sequential diagnostic classification models (DCMs) have been developed for such data. As student learning processes may be aligned with a hierarchy of measured traits, this study aimed to develop a sequential hierarchical DCM (sequential HDCM), which combines a sequential DCM with the HDCM, and investigate classification accuracy of the model in the presence of hierarchies when multiple attempts are allowed in dynamic assessment. We investigated the model's impact on classification accuracy when hierarchical structures are correctly specified, misspecified, or overspecified. The results indicate that (1) a sequential HDCM accurately classified students as masters and nonmasters when the data had a hierarchical structure; (2) a sequential HDCM produced similar or slightly higher classification accuracy than nonhierarchical sequential LCDM when the data had hierarchical structures; and (3) the misspecification of the hierarchical structure of the data resulted in lower classification accuracy when the misspecified model had fewer attribute profiles than the true model. We discuss limitations and make recommendations on using the proposed model in practice. This study provides practitioners with information about the possibilities for psychometric modeling of dynamic classroom assessment data. Â© 2024 by the National Council on Measurement in Education.",Modeling Hierarchical Attribute Structures in Diagnostic Classification Models with Multiple Attempts,"In classroom assessments, examinees can often answer test items multiple times, resulting in sequential multiple-attempt data. Sequential diagnostic classification models (DCMs) have been developed for such data. As student learning processes may be aligned with a hierarchy of measured traits, this study aimed to develop a sequential hierarchical DCM (sequential HDCM), which combines a sequential DCM with the HDCM, and investigate classification accuracy of the model in the presence of hierarchies when multiple attempts are allowed in dynamic assessment. We investigated the model's impact on classification accuracy when hierarchical structures are correctly specified, misspecified, or overspecified. The results indicate that (1) a sequential HDCM accurately classified students as masters and nonmasters when the data had a hierarchical structure; (2) a sequential HDCM produced similar or slightly higher classification accuracy than nonhierarchical sequential LCDM when the data had hierarchical structures; and (3) the misspecification of the hierarchical structure of the data resulted in lower classification accuracy when the misspecified model had fewer attribute profiles than the true model. We discuss limitations and make recommendations on using the proposed model in practice. This study provides practitioners with information about the possibilities for psychometric modeling of dynamic classroom assessment data. Â© 2024 by the National Council on Measurement in Education.","['classroom', 'assessment', 'examinee', 'answer', 'test', 'item', 'multiple', 'time', 'result', 'sequential', 'multipleattempt', 'datum', 'sequential', 'diagnostic', 'classification', 'dcm', 'develop', 'datum', 'student', 'learning', 'process', 'align', 'hierarchy', 'measure', 'trait', 'study', 'aim', 'develop', 'sequential', 'hierarchical', 'dcm', 'sequential', 'hdcm', 'combine', 'sequential', 'dcm', 'hdcm', 'investigate', 'classification', 'accuracy', 'presence', 'hierarchy', 'multiple', 'attempt', 'allow', 'dynamic', 'assessment', 'investigate', 'impact', 'classification', 'accuracy', 'hierarchical', 'structure', 'correctly', 'specify', 'misspecifie', 'overspecifie', 'result', 'indicate', '1', 'sequential', 'hdcm', 'accurately', 'classify', 'student', 'master', 'nonmaster', 'datum', 'hierarchical', 'structure', '2', 'sequential', 'hdcm', 'produce', 'similar', 'slightly', 'high', 'classification', 'accuracy', 'nonhierarchical', 'sequential', 'LCDM', 'datum', 'hierarchical', 'structure', '3', 'misspecification', 'hierarchical', 'structure', 'datum', 'result', 'low', 'classification', 'accuracy', 'misspecifie', 'attribute', 'profile', 'true', 'discuss', 'limitation', 'recommendation', 'propose', 'practice', 'study', 'provide', 'practitioner', 'information', 'possibility', 'psychometric', 'modeling', 'dynamic', 'classroom', 'assessment', 'datum', 'Â©', '2024', 'National', 'Council']","['Hierarchical', 'Attribute', 'Structures', 'Diagnostic', 'Classification', 'Models', 'multiple', 'attempt']",classroom assessment examinee answer test item multiple time result sequential multipleattempt datum sequential diagnostic classification dcm develop datum student learning process align hierarchy measure trait study aim develop sequential hierarchical dcm sequential hdcm combine sequential dcm hdcm investigate classification accuracy presence hierarchy multiple attempt allow dynamic assessment investigate impact classification accuracy hierarchical structure correctly specify misspecifie overspecifie result indicate 1 sequential hdcm accurately classify student master nonmaster datum hierarchical structure 2 sequential hdcm produce similar slightly high classification accuracy nonhierarchical sequential LCDM datum hierarchical structure 3 misspecification hierarchical structure datum result low classification accuracy misspecifie attribute profile true discuss limitation recommendation propose practice study provide practitioner information possibility psychometric modeling dynamic classroom assessment datum Â© 2024 National Council,Hierarchical Attribute Structures Diagnostic Classification Models multiple attempt,0.030438307,0.882126577,0.028821421,0.029638722,0.028974973,0.02039584,0.063757612,0,0.014197206,0
Grochowalski J.H.; Hendrickson A.,Detecting Group Collaboration Using Multiple Correspondence Analysis,2023,60,"Test takers wishing to gain an unfair advantage often share answers with other test takers, either sharing all answers (a full key) or some (a partial key). Detecting key sharing during a tight testing window requires an efficient, easily interpretable, and rich form of analysis that is descriptive and inferential. We introduce a detection method based on multiple correspondence analysis (MCA) that identifies test takers with unusual response similarities. The method simultaneously detects multiple shared keys (partial or full), plots results, and is computationally efficient as it requires only matrix operations. We describe the method, evaluate its detection accuracy under various simulation conditions, and demonstrate the procedure on a real data set with known test-taking misbehavior. The simulation results showed that the MCA method had reasonably high power under realistic conditions and maintained the nominal false-positive level, except when the group size was very large or partial shared keys had more than 50% of the items. The real data analysis illustrated visual detection procedures and inference about the item responses possibly shared in the key, which was likely shared among 91 test takers, many of whom were confirmed by nonstatistical investigation to have engaged in test-taking misconduct. Â© 2023 by the National Council on Measurement in Education.",Detecting Group Collaboration Using Multiple Correspondence Analysis,"Test takers wishing to gain an unfair advantage often share answers with other test takers, either sharing all answers (a full key) or some (a partial key). Detecting key sharing during a tight testing window requires an efficient, easily interpretable, and rich form of analysis that is descriptive and inferential. We introduce a detection method based on multiple correspondence analysis (MCA) that identifies test takers with unusual response similarities. The method simultaneously detects multiple shared keys (partial or full), plots results, and is computationally efficient as it requires only matrix operations. We describe the method, evaluate its detection accuracy under various simulation conditions, and demonstrate the procedure on a real data set with known test-taking misbehavior. The simulation results showed that the MCA method had reasonably high power under realistic conditions and maintained the nominal false-positive level, except when the group size was very large or partial shared keys had more than 50% of the items. The real data analysis illustrated visual detection procedures and inference about the item responses possibly shared in the key, which was likely shared among 91 test takers, many of whom were confirmed by nonstatistical investigation to have engaged in test-taking misconduct. Â© 2023 by the National Council on Measurement in Education.","['test', 'taker', 'wish', 'gain', 'unfair', 'advantage', 'share', 'answer', 'test', 'taker', 'share', 'answer', 'key', 'partial', 'key', 'detect', 'key', 'sharing', 'tight', 'testing', 'window', 'require', 'efficient', 'easily', 'interpretable', 'rich', 'form', 'analysis', 'descriptive', 'inferential', 'introduce', 'detection', 'method', 'base', 'multiple', 'correspondence', 'analysis', 'MCA', 'identify', 'test', 'taker', 'unusual', 'response', 'similaritie', 'method', 'simultaneously', 'detect', 'multiple', 'share', 'key', 'partial', 'plot', 'result', 'computationally', 'efficient', 'require', 'matrix', 'operation', 'describe', 'method', 'evaluate', 'detection', 'accuracy', 'simulation', 'condition', 'demonstrate', 'procedure', 'real', 'datum', 'set', 'know', 'testtaking', 'misbehavior', 'simulation', 'result', 'MCA', 'method', 'reasonably', 'high', 'power', 'realistic', 'condition', 'maintain', 'nominal', 'falsepositive', 'level', 'group', 'size', 'large', 'partial', 'share', 'key', '50', 'item', 'real', 'data', 'analysis', 'illustrate', 'visual', 'detection', 'procedure', 'inference', 'item', 'response', 'possibly', 'share', 'key', 'likely', 'share', '91', 'test', 'taker', 'confirm', 'nonstatistical', 'investigation', 'engage', 'testtake', 'misconduct', 'Â©', '2023', 'National', 'Council']","['Detecting', 'Group', 'Collaboration', 'multiple', 'Correspondence', 'Analysis']",test taker wish gain unfair advantage share answer test taker share answer key partial key detect key sharing tight testing window require efficient easily interpretable rich form analysis descriptive inferential introduce detection method base multiple correspondence analysis MCA identify test taker unusual response similaritie method simultaneously detect multiple share key partial plot result computationally efficient require matrix operation describe method evaluate detection accuracy simulation condition demonstrate procedure real datum set know testtaking misbehavior simulation result MCA method reasonably high power realistic condition maintain nominal falsepositive level group size large partial share key 50 item real data analysis illustrate visual detection procedure inference item response possibly share key likely share 91 test taker confirm nonstatistical investigation engage testtake misconduct Â© 2023 National Council,Detecting Group Collaboration multiple Correspondence Analysis,0.89922439,0.025060803,0.025234257,0.025233308,0.025247242,0.022806866,0.076378153,0.00236513,0,0.012672899
Ferrara S.; Qunbar S.,Validity Arguments for AI-Based Automated Scores: Essay Scoring as an Illustration,2022,59,"In this article, we argue that automated scoring engines should be transparent and construct relevant?”that is, as much as is currently feasible. Many current automated scoring engines cannot achieve high degrees of scoring accuracy without allowing in some features that may not be easily explained and understood and may not be obviously and directly relevant to the target assessment construct. We address the current limitations on evidence and validity arguments for scores from automated scoring engines from the points of view of the Standards for Educational and Psychological Testing (i.e., construct relevance, construct representation, and fairness) and emerging principles in Artificial Intelligence (e.g., explainable AI, an examinee's right to explanations, and principled AI). We illustrate these concepts and arguments for automated essay scores. Â© 2022 by the National Council on Measurement in Education.",Validity Arguments for AI-Based Automated Scores: Essay Scoring as an Illustration,"In this article, we argue that automated scoring engines should be transparent and construct relevant?”that is, as much as is currently feasible. Many current automated scoring engines cannot achieve high degrees of scoring accuracy without allowing in some features that may not be easily explained and understood and may not be obviously and directly relevant to the target assessment construct. We address the current limitations on evidence and validity arguments for scores from automated scoring engines from the points of view of the Standards for Educational and Psychological Testing (i.e., construct relevance, construct representation, and fairness) and emerging principles in Artificial Intelligence (e.g., explainable AI, an examinee's right to explanations, and principled AI). We illustrate these concepts and arguments for automated essay scores. Â© 2022 by the National Council on Measurement in Education.","['article', 'argue', 'automate', 'scoring', 'engine', 'transparent', 'construct', 'relevant', '??, 'currently', 'feasible', 'current', 'automate', 'scoring', 'engine', 'achieve', 'high', 'degree', 'scoring', 'accuracy', 'allow', 'feature', 'easily', 'explain', 'understand', 'obviously', 'directly', 'relevant', 'target', 'assessment', 'construct', 'address', 'current', 'limitation', 'evidence', 'validity', 'argument', 'score', 'automate', 'scoring', 'engine', 'point', 'view', 'Standards', 'Educational', 'Psychological', 'Testing', 'ie', 'construct', 'relevance', 'construct', 'representation', 'fairness', 'emerge', 'principle', 'Artificial', 'Intelligence', 'eg', 'explainable', 'AI', 'examinee', 'right', 'explanation', 'principle', 'AI', 'illustrate', 'concept', 'argument', 'automate', 'essay', 'score', 'Â©', '2022', 'National', 'Council']","['validity', 'argument', 'AIBased', 'Automated', 'Scores', 'Essay', 'Scoring', 'illustration']",article argue automate scoring engine transparent construct relevant ??currently feasible current automate scoring engine achieve high degree scoring accuracy allow feature easily explain understand obviously directly relevant target assessment construct address current limitation evidence validity argument score automate scoring engine point view Standards Educational Psychological Testing ie construct relevance construct representation fairness emerge principle Artificial Intelligence eg explainable AI examinee right explanation principle AI illustrate concept argument automate essay score Â© 2022 National Council,validity argument AIBased Automated Scores Essay Scoring illustration,0.027276592,0.02724519,0.027456738,0.89069651,0.027324971,0,0,0,0.184428033,0
Sinharay S.; Johnson M.S.,Computation and Accuracy Evaluation of Comparable Scores on Culturally Responsive Assessments,2024,61,"Culturally responsive assessments have been proposed as potential tools to ensure equity and fairness for examinees from all backgrounds including those from traditionally underserved or minoritized groups. However, these assessments are relatively new and, with few exceptions, are yet to be implemented in large scale. Consequently, there is a lack of guidance on how one can compute comparable scores on various versions of these assessments. In this paper, the multigroup multidimensional Rasch model is repurposed for modeling data originating from various versions of a culturally responsive assessment and for analyzing such data to compute comparable scores. Two simulation studies are performed to evaluate the performance of the model for data simulated from hypothetical culturally responsive assessments and to find the conditions under which the computed scores are accurate. Recommendations are made for measurement practitioners interested in culturally responsive assessments. Â© 2023 by the National Council on Measurement in Education.",Computation and Accuracy Evaluation of Comparable Scores on Culturally Responsive Assessments,"Culturally responsive assessments have been proposed as potential tools to ensure equity and fairness for examinees from all backgrounds including those from traditionally underserved or minoritized groups. However, these assessments are relatively new and, with few exceptions, are yet to be implemented in large scale. Consequently, there is a lack of guidance on how one can compute comparable scores on various versions of these assessments. In this paper, the multigroup multidimensional Rasch model is repurposed for modeling data originating from various versions of a culturally responsive assessment and for analyzing such data to compute comparable scores. Two simulation studies are performed to evaluate the performance of the model for data simulated from hypothetical culturally responsive assessments and to find the conditions under which the computed scores are accurate. Recommendations are made for measurement practitioners interested in culturally responsive assessments. Â© 2023 by the National Council on Measurement in Education.","['culturally', 'responsive', 'assessment', 'propose', 'potential', 'tool', 'ensure', 'equity', 'fairness', 'examinee', 'background', 'include', 'traditionally', 'underserved', 'minoritized', 'group', 'assessment', 'relatively', 'new', 'exception', 'implement', 'large', 'scale', 'consequently', 'lack', 'guidance', 'compute', 'comparable', 'score', 'version', 'assessment', 'paper', 'multigroup', 'multidimensional', 'Rasch', 'repurpose', 'datum', 'originate', 'version', 'culturally', 'responsive', 'assessment', 'analyze', 'datum', 'compute', 'comparable', 'score', 'simulation', 'study', 'perform', 'evaluate', 'performance', 'datum', 'simulate', 'hypothetical', 'culturally', 'responsive', 'assessment', 'find', 'condition', 'compute', 'score', 'accurate', 'Recommendations', 'practitioner', 'interested', 'culturally', 'responsive', 'assessment', 'Â©', '2023', 'National', 'Council']","['computation', 'Accuracy', 'Evaluation', 'Comparable', 'Scores', 'culturally', 'responsive', 'assessment']",culturally responsive assessment propose potential tool ensure equity fairness examinee background include traditionally underserved minoritized group assessment relatively new exception implement large scale consequently lack guidance compute comparable score version assessment paper multigroup multidimensional Rasch repurpose datum originate version culturally responsive assessment analyze datum compute comparable score simulation study perform evaluate performance datum simulate hypothetical culturally responsive assessment find condition compute score accurate Recommendations practitioner interested culturally responsive assessment Â© 2023 National Council,computation Accuracy Evaluation Comparable Scores culturally responsive assessment,0.032422884,0.874928495,0.030539022,0.031394926,0.030714673,0.000928961,0.030283759,0.003197089,0.068985442,0.005137942
Guo J.; Xu X.; Xin T.,A Note on Latent Traits Estimates under IRT Models with Missingness,2023,60,"Missingness due to not-reached items and omitted items has received much attention in the recent psychometric literature. Such missingness, if not handled properly, would lead to biased parameter estimation, as well as inaccurate inference of examinees, and further erode the validity of the test. This paper reviews some commonly used IRT based models allowing missingness, followed by three popular examinee scoring methods, including maximum likelihood estimation, maximum a posteriori, and expected a posteriori. Simulation studies were conducted to compare these examinee scoring methods across these commonly used models in the presence of missingness. Results showed that all the methods could infer examinees' ability accurately when the missingness is ignorable. If the missingness is nonignorable, incorporating those missing responses would improve the precision in estimating abilities for examinees with missingness, especially when the test length is short. In terms of examinee scoring methods, expected a posteriori method performed better for evaluating latent traits under models allowing missingness. An empirical study based on the PISA 2015 Science Test was furtherÂ performed. Â© 2023 by the National Council on Measurement in Education.",A Note on Latent Traits Estimates under IRT Models with Missingness,"Missingness due to not-reached items and omitted items has received much attention in the recent psychometric literature. Such missingness, if not handled properly, would lead to biased parameter estimation, as well as inaccurate inference of examinees, and further erode the validity of the test. This paper reviews some commonly used IRT based models allowing missingness, followed by three popular examinee scoring methods, including maximum likelihood estimation, maximum a posteriori, and expected a posteriori. Simulation studies were conducted to compare these examinee scoring methods across these commonly used models in the presence of missingness. Results showed that all the methods could infer examinees' ability accurately when the missingness is ignorable. If the missingness is nonignorable, incorporating those missing responses would improve the precision in estimating abilities for examinees with missingness, especially when the test length is short. In terms of examinee scoring methods, expected a posteriori method performed better for evaluating latent traits under models allowing missingness. An empirical study based on the PISA 2015 Science Test was furtherÂ performed. Â© 2023 by the National Council on Measurement in Education.","['missingness', 'notreached', 'item', 'omit', 'item', 'receive', 'attention', 'recent', 'psychometric', 'literature', 'missingness', 'handle', 'properly', 'lead', 'biased', 'parameter', 'estimation', 'inaccurate', 'inference', 'examinee', 'far', 'erode', 'validity', 'test', 'paper', 'review', 'commonly', 'IRT', 'base', 'allow', 'missingness', 'follow', 'popular', 'examinee', 'scoring', 'method', 'include', 'maximum', 'likelihood', 'estimation', 'maximum', 'posteriori', 'expect', 'posteriori', 'Simulation', 'study', 'conduct', 'compare', 'examinee', 'scoring', 'method', 'commonly', 'presence', 'missingness', 'result', 'method', 'infer', 'examine', 'ability', 'accurately', 'missingness', 'ignorable', 'missingness', 'nonignorable', 'incorporate', 'miss', 'response', 'improve', 'precision', 'estimate', 'ability', 'examinee', 'missingness', 'especially', 'test', 'length', 'short', 'term', 'examinee', 'scoring', 'method', 'expect', 'posteriori', 'method', 'perform', 'evaluate', 'latent', 'trait', 'allow', 'missingness', 'empirical', 'study', 'base', 'PISA', '2015', 'Science', 'Test', 'far', 'perform', 'Â©', '2023', 'National', 'Council']","['note', 'Latent', 'Traits', 'Estimates', 'IRT', 'Models', 'missingness']",missingness notreached item omit item receive attention recent psychometric literature missingness handle properly lead biased parameter estimation inaccurate inference examinee far erode validity test paper review commonly IRT base allow missingness follow popular examinee scoring method include maximum likelihood estimation maximum posteriori expect posteriori Simulation study conduct compare examinee scoring method commonly presence missingness result method infer examine ability accurately missingness ignorable missingness nonignorable incorporate miss response improve precision estimate ability examinee missingness especially test length short term examinee scoring method expect posteriori method perform evaluate latent trait allow missingness empirical study base PISA 2015 Science Test far perform Â© 2023 National Council,note Latent Traits Estimates IRT Models missingness,0.031915575,0.029781315,0.876686619,0.031031156,0.030585335,0.009815425,0.070934945,0,0.034053199,0.005150931
Johnson M.S.; Liu X.; McCaffrey D.F.,Psychometric Methods to Evaluate Measurement and Algorithmic Bias in Automated Scoring,2022,59,"With the increasing use of automated scores in operational testing settings comes the need to understand the ways in which they can yield biased and unfair results. In this paper, we provide a brief survey of some of the ways in which the predictive methods used in automated scoring can lead to biased, and thus unfair automated scores. After providing definitions of fairness from machine learning and a psychometric framework to study them, we demonstrate how modeling decisions, like omitting variables, using proxy measures or confounded variables, and even the optimization criterion in estimation can lead to biased and unfair automated scores. We then introduce two simple methods for evaluating bias, evaluate their statistical properties through simulation, and apply to an item from a large-scale reading assessment. Â© 2022 by the National Council on Measurement in Education.",Psychometric Methods to Evaluate Measurement and Algorithmic Bias in Automated Scoring,"With the increasing use of automated scores in operational testing settings comes the need to understand the ways in which they can yield biased and unfair results. In this paper, we provide a brief survey of some of the ways in which the predictive methods used in automated scoring can lead to biased, and thus unfair automated scores. After providing definitions of fairness from machine learning and a psychometric framework to study them, we demonstrate how modeling decisions, like omitting variables, using proxy measures or confounded variables, and even the optimization criterion in estimation can lead to biased and unfair automated scores. We then introduce two simple methods for evaluating bias, evaluate their statistical properties through simulation, and apply to an item from a large-scale reading assessment. Â© 2022 by the National Council on Measurement in Education.","['increase', 'automate', 'score', 'operational', 'testing', 'setting', 'come', 'need', 'understand', 'way', 'yield', 'biased', 'unfair', 'result', 'paper', 'provide', 'brief', 'survey', 'way', 'predictive', 'method', 'automate', 'scoring', 'lead', 'biased', 'unfair', 'automate', 'score', 'provide', 'definition', 'fairness', 'machine', 'learning', 'psychometric', 'framework', 'study', 'demonstrate', 'modeling', 'decision', 'like', 'omit', 'variable', 'proxy', 'measure', 'confound', 'variable', 'optimization', 'criterion', 'estimation', 'lead', 'biased', 'unfair', 'automate', 'score', 'introduce', 'simple', 'method', 'evaluate', 'bias', 'evaluate', 'statistical', 'property', 'simulation', 'apply', 'item', 'largescale', 'reading', 'assessment', 'Â©', '2022', 'National', 'Council']","['Psychometric', 'Methods', 'evaluate', 'Algorithmic', 'Bias', 'Automated', 'Scoring']",increase automate score operational testing setting come need understand way yield biased unfair result paper provide brief survey way predictive method automate scoring lead biased unfair automate score provide definition fairness machine learning psychometric framework study demonstrate modeling decision like omit variable proxy measure confound variable optimization criterion estimation lead biased unfair automate score introduce simple method evaluate bias evaluate statistical property simulation apply item largescale reading assessment Â© 2022 National Council,Psychometric Methods evaluate Algorithmic Bias Automated Scoring,0.028401112,0.028051449,0.886411875,0.028812173,0.028323391,0,0.031176717,0,0.119804848,0.017545471
Qiao X.; Jiao H.,Explanatory Cognitive Diagnostic Modeling Incorporating Response Times,2021,58,"This study proposes explanatory cognitive diagnostic model (CDM) jointly incorporating responses and response times (RTs) with the inclusion of item covariates related to both item responses and RTs. The joint modeling of item responses and RTs intends to provide more information for cognitive diagnosis while item covariates can be used to predict item parameters when item calibration is not feasible in diagnostic assessments or item parameter estimation errors could be too large due to small sample sizes for calibration. In addition, the inclusion of the item covariates allows the evaluation of cognitive theories underlying the test design in item development. Model parameter estimation is explored using the Bayesian Markov chain Monte Carlo (MCMC) method. A Monte Carlo simulation study is conducted to examine the parameter recovery of the proposed model under different simulated conditions in comparison to alternative competing models. Further, the application of the proposed model is illustrated using the Programme for International Student Assessment (PISA) 2012 problem-solving items modeling both item response and RT data. The study results indicate that model parameters can be well recovered using the MCMC algorithm and the explanatory CDM jointly incorporating item responses and RTs with item covariates holds promising applications in digital-based diagnostic assessments. Â© 2022 by the National Council on Measurement in Education",Explanatory Cognitive Diagnostic Modeling Incorporating Response Times,"This study proposes explanatory cognitive diagnostic model (CDM) jointly incorporating responses and response times (RTs) with the inclusion of item covariates related to both item responses and RTs. The joint modeling of item responses and RTs intends to provide more information for cognitive diagnosis while item covariates can be used to predict item parameters when item calibration is not feasible in diagnostic assessments or item parameter estimation errors could be too large due to small sample sizes for calibration. In addition, the inclusion of the item covariates allows the evaluation of cognitive theories underlying the test design in item development. Model parameter estimation is explored using the Bayesian Markov chain Monte Carlo (MCMC) method. A Monte Carlo simulation study is conducted to examine the parameter recovery of the proposed model under different simulated conditions in comparison to alternative competing models. Further, the application of the proposed model is illustrated using the Programme for International Student Assessment (PISA) 2012 problem-solving items modeling both item response and RT data. The study results indicate that model parameters can be well recovered using the MCMC algorithm and the explanatory CDM jointly incorporating item responses and RTs with item covariates holds promising applications in digital-based diagnostic assessments. Â© 2022 by the National Council on Measurement in Education","['study', 'propose', 'explanatory', 'cognitive', 'diagnostic', 'CDM', 'jointly', 'incorporate', 'response', 'response', 'time', 'rt', 'inclusion', 'item', 'covariate', 'relate', 'item', 'response', 'rt', 'joint', 'modeling', 'item', 'response', 'rt', 'intend', 'provide', 'information', 'cognitive', 'diagnosis', 'item', 'covariate', 'predict', 'item', 'parameter', 'item', 'calibration', 'feasible', 'diagnostic', 'assessment', 'item', 'parameter', 'estimation', 'error', 'large', 'small', 'sample', 'size', 'calibration', 'addition', 'inclusion', 'item', 'covariate', 'allow', 'evaluation', 'cognitive', 'theory', 'underlie', 'test', 'design', 'item', 'development', 'parameter', 'estimation', 'explore', 'Bayesian', 'Markov', 'chain', 'Monte', 'Carlo', 'MCMC', 'method', 'A', 'Monte', 'Carlo', 'simulation', 'study', 'conduct', 'examine', 'parameter', 'recovery', 'propose', 'different', 'simulated', 'condition', 'comparison', 'alternative', 'compete', 'far', 'application', 'propose', 'illustrate', 'Programme', 'International', 'Student', 'Assessment', 'PISA', '2012', 'problemsolving', 'item', 'item', 'response', 'RT', 'datum', 'study', 'result', 'indicate', 'parameter', 'recover', 'MCMC', 'algorithm', 'explanatory', 'CDM', 'jointly', 'incorporate', 'item', 'response', 'rt', 'item', 'covariate', 'hold', 'promise', 'application', 'digitalbase', 'diagnostic', 'assessment', 'Â©', '2022', 'National', 'Council']","['explanatory', 'Cognitive', 'Diagnostic', 'Modeling', 'incorporate', 'Response', 'Times']",study propose explanatory cognitive diagnostic CDM jointly incorporate response response time rt inclusion item covariate relate item response rt joint modeling item response rt intend provide information cognitive diagnosis item covariate predict item parameter item calibration feasible diagnostic assessment item parameter estimation error large small sample size calibration addition inclusion item covariate allow evaluation cognitive theory underlie test design item development parameter estimation explore Bayesian Markov chain Monte Carlo MCMC method A Monte Carlo simulation study conduct examine parameter recovery propose different simulated condition comparison alternative compete far application propose illustrate Programme International Student Assessment PISA 2012 problemsolving item item response RT datum study result indicate parameter recover MCMC algorithm explanatory CDM jointly incorporate item response rt item covariate hold promise application digitalbase diagnostic assessment Â© 2022 National Council,explanatory Cognitive Diagnostic Modeling incorporate Response Times,0.900645425,0.024732824,0.024695982,0.025088993,0.024836777,0.097425097,0.049783137,0,0,0.001292861
Lim H.; Davey T.; Wells C.S.,A Recursion-Based Analytical Approach to Evaluate the Performance of MST,2021,58,"This study proposed a recursion-based analytical approach to assess measurement precision of ability estimation and classification accuracy in multistage adaptive tests (MSTs). A simulation study was conducted to compare the proposed recursion-based analytical method with an analytical method proposed by Park, Kim, Chung, and Dodd and with the more commonly used Monte Carlo (MC) simulation approaches. The results from the simulation study indicate that the recursion-based analytical method produced more stable measurement properties in MST than the MC-based simulation method. It also produced more credible measurement performance than the analytical approach of Park, Kim, Chung, and Dodd. We expect that the recursion-based analytical method would be especially efficient in instances when multiple MST designs need to be compared to determine which one has better measurement performance. Â© 2020 by the National Council on Measurement in Education",A Recursion-Based Analytical Approach to Evaluate the Performance of MST,"This study proposed a recursion-based analytical approach to assess measurement precision of ability estimation and classification accuracy in multistage adaptive tests (MSTs). A simulation study was conducted to compare the proposed recursion-based analytical method with an analytical method proposed by Park, Kim, Chung, and Dodd and with the more commonly used Monte Carlo (MC) simulation approaches. The results from the simulation study indicate that the recursion-based analytical method produced more stable measurement properties in MST than the MC-based simulation method. It also produced more credible measurement performance than the analytical approach of Park, Kim, Chung, and Dodd. We expect that the recursion-based analytical method would be especially efficient in instances when multiple MST designs need to be compared to determine which one has better measurement performance. Â© 2020 by the National Council on Measurement in Education","['study', 'propose', 'recursionbase', 'analytical', 'approach', 'assess', 'precision', 'ability', 'estimation', 'classification', 'accuracy', 'multistage', 'adaptive', 'test', 'MSTs', 'simulation', 'study', 'conduct', 'compare', 'propose', 'recursionbase', 'analytical', 'method', 'analytical', 'method', 'propose', 'Park', 'Kim', 'Chung', 'Dodd', 'commonly', 'Monte', 'Carlo', 'MC', 'simulation', 'approach', 'result', 'simulation', 'study', 'indicate', 'recursionbase', 'analytical', 'method', 'produce', 'stable', 'property', 'MST', 'MCbased', 'simulation', 'method', 'produce', 'credible', 'performance', 'analytical', 'approach', 'Park', 'Kim', 'Chung', 'Dodd', 'expect', 'recursionbase', 'analytical', 'method', 'especially', 'efficient', 'instance', 'multiple', 'MST', 'design', 'need', 'compare', 'determine', 'performance', 'Â©', '2020', 'National', 'Council']","['RecursionBased', 'Analytical', 'Approach', 'evaluate', 'performance', 'MST']",study propose recursionbase analytical approach assess precision ability estimation classification accuracy multistage adaptive test MSTs simulation study conduct compare propose recursionbase analytical method analytical method propose Park Kim Chung Dodd commonly Monte Carlo MC simulation approach result simulation study indicate recursionbase analytical method produce stable property MST MCbased simulation method produce credible performance analytical approach Park Kim Chung Dodd expect recursionbase analytical method especially efficient instance multiple MST design need compare determine performance Â© 2020 National Council,RecursionBased Analytical Approach evaluate performance MST,0.033759601,0.033132664,0.033089786,0.866434743,0.033583205,0,0.1054064,0.013598826,0,0
Gorney K.; Wollack J.A.,Using Item Scores and Distractors in Person-Fit Assessment,2023,60,"In order to detect a wide range of aberrant behaviors, it can be useful to incorporate information beyond the dichotomous item scores. In this paper, we extend the (Formula presented.) and (Formula presented.) person-fit statistics so that unusual behavior in item scores and unusual behavior in item distractors can be used as indicators of aberrance. Through detailed simulations, we show that the new statistics are more powerful than existing statistics in detecting several types of aberrant behavior, and that they are able to control the Type I error rate in instances where the model does not exactly fit the data. A real data example is also provided to demonstrate the utility of the new statistics in an operationalÂ setting. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Using Item Scores and Distractors in Person-Fit Assessment,"In order to detect a wide range of aberrant behaviors, it can be useful to incorporate information beyond the dichotomous item scores. In this paper, we extend the (Formula presented.) and (Formula presented.) person-fit statistics so that unusual behavior in item scores and unusual behavior in item distractors can be used as indicators of aberrance. Through detailed simulations, we show that the new statistics are more powerful than existing statistics in detecting several types of aberrant behavior, and that they are able to control the Type I error rate in instances where the model does not exactly fit the data. A real data example is also provided to demonstrate the utility of the new statistics in an operationalÂ setting. Â© 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['order', 'detect', 'wide', 'range', 'aberrant', 'behavior', 'useful', 'incorporate', 'information', 'dichotomous', 'item', 'score', 'paper', 'extend', 'Formula', 'present', 'Formula', 'present', 'personfit', 'statistic', 'unusual', 'behavior', 'item', 'score', 'unusual', 'behavior', 'item', 'distractor', 'indicator', 'aberrance', 'detailed', 'simulation', 'new', 'statistic', 'powerful', 'exist', 'statistic', 'detect', 'type', 'aberrant', 'behavior', 'able', 'control', 'type', 'I', 'error', 'rate', 'instance', 'exactly', 'fit', 'datum', 'real', 'datum', 'example', 'provide', 'demonstrate', 'utility', 'new', 'statistic', 'operational', 'setting', 'Â©', '2022', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['Item', 'Scores', 'Distractors', 'PersonFit', 'Assessment']",order detect wide range aberrant behavior useful incorporate information dichotomous item score paper extend Formula present Formula present personfit statistic unusual behavior item score unusual behavior item distractor indicator aberrance detailed simulation new statistic powerful exist statistic detect type aberrant behavior able control type I error rate instance exactly fit datum real datum example provide demonstrate utility new statistic operational setting Â© 2022 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,Item Scores Distractors PersonFit Assessment,0.027823525,0.890639298,0.026882431,0.027496291,0.027158455,0.044591075,0,0.083994973,0,0.062756872
Yamaguchi K.; Zhang J.,Fully Gibbs Sampling Algorithms for Bayesian Variable Selection in Latent Regression Models,2023,60,"This study proposed Gibbs sampling algorithms for variable selection in a latent regression model under a unidimensional two-parameter logistic item response theory model. Three types of shrinkage priors were employed to obtain shrinkage estimates: double-exponential (i.e., Laplace), horseshoe, and horseshoe+ priors. These shrinkage priors were compared to a uniform prior case in both simulation and real data analysis. The simulation study revealed that two types of horseshoe priors had a smaller root mean square errors and shorter 95% credible interval lengths than double-exponential or uniform priors. In addition, the horseshoe+Â prior was slightly more stable than the horseshoe prior. The real data example successfully proved the utility of horseshoe and horseshoe+ priors in selecting effective predictive covariates for math achievement. Â© 2022 by the National Council on Measurement in Education.",Fully Gibbs Sampling Algorithms for Bayesian Variable Selection in Latent Regression Models,"This study proposed Gibbs sampling algorithms for variable selection in a latent regression model under a unidimensional two-parameter logistic item response theory model. Three types of shrinkage priors were employed to obtain shrinkage estimates: double-exponential (i.e., Laplace), horseshoe, and horseshoe+ priors. These shrinkage priors were compared to a uniform prior case in both simulation and real data analysis. The simulation study revealed that two types of horseshoe priors had a smaller root mean square errors and shorter 95% credible interval lengths than double-exponential or uniform priors. In addition, the horseshoe+Â prior was slightly more stable than the horseshoe prior. The real data example successfully proved the utility of horseshoe and horseshoe+ priors in selecting effective predictive covariates for math achievement. Â© 2022 by the National Council on Measurement in Education.","['study', 'propose', 'Gibbs', 'sample', 'algorithm', 'variable', 'selection', 'latent', 'regression', 'unidimensional', 'twoparameter', 'logistic', 'item', 'response', 'theory', 'type', 'shrinkage', 'prior', 'employ', 'obtain', 'shrinkage', 'estimate', 'doubleexponential', 'ie', 'Laplace', 'horseshoe', 'horseshoe', 'prior', 'shrinkage', 'prior', 'compare', 'uniform', 'prior', 'case', 'simulation', 'real', 'datum', 'analysis', 'simulation', 'study', 'reveal', 'type', 'horseshoe', 'prior', 'small', 'root', 'mean', 'square', 'error', 'short', '95', 'credible', 'interval', 'length', 'doubleexponential', 'uniform', 'prior', 'addition', 'horseshoe', 'prior', 'slightly', 'stable', 'horseshoe', 'prior', 'real', 'datum', 'example', 'successfully', 'prove', 'utility', 'horseshoe', 'horseshoe', 'prior', 'select', 'effective', 'predictive', 'covariate', 'math', 'achievement', 'Â©', '2022', 'National', 'Council']","['fully', 'Gibbs', 'Sampling', 'Algorithms', 'Bayesian', 'Variable', 'Selection', 'Latent', 'Regression', 'Models']",study propose Gibbs sample algorithm variable selection latent regression unidimensional twoparameter logistic item response theory type shrinkage prior employ obtain shrinkage estimate doubleexponential ie Laplace horseshoe horseshoe prior shrinkage prior compare uniform prior case simulation real datum analysis simulation study reveal type horseshoe prior small root mean square error short 95 credible interval length doubleexponential uniform prior addition horseshoe prior slightly stable horseshoe prior real datum example successfully prove utility horseshoe horseshoe prior select effective predictive covariate math achievement Â© 2022 National Council,fully Gibbs Sampling Algorithms Bayesian Variable Selection Latent Regression Models,0.034359461,0.033632023,0.033669989,0.033974117,0.864364411,0.00687861,0.029031517,0.024529425,0,0.018432262
Tang X.; Zheng Y.; Wu T.; Hau K.-T.; Chang H.-H.,Utilizing Response Time for Item Selection in On-the-Fly Multistage Adaptive Testing for PISA Assessment,2024,,"Multistage adaptive testing (MST) has been recently adopted for international large-scale assessments such as Programme for International Student Assessment (PISA). MST offers improved measurement efficiency over traditional nonadaptive tests and improved practical convenience over single-item-adaptive computerized adaptive testing (CAT). As a third alternative adaptive test design to MST and CAT, Zheng and Chang proposed the ?œon-the-fly multistage adaptive testing??(OMST), which combines the benefits of MST and CAT and offsets their limitations. In this study, we adopted the OMST design while also incorporating response time (RT) in item selection. Via simulations emulating the PISA 2018 reading test, including using the real item attributes and replicating PISA 2018 reading test's MST design, we compared the performance of our OMST designs against the simulated MST design in (1) measurement accuracy of test takers??ability, (2) test time efficiency and consistency, and (3) expected gains in precision by design. We also investigated the performance of OMST in item bank usage and constraints management. Results show great potential for the proposed RT-incorporated OMST designs to be used for PISA and potentially other international large-scale assessments. Â© 2024 by the National Council on Measurement in Education.",Utilizing Response Time for Item Selection in On-the-Fly Multistage Adaptive Testing for PISA Assessment,"Multistage adaptive testing (MST) has been recently adopted for international large-scale assessments such as Programme for International Student Assessment (PISA). MST offers improved measurement efficiency over traditional nonadaptive tests and improved practical convenience over single-item-adaptive computerized adaptive testing (CAT). As a third alternative adaptive test design to MST and CAT, Zheng and Chang proposed the ?œon-the-fly multistage adaptive testing??(OMST), which combines the benefits of MST and CAT and offsets their limitations. In this study, we adopted the OMST design while also incorporating response time (RT) in item selection. Via simulations emulating the PISA 2018 reading test, including using the real item attributes and replicating PISA 2018 reading test's MST design, we compared the performance of our OMST designs against the simulated MST design in (1) measurement accuracy of test takers??ability, (2) test time efficiency and consistency, and (3) expected gains in precision by design. We also investigated the performance of OMST in item bank usage and constraints management. Results show great potential for the proposed RT-incorporated OMST designs to be used for PISA and potentially other international large-scale assessments. Â© 2024 by the National Council on Measurement in Education.","['multistage', 'adaptive', 'testing', 'MST', 'recently', 'adopt', 'international', 'largescale', 'assessment', 'Programme', 'International', 'Student', 'Assessment', 'PISA', 'MST', 'offer', 'improve', 'efficiency', 'traditional', 'nonadaptive', 'test', 'improve', 'practical', 'convenience', 'singleitemadaptive', 'computerized', 'adaptive', 'testing', 'CAT', 'alternative', 'adaptive', 'test', 'design', 'MST', 'CAT', 'Zheng', 'Chang', 'propose', '""', 'onthefly', 'multistage', 'adaptive', 'testing', '""', 'omst', 'combine', 'benefit', 'MST', 'CAT', 'offset', 'limitation', 'study', 'adopt', 'OMST', 'design', 'incorporate', 'response', 'time', 'RT', 'item', 'selection', 'Via', 'simulation', 'emulate', 'PISA', '2018', 'reading', 'test', 'include', 'real', 'item', 'attribute', 'replicate', 'PISA', '2018', 'read', 'test', 'MST', 'design', 'compare', 'performance', 'OMST', 'design', 'simulated', 'MST', 'design', '1', 'accuracy', 'test', 'taker', '??, 'ability', '2', 'test', 'time', 'efficiency', 'consistency', '3', 'expect', 'gain', 'precision', 'design', 'investigate', 'performance', 'OMST', 'item', 'bank', 'usage', 'constraint', 'management', 'result', 'great', 'potential', 'propose', 'RTincorporated', 'OMST', 'design', 'PISA', 'potentially', 'international', 'largescale', 'assessment', 'Â©', '2024', 'National', 'Council']","['utilize', 'Response', 'Time', 'Item', 'Selection', 'OntheFly', 'Multistage', 'Adaptive', 'Testing', 'PISA', 'Assessment']","multistage adaptive testing MST recently adopt international largescale assessment Programme International Student Assessment PISA MST offer improve efficiency traditional nonadaptive test improve practical convenience singleitemadaptive computerized adaptive testing CAT alternative adaptive test design MST CAT Zheng Chang propose "" onthefly multistage adaptive testing "" omst combine benefit MST CAT offset limitation study adopt OMST design incorporate response time RT item selection Via simulation emulate PISA 2018 reading test include real item attribute replicate PISA 2018 read test MST design compare performance OMST design simulated MST design 1 accuracy test taker ??ability 2 test time efficiency consistency 3 expect gain precision design investigate performance OMST item bank usage constraint management result great potential propose RTincorporated OMST design PISA potentially international largescale assessment Â© 2024 National Council",utilize Response Time Item Selection OntheFly Multistage Adaptive Testing PISA Assessment,0.026488407,0.025704155,0.025661682,0.026445834,0.895699922,0.016152775,0.147606047,0,0,0
Feng T.; Cai L.,Sensemaking of Process Data from Evaluation Studies of Educational Games: An Application of Cross-Classified Item Response Theory Modeling,2024,,"Process information collected from educational games can illuminate how students approach interactive tasks, complementing assessment outcomes routinely examined in evaluation studies. However, the two sources of information are historically analyzed and interpreted separately, and diagnostic process information is often underused. To tackle these issues, we present a new application of cross-classified item response theory modeling, using indicators of knowledge misconceptions and item-level assessment data collected from a multisite game-based randomized controlled trial. This application addresses (a) the joint modeling of students' pretest and posttest item responses and game-based processes described by indicators of misconceptions; (b) integration of gameplay information when gauging the intervention effect of an educational game; (c) relationships among game-based misconception, pretest initial status, and pre-to-post change; and (d) nesting of students within schools, a common aspect in multisite research. We also demonstrate how to structure the data and set up the model to enable our proposed application, and how our application compares to three other approaches to analyzing gameplay and assessment data. Lastly, we note the implications for future evaluation studies and for using analytic results to inform learning andÂ instruction. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",Sensemaking of Process Data from Evaluation Studies of Educational Games: An Application of Cross-Classified Item Response Theory Modeling,"Process information collected from educational games can illuminate how students approach interactive tasks, complementing assessment outcomes routinely examined in evaluation studies. However, the two sources of information are historically analyzed and interpreted separately, and diagnostic process information is often underused. To tackle these issues, we present a new application of cross-classified item response theory modeling, using indicators of knowledge misconceptions and item-level assessment data collected from a multisite game-based randomized controlled trial. This application addresses (a) the joint modeling of students' pretest and posttest item responses and game-based processes described by indicators of misconceptions; (b) integration of gameplay information when gauging the intervention effect of an educational game; (c) relationships among game-based misconception, pretest initial status, and pre-to-post change; and (d) nesting of students within schools, a common aspect in multisite research. We also demonstrate how to structure the data and set up the model to enable our proposed application, and how our application compares to three other approaches to analyzing gameplay and assessment data. Lastly, we note the implications for future evaluation studies and for using analytic results to inform learning andÂ instruction. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['process', 'information', 'collect', 'educational', 'game', 'illuminate', 'student', 'approach', 'interactive', 'task', 'complement', 'assessment', 'outcome', 'routinely', 'examine', 'evaluation', 'study', 'source', 'information', 'historically', 'analyze', 'interpret', 'separately', 'diagnostic', 'process', 'information', 'underused', 'tackle', 'issue', 'present', 'new', 'application', 'crossclassified', 'item', 'response', 'theory', 'indicator', 'knowledge', 'misconception', 'itemlevel', 'assessment', 'datum', 'collect', 'multisite', 'gamebase', 'randomize', 'control', 'trial', 'application', 'address', 'joint', 'modeling', 'student', 'pret', 'postt', 'item', 'response', 'gamebase', 'process', 'describe', 'indicator', 'misconception', 'b', 'integration', 'gameplay', 'information', 'gauge', 'intervention', 'effect', 'educational', 'game', 'c', 'relationship', 'gamebase', 'misconception', 'pret', 'initial', 'status', 'pretopost', 'change', 'd', 'nesting', 'student', 'school', 'common', 'aspect', 'multisite', 'research', 'demonstrate', 'structure', 'datum', 'set', 'enable', 'propose', 'application', 'application', 'compare', 'approach', 'analyze', 'gameplay', 'assessment', 'datum', 'lastly', 'note', 'implication', 'future', 'evaluation', 'study', 'analytic', 'result', 'inform', 'learning', 'instruction', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['sensemaking', 'Process', 'Data', 'Evaluation', 'Studies', 'Educational', 'Games', 'Application', 'CrossClassified', 'Item', 'Response', 'Theory', 'Modeling']",process information collect educational game illuminate student approach interactive task complement assessment outcome routinely examine evaluation study source information historically analyze interpret separately diagnostic process information underused tackle issue present new application crossclassified item response theory indicator knowledge misconception itemlevel assessment datum collect multisite gamebase randomize control trial application address joint modeling student pret postt item response gamebase process describe indicator misconception b integration gameplay information gauge intervention effect educational game c relationship gamebase misconception pret initial status pretopost change d nesting student school common aspect multisite research demonstrate structure datum set enable propose application application compare approach analyze gameplay assessment datum lastly note implication future evaluation study analytic result inform learning instruction Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,sensemaking Process Data Evaluation Studies Educational Games Application CrossClassified Item Response Theory Modeling,0.913954341,0.02144441,0.021371048,0.021698942,0.021531259,0.064601642,0.002686334,0.003942567,0.033245658,0
Xu L.; Wang S.; Cai Y.; Tu D.,The Automated Test Assembly and Routing Rule for Multistage Adaptive Testing with Multidimensional Item Response Theory,2021,58,"Designing a multidimensional adaptive test (M-MST) based on a multidimensional item response theory (MIRT) model is critical to make full use of the advantages of both MST and MIRT in implementing multidimensional assessments. This study proposed two types of automated test assembly (ATA) algorithms and one set of routing rules that can facilitate the development of an M-MST. Different M-MST designs were developed based on the proposed ATA algorithms and routing rules and were evaluated through two sets of simulation studies. Study 1 used simulated item banks and considered a variety of testing factors, and results from which can inform us the theoretical performance of the proposed M-MST designs. Study 2 was designed based on a real multidimensional assessment. In addition, a MCAT was simulated as a baseline design to demonstrate the advantage of the proposed M-MST design in each study. Our simulation results indicate that the proposed ATA algorithms and routing rule can generate M-MSTs with a good-quality control and the same or even better ability estimation results than the MCAT given the same condition. This demonstrates the advantage of using M-MST for multidimensional assessment especially for the one that needs to satisfy many nonstatistical constraints. Â© 2022 by the National Council on Measurement in Education",The Automated Test Assembly and Routing Rule for Multistage Adaptive Testing with Multidimensional Item Response Theory,"Designing a multidimensional adaptive test (M-MST) based on a multidimensional item response theory (MIRT) model is critical to make full use of the advantages of both MST and MIRT in implementing multidimensional assessments. This study proposed two types of automated test assembly (ATA) algorithms and one set of routing rules that can facilitate the development of an M-MST. Different M-MST designs were developed based on the proposed ATA algorithms and routing rules and were evaluated through two sets of simulation studies. Study 1 used simulated item banks and considered a variety of testing factors, and results from which can inform us the theoretical performance of the proposed M-MST designs. Study 2 was designed based on a real multidimensional assessment. In addition, a MCAT was simulated as a baseline design to demonstrate the advantage of the proposed M-MST design in each study. Our simulation results indicate that the proposed ATA algorithms and routing rule can generate M-MSTs with a good-quality control and the same or even better ability estimation results than the MCAT given the same condition. This demonstrates the advantage of using M-MST for multidimensional assessment especially for the one that needs to satisfy many nonstatistical constraints. Â© 2022 by the National Council on Measurement in Education","['design', 'multidimensional', 'adaptive', 'test', 'MMST', 'base', 'multidimensional', 'item', 'response', 'theory', 'MIRT', 'critical', 'advantage', 'MST', 'MIRT', 'implement', 'multidimensional', 'assessment', 'study', 'propose', 'type', 'automate', 'test', 'assembly', 'ATA', 'algorithm', 'set', 'route', 'rule', 'facilitate', 'development', 'mmst', 'different', 'mmst', 'design', 'develop', 'base', 'propose', 'ATA', 'algorithm', 'route', 'rule', 'evaluate', 'set', 'simulation', 'study', 'Study', '1', 'simulated', 'item', 'bank', 'consider', 'variety', 'testing', 'factor', 'result', 'inform', 'theoretical', 'performance', 'propose', 'mmst', 'design', 'Study', '2', 'design', 'base', 'real', 'multidimensional', 'assessment', 'addition', 'MCAT', 'simulate', 'baseline', 'design', 'demonstrate', 'advantage', 'propose', 'mmst', 'design', 'study', 'simulation', 'result', 'indicate', 'propose', 'ATA', 'algorithm', 'route', 'rule', 'generate', 'mmst', 'goodquality', 'control', 'ability', 'estimation', 'result', 'MCAT', 'condition', 'demonstrate', 'advantage', 'mmst', 'multidimensional', 'assessment', 'especially', 'need', 'satisfy', 'nonstatistical', 'constraint', 'Â©', '2022', 'National', 'Council']","['Automated', 'Test', 'Assembly', 'Routing', 'Rule', 'Multistage', 'Adaptive', 'Testing', 'Multidimensional', 'Item', 'Response', 'Theory']",design multidimensional adaptive test MMST base multidimensional item response theory MIRT critical advantage MST MIRT implement multidimensional assessment study propose type automate test assembly ATA algorithm set route rule facilitate development mmst different mmst design develop base propose ATA algorithm route rule evaluate set simulation study Study 1 simulated item bank consider variety testing factor result inform theoretical performance propose mmst design Study 2 design base real multidimensional assessment addition MCAT simulate baseline design demonstrate advantage propose mmst design study simulation result indicate propose ATA algorithm route rule generate mmst goodquality control ability estimation result MCAT condition demonstrate advantage mmst multidimensional assessment especially need satisfy nonstatistical constraint Â© 2022 National Council,Automated Test Assembly Routing Rule Multistage Adaptive Testing Multidimensional Item Response Theory,0.880156551,0.029757894,0.029923214,0.030247839,0.029914502,0.00914946,0.109351132,0.006226059,0.009325771,0
Hong M.; RebouÃ§as D.A.; Cheng Y.,Robust Estimation for Response Time Modeling,2021,58,"Response time has started to play an increasingly important role in educational and psychological testing, which prompts many response time models to be proposed in recent years. However, response time modeling can be adversely impacted by aberrant response behavior. For example, test speededness can cause response time to certain items to deviate from the hypothesized model. In this article, we introduce a robust estimation approach when estimating a respondent's working speed under the log-normal model by down-weighting aberrant response times. A simulation study is carried out to compare the performance of two weighting schemes and a real data example is provided to showcase the use of the new robust estimation method. Limitations and future directions are alsoÂ discussed. Â© 2020 by the National Council on Measurement in Education",,"Response time has started to play an increasingly important role in educational and psychological testing, which prompts many response time models to be proposed in recent years. However, response time modeling can be adversely impacted by aberrant response behavior. For example, test speededness can cause response time to certain items to deviate from the hypothesized model. In this article, we introduce a robust estimation approach when estimating a respondent's working speed under the log-normal model by down-weighting aberrant response times. A simulation study is carried out to compare the performance of two weighting schemes and a real data example is provided to showcase the use of the new robust estimation method. Limitations and future directions are alsoÂ discussed. Â© 2020 by the National Council on Measurement in Education","['response', 'time', 'start', 'play', 'increasingly', 'important', 'role', 'educational', 'psychological', 'testing', 'prompt', 'response', 'time', 'propose', 'recent', 'year', 'response', 'time', 'modeling', 'adversely', 'impact', 'aberrant', 'response', 'behavior', 'example', 'test', 'speededness', 'cause', 'response', 'time', 'certain', 'item', 'deviate', 'hypothesized', 'article', 'introduce', 'robust', 'estimation', 'approach', 'estimate', 'respondent', 'work', 'speed', 'lognormal', 'downweighte', 'aberrant', 'response', 'time', 'simulation', 'study', 'carry', 'compare', 'performance', 'weighting', 'scheme', 'real', 'datum', 'example', 'provide', 'showcase', 'new', 'robust', 'estimation', 'method', 'Limitations', 'future', 'direction', 'discuss', 'Â©', '2020', 'National', 'Council']",,response time start play increasingly important role educational psychological testing prompt response time propose recent year response time modeling adversely impact aberrant response behavior example test speededness cause response time certain item deviate hypothesized article introduce robust estimation approach estimate respondent work speed lognormal downweighte aberrant response time simulation study carry compare performance weighting scheme real datum example provide showcase new robust estimation method Limitations future direction discuss Â© 2020 National Council,,0.027230084,0.89235799,0.026393292,0.027094975,0.026923659,0.138539585,0,0,0,0
Lee S.; Han S.; Choi S.W.,A Bayesian Moderated Nonlinear Factor Analysis Approach for DIF Detection under Violation of the Equal Variance Assumption,2024,61,"Research has shown that multiple-indicator multiple-cause (MIMIC) models can result in inflated Type I error rates in detecting differential item functioning (DIF) when the assumption of equal latent variance is violated. This study explains how the violation of the equal variance assumption adversely impacts the detection of nonuniform DIF and how it can be addressed through moderated nonlinear factor analysis (MNLFA) model via Bayesian estimation approach to overcome limitations from the restrictive assumption. The Bayesian MNLFA approach suggested in this study better control Type I errors by freely estimating latent factor variances across different groups. Our experimentation with simulated data demonstrates that the BMNFA models outperform the existing MIMIC models, in terms of Type I error control as well as parameter recovery. The results suggest that the MNLFA models have the potential to be a superior choice to the existing MIMIC models, especially in situations where the assumption of equal latent variance assumption is not likely to hold. Â© 2024 by the National Council on Measurement in Education.",A Bayesian Moderated Nonlinear Factor Analysis Approach for DIF Detection under Violation of the Equal Variance Assumption,"Research has shown that multiple-indicator multiple-cause (MIMIC) models can result in inflated Type I error rates in detecting differential item functioning (DIF) when the assumption of equal latent variance is violated. This study explains how the violation of the equal variance assumption adversely impacts the detection of nonuniform DIF and how it can be addressed through moderated nonlinear factor analysis (MNLFA) model via Bayesian estimation approach to overcome limitations from the restrictive assumption. The Bayesian MNLFA approach suggested in this study better control Type I errors by freely estimating latent factor variances across different groups. Our experimentation with simulated data demonstrates that the BMNFA models outperform the existing MIMIC models, in terms of Type I error control as well as parameter recovery. The results suggest that the MNLFA models have the potential to be a superior choice to the existing MIMIC models, especially in situations where the assumption of equal latent variance assumption is not likely to hold. Â© 2024 by the National Council on Measurement in Education.","['research', 'multipleindicator', 'multiplecause', 'MIMIC', 'result', 'inflated', 'Type', 'I', 'error', 'rate', 'detect', 'differential', 'item', 'function', 'dif', 'assumption', 'equal', 'latent', 'variance', 'violate', 'study', 'explain', 'violation', 'equal', 'variance', 'assumption', 'adversely', 'impact', 'detection', 'nonuniform', 'DIF', 'address', 'moderated', 'nonlinear', 'factor', 'analysis', 'MNLFA', 'bayesian', 'estimation', 'approach', 'overcome', 'limitation', 'restrictive', 'assumption', 'Bayesian', 'MNLFA', 'approach', 'suggest', 'study', 'control', 'Type', 'I', 'error', 'freely', 'estimate', 'latent', 'factor', 'variance', 'different', 'group', 'experimentation', 'simulate', 'datum', 'demonstrate', 'BMNFA', 'outperform', 'exist', 'MIMIC', 'term', 'Type', 'I', 'error', 'control', 'parameter', 'recovery', 'result', 'suggest', 'MNLFA', 'potential', 'superior', 'choice', 'exist', 'MIMIC', 'especially', 'situation', 'assumption', 'equal', 'latent', 'variance', 'assumption', 'likely', 'hold', 'Â©', '2024', 'National', 'Council']","['Bayesian', 'Moderated', 'Nonlinear', 'Factor', 'Analysis', 'Approach', 'DIF', 'Detection', 'violation', 'Equal', 'Variance', 'Assumption']",research multipleindicator multiplecause MIMIC result inflated Type I error rate detect differential item function dif assumption equal latent variance violate study explain violation equal variance assumption adversely impact detection nonuniform DIF address moderated nonlinear factor analysis MNLFA bayesian estimation approach overcome limitation restrictive assumption Bayesian MNLFA approach suggest study control Type I error freely estimate latent factor variance different group experimentation simulate datum demonstrate BMNFA outperform exist MIMIC term Type I error control parameter recovery result suggest MNLFA potential superior choice exist MIMIC especially situation assumption equal latent variance assumption likely hold Â© 2024 National Council,Bayesian Moderated Nonlinear Factor Analysis Approach DIF Detection violation Equal Variance Assumption,0.894025395,0.026361223,0.026345929,0.026754677,0.026512776,0.006398002,0,0.045511346,0.009763627,0.114646835
Jin K.-Y.; Eckes T.,Measuring the Impact of Peer Interaction in Group Oral Assessments with an Extended Many-Facet Rasch Model,2024,61,"Many language proficiency tests include group oral assessments involving peer interaction. In such an assessment, examinees discuss a common topic with others. Human raters score each examinee's spoken performance on specially designed criteria. However, measurement models for analyzing group assessment data usually assume local person independence and thus fail to consider the impact of peer interaction on the assessment outcomes. This research advances an extended many-facet Rasch model for group assessments (MFRM-GA), accounting for local person dependence. In a series of simulations, we examined the MFRM-GA's parameter recovery and the consequences of ignoring peer interactions under the traditional modeling approach. We also used a real dataset from the English-speaking test of the Language Proficiency Assessment for Teachers (LPAT) routinely administered in Hong Kong to illustrate the efficiency of the new model. The discussion focuses on the model's usefulness for measuring oral language proficiency, practical implications, and future research perspectives. Â© 2023 by the National Council on Measurement in Education.",Measuring the Impact of Peer Interaction in Group Oral Assessments with an Extended Many-Facet Rasch Model,"Many language proficiency tests include group oral assessments involving peer interaction. In such an assessment, examinees discuss a common topic with others. Human raters score each examinee's spoken performance on specially designed criteria. However, measurement models for analyzing group assessment data usually assume local person independence and thus fail to consider the impact of peer interaction on the assessment outcomes. This research advances an extended many-facet Rasch model for group assessments (MFRM-GA), accounting for local person dependence. In a series of simulations, we examined the MFRM-GA's parameter recovery and the consequences of ignoring peer interactions under the traditional modeling approach. We also used a real dataset from the English-speaking test of the Language Proficiency Assessment for Teachers (LPAT) routinely administered in Hong Kong to illustrate the efficiency of the new model. The discussion focuses on the model's usefulness for measuring oral language proficiency, practical implications, and future research perspectives. Â© 2023 by the National Council on Measurement in Education.","['language', 'proficiency', 'test', 'include', 'group', 'oral', 'assessment', 'involve', 'peer', 'interaction', 'assessment', 'examine', 'discuss', 'common', 'topic', 'human', 'rater', 'score', 'examine', 'speak', 'performance', 'specially', 'design', 'criterion', 'analyze', 'group', 'assessment', 'datum', 'usually', 'assume', 'local', 'person', 'independence', 'fail', 'consider', 'impact', 'peer', 'interaction', 'assessment', 'outcome', 'research', 'advance', 'extended', 'manyfacet', 'Rasch', 'group', 'assessment', 'MFRMGA', 'accounting', 'local', 'person', 'dependence', 'series', 'simulation', 'examine', 'MFRMGAs', 'parameter', 'recovery', 'consequence', 'ignore', 'peer', 'interaction', 'traditional', 'modeling', 'approach', 'real', 'dataset', 'englishspeake', 'test', 'Language', 'Proficiency', 'Assessment', 'Teachers', 'LPAT', 'routinely', 'administer', 'Hong', 'Kong', 'illustrate', 'efficiency', 'new', 'discussion', 'focus', 'usefulness', 'measure', 'oral', 'language', 'proficiency', 'practical', 'implication', 'future', 'research', 'perspective', 'Â©', '2023', 'National', 'Council']","['measure', 'Impact', 'Peer', 'Interaction', 'Group', 'oral', 'assessment', 'Extended', 'ManyFacet', 'Rasch']",language proficiency test include group oral assessment involve peer interaction assessment examine discuss common topic human rater score examine speak performance specially design criterion analyze group assessment datum usually assume local person independence fail consider impact peer interaction assessment outcome research advance extended manyfacet Rasch group assessment MFRMGA accounting local person dependence series simulation examine MFRMGAs parameter recovery consequence ignore peer interaction traditional modeling approach real dataset englishspeake test Language Proficiency Assessment Teachers LPAT routinely administer Hong Kong illustrate efficiency new discussion focus usefulness measure oral language proficiency practical implication future research perspective Â© 2023 National Council,measure Impact Peer Interaction Group oral assessment Extended ManyFacet Rasch,0.024704502,0.024413669,0.024188027,0.901855912,0.02483789,0.006793657,0.02463164,0.007613118,0.081133437,0.001964071
Wang S.; Zhang M.; Lee W.-C.; Huang F.; Li Z.; Li Y.; Yu S.,Two IRT Characteristic Curve Linking Methods Weighted by Information,2022,59,"Traditional IRT characteristic curve linking methods ignore parameter estimation errors, which may undermine the accuracy of estimated linking constants. Two new linking methods are proposed that take into account parameter estimation errors. The item- (IWCC) and test-information-weighted characteristic curve (TWCC) methods employ weighting components in the loss function from traditional methods by their corresponding item and test information, respectively. Monte Carlo simulation was conducted to evaluate the performances of the new linking methods and compare them with traditional ones. Ability difference between linking groups, sample size, and test length were manipulated under the common-item nonequivalent groups design. Results showed that the two information-weighted characteristic curve methods outperformed traditional methods, in general. TWCC was found to be more accurate and stable than IWCC. A pseudo-form pseudo-group analysis was also performed, and similar results were observed. Finally, guidelines for practice and future directions are discussed. Â© 2022 by the National Council on Measurement in Education.",Two IRT Characteristic Curve Linking Methods Weighted by Information,"Traditional IRT characteristic curve linking methods ignore parameter estimation errors, which may undermine the accuracy of estimated linking constants. Two new linking methods are proposed that take into account parameter estimation errors. The item- (IWCC) and test-information-weighted characteristic curve (TWCC) methods employ weighting components in the loss function from traditional methods by their corresponding item and test information, respectively. Monte Carlo simulation was conducted to evaluate the performances of the new linking methods and compare them with traditional ones. Ability difference between linking groups, sample size, and test length were manipulated under the common-item nonequivalent groups design. Results showed that the two information-weighted characteristic curve methods outperformed traditional methods, in general. TWCC was found to be more accurate and stable than IWCC. A pseudo-form pseudo-group analysis was also performed, and similar results were observed. Finally, guidelines for practice and future directions are discussed. Â© 2022 by the National Council on Measurement in Education.","['traditional', 'IRT', 'characteristic', 'curve', 'link', 'method', 'ignore', 'parameter', 'estimation', 'error', 'undermine', 'accuracy', 'estimate', 'linking', 'constant', 'new', 'linking', 'method', 'propose', 'account', 'parameter', 'estimation', 'error', 'item', 'iwcc', 'testinformationweighted', 'characteristic', 'curve', 'TWCC', 'method', 'employ', 'weighting', 'component', 'loss', 'function', 'traditional', 'method', 'correspond', 'item', 'test', 'information', 'respectively', 'Monte', 'Carlo', 'simulation', 'conduct', 'evaluate', 'performance', 'new', 'linking', 'method', 'compare', 'traditional', 'ability', 'difference', 'link', 'group', 'sample', 'size', 'test', 'length', 'manipulate', 'commonitem', 'nonequivalent', 'group', 'design', 'result', 'informationweighte', 'characteristic', 'curve', 'method', 'outperform', 'traditional', 'method', 'general', 'TWCC', 'find', 'accurate', 'stable', 'IWCC', 'pseudoform', 'pseudogroup', 'analysis', 'perform', 'similar', 'result', 'observe', 'finally', 'guideline', 'practice', 'future', 'direction', 'discuss', 'Â©', '2022', 'National', 'Council']","['IRT', 'Characteristic', 'curve', 'Linking', 'Methods', 'weight', 'information']",traditional IRT characteristic curve link method ignore parameter estimation error undermine accuracy estimate linking constant new linking method propose account parameter estimation error item iwcc testinformationweighted characteristic curve TWCC method employ weighting component loss function traditional method correspond item test information respectively Monte Carlo simulation conduct evaluate performance new linking method compare traditional ability difference link group sample size test length manipulate commonitem nonequivalent group design result informationweighte characteristic curve method outperform traditional method general TWCC find accurate stable IWCC pseudoform pseudogroup analysis perform similar result observe finally guideline practice future direction discuss Â© 2022 National Council,IRT Characteristic curve Linking Methods weight information,0.899431501,0.025029666,0.025013096,0.025303603,0.025222134,0,0.116753705,0.04634153,0,0.006158071
Binici S.; Cuhadar I.,Validating Performance Standards via Latent Class Analysis,2022,59,"Validity of performance standards is a key element for the defensibility of standard setting results, and validating performance standards requires collecting multiple pieces of evidence at every step during the standard setting process. This study employs a statistical procedure, latent class analysis, to set performance standards and compares latent class analysis results with previously established performance standards via the modified-Angoff method for cross-validation. The context of the study is an operational large-scale science assessment administered in one of the southern states in the United States. Results show that the number of classes that emerged in the latent class analysis concurs with the number of existing performance levels. In addition, there is a substantial level of agreement between latent class analysis results and modified-Angoff method in terms of classifying students into the same performance levels. Overall, the findings establish evidence for the validity of the performance standards identified via the modified-Angoff method. Practical implications of the study findings are discussed. Â© 2022 by the National Council on Measurement in Education.",Validating Performance Standards via Latent Class Analysis,"Validity of performance standards is a key element for the defensibility of standard setting results, and validating performance standards requires collecting multiple pieces of evidence at every step during the standard setting process. This study employs a statistical procedure, latent class analysis, to set performance standards and compares latent class analysis results with previously established performance standards via the modified-Angoff method for cross-validation. The context of the study is an operational large-scale science assessment administered in one of the southern states in the United States. Results show that the number of classes that emerged in the latent class analysis concurs with the number of existing performance levels. In addition, there is a substantial level of agreement between latent class analysis results and modified-Angoff method in terms of classifying students into the same performance levels. Overall, the findings establish evidence for the validity of the performance standards identified via the modified-Angoff method. Practical implications of the study findings are discussed. Â© 2022 by the National Council on Measurement in Education.","['validity', 'performance', 'standard', 'key', 'element', 'defensibility', 'standard', 'setting', 'result', 'validate', 'performance', 'standard', 'require', 'collect', 'multiple', 'piece', 'evidence', 'step', 'standard', 'setting', 'process', 'study', 'employ', 'statistical', 'procedure', 'latent', 'class', 'analysis', 'set', 'performance', 'standard', 'compare', 'latent', 'class', 'analysis', 'result', 'previously', 'establish', 'performance', 'standard', 'modifiedangoff', 'method', 'crossvalidation', 'context', 'study', 'operational', 'largescale', 'science', 'assessment', 'administer', 'southern', 'state', 'United', 'States', 'Results', 'number', 'class', 'emerge', 'latent', 'class', 'analysis', 'concur', 'number', 'exist', 'performance', 'level', 'addition', 'substantial', 'level', 'agreement', 'latent', 'class', 'analysis', 'result', 'modifiedangoff', 'method', 'term', 'classify', 'student', 'performance', 'level', 'overall', 'finding', 'establish', 'evidence', 'validity', 'performance', 'standard', 'identify', 'modifiedangoff', 'method', 'practical', 'implication', 'study', 'finding', 'discuss', 'Â©', '2022', 'National', 'Council']","['validate', 'Performance', 'Standards', 'Latent', 'Class', 'Analysis']",validity performance standard key element defensibility standard setting result validate performance standard require collect multiple piece evidence step standard setting process study employ statistical procedure latent class analysis set performance standard compare latent class analysis result previously establish performance standard modifiedangoff method crossvalidation context study operational largescale science assessment administer southern state United States Results number class emerge latent class analysis concur number exist performance level addition substantial level agreement latent class analysis result modifiedangoff method term classify student performance level overall finding establish evidence validity performance standard identify modifiedangoff method practical implication study finding discuss Â© 2022 National Council,validate Performance Standards Latent Class Analysis,0.028667351,0.028169713,0.028217192,0.028641941,0.886303802,0.005531267,0.0319623,0.048037319,0.04997815,1.24E-05
Ercikan K.; McCaffrey D.F.,Optimizing Implementation of Artificial-Intelligence-Based Automated Scoring: An Evidence Centered Design Approach for Designing Assessments for AI-based Scoring,2022,59,"Artificial-intelligence-based automated scoring is often an afterthought and is considered after assessments have been developed, resulting in nonoptimal possibility of implementing automated scoring solutions. In this article, we provide a review of Artificial intelligence (AI)-based methodologies for scoring in educational assessments. We then propose an evidence-centered design framework for developing assessments to align conceptualization, scoring, and ultimate assessment interpretation and use with the advantages and limitations of AI-based scoring in mind. We provide recommendations for defining construct, task, and evidence models to guide task and assessment design that optimize the development and implementation of AI-based automated scoring of constructed response items and support the validity of inferences from and uses of scores. Â© 2022 by the National Council on Measurement in Education.",Optimizing Implementation of Artificial-Intelligence-Based Automated Scoring: An Evidence Centered Design Approach for Designing Assessments for AI-based Scoring,"Artificial-intelligence-based automated scoring is often an afterthought and is considered after assessments have been developed, resulting in nonoptimal possibility of implementing automated scoring solutions. In this article, we provide a review of Artificial intelligence (AI)-based methodologies for scoring in educational assessments. We then propose an evidence-centered design framework for developing assessments to align conceptualization, scoring, and ultimate assessment interpretation and use with the advantages and limitations of AI-based scoring in mind. We provide recommendations for defining construct, task, and evidence models to guide task and assessment design that optimize the development and implementation of AI-based automated scoring of constructed response items and support the validity of inferences from and uses of scores. Â© 2022 by the National Council on Measurement in Education.","['artificialintelligencebase', 'automate', 'scoring', 'afterthought', 'consider', 'assessment', 'develop', 'result', 'nonoptimal', 'possibility', 'implement', 'automate', 'scoring', 'solution', 'article', 'provide', 'review', 'Artificial', 'intelligence', 'AIbased', 'methodology', 'scoring', 'educational', 'assessment', 'propose', 'evidencecentere', 'design', 'framework', 'develop', 'assessment', 'align', 'conceptualization', 'scoring', 'ultimate', 'assessment', 'interpretation', 'advantage', 'limitation', 'AIbased', 'scoring', 'mind', 'provide', 'recommendation', 'define', 'construct', 'task', 'evidence', 'guide', 'task', 'assessment', 'design', 'optimize', 'development', 'implementation', 'AIbased', 'automate', 'scoring', 'construct', 'response', 'item', 'support', 'validity', 'inference', 'score', 'Â©', '2022', 'National', 'Council']","['optimize', 'implementation', 'artificialintelligencebased', 'Automated', 'Scoring', 'Evidence', 'center', 'Design', 'Approach', 'Designing', 'assessment', 'AIbased', 'Scoring']",artificialintelligencebase automate scoring afterthought consider assessment develop result nonoptimal possibility implement automate scoring solution article provide review Artificial intelligence AIbased methodology scoring educational assessment propose evidencecentere design framework develop assessment align conceptualization scoring ultimate assessment interpretation advantage limitation AIbased scoring mind provide recommendation define construct task evidence guide task assessment design optimize development implementation AIbased automate scoring construct response item support validity inference score Â© 2022 National Council,optimize implementation artificialintelligencebased Automated Scoring Evidence center Design Approach Designing assessment AIbased Scoring,0.030821227,0.029765416,0.879220905,0.030246351,0.029946102,0,0,0,0.186902755,0
Luo X.,Automated Test Assembly with Mixed-Integer Programming: The Effects of Modeling Approaches and Solvers,2020,57,"Automated test assembly (ATA) is a modern approach to test assembly that applies advanced optimization algorithms on computers to build test forms automatically. ATA greatly improves the efficiency and accuracy of the test assembly. This study investigated the effects of the modeling methods and solvers in the mixed-integer programming (MIP) approach to ATA in the context of assembling parallel linear test forms and multistage testing (MST) panels. The results of two simulation studies indicated that the newly proposed maximin modeling method significantly improved the parallelism of the test information functions (TIFs) among assembled test forms while maintaining relatively high overall TIFs, and the newly proposed binary minimax method considerably reduced the overall discrepancies from the targets. A comparison of four freely available noncommercial MIP solvers from the utilitarian, as opposed to the benchmarking, perspective was also included in this study. Â© 2019 by the National Council on Measurement in Education",Automated Test Assembly with Mixed-Integer Programming: The Effects of Modeling Approaches and Solvers,"Automated test assembly (ATA) is a modern approach to test assembly that applies advanced optimization algorithms on computers to build test forms automatically. ATA greatly improves the efficiency and accuracy of the test assembly. This study investigated the effects of the modeling methods and solvers in the mixed-integer programming (MIP) approach to ATA in the context of assembling parallel linear test forms and multistage testing (MST) panels. The results of two simulation studies indicated that the newly proposed maximin modeling method significantly improved the parallelism of the test information functions (TIFs) among assembled test forms while maintaining relatively high overall TIFs, and the newly proposed binary minimax method considerably reduced the overall discrepancies from the targets. A comparison of four freely available noncommercial MIP solvers from the utilitarian, as opposed to the benchmarking, perspective was also included in this study. Â© 2019 by the National Council on Measurement in Education","['automate', 'test', 'assembly', 'ATA', 'modern', 'approach', 'test', 'assembly', 'apply', 'advanced', 'optimization', 'algorithm', 'computer', 'build', 'test', 'form', 'automatically', 'ATA', 'greatly', 'improve', 'efficiency', 'accuracy', 'test', 'assembly', 'study', 'investigate', 'effect', 'modeling', 'method', 'solver', 'mixedinteger', 'programming', 'MIP', 'approach', 'ATA', 'context', 'assemble', 'parallel', 'linear', 'test', 'form', 'multistage', 'testing', 'mst', 'panel', 'result', 'simulation', 'study', 'indicate', 'newly', 'propose', 'maximin', 'modeling', 'method', 'significantly', 'improve', 'parallelism', 'test', 'information', 'function', 'tif', 'assemble', 'test', 'form', 'maintain', 'relatively', 'high', 'overall', 'tif', 'newly', 'propose', 'binary', 'minimax', 'method', 'considerably', 'reduce', 'overall', 'discrepancy', 'target', 'comparison', 'freely', 'available', 'noncommercial', 'MIP', 'solver', 'utilitarian', 'oppose', 'benchmarking', 'perspective', 'include', 'study', 'Â©', '2019', 'National', 'Council']","['Automated', 'Test', 'Assembly', 'MixedInteger', 'Programming', 'Effects', 'Modeling', 'Approaches', 'solver']",automate test assembly ATA modern approach test assembly apply advanced optimization algorithm computer build test form automatically ATA greatly improve efficiency accuracy test assembly study investigate effect modeling method solver mixedinteger programming MIP approach ATA context assemble parallel linear test form multistage testing mst panel result simulation study indicate newly propose maximin modeling method significantly improve parallelism test information function tif assemble test form maintain relatively high overall tif newly propose binary minimax method considerably reduce overall discrepancy target comparison freely available noncommercial MIP solver utilitarian oppose benchmarking perspective include study Â© 2019 National Council,Automated Test Assembly MixedInteger Programming Effects Modeling Approaches solver,0.02459203,0.024215441,0.024247861,0.902726903,0.024217765,0,0.127737408,0,0,0.001337966
Chen Y.; Zhang J.; Yang Y.; Lee Y.-S.,Latent Space Model for Process Data,2022,59,"The development of human-computer interactive items in educational assessments provides opportunities to extract useful process information for problem-solving. However, the complex, intensive, and noisy nature of process data makes it challenging to model with the traditional psychometric methods. Social network methods have been appliedÂ to visualize and analyze process data. Nonetheless, research about statistical modeling of process information using social network methods is still limited. This article explored the application of the latent space model (LSM) for analyzing process data in educational assessment. The adjacent matrix of transitions between actions was created based on the weighted and directed network of action sequences and related auxiliary information. Then, the adjacent matrix was modeled with LSM to identify the lower-dimensional latent positions of actions. Three applications based on the results from LSM were introduced: action clustering, error analysis, and performance measurement. The simulation study showed that LSM can cluster actions from the same problem-solving strategy and measure students??performance by comparing their action sequences with the optimal strategy. Finally, we analyzed the empirical data from PISA 2012 as a real case scenario to illustrate how to use LSM. Â© 2022 by the National Council on Measurement in Education.",,"The development of human-computer interactive items in educational assessments provides opportunities to extract useful process information for problem-solving. However, the complex, intensive, and noisy nature of process data makes it challenging to model with the traditional psychometric methods. Social network methods have been appliedÂ to visualize and analyze process data. Nonetheless, research about statistical modeling of process information using social network methods is still limited. This article explored the application of the latent space model (LSM) for analyzing process data in educational assessment. The adjacent matrix of transitions between actions was created based on the weighted and directed network of action sequences and related auxiliary information. Then, the adjacent matrix was modeled with LSM to identify the lower-dimensional latent positions of actions. Three applications based on the results from LSM were introduced: action clustering, error analysis, and performance measurement. The simulation study showed that LSM can cluster actions from the same problem-solving strategy and measure students??performance by comparing their action sequences with the optimal strategy. Finally, we analyzed the empirical data from PISA 2012 as a real case scenario to illustrate how to use LSM. Â© 2022 by the National Council on Measurement in Education.","['development', 'humancomputer', 'interactive', 'item', 'educational', 'assessment', 'provide', 'opportunity', 'extract', 'useful', 'process', 'information', 'problemsolve', 'complex', 'intensive', 'noisy', 'nature', 'process', 'datum', 'challenge', 'traditional', 'psychometric', 'method', 'Social', 'network', 'method', 'apply', 'visualize', 'analyze', 'process', 'datum', 'nonetheless', 'research', 'statistical', 'modeling', 'process', 'information', 'social', 'network', 'method', 'limit', 'article', 'explore', 'application', 'latent', 'space', 'LSM', 'analyze', 'process', 'datum', 'educational', 'assessment', 'adjacent', 'matrix', 'transition', 'action', 'create', 'base', 'weighted', 'directed', 'network', 'action', 'sequence', 'relate', 'auxiliary', 'information', 'adjacent', 'matrix', 'LSM', 'identify', 'lowerdimensional', 'latent', 'position', 'action', 'application', 'base', 'result', 'LSM', 'introduce', 'action', 'cluster', 'error', 'analysis', 'performance', 'simulation', 'study', 'LSM', 'cluster', 'action', 'problemsolving', 'strategy', 'measure', 'student', '??, 'performance', 'compare', 'action', 'sequence', 'optimal', 'strategy', 'finally', 'analyze', 'empirical', 'datum', 'PISA', '2012', 'real', 'case', 'scenario', 'illustrate', 'LSM', 'Â©', '2022', 'National', 'Council']",,development humancomputer interactive item educational assessment provide opportunity extract useful process information problemsolve complex intensive noisy nature process datum challenge traditional psychometric method Social network method apply visualize analyze process datum nonetheless research statistical modeling process information social network method limit article explore application latent space LSM analyze process datum educational assessment adjacent matrix transition action create base weighted directed network action sequence relate auxiliary information adjacent matrix LSM identify lowerdimensional latent position action application base result LSM introduce action cluster error analysis performance simulation study LSM cluster action problemsolving strategy measure student ??performance compare action sequence optimal strategy finally analyze empirical datum PISA 2012 real case scenario illustrate LSM Â© 2022 National Council,,0.027477751,0.892399424,0.026466875,0.02679852,0.02685743,0.03066139,0.019327734,0.021442711,0.022717594,0
Li L.; Becker B.J.,Assessing Differential Bundle Functioning Using Meta-Analysis,2021,58,"Differential bundle functioning (DBF) has been proposed to quantify the accumulated amount of differential item functioning (DIF) in an item cluster/bundle (Douglas, Roussos, and Stout). The simultaneous item bias test (SIBTEST, Shealy and Stout) has been used to test for DBF (e.g., Walker, Zhang, and Surber). Research on DBF may have the potential to reveal the mechanism underlying DIF. However, an unresolved issue is the lack of an effect size for DBF, making it difficult to assess and compare the amounts of DBF within and between tests. We propose using meta-analysis techniques to study DBF. By meta-analyzing DIF indices, we can examine the heterogeneity of DIF across items, using the weighted average of DIF indices in an item bundle as a measure of effect size for DBF. A Monte Carlo simulation study compared the performance of our proposed effect size for DBF and a new test of nonzero average DIF in an item bundle with that of a DBF test using SIBTEST. When the primary and secondary dimensions were moderately correlated, our proposed effect size for DBF had little bias; the test of nonzero average DIF also showed power and Type I error levels comparable to those of the DBF test. Â© 2021 by the National Council on Measurement in Education",Assessing Differential Bundle Functioning Using Meta-Analysis,"Differential bundle functioning (DBF) has been proposed to quantify the accumulated amount of differential item functioning (DIF) in an item cluster/bundle (Douglas, Roussos, and Stout). The simultaneous item bias test (SIBTEST, Shealy and Stout) has been used to test for DBF (e.g., Walker, Zhang, and Surber). Research on DBF may have the potential to reveal the mechanism underlying DIF. However, an unresolved issue is the lack of an effect size for DBF, making it difficult to assess and compare the amounts of DBF within and between tests. We propose using meta-analysis techniques to study DBF. By meta-analyzing DIF indices, we can examine the heterogeneity of DIF across items, using the weighted average of DIF indices in an item bundle as a measure of effect size for DBF. A Monte Carlo simulation study compared the performance of our proposed effect size for DBF and a new test of nonzero average DIF in an item bundle with that of a DBF test using SIBTEST. When the primary and secondary dimensions were moderately correlated, our proposed effect size for DBF had little bias; the test of nonzero average DIF also showed power and Type I error levels comparable to those of the DBF test. Â© 2021 by the National Council on Measurement in Education","['differential', 'bundle', 'function', 'DBF', 'propose', 'quantify', 'accumulate', 'differential', 'item', 'function', 'DIF', 'item', 'clusterbundle', 'Douglas', 'Roussos', 'Stout', 'simultaneous', 'item', 'bias', 'test', 'SIBTEST', 'Shealy', 'Stout', 'test', 'DBF', 'eg', 'Walker', 'Zhang', 'Surber', 'Research', 'DBF', 'potential', 'reveal', 'mechanism', 'underlie', 'dif', 'unresolved', 'issue', 'lack', 'effect', 'size', 'DBF', 'difficult', 'assess', 'compare', 'DBF', 'test', 'propose', 'metaanalysis', 'technique', 'study', 'DBF', 'metaanalyze', 'DIF', 'index', 'examine', 'heterogeneity', 'dif', 'item', 'weighted', 'average', 'DIF', 'index', 'item', 'bundle', 'measure', 'effect', 'size', 'DBF', 'A', 'Monte', 'Carlo', 'simulation', 'study', 'compare', 'performance', 'propose', 'effect', 'size', 'DBF', 'new', 'test', 'nonzero', 'average', 'DIF', 'item', 'bundle', 'DBF', 'test', 'SIBTEST', 'primary', 'secondary', 'dimension', 'moderately', 'correlate', 'propose', 'effect', 'size', 'DBF', 'little', 'bias', 'test', 'nonzero', 'average', 'DIF', 'power', 'Type', 'I', 'error', 'level', 'comparable', 'DBF', 'test', 'Â©', '2021', 'National', 'Council']","['assess', 'Differential', 'Bundle', 'Functioning', 'MetaAnalysis']",differential bundle function DBF propose quantify accumulate differential item function DIF item clusterbundle Douglas Roussos Stout simultaneous item bias test SIBTEST Shealy Stout test DBF eg Walker Zhang Surber Research DBF potential reveal mechanism underlie dif unresolved issue lack effect size DBF difficult assess compare DBF test propose metaanalysis technique study DBF metaanalyze DIF index examine heterogeneity dif item weighted average DIF index item bundle measure effect size DBF A Monte Carlo simulation study compare performance propose effect size DBF new test nonzero average DIF item bundle DBF test SIBTEST primary secondary dimension moderately correlate propose effect size DBF little bias test nonzero average DIF power Type I error level comparable DBF test Â© 2021 National Council,assess Differential Bundle Functioning MetaAnalysis,0.032581717,0.032455453,0.032490086,0.869758979,0.032713765,0,0,0,0,0.187768698
Casabianca J.M.; Donoghue J.R.; Shin H.J.; Chao S.-F.; Choi I.,Using Linkage Sets to Improve Connectedness in Rater Response Model Estimation,2023,60,"Using item-response theory to model rater effects provides an alternative solution for rater monitoring and diagnosis, compared to using standard performance metrics. In order to fit such models, the ratings data must be sufficiently connected in order to estimate rater effects. Due to popular rating designs used in large-scale testing scenarios, there tends to be a large proportion of missing data, yielding sparse matrices and estimation issues. In this article, we explore the impact of different types of connectedness, or linkage, brought about by using a linkage set?”a collection of responses scored by most or all raters. We also explore the impact of the properties and composition of the linkage set, the different connectedness yielded from different rating designs, and the role of scores from automated scoring engines. In designing monitoring systems using the rater response version of the generalized partial credit model, the study results suggest use of a linkage set, especially a large one that is comprised of responses representing the full score scale. Results also show that a double-human-scoring design provides more connectedness than a design with one human and an automated scoring engine. Furthermore, scores from automated scoring engines do not provide adequate connectedness. We discuss considerations for operational implementation and further study. Â© 2023 by the National Council on Measurement in Education.",Using Linkage Sets to Improve Connectedness in Rater Response Model Estimation,"Using item-response theory to model rater effects provides an alternative solution for rater monitoring and diagnosis, compared to using standard performance metrics. In order to fit such models, the ratings data must be sufficiently connected in order to estimate rater effects. Due to popular rating designs used in large-scale testing scenarios, there tends to be a large proportion of missing data, yielding sparse matrices and estimation issues. In this article, we explore the impact of different types of connectedness, or linkage, brought about by using a linkage set?”a collection of responses scored by most or all raters. We also explore the impact of the properties and composition of the linkage set, the different connectedness yielded from different rating designs, and the role of scores from automated scoring engines. In designing monitoring systems using the rater response version of the generalized partial credit model, the study results suggest use of a linkage set, especially a large one that is comprised of responses representing the full score scale. Results also show that a double-human-scoring design provides more connectedness than a design with one human and an automated scoring engine. Furthermore, scores from automated scoring engines do not provide adequate connectedness. We discuss considerations for operational implementation and further study. Â© 2023 by the National Council on Measurement in Education.","['itemresponse', 'theory', 'rater', 'effect', 'provide', 'alternative', 'solution', 'rater', 'monitoring', 'diagnosis', 'compare', 'standard', 'performance', 'metric', 'order', 'fit', 'rating', 'datum', 'sufficiently', 'connect', 'order', 'estimate', 'rater', 'effect', 'popular', 'rating', 'design', 'largescale', 'testing', 'scenario', 'tend', 'large', 'proportion', 'miss', 'datum', 'yield', 'sparse', 'matrix', 'estimation', 'issue', 'article', 'explore', 'impact', 'different', 'type', 'connectedness', 'linkage', 'bring', 'linkage', 'set', '??, 'collection', 'response', 'score', 'rater', 'explore', 'impact', 'property', 'composition', 'linkage', 'set', 'different', 'connectedness', 'yield', 'different', 'rating', 'design', 'role', 'score', 'automate', 'scoring', 'engine', 'design', 'monitoring', 'system', 'rater', 'response', 'version', 'generalized', 'partial', 'credit', 'study', 'result', 'suggest', 'linkage', 'set', 'especially', 'large', 'comprise', 'response', 'represent', 'score', 'scale', 'result', 'doublehumanscoring', 'design', 'provide', 'connectedness', 'design', 'human', 'automate', 'scoring', 'engine', 'furthermore', 'score', 'automate', 'scoring', 'engine', 'provide', 'adequate', 'connectedness', 'discuss', 'consideration', 'operational', 'implementation', 'study', 'Â©', '2023', 'National', 'Council']","['linkage', 'set', 'improve', 'Connectedness', 'Rater', 'Response', 'Estimation']",itemresponse theory rater effect provide alternative solution rater monitoring diagnosis compare standard performance metric order fit rating datum sufficiently connect order estimate rater effect popular rating design largescale testing scenario tend large proportion miss datum yield sparse matrix estimation issue article explore impact different type connectedness linkage bring linkage set ??collection response score rater explore impact property composition linkage set different connectedness yield different rating design role score automate scoring engine design monitoring system rater response version generalized partial credit study result suggest linkage set especially large comprise response represent score scale result doublehumanscoring design provide connectedness design human automate scoring engine furthermore score automate scoring engine provide adequate connectedness discuss consideration operational implementation study Â© 2023 National Council,linkage set improve Connectedness Rater Response Estimation,0.02596551,0.896308385,0.025686586,0.026054328,0.025985191,0.001574861,0,0.010341245,0.175189953,0
Bolt D.M.; Liao X.,On the Positive Correlation between DIF and Difficulty: A New Theory on the Correlation as Methodological Artifact,2021,58,"We revisit the empirically observed positive correlation between DIF and difficulty studied by Freedle and commonly seen in tests of verbal proficiency when comparing populations of different mean latent proficiency levels. It is shown that a positive correlation between DIF and difficulty estimates is actually an expected result (absent any true DIF) in the presence of systematically negative ICC asymmetry. Under such conditions, conditional upon sum score, correct responses to easier items are indicative of higher latent proficiency than correct responses to more difficult items. Negative ICC asymmetry, which can occur as a result of disjunctively interacting latent subprocesses (including disjunctive combinations of proficiency-based guessing and problem-solving processes, for example), is suggested to be widely present among items used in many verbal tests due to their general lack of problem-solving complexity and the anticipated presence of proficiency-related guessing. When systematic, negative ICC asymmetries result in a shrinkage of units at the lower end of the IRT metric when traditional symmetric IRT models are applied. We demonstrate by simulation an ensuing DIF-difficulty correlation artifact in application of a model-based DIF approach, as well as for the Mantel-Haenszel and Standardization DIF detection methods. A sensitivity analysis illustration demonstrates how assuming a fixed level of negative asymmetry in the ICCs effectively makes an observed positive correlation between DIF and difficulty seen when fitting symmetric ICCs go away. Â© 2021 by the National Council on Measurement in Education",On the Positive Correlation between DIF and Difficulty: A New Theory on the Correlation as Methodological Artifact,"We revisit the empirically observed positive correlation between DIF and difficulty studied by Freedle and commonly seen in tests of verbal proficiency when comparing populations of different mean latent proficiency levels. It is shown that a positive correlation between DIF and difficulty estimates is actually an expected result (absent any true DIF) in the presence of systematically negative ICC asymmetry. Under such conditions, conditional upon sum score, correct responses to easier items are indicative of higher latent proficiency than correct responses to more difficult items. Negative ICC asymmetry, which can occur as a result of disjunctively interacting latent subprocesses (including disjunctive combinations of proficiency-based guessing and problem-solving processes, for example), is suggested to be widely present among items used in many verbal tests due to their general lack of problem-solving complexity and the anticipated presence of proficiency-related guessing. When systematic, negative ICC asymmetries result in a shrinkage of units at the lower end of the IRT metric when traditional symmetric IRT models are applied. We demonstrate by simulation an ensuing DIF-difficulty correlation artifact in application of a model-based DIF approach, as well as for the Mantel-Haenszel and Standardization DIF detection methods. A sensitivity analysis illustration demonstrates how assuming a fixed level of negative asymmetry in the ICCs effectively makes an observed positive correlation between DIF and difficulty seen when fitting symmetric ICCs go away. Â© 2021 by the National Council on Measurement in Education","['revisit', 'empirically', 'observe', 'positive', 'correlation', 'DIF', 'difficulty', 'study', 'Freedle', 'commonly', 'test', 'verbal', 'proficiency', 'compare', 'population', 'different', 'mean', 'latent', 'proficiency', 'level', 'positive', 'correlation', 'DIF', 'difficulty', 'estimate', 'actually', 'expected', 'result', 'absent', 'true', 'DIF', 'presence', 'systematically', 'negative', 'ICC', 'asymmetry', 'condition', 'conditional', 'sum', 'score', 'correct', 'response', 'easy', 'item', 'indicative', 'high', 'latent', 'proficiency', 'correct', 'response', 'difficult', 'item', 'negative', 'ICC', 'asymmetry', 'occur', 'result', 'disjunctively', 'interact', 'latent', 'subprocesse', 'include', 'disjunctive', 'combination', 'proficiencybased', 'guessing', 'problemsolving', 'process', 'example', 'suggest', 'widely', 'present', 'item', 'verbal', 'test', 'general', 'lack', 'problemsolve', 'complexity', 'anticipated', 'presence', 'proficiencyrelate', 'guess', 'systematic', 'negative', 'ICC', 'asymmetry', 'result', 'shrinkage', 'unit', 'low', 'end', 'IRT', 'metric', 'traditional', 'symmetric', 'IRT', 'apply', 'demonstrate', 'simulation', 'ensue', 'difdifficulty', 'correlation', 'artifact', 'application', 'modelbase', 'dif', 'approach', 'MantelHaenszel', 'Standardization', 'DIF', 'detection', 'method', 'sensitivity', 'analysis', 'illustration', 'demonstrate', 'assume', 'fix', 'level', 'negative', 'asymmetry', 'icc', 'effectively', 'observed', 'positive', 'correlation', 'DIF', 'difficulty', 'fitting', 'symmetric', 'icc', 'away', 'Â©', '2021', 'National', 'Council']","['Positive', 'Correlation', 'DIF', 'Difficulty', 'New', 'Theory', 'Correlation', 'Methodological', 'Artifact']",revisit empirically observe positive correlation DIF difficulty study Freedle commonly test verbal proficiency compare population different mean latent proficiency level positive correlation DIF difficulty estimate actually expected result absent true DIF presence systematically negative ICC asymmetry condition conditional sum score correct response easy item indicative high latent proficiency correct response difficult item negative ICC asymmetry occur result disjunctively interact latent subprocesse include disjunctive combination proficiencybased guessing problemsolving process example suggest widely present item verbal test general lack problemsolve complexity anticipated presence proficiencyrelate guess systematic negative ICC asymmetry result shrinkage unit low end IRT metric traditional symmetric IRT apply demonstrate simulation ensue difdifficulty correlation artifact application modelbase dif approach MantelHaenszel Standardization DIF detection method sensitivity analysis illustration demonstrate assume fix level negative asymmetry icc effectively observed positive correlation DIF difficulty fitting symmetric icc away Â© 2021 National Council,Positive Correlation DIF Difficulty New Theory Correlation Methodological Artifact,0.023828,0.90562344,0.023329206,0.023755662,0.023463692,0.008717309,0,0,0.000920647,0.164981348
Guo W.; Wind S.A.,Examining the Impacts of Ignoring Rater Effects in Mixed-Format Tests,2021,58,"The use of mixed-format tests made up of multiple-choice (MC) items and constructed response (CR) items is popular in large-scale testing programs, including the National Assessment of Educational Progress (NAEP) and many district- and state-level assessments in the United States. Rater effects, or raters??scoring tendencies that result in performances receiving different scores than are warranted given their quality, are concerns for the interpretation of scores on CR items. However, there are few published studies in which researchers have systematically considered the impact of ignoring rater effects when they are present on estimates of student ability using large-scale mixed-format assessments. Using results from an analysis of NAEP data, we systematically explored the impacts of rater effects on student achievement estimates. Our results suggest that in conditions that reflect many large-scale mixed-format assessments, directly modeling rater effects yields more accurate student achievement estimates than estimation procedures that do not incorporate raters. We consider the implications of our findings for research and practice. Â© 2021 by the National Council on Measurement in Education",Examining the Impacts of Ignoring Rater Effects in Mixed-Format Tests,"The use of mixed-format tests made up of multiple-choice (MC) items and constructed response (CR) items is popular in large-scale testing programs, including the National Assessment of Educational Progress (NAEP) and many district- and state-level assessments in the United States. Rater effects, or raters??scoring tendencies that result in performances receiving different scores than are warranted given their quality, are concerns for the interpretation of scores on CR items. However, there are few published studies in which researchers have systematically considered the impact of ignoring rater effects when they are present on estimates of student ability using large-scale mixed-format assessments. Using results from an analysis of NAEP data, we systematically explored the impacts of rater effects on student achievement estimates. Our results suggest that in conditions that reflect many large-scale mixed-format assessments, directly modeling rater effects yields more accurate student achievement estimates than estimation procedures that do not incorporate raters. We consider the implications of our findings for research and practice. Â© 2021 by the National Council on Measurement in Education","['mixedformat', 'test', 'multiplechoice', 'MC', 'item', 'construct', 'response', 'CR', 'item', 'popular', 'largescale', 'testing', 'program', 'include', 'National', 'Assessment', 'Educational', 'Progress', 'naep', 'district', 'statelevel', 'assessment', 'United', 'States', 'Rater', 'effect', 'rater', ""'"", 'scoring', 'tendency', 'result', 'performance', 'receive', 'different', 'score', 'warrant', 'quality', 'concern', 'interpretation', 'score', 'CR', 'item', 'publish', 'study', 'researcher', 'systematically', 'consider', 'impact', 'ignore', 'rater', 'effect', 'present', 'estimate', 'student', 'ability', 'largescale', 'mixedformat', 'assessment', 'result', 'analysis', 'naep', 'datum', 'systematically', 'explore', 'impact', 'rater', 'effect', 'student', 'achievement', 'estimate', 'result', 'suggest', 'condition', 'reflect', 'largescale', 'mixedformat', 'assessment', 'directly', 'rater', 'effect', 'yield', 'accurate', 'student', 'achievement', 'estimate', 'estimation', 'procedure', 'incorporate', 'rater', 'consider', 'implication', 'finding', 'research', 'practice', 'Â©', '2021', 'National', 'Council']","['examine', 'impact', 'ignore', 'Rater', 'Effects', 'MixedFormat', 'test']",mixedformat test multiplechoice MC item construct response CR item popular largescale testing program include National Assessment Educational Progress naep district statelevel assessment United States Rater effect rater ' scoring tendency result performance receive different score warrant quality concern interpretation score CR item publish study researcher systematically consider impact ignore rater effect present estimate student ability largescale mixedformat assessment result analysis naep datum systematically explore impact rater effect student achievement estimate result suggest condition reflect largescale mixedformat assessment directly rater effect yield accurate student achievement estimate estimation procedure incorporate rater consider implication finding research practice Â© 2021 National Council,examine impact ignore Rater Effects MixedFormat test,0.027826935,0.027526631,0.027536425,0.028422619,0.888687389,0.016083135,0.01271766,0,0.155100384,0
Choe E.M.; Han K.C.T.,Constructing a Robust Score Scale from IRT Scores with Informed Boundaries,2022,59,"In operational testing, item response theory (IRT) models for dichotomous responses are popular for measuring a single latent construct (Formula presented.), such as cognitive ability in a content domain. Estimates of (Formula presented.), also called IRT scores or (Formula presented.), can be computed using estimators based on the likelihood function, such as maximum likelihood (ML), weighted likelihood (WL), maximum a posteriori (MAP), and expected a posteriori (EAP). Although the parameter space of (Formula presented.) is theoretically unrestricted, the range of finite (Formula presented.) is constrained by the estimator and test form properties, which is important to consider but often overlooked when developing a score scale for reporting purposes. Irrespective of the estimator or test forms at hand, a common practice is to fix arbitrary points symmetric about zero (e.g., ?? and 4) as anchors for deriving a score transformation, possibly resulting in unintended gaps or truncations at the extremes. Therefore, a systematic framework is proposed for using IRT scores to construct a robust score scale with informed boundaries that are logical and consistent across testÂ forms. Â© 2022 by the National Council on Measurement in Education.",Constructing a Robust Score Scale from IRT Scores with Informed Boundaries,"In operational testing, item response theory (IRT) models for dichotomous responses are popular for measuring a single latent construct (Formula presented.), such as cognitive ability in a content domain. Estimates of (Formula presented.), also called IRT scores or (Formula presented.), can be computed using estimators based on the likelihood function, such as maximum likelihood (ML), weighted likelihood (WL), maximum a posteriori (MAP), and expected a posteriori (EAP). Although the parameter space of (Formula presented.) is theoretically unrestricted, the range of finite (Formula presented.) is constrained by the estimator and test form properties, which is important to consider but often overlooked when developing a score scale for reporting purposes. Irrespective of the estimator or test forms at hand, a common practice is to fix arbitrary points symmetric about zero (e.g., ?? and 4) as anchors for deriving a score transformation, possibly resulting in unintended gaps or truncations at the extremes. Therefore, a systematic framework is proposed for using IRT scores to construct a robust score scale with informed boundaries that are logical and consistent across testÂ forms. Â© 2022 by the National Council on Measurement in Education.","['operational', 'testing', 'item', 'response', 'theory', 'IRT', 'dichotomous', 'response', 'popular', 'measure', 'single', 'latent', 'construct', 'Formula', 'present', 'cognitive', 'ability', 'content', 'domain', 'estimate', 'Formula', 'present', 'IRT', 'score', 'Formula', 'present', 'compute', 'estimator', 'base', 'likelihood', 'function', 'maximum', 'likelihood', 'ML', 'weight', 'likelihood', 'WL', 'maximum', 'posteriori', 'MAP', 'expect', 'posteriori', 'EAP', 'parameter', 'space', 'Formula', 'present', 'theoretically', 'unrestricte', 'range', 'finite', 'Formula', 'present', 'constrain', 'estimator', 'test', 'form', 'property', 'important', 'consider', 'overlook', 'develop', 'score', 'scale', 'report', 'purpose', 'irrespective', 'estimator', 'test', 'form', 'hand', 'common', 'practice', 'fix', 'arbitrary', 'point', 'symmetric', 'zero', 'eg', '??', '4', 'anchor', 'derive', 'score', 'transformation', 'possibly', 'result', 'unintended', 'gap', 'truncation', 'extreme', 'systematic', 'framework', 'propose', 'IRT', 'score', 'construct', 'robust', 'score', 'scale', 'informed', 'boundary', 'logical', 'consistent', 'test', 'form', 'Â©', '2022', 'National', 'Council']","['construct', 'Robust', 'Score', 'Scale', 'IRT', 'score', 'Informed', 'Boundaries']",operational testing item response theory IRT dichotomous response popular measure single latent construct Formula present cognitive ability content domain estimate Formula present IRT score Formula present compute estimator base likelihood function maximum likelihood ML weight likelihood WL maximum posteriori MAP expect posteriori EAP parameter space Formula present theoretically unrestricte range finite Formula present constrain estimator test form property important consider overlook develop score scale report purpose irrespective estimator test form hand common practice fix arbitrary point symmetric zero eg ?? 4 anchor derive score transformation possibly result unintended gap truncation extreme systematic framework propose IRT score construct robust score scale informed boundary logical consistent test form Â© 2022 National Council,construct Robust Score Scale IRT score Informed Boundaries,0.023336467,0.023267472,0.023156492,0.906824119,0.02341545,0.007824493,0.06441555,0,0.019397456,0.080540622
Frey A.; KÃ¶nig C.; Fink A.,A Highly Adaptive Testing Design for PISA,2023,,"The highly adaptive testing (HAT) design is introduced as an alternative test design for the Programme for International Student Assessment (PISA). The principle of HAT is to be as adaptive as possible when selecting items while accounting for PISA's nonstatistical constraints and addressing issues concerning PISA such as item position effects. HAT combines established methods from the field of computerized adaptive testing. It is implemented in R and code is provided. HAT was compared to the PISA 2018 multistage design (MST) in a simulation study based on a factorial design with the independent variables response probability (RP;.50,.62), item pool optimality (PISA 2018, optimal), and ability level (low, medium, high). PISA-specific conditions regarding sample size, missing responses, and nonstatistical constraints were implemented. HAT clearly outperformed MST regarding test information, RMSE, and constraint management across ability groups but it showed slightly weaker item exposure. Raising RP to.62 did not decrease test information much and is therefore a viable option to foster students??test-taking experience with HAT. Test information for HAT was up to three times higher than for MST when using a hypothetical optimal item pool. Summarizing, HAT proved to be a promising and applicable test design for PISA. Â© 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,"The highly adaptive testing (HAT) design is introduced as an alternative test design for the Programme for International Student Assessment (PISA). The principle of HAT is to be as adaptive as possible when selecting items while accounting for PISA's nonstatistical constraints and addressing issues concerning PISA such as item position effects. HAT combines established methods from the field of computerized adaptive testing. It is implemented in R and code is provided. HAT was compared to the PISA 2018 multistage design (MST) in a simulation study based on a factorial design with the independent variables response probability (RP;.50,.62), item pool optimality (PISA 2018, optimal), and ability level (low, medium, high). PISA-specific conditions regarding sample size, missing responses, and nonstatistical constraints were implemented. HAT clearly outperformed MST regarding test information, RMSE, and constraint management across ability groups but it showed slightly weaker item exposure. Raising RP to.62 did not decrease test information much and is therefore a viable option to foster students??test-taking experience with HAT. Test information for HAT was up to three times higher than for MST when using a hypothetical optimal item pool. Summarizing, HAT proved to be a promising and applicable test design for PISA. Â© 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['highly', 'adaptive', 'testing', 'HAT', 'design', 'introduce', 'alternative', 'test', 'design', 'Programme', 'International', 'Student', 'Assessment', 'PISA', 'principle', 'HAT', 'adaptive', 'possible', 'select', 'item', 'account', 'PISAs', 'nonstatistical', 'constraint', 'address', 'issue', 'concern', 'PISA', 'item', 'position', 'effect', 'HAT', 'combine', 'establish', 'method', 'field', 'computerized', 'adaptive', 'testing', 'implement', 'R', 'code', 'provide', 'HAT', 'compare', 'PISA', '2018', 'multistage', 'design', 'mst', 'simulation', 'study', 'base', 'factorial', 'design', 'independent', 'variable', 'response', 'probability', 'RP5062', 'item', 'pool', 'optimality', 'PISA', '2018', 'optimal', 'ability', 'level', 'low', 'medium', 'high', 'pisaspecific', 'condition', 'regard', 'sample', 'size', 'miss', 'response', 'nonstatistical', 'constraint', 'implement', 'HAT', 'clearly', 'outperform', 'MST', 'regard', 'test', 'information', 'RMSE', 'constraint', 'management', 'ability', 'group', 'slightly', 'weak', 'item', 'exposure', 'raise', 'rp', 'to62', 'decrease', 'test', 'information', 'viable', 'option', 'foster', 'student', ""'"", 'testtaking', 'experience', 'HAT', 'Test', 'information', 'HAT', 'time', 'high', 'MST', 'hypothetical', 'optimal', 'item', 'pool', 'Summarizing', 'HAT', 'prove', 'promising', 'applicable', 'test', 'design', 'PISA', 'Â©', '2023', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']",,highly adaptive testing HAT design introduce alternative test design Programme International Student Assessment PISA principle HAT adaptive possible select item account PISAs nonstatistical constraint address issue concern PISA item position effect HAT combine establish method field computerized adaptive testing implement R code provide HAT compare PISA 2018 multistage design mst simulation study base factorial design independent variable response probability RP5062 item pool optimality PISA 2018 optimal ability level low medium high pisaspecific condition regard sample size miss response nonstatistical constraint implement HAT clearly outperform MST regard test information RMSE constraint management ability group slightly weak item exposure raise rp to62 decrease test information viable option foster student ' testtaking experience HAT Test information HAT time high MST hypothetical optimal item pool Summarizing HAT prove promising applicable test design PISA Â© 2023 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,,0.90241438,0.024206948,0.024208741,0.024637527,0.024532404,0.030292594,0.099428026,0,0,0
Sinharay S.,Measuring the Uncertainty of Imputed Scores,2023,60,"Technical difficulties and other unforeseen events occasionally lead to incomplete data on educational tests, which necessitates the reporting of imputed scores to some examinees. While there exist several approaches for reporting imputed scores, there is a lack of any guidance on the reporting of the uncertainty of imputed scores. In this paper, several approaches are suggested for quantifying the uncertainty of imputed scores using measures that are similar in spirit to estimates of reliability and standard error of measurement. A simulation study is performed to examine the properties of the approaches. The approaches are then applied to data from a state test on which some examinees' scores had to be imputed following computer problems. Several recommendations are made forÂ practice. Â© 2022 by the National Council on Measurement in Education.",,"Technical difficulties and other unforeseen events occasionally lead to incomplete data on educational tests, which necessitates the reporting of imputed scores to some examinees. While there exist several approaches for reporting imputed scores, there is a lack of any guidance on the reporting of the uncertainty of imputed scores. In this paper, several approaches are suggested for quantifying the uncertainty of imputed scores using measures that are similar in spirit to estimates of reliability and standard error of measurement. A simulation study is performed to examine the properties of the approaches. The approaches are then applied to data from a state test on which some examinees' scores had to be imputed following computer problems. Several recommendations are made forÂ practice. Â© 2022 by the National Council on Measurement in Education.","['technical', 'difficulty', 'unforeseen', 'event', 'occasionally', 'lead', 'incomplete', 'datum', 'educational', 'test', 'necessitate', 'reporting', 'impute', 'score', 'examinee', 'exist', 'approach', 'report', 'impute', 'score', 'lack', 'guidance', 'reporting', 'uncertainty', 'impute', 'score', 'paper', 'approach', 'suggest', 'quantify', 'uncertainty', 'impute', 'score', 'measure', 'similar', 'spirit', 'estimate', 'reliability', 'standard', 'error', 'simulation', 'study', 'perform', 'examine', 'property', 'approach', 'approach', 'apply', 'datum', 'state', 'test', 'examine', 'score', 'impute', 'follow', 'computer', 'problem', 'recommendation', 'practice', 'Â©', '2022', 'National', 'Council']",,technical difficulty unforeseen event occasionally lead incomplete datum educational test necessitate reporting impute score examinee exist approach report impute score lack guidance reporting uncertainty impute score paper approach suggest quantify uncertainty impute score measure similar spirit estimate reliability standard error simulation study perform examine property approach approach apply datum state test examine score impute follow computer problem recommendation practice Â© 2022 National Council,,0.033415547,0.033081647,0.866663149,0.033475961,0.033363696,0,0.062348079,0.023998007,0.03324906,0
Shear B.R.,"Gender Bias in Test Item Formats: Evidence from PISA 2009, 2012, and 2015 Math and Reading Tests",2023,60,"Large-scale standardized tests are regularly used to measure student achievement overall and for student subgroups. These uses assume tests provide comparable measures of outcomes across student subgroups, but prior research suggests score comparisons across gender groups may be complicated by the type of test items used. This paper presents evidence that among nationally representative samples of 15-year-olds in the United States participating in the 2009, 2012, and 2015 PISA math and reading tests, there are consistent item format by gender differences. On average, male students answer multiple-choice items correctly relatively more often and female students answer constructed-response items correctly relatively more often. These patterns were consistent across 34 additional participating PISA jurisdictions, although the size of the format differences varied and were larger on average in reading than math. The average magnitude of the format differences is not large enough to be flagged in routine differential item functioning analyses intended to detect test bias but is large enough to raise questions about the validity of inferences based on comparisons of scores across gender groups. Researchers and other test users should account for test item format, particularly when comparing scores across gender groups. Â© 2023 by the National Council on Measurement in Education.","Gender Bias in Test Item Formats: Evidence from PISA 2009, 2012, and 2015 Math and Reading Tests","Large-scale standardized tests are regularly used to measure student achievement overall and for student subgroups. These uses assume tests provide comparable measures of outcomes across student subgroups, but prior research suggests score comparisons across gender groups may be complicated by the type of test items used. This paper presents evidence that among nationally representative samples of 15-year-olds in the United States participating in the 2009, 2012, and 2015 PISA math and reading tests, there are consistent item format by gender differences. On average, male students answer multiple-choice items correctly relatively more often and female students answer constructed-response items correctly relatively more often. These patterns were consistent across 34 additional participating PISA jurisdictions, although the size of the format differences varied and were larger on average in reading than math. The average magnitude of the format differences is not large enough to be flagged in routine differential item functioning analyses intended to detect test bias but is large enough to raise questions about the validity of inferences based on comparisons of scores across gender groups. Researchers and other test users should account for test item format, particularly when comparing scores across gender groups. Â© 2023 by the National Council on Measurement in Education.","['Largescale', 'standardized', 'test', 'regularly', 'measure', 'student', 'achievement', 'overall', 'student', 'subgroup', 'assume', 'test', 'provide', 'comparable', 'measure', 'outcome', 'student', 'subgroup', 'prior', 'research', 'suggest', 'score', 'comparison', 'gender', 'group', 'complicate', 'type', 'test', 'item', 'paper', 'present', 'evidence', 'nationally', 'representative', 'sample', '15yearolds', 'United', 'States', 'participate', '2009', '2012', '2015', 'PISA', 'math', 'reading', 'test', 'consistent', 'item', 'format', 'gender', 'difference', 'average', 'male', 'student', 'answer', 'multiplechoice', 'item', 'correctly', 'relatively', 'female', 'student', 'answer', 'constructedresponse', 'item', 'correctly', 'relatively', 'pattern', 'consistent', '34', 'additional', 'participate', 'PISA', 'jurisdiction', 'size', 'format', 'difference', 'varied', 'large', 'average', 'reading', 'math', 'average', 'magnitude', 'format', 'difference', 'large', 'flag', 'routine', 'differential', 'item', 'function', 'analysis', 'intend', 'detect', 'test', 'bias', 'large', 'raise', 'question', 'validity', 'inference', 'base', 'comparison', 'score', 'gender', 'group', 'Researchers', 'test', 'user', 'account', 'test', 'item', 'format', 'particularly', 'compare', 'score', 'gender', 'group', 'Â©', '2023', 'National', 'Council']","['Gender', 'Bias', 'Test', 'Item', 'Formats', 'Evidence', 'PISA', '2009', '2012', '2015', 'Math', 'Reading', 'Tests']",Largescale standardized test regularly measure student achievement overall student subgroup assume test provide comparable measure outcome student subgroup prior research suggest score comparison gender group complicate type test item paper present evidence nationally representative sample 15yearolds United States participate 2009 2012 2015 PISA math reading test consistent item format gender difference average male student answer multiplechoice item correctly relatively female student answer constructedresponse item correctly relatively pattern consistent 34 additional participate PISA jurisdiction size format difference varied large average reading math average magnitude format difference large flag routine differential item function analysis intend detect test bias large raise question validity inference base comparison score gender group Researchers test user account test item format particularly compare score gender group Â© 2023 National Council,Gender Bias Test Item Formats Evidence PISA 2009 2012 2015 Math Reading Tests,0.024628336,0.023402048,0.903981684,0.023916219,0.024071713,0.016091386,0.079367425,0.003644616,0.017267059,0.03135153
He Y.,An Exponentially Weighted Moving Average Procedure for Detecting Back Random Responding Behavior,2023,60,"Back random responding (BRR) behavior is one of the commonly observed careless response behaviors. Accurately detecting BRR behavior can improve test validities. Yu and Cheng (2019) showed that the change point analysis (CPA) procedure based on weighted residual (CPA-WR) performed well in detecting BRR. Compared with the CPA procedure, the exponentially weighted moving average (EWMA) obtains more detailed information. This study equipped the weighted residual statistic with EWMA, and proposed the EWMA-WR method to detect BRR. To make the critical values adaptive to the ability levels, this study proposed the Monte Carlo simulation with ability stratification (MC-stratification) method for calculating critical values. Compared to the original Monte Carlo simulation (MC) method, the newly proposed MC-stratification method generated a larger number of satisfactory results. The performances of CPA-WR and EWMA-WR were evaluated under different conditions that varied in the test lengths, abnormal proportions, critical values and smoothing constants used in the EWMA-WR method. The results showed that EWMA-WR was more powerful than CPA-WR in detecting BRR. Moreover, an empirical study was conducted to illustrate the utility of EWMA-WR for detecting BRR. Â© 2022 by the National Council on Measurement in Education.",An Exponentially Weighted Moving Average Procedure for Detecting Back Random Responding Behavior,"Back random responding (BRR) behavior is one of the commonly observed careless response behaviors. Accurately detecting BRR behavior can improve test validities. Yu and Cheng (2019) showed that the change point analysis (CPA) procedure based on weighted residual (CPA-WR) performed well in detecting BRR. Compared with the CPA procedure, the exponentially weighted moving average (EWMA) obtains more detailed information. This study equipped the weighted residual statistic with EWMA, and proposed the EWMA-WR method to detect BRR. To make the critical values adaptive to the ability levels, this study proposed the Monte Carlo simulation with ability stratification (MC-stratification) method for calculating critical values. Compared to the original Monte Carlo simulation (MC) method, the newly proposed MC-stratification method generated a larger number of satisfactory results. The performances of CPA-WR and EWMA-WR were evaluated under different conditions that varied in the test lengths, abnormal proportions, critical values and smoothing constants used in the EWMA-WR method. The results showed that EWMA-WR was more powerful than CPA-WR in detecting BRR. Moreover, an empirical study was conducted to illustrate the utility of EWMA-WR for detecting BRR. Â© 2022 by the National Council on Measurement in Education.","['random', 'respond', 'BRR', 'behavior', 'commonly', 'observe', 'careless', 'response', 'behavior', 'accurately', 'detect', 'BRR', 'behavior', 'improve', 'test', 'validity', 'Yu', 'Cheng', '2019', 'change', 'point', 'analysis', 'cpa', 'procedure', 'base', 'weight', 'residual', 'cpawr', 'perform', 'detect', 'BRR', 'compare', 'cpa', 'procedure', 'exponentially', 'weight', 'average', 'ewma', 'obtain', 'detailed', 'information', 'study', 'equip', 'weighted', 'residual', 'statistic', 'ewma', 'propose', 'ewmawr', 'method', 'detect', 'BRR', 'critical', 'value', 'adaptive', 'ability', 'level', 'study', 'propose', 'Monte', 'Carlo', 'simulation', 'ability', 'stratification', 'MCstratification', 'method', 'calculate', 'critical', 'value', 'compare', 'original', 'Monte', 'Carlo', 'simulation', 'MC', 'method', 'newly', 'propose', 'MCstratification', 'method', 'generate', 'large', 'number', 'satisfactory', 'result', 'performance', 'cpawr', 'EWMAWR', 'evaluate', 'different', 'condition', 'vary', 'test', 'length', 'abnormal', 'proportion', 'critical', 'value', 'smooth', 'constant', 'ewmawr', 'method', 'result', 'EWMAWR', 'powerful', 'CPAWR', 'detect', 'BRR', 'Moreover', 'empirical', 'study', 'conduct', 'illustrate', 'utility', 'EWMAWR', 'detect', 'BRR', 'Â©', '2022', 'National', 'Council']","['exponentially', 'Weighted', 'Moving', 'average', 'Procedure', 'detect', 'Random', 'Responding', 'Behavior']",random respond BRR behavior commonly observe careless response behavior accurately detect BRR behavior improve test validity Yu Cheng 2019 change point analysis cpa procedure base weight residual cpawr perform detect BRR compare cpa procedure exponentially weight average ewma obtain detailed information study equip weighted residual statistic ewma propose ewmawr method detect BRR critical value adaptive ability level study propose Monte Carlo simulation ability stratification MCstratification method calculate critical value compare original Monte Carlo simulation MC method newly propose MCstratification method generate large number satisfactory result performance cpawr EWMAWR evaluate different condition vary test length abnormal proportion critical value smooth constant ewmawr method result EWMAWR powerful CPAWR detect BRR Moreover empirical study conduct illustrate utility EWMAWR detect BRR Â© 2022 National Council,exponentially Weighted Moving average Procedure detect Random Responding Behavior,0.891991326,0.026862525,0.026926879,0.027358122,0.026861149,0.017392048,0.035675818,0.041982045,0,0.026593166
Combs A.,A New Bayesian Person-Fit Analysis Method Using Pivotal Discrepancy Measures,2023,60,"A common method of checking person-fit in Bayesian item response theory (IRT) is the posterior-predictive (PP) method. In recent years, more powerful approaches have been proposed that areÂ based on resampling methods using the popular (Formula presented.) statistic. There has also been proposed a new Bayesian model checking method based on pivotal discrepancy measures (PDMs). A PDM T is a discrepancy measure that is a pivotal quantity with a known reference distribution. A posterior sample of T can be generated using standard Markov chain Monte Carlo output, and a p-value is obtained from probability bounds computed on order statistics of the sample. In this paper, we propose a general procedure to apply this PDM method to person-fit checking in IRT models. We illustrate this using the (Formula presented.) and (Formula presented.) measures. Simulation studies are done comparing these with the PP method and one of the more recent resampling methods. The results show that the PDM method is more powerful than the PP method. Under certain conditions, it is more powerful than the resampling method, while in others, it is less. The PDM method is also applied to a real dataÂ set. Â© 2022 by the National Council on Measurement in Education.",A New Bayesian Person-Fit Analysis Method Using Pivotal Discrepancy Measures,"A common method of checking person-fit in Bayesian item response theory (IRT) is the posterior-predictive (PP) method. In recent years, more powerful approaches have been proposed that areÂ based on resampling methods using the popular (Formula presented.) statistic. There has also been proposed a new Bayesian model checking method based on pivotal discrepancy measures (PDMs). A PDM T is a discrepancy measure that is a pivotal quantity with a known reference distribution. A posterior sample of T can be generated using standard Markov chain Monte Carlo output, and a p-value is obtained from probability bounds computed on order statistics of the sample. In this paper, we propose a general procedure to apply this PDM method to person-fit checking in IRT models. We illustrate this using the (Formula presented.) and (Formula presented.) measures. Simulation studies are done comparing these with the PP method and one of the more recent resampling methods. The results show that the PDM method is more powerful than the PP method. Under certain conditions, it is more powerful than the resampling method, while in others, it is less. The PDM method is also applied to a real dataÂ set. Â© 2022 by the National Council on Measurement in Education.","['common', 'method', 'check', 'personfit', 'bayesian', 'item', 'response', 'theory', 'IRT', 'posteriorpredictive', 'pp', 'method', 'recent', 'year', 'powerful', 'approach', 'propose', 'base', 'resampling', 'method', 'popular', 'Formula', 'present', 'statistic', 'propose', 'new', 'bayesian', 'check', 'method', 'base', 'pivotal', 'discrepancy', 'measure', 'pdm', 'PDM', 't', 'discrepancy', 'measure', 'pivotal', 'quantity', 'know', 'reference', 'distribution', 'posterior', 'sample', 't', 'generate', 'standard', 'Markov', 'chain', 'Monte', 'Carlo', 'output', 'pvalue', 'obtain', 'probability', 'bound', 'compute', 'order', 'statistic', 'sample', 'paper', 'propose', 'general', 'procedure', 'apply', 'PDM', 'method', 'personfit', 'checking', 'IRT', 'illustrate', 'Formula', 'present', 'Formula', 'present', 'measure', 'Simulation', 'study', 'compare', 'PP', 'method', 'recent', 'resampling', 'method', 'result', 'PDM', 'method', 'powerful', 'PP', 'method', 'certain', 'condition', 'powerful', 'resampling', 'method', 'PDM', 'method', 'apply', 'real', 'data', 'set', 'Â©', '2022', 'National', 'Council']","['New', 'Bayesian', 'PersonFit', 'Analysis', 'Method', 'Pivotal', 'Discrepancy', 'Measures']",common method check personfit bayesian item response theory IRT posteriorpredictive pp method recent year powerful approach propose base resampling method popular Formula present statistic propose new bayesian check method base pivotal discrepancy measure pdm PDM t discrepancy measure pivotal quantity know reference distribution posterior sample t generate standard Markov chain Monte Carlo output pvalue obtain probability bound compute order statistic sample paper propose general procedure apply PDM method personfit checking IRT illustrate Formula present Formula present measure Simulation study compare PP method recent resampling method result PDM method powerful PP method certain condition powerful resampling method PDM method apply real data set Â© 2022 National Council,New Bayesian PersonFit Analysis Method Pivotal Discrepancy Measures,0.030001441,0.029201263,0.882025752,0.029504794,0.02926675,0,0.060076242,0.040874602,0,0.056755579
He Y.; Qi Y.,Using Response Time in Multidimensional Computerized Adaptive Testing,2023,60,"In multidimensional computerized adaptive testing (MCAT), item selection strategies are generally constructed based on responses, and they do not consider the response times required by items. This study constructed two new criteria (referred to as DT-inc and DT) for MCAT item selection by utilizing information from response times. The new designs maximize the amount of information per unit time. Furthermore, these two new designs were extended to the DTS-inc and DTS designs to efficiently estimate intentional abilities. Moreover, the EAP method for ability estimation was also equipped with response time. The performances of the response-time-based EAP (RT-based EAP) and the new designs were evaluated in simulation and empirical studies. The results showed that the RT-based EAP significantly improved the ability estimation precision compared with the EAP without using response time, and the new designs dramatically saved testing times for examinees with a small sacrifice of ability estimation precision and item pool usage. Â© 2023 by the National Council on Measurement in Education.",Using Response Time in Multidimensional Computerized Adaptive Testing,"In multidimensional computerized adaptive testing (MCAT), item selection strategies are generally constructed based on responses, and they do not consider the response times required by items. This study constructed two new criteria (referred to as DT-inc and DT) for MCAT item selection by utilizing information from response times. The new designs maximize the amount of information per unit time. Furthermore, these two new designs were extended to the DTS-inc and DTS designs to efficiently estimate intentional abilities. Moreover, the EAP method for ability estimation was also equipped with response time. The performances of the response-time-based EAP (RT-based EAP) and the new designs were evaluated in simulation and empirical studies. The results showed that the RT-based EAP significantly improved the ability estimation precision compared with the EAP without using response time, and the new designs dramatically saved testing times for examinees with a small sacrifice of ability estimation precision and item pool usage. Â© 2023 by the National Council on Measurement in Education.","['multidimensional', 'computerized', 'adaptive', 'testing', 'MCAT', 'item', 'selection', 'strategy', 'generally', 'construct', 'base', 'response', 'consider', 'response', 'time', 'require', 'item', 'study', 'construct', 'new', 'criterion', 'refer', 'DTinc', 'DT', 'MCAT', 'item', 'selection', 'utilize', 'information', 'response', 'time', 'new', 'design', 'maximize', 'information', 'unit', 'time', 'furthermore', 'new', 'design', 'extend', 'DTSinc', 'DTS', 'design', 'efficiently', 'estimate', 'intentional', 'ability', 'EAP', 'method', 'ability', 'estimation', 'equip', 'response', 'time', 'performance', 'responsetimebased', 'EAP', 'RTbased', 'EAP', 'new', 'design', 'evaluate', 'simulation', 'empirical', 'study', 'result', 'RTbased', 'EAP', 'significantly', 'improve', 'ability', 'estimation', 'precision', 'compare', 'EAP', 'response', 'time', 'new', 'design', 'dramatically', 'save', 'testing', 'time', 'examinee', 'small', 'sacrifice', 'ability', 'estimation', 'precision', 'item', 'pool', 'usage', 'Â©', '2023', 'National', 'Council']","['Response', 'Time', 'Multidimensional', 'Computerized', 'Adaptive', 'Testing']",multidimensional computerized adaptive testing MCAT item selection strategy generally construct base response consider response time require item study construct new criterion refer DTinc DT MCAT item selection utilize information response time new design maximize information unit time furthermore new design extend DTSinc DTS design efficiently estimate intentional ability EAP method ability estimation equip response time performance responsetimebased EAP RTbased EAP new design evaluate simulation empirical study result RTbased EAP significantly improve ability estimation precision compare EAP response time new design dramatically save testing time examinee small sacrifice ability estimation precision item pool usage Â© 2023 National Council,Response Time Multidimensional Computerized Adaptive Testing,0.889587478,0.027454303,0.027421447,0.027879993,0.027656779,0.096038232,0.035457099,0.00534656,0,0
DeCarlo L.T.,Classical Item Analysis from a Signal Detection Perspective,2023,60,"A conceptualization of multiple-choice exams in terms of signal detection theory (SDT) leads to simple measures of item difficulty and item discrimination that are closely related to, but also distinct from, those used in classical item analysis (CIA). The theory defines a ?œtrue split,??depending on whether or not examinees know an item, and so it provides a basis for using total scores to split item tables, as done in CIA, while also clarifying benefits and limitations of the approach. The SDT item difficulty and discrimination measures differ from those used in CIA in that they explicitly consider the role of distractors and avoid limitations due to range restrictions. A new screening measure is also introduced. The measures are theoretically well-grounded and are simple to compute by hand calculations or with standard software for choice models; simulations show that they offer advantages over traditional measures. Â© 2023 by the National Council on Measurement in Education.",Classical Item Analysis from a Signal Detection Perspective,"A conceptualization of multiple-choice exams in terms of signal detection theory (SDT) leads to simple measures of item difficulty and item discrimination that are closely related to, but also distinct from, those used in classical item analysis (CIA). The theory defines a ?œtrue split,??depending on whether or not examinees know an item, and so it provides a basis for using total scores to split item tables, as done in CIA, while also clarifying benefits and limitations of the approach. The SDT item difficulty and discrimination measures differ from those used in CIA in that they explicitly consider the role of distractors and avoid limitations due to range restrictions. A new screening measure is also introduced. The measures are theoretically well-grounded and are simple to compute by hand calculations or with standard software for choice models; simulations show that they offer advantages over traditional measures. Â© 2023 by the National Council on Measurement in Education.","['conceptualization', 'multiplechoice', 'exam', 'term', 'signal', 'detection', 'theory', 'SDT', 'lead', 'simple', 'measure', 'item', 'difficulty', 'item', 'discrimination', 'closely', 'relate', 'distinct', 'classical', 'item', 'analysis', 'CIA', 'theory', 'define', '""', 'true', 'split', '""', 'depend', 'examinee', 'know', 'item', 'provide', 'basis', 'total', 'score', 'split', 'item', 'table', 'CIA', 'clarify', 'benefit', 'limitation', 'approach', 'SDT', 'item', 'difficulty', 'discrimination', 'measure', 'differ', 'CIA', 'explicitly', 'consider', 'role', 'distractor', 'avoid', 'limitation', 'range', 'restriction', 'new', 'screening', 'measure', 'introduce', 'measure', 'theoretically', 'wellgrounde', 'simple', 'compute', 'hand', 'calculation', 'standard', 'software', 'choice', 'simulation', 'offer', 'advantage', 'traditional', 'measure', 'Â©', '2023', 'National', 'Council']","['Classical', 'Item', 'Analysis', 'Signal', 'Detection', 'Perspective']","conceptualization multiplechoice exam term signal detection theory SDT lead simple measure item difficulty item discrimination closely relate distinct classical item analysis CIA theory define "" true split "" depend examinee know item provide basis total score split item table CIA clarify benefit limitation approach SDT item difficulty discrimination measure differ CIA explicitly consider role distractor avoid limitation range restriction new screening measure introduce measure theoretically wellgrounde simple compute hand calculation standard software choice simulation offer advantage traditional measure Â© 2023 National Council",Classical Item Analysis Signal Detection Perspective,0.025708486,0.896552958,0.025661664,0.026377846,0.025699046,0.025673207,0.052136784,0,0.010816974,0.012243662
Yaneva V.; Clauser B.E.; Morales A.; Paniagua M.,Using Eye-Tracking Data as Part of the Validity Argument for Multiple-Choice Questions: A Demonstration,2021,58,"Eye-tracking technology can create a record of the location and duration of visual fixations as a test-taker reads test questions. Although the cognitive process the test-taker is using cannot be directly observed, eye-tracking data can support inferences about these unobserved cognitive processes. This type of information has the potential to support improved test design and to contribute to an overall validity argument for the inferences and uses made based on test scores. Although several authors have referred to the potential usefulness of eye-tracking data, there are relatively few published studies that provide examples of that use. In this paper, we report the results an eye-tracking study designed to evaluate how the presence of the options in multiple-choice questions impacts the way medical students responded to questions designed to evaluate clinical reasoning. Examples of the types of data that can be extracted are presented. We then discuss the implications of these results for evaluating the validity of inferences made based on the type of items used in this study. Â© 2021 National Board of Medical Examiners. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education",Using Eye-Tracking Data as Part of the Validity Argument for Multiple-Choice Questions: A Demonstration,"Eye-tracking technology can create a record of the location and duration of visual fixations as a test-taker reads test questions. Although the cognitive process the test-taker is using cannot be directly observed, eye-tracking data can support inferences about these unobserved cognitive processes. This type of information has the potential to support improved test design and to contribute to an overall validity argument for the inferences and uses made based on test scores. Although several authors have referred to the potential usefulness of eye-tracking data, there are relatively few published studies that provide examples of that use. In this paper, we report the results an eye-tracking study designed to evaluate how the presence of the options in multiple-choice questions impacts the way medical students responded to questions designed to evaluate clinical reasoning. Examples of the types of data that can be extracted are presented. We then discuss the implications of these results for evaluating the validity of inferences made based on the type of items used in this study. Â© 2021 National Board of Medical Examiners. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education","['eyetracke', 'technology', 'create', 'record', 'location', 'duration', 'visual', 'fixation', 'testtaker', 'read', 'test', 'question', 'cognitive', 'process', 'testtaker', 'directly', 'observed', 'eyetracke', 'datum', 'support', 'inference', 'unobserved', 'cognitive', 'process', 'type', 'information', 'potential', 'support', 'improve', 'test', 'design', 'contribute', 'overall', 'validity', 'argument', 'inference', 'base', 'test', 'score', 'author', 'refer', 'potential', 'usefulness', 'eyetracke', 'datum', 'relatively', 'publish', 'study', 'provide', 'example', 'paper', 'report', 'result', 'eyetracke', 'study', 'design', 'evaluate', 'presence', 'option', 'multiplechoice', 'question', 'impact', 'way', 'medical', 'student', 'respond', 'question', 'design', 'evaluate', 'clinical', 'reasoning', 'Examples', 'type', 'datum', 'extract', 'present', 'discuss', 'implication', 'result', 'evaluate', 'validity', 'inference', 'base', 'type', 'item', 'study', 'Â©', '2021', 'National', 'Board', 'Medical', 'Examiners', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['EyeTracking', 'datum', 'Part', 'Validity', 'Argument', 'MultipleChoice', 'Questions', 'A', 'demonstration']",eyetracke technology create record location duration visual fixation testtaker read test question cognitive process testtaker directly observed eyetracke datum support inference unobserved cognitive process type information potential support improve test design contribute overall validity argument inference base test score author refer potential usefulness eyetracke datum relatively publish study provide example paper report result eyetracke study design evaluate presence option multiplechoice question impact way medical student respond question design evaluate clinical reasoning Examples type datum extract present discuss implication result evaluate validity inference base type item study Â© 2021 National Board Medical Examiners Journal Educational publish Wiley Periodicals LLC behalf National Council,EyeTracking datum Part Validity Argument MultipleChoice Questions A demonstration,0.899948295,0.024825507,0.024889279,0.025472475,0.024864443,0.026412162,0.037210826,0.015180121,0.047525163,0.010651365
Strachan T.; Cho U.H.; Kim K.Y.; Willse J.T.; Chen S.-H.; Ip E.H.; Ackerman T.A.; Weeks J.P.,Using a Projection IRT Method for Vertical Scaling When Construct Shift Is Present,2021,58,"In vertical scaling, results of tests from several different grade levels are placed on a common scale. Most vertical scaling methodologies rely heavily on the assumption that the construct being measured is unidimensional. In many testing situations, however, such an assumption could be problematic. For instance, the construct measured at one grade level may differ from that measured in another grade (e.g., construct shift). On the other hand, dimensions that involve low-level skills are usually mastered by almost all students as they progress to higher grades. These types of changes in the multidimensional structure, within and across grades, create challenges for developing a vertical scale. In this article, we propose the use of projective IRT (PIRT) as a potential solution to the problem. Assuming that a test measures a primary dimension of substantive interest as well as some peripheral dimensions, the idea underlying PIRT is to integrate out the secondary dimensions such that the model provides both item parameters and ability estimates for the primary dimension. A simulation study was conducted to evaluate the effectiveness of the PIRT as a method for vertical scaling. An example using empirical data from a measure of foundational reading skills is also presented. Â© 2020 by the National Council on Measurement in Education",Using a Projection IRT Method for Vertical Scaling When Construct Shift Is Present,"In vertical scaling, results of tests from several different grade levels are placed on a common scale. Most vertical scaling methodologies rely heavily on the assumption that the construct being measured is unidimensional. In many testing situations, however, such an assumption could be problematic. For instance, the construct measured at one grade level may differ from that measured in another grade (e.g., construct shift). On the other hand, dimensions that involve low-level skills are usually mastered by almost all students as they progress to higher grades. These types of changes in the multidimensional structure, within and across grades, create challenges for developing a vertical scale. In this article, we propose the use of projective IRT (PIRT) as a potential solution to the problem. Assuming that a test measures a primary dimension of substantive interest as well as some peripheral dimensions, the idea underlying PIRT is to integrate out the secondary dimensions such that the model provides both item parameters and ability estimates for the primary dimension. A simulation study was conducted to evaluate the effectiveness of the PIRT as a method for vertical scaling. An example using empirical data from a measure of foundational reading skills is also presented. Â© 2020 by the National Council on Measurement in Education","['vertical', 'scaling', 'result', 'test', 'different', 'grade', 'level', 'place', 'common', 'scale', 'vertical', 'scaling', 'methodology', 'rely', 'heavily', 'assumption', 'construct', 'measure', 'unidimensional', 'testing', 'situation', 'assumption', 'problematic', 'instance', 'construct', 'measure', 'grade', 'level', 'differ', 'measure', 'grade', 'eg', 'construct', 'shift', 'hand', 'dimension', 'involve', 'lowlevel', 'skill', 'usually', 'master', 'student', 'progress', 'high', 'grade', 'type', 'change', 'multidimensional', 'structure', 'grade', 'create', 'challenge', 'develop', 'vertical', 'scale', 'article', 'propose', 'projective', 'IRT', 'PIRT', 'potential', 'solution', 'problem', 'assume', 'test', 'measure', 'primary', 'dimension', 'substantive', 'interest', 'peripheral', 'dimension', 'idea', 'underlie', 'PIRT', 'integrate', 'secondary', 'dimension', 'provide', 'item', 'parameter', 'ability', 'estimate', 'primary', 'dimension', 'simulation', 'study', 'conduct', 'evaluate', 'effectiveness', 'PIRT', 'method', 'vertical', 'scaling', 'example', 'empirical', 'datum', 'measure', 'foundational', 'reading', 'skill', 'present', 'Â©', '2020', 'National', 'Council']","['Projection', 'IRT', 'Method', 'Vertical', 'Scaling', 'Construct', 'shift', 'present']",vertical scaling result test different grade level place common scale vertical scaling methodology rely heavily assumption construct measure unidimensional testing situation assumption problematic instance construct measure grade level differ measure grade eg construct shift hand dimension involve lowlevel skill usually master student progress high grade type change multidimensional structure grade create challenge develop vertical scale article propose projective IRT PIRT potential solution problem assume test measure primary dimension substantive interest peripheral dimension idea underlie PIRT integrate secondary dimension provide item parameter ability estimate primary dimension simulation study conduct evaluate effectiveness PIRT method vertical scaling example empirical datum measure foundational reading skill present Â© 2020 National Council,Projection IRT Method Vertical Scaling Construct shift present,0.026336858,0.026041242,0.025991156,0.895553861,0.026076883,0.00346008,0.068386746,0,0.018659446,0.023753939
Almehrizi R.S.,"Standard Errors of Variance Components, Measurement Errors and Generalizability Coefficients for Crossed Designs",2021,58,"Estimates of various variance components, universe score variance, measurement error variances, and generalizability coefficients, like all statistics, are subject to sampling variability, particularly in small samples. Such variability is quantified traditionally through estimated standard errors and/or confidence intervals. The paper derived new standard errors for all estimated statistics for two crossed designs (single-facet design and two-facet design) in generalizability theory. The derivation was based on the assumption of multivariate normal distribution for observation scores using delta method. The derivation was differentiating between fixed and random facets. The adequacy of the derived standard errors was examined using Mont Carlo simulation for the two designs under several test conditions and compared to the traditional existing methods as well as implementation on real data. Results showed that the derived standard errors for all estimators are converging to the empirical standard errors for all simulation conditions with the two designs for both normal and non-normal continuous data. Â© 2020 by the National Council on Measurement in Education","Standard Errors of Variance Components, Measurement Errors and Generalizability Coefficients for Crossed Designs","Estimates of various variance components, universe score variance, measurement error variances, and generalizability coefficients, like all statistics, are subject to sampling variability, particularly in small samples. Such variability is quantified traditionally through estimated standard errors and/or confidence intervals. The paper derived new standard errors for all estimated statistics for two crossed designs (single-facet design and two-facet design) in generalizability theory. The derivation was based on the assumption of multivariate normal distribution for observation scores using delta method. The derivation was differentiating between fixed and random facets. The adequacy of the derived standard errors was examined using Mont Carlo simulation for the two designs under several test conditions and compared to the traditional existing methods as well as implementation on real data. Results showed that the derived standard errors for all estimators are converging to the empirical standard errors for all simulation conditions with the two designs for both normal and non-normal continuous data. Â© 2020 by the National Council on Measurement in Education","['estimate', 'variance', 'component', 'universe', 'score', 'variance', 'error', 'variance', 'generalizability', 'coefficient', 'like', 'statistic', 'subject', 'sample', 'variability', 'particularly', 'small', 'sample', 'variability', 'quantify', 'traditionally', 'estimate', 'standard', 'error', 'andor', 'confidence', 'interval', 'paper', 'derive', 'new', 'standard', 'error', 'estimate', 'statistic', 'cross', 'design', 'singlefacet', 'design', 'twofacet', 'design', 'generalizability', 'theory', 'derivation', 'base', 'assumption', 'multivariate', 'normal', 'distribution', 'observation', 'score', 'delta', 'method', 'derivation', 'differentiate', 'fix', 'random', 'facet', 'adequacy', 'derive', 'standard', 'error', 'examine', 'Mont', 'Carlo', 'simulation', 'design', 'test', 'condition', 'compare', 'traditional', 'exist', 'method', 'implementation', 'real', 'datum', 'result', 'derive', 'standard', 'error', 'estimator', 'converge', 'empirical', 'standard', 'error', 'simulation', 'condition', 'design', 'normal', 'nonnormal', 'continuous', 'datum', 'Â©', '2020', 'National', 'Council']","['Standard', 'Errors', 'Variance', 'Components', 'Errors', 'Generalizability', 'Coefficients', 'crossed', 'design']",estimate variance component universe score variance error variance generalizability coefficient like statistic subject sample variability particularly small sample variability quantify traditionally estimate standard error andor confidence interval paper derive new standard error estimate statistic cross design singlefacet design twofacet design generalizability theory derivation base assumption multivariate normal distribution observation score delta method derivation differentiate fix random facet adequacy derive standard error examine Mont Carlo simulation design test condition compare traditional exist method implementation real datum result derive standard error estimator converge empirical standard error simulation condition design normal nonnormal continuous datum Â© 2020 National Council,Standard Errors Variance Components Errors Generalizability Coefficients crossed design,0.026184553,0.025611492,0.025625087,0.025787719,0.89679115,0,0.00712527,0.176991983,0,0
Lee Y.-H.; Haberman S.J.,Studying Score Stability with a Harmonic Regression Family: A Comparison of Three Approaches to Adjustment of Examinee-Specific Demographic Data,2021,58,"For assessments that use different forms in different administrations, equating methods are applied to ensure comparability of scores over time. Ideally, a score scale is well maintained throughout the life of a testing program. In reality, instability of a score scale can result from a variety of causes, some are expected while others may be unforeseen. The situation is more challenging for assessments that assemble many different forms and deliver frequent administrations per year. Harmonic regression, a seasonal-adjustment method, has been found useful in achieving the goal of differentiating between possible known sources of variability and unknown sources so as to study score stability for such assessments. As an extension, this paper presents a family of three approaches that incorporate examinees' demographic data into harmonic regression in different ways. A generic evaluation method based on jackknifing is developed to compare the approaches within the family. The three approaches are compared using real data from an international language assessment. Results suggest that all approaches perform similarly and are effective in meeting the goal. The paper also discusses the properties and limitations of the three approaches, along with inferences about score (in)stability based on the harmonic regressionÂ results. Â© 2020 by the National Council on Measurement in Education",Studying Score Stability with a Harmonic Regression Family: A Comparison of Three Approaches to Adjustment of Examinee-Specific Demographic Data,"For assessments that use different forms in different administrations, equating methods are applied to ensure comparability of scores over time. Ideally, a score scale is well maintained throughout the life of a testing program. In reality, instability of a score scale can result from a variety of causes, some are expected while others may be unforeseen. The situation is more challenging for assessments that assemble many different forms and deliver frequent administrations per year. Harmonic regression, a seasonal-adjustment method, has been found useful in achieving the goal of differentiating between possible known sources of variability and unknown sources so as to study score stability for such assessments. As an extension, this paper presents a family of three approaches that incorporate examinees' demographic data into harmonic regression in different ways. A generic evaluation method based on jackknifing is developed to compare the approaches within the family. The three approaches are compared using real data from an international language assessment. Results suggest that all approaches perform similarly and are effective in meeting the goal. The paper also discusses the properties and limitations of the three approaches, along with inferences about score (in)stability based on the harmonic regressionÂ results. Â© 2020 by the National Council on Measurement in Education","['assessment', 'different', 'form', 'different', 'administration', 'equate', 'method', 'apply', 'ensure', 'comparability', 'score', 'time', 'ideally', 'score', 'scale', 'maintain', 'life', 'testing', 'program', 'reality', 'instability', 'score', 'scale', 'result', 'variety', 'cause', 'expect', 'unforeseen', 'situation', 'challenging', 'assessment', 'assemble', 'different', 'form', 'deliver', 'frequent', 'administration', 'year', 'harmonic', 'regression', 'seasonaladjustment', 'method', 'find', 'useful', 'achieve', 'goal', 'differentiate', 'possible', 'know', 'source', 'variability', 'unknown', 'source', 'study', 'score', 'stability', 'assessment', 'extension', 'paper', 'present', 'family', 'approach', 'incorporate', 'examine', 'demographic', 'datum', 'harmonic', 'regression', 'different', 'way', 'generic', 'evaluation', 'method', 'base', 'jackknifing', 'develop', 'compare', 'approach', 'family', 'approach', 'compare', 'real', 'datum', 'international', 'language', 'assessment', 'result', 'suggest', 'approach', 'perform', 'similarly', 'effective', 'meet', 'goal', 'paper', 'discuss', 'property', 'limitation', 'approach', 'inference', 'score', 'instability', 'base', 'harmonic', 'regression', 'result', 'Â©', '2020', 'National', 'Council']","['study', 'Score', 'Stability', 'harmonic', 'regression', 'Family', 'Comparison', 'Three', 'Approaches', 'Adjustment', 'ExamineeSpecific', 'Demographic', 'Data']",assessment different form different administration equate method apply ensure comparability score time ideally score scale maintain life testing program reality instability score scale result variety cause expect unforeseen situation challenging assessment assemble different form deliver frequent administration year harmonic regression seasonaladjustment method find useful achieve goal differentiate possible know source variability unknown source study score stability assessment extension paper present family approach incorporate examine demographic datum harmonic regression different way generic evaluation method base jackknifing develop compare approach family approach compare real datum international language assessment result suggest approach perform similarly effective meet goal paper discuss property limitation approach inference score instability base harmonic regression result Â© 2020 National Council,study Score Stability harmonic regression Family Comparison Three Approaches Adjustment ExamineeSpecific Demographic Data,0.906953485,0.023152361,0.023143285,0.023365652,0.023385217,0.007186599,0.061167579,0.025324345,0.06367835,0
Shermis M.D.,Anchoring Validity Evidence for Automated Essay Scoring,2022,59,"One of the challenges of discussing validity arguments for machine scoring of essays centers on the absence of a commonly held definition and theory of good writing. At best, the algorithms attempt to measure select attributes of writing and calibrate them against human ratings with the goal of accurate prediction of scores for new essays. Sometimes these attributes are based on the fundamentals of writing (e.g., fluency), but quite often they are based on locally developed rubrics that may be confounded with specific content coverage expectations. This lack of transparency makes it difficult to provide systematic evidence that machine scoring is assessing writing, but slices or correlates of writing performance. Â© 2022 by the National Council on Measurement in Education.",Anchoring Validity Evidence for Automated Essay Scoring,"One of the challenges of discussing validity arguments for machine scoring of essays centers on the absence of a commonly held definition and theory of good writing. At best, the algorithms attempt to measure select attributes of writing and calibrate them against human ratings with the goal of accurate prediction of scores for new essays. Sometimes these attributes are based on the fundamentals of writing (e.g., fluency), but quite often they are based on locally developed rubrics that may be confounded with specific content coverage expectations. This lack of transparency makes it difficult to provide systematic evidence that machine scoring is assessing writing, but slices or correlates of writing performance. Â© 2022 by the National Council on Measurement in Education.","['challenge', 'discuss', 'validity', 'argument', 'machine', 'scoring', 'essay', 'center', 'absence', 'commonly', 'hold', 'definition', 'theory', 'good', 'writing', 'good', 'algorithm', 'attempt', 'measure', 'select', 'attribute', 'writing', 'calibrate', 'human', 'rating', 'goal', 'accurate', 'prediction', 'score', 'new', 'essay', 'attribute', 'base', 'fundamental', 'write', 'eg', 'fluency', 'base', 'locally', 'develop', 'rubric', 'confound', 'specific', 'content', 'coverage', 'expectation', 'lack', 'transparency', 'difficult', 'provide', 'systematic', 'evidence', 'machine', 'scoring', 'assess', 'writing', 'slice', 'correlate', 'write', 'performance', 'Â©', '2022', 'National', 'Council']","['anchor', 'Validity', 'Evidence', 'Automated', 'Essay', 'Scoring']",challenge discuss validity argument machine scoring essay center absence commonly hold definition theory good writing good algorithm attempt measure select attribute writing calibrate human rating goal accurate prediction score new essay attribute base fundamental write eg fluency base locally develop rubric confound specific content coverage expectation lack transparency difficult provide systematic evidence machine scoring assess writing slice correlate write performance Â© 2022 National Council,anchor Validity Evidence Automated Essay Scoring,0.02810011,0.027977105,0.028064017,0.028502721,0.887356048,0,0.009038682,0,0.085308868,0.001100741
KÃ¶hler C.; Khorramdel L.; Pokropek A.; Hartig J.,DIF Detection for Multiple Groups: Comparing Three-Level GLMMs and Multiple-Group IRT Models,2024,61,"For assessment scales applied to different groups (e.g., students from different states; patients in different countries), multigroup differential item functioning (MG-DIF) needs to be evaluated in order to ensure that respondents with the same trait level but from different groups have equal response probabilities on a particular item. The current study compares two approaches for DIF detection: a multiple-group item response theory (MG-IRT) model and a generalized linear mixed model (GLMM). In the MG-IRT model approach, item parameters are constrained to be equal across groups and DIF is evaluated for each item in each group. In the GLMM, groups are treated as random, and item difficulties are modeled as correlated random effects with a joint multivariate normal distribution. Its nested structure allows the estimation of item difficulty variances and covariances at the group level. We use an excerpt from the PISA 2015 reading domain as an exemplary empirical investigation, and conduct a simulation study to compare the performance of the two approaches. Results from the empirical investigation show that the detection of countries with DIF is similar in both approaches. Results from the simulation study confirm this finding and indicate slight advantages of the MG-IRT model approach. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",DIF Detection for Multiple Groups: Comparing Three-Level GLMMs and Multiple-Group IRT Models,"For assessment scales applied to different groups (e.g., students from different states; patients in different countries), multigroup differential item functioning (MG-DIF) needs to be evaluated in order to ensure that respondents with the same trait level but from different groups have equal response probabilities on a particular item. The current study compares two approaches for DIF detection: a multiple-group item response theory (MG-IRT) model and a generalized linear mixed model (GLMM). In the MG-IRT model approach, item parameters are constrained to be equal across groups and DIF is evaluated for each item in each group. In the GLMM, groups are treated as random, and item difficulties are modeled as correlated random effects with a joint multivariate normal distribution. Its nested structure allows the estimation of item difficulty variances and covariances at the group level. We use an excerpt from the PISA 2015 reading domain as an exemplary empirical investigation, and conduct a simulation study to compare the performance of the two approaches. Results from the empirical investigation show that the detection of countries with DIF is similar in both approaches. Results from the simulation study confirm this finding and indicate slight advantages of the MG-IRT model approach. Â© 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.","['assessment', 'scale', 'apply', 'different', 'group', 'eg', 'student', 'different', 'state', 'patient', 'different', 'country', 'multigroup', 'differential', 'item', 'function', 'MGDIF', 'need', 'evaluate', 'order', 'ensure', 'respondent', 'trait', 'level', 'different', 'group', 'equal', 'response', 'probability', 'particular', 'item', 'current', 'study', 'compare', 'approach', 'DIF', 'detection', 'multiplegroup', 'item', 'response', 'theory', 'MGIRT', 'generalized', 'linear', 'mixed', 'GLMM', 'MGIRT', 'approach', 'item', 'parameter', 'constrain', 'equal', 'group', 'DIF', 'evaluate', 'item', 'group', 'GLMM', 'group', 'treat', 'random', 'item', 'difficulty', 'correlate', 'random', 'effect', 'joint', 'multivariate', 'normal', 'distribution', 'nested', 'structure', 'allow', 'estimation', 'item', 'difficulty', 'variance', 'covariance', 'group', 'level', 'excerpt', 'PISA', '2015', 'read', 'domain', 'exemplary', 'empirical', 'investigation', 'conduct', 'simulation', 'study', 'compare', 'performance', 'approach', 'result', 'empirical', 'investigation', 'detection', 'country', 'DIF', 'similar', 'approach', 'result', 'simulation', 'study', 'confirm', 'finding', 'indicate', 'slight', 'advantage', 'MGIRT', 'approach', 'Â©', '2024', 'Authors', 'Journal', 'Educational', 'publish', 'Wiley', 'Periodicals', 'LLC', 'behalf', 'National', 'Council']","['DIF', 'Detection', 'Multiple', 'Groups', 'compare', 'ThreeLevel', 'glmm', 'MultipleGroup', 'IRT']",assessment scale apply different group eg student different state patient different country multigroup differential item function MGDIF need evaluate order ensure respondent trait level different group equal response probability particular item current study compare approach DIF detection multiplegroup item response theory MGIRT generalized linear mixed GLMM MGIRT approach item parameter constrain equal group DIF evaluate item group GLMM group treat random item difficulty correlate random effect joint multivariate normal distribution nested structure allow estimation item difficulty variance covariance group level excerpt PISA 2015 read domain exemplary empirical investigation conduct simulation study compare performance approach result empirical investigation detection country DIF similar approach result simulation study confirm finding indicate slight advantage MGIRT approach Â© 2024 Authors Journal Educational publish Wiley Periodicals LLC behalf National Council,DIF Detection Multiple Groups compare ThreeLevel glmm MultipleGroup IRT,0.90994621,0.022436981,0.022372263,0.022688311,0.022556236,0.05552389,0.007948141,0.012286186,0.004546988,0.126379409
Kaufmann E.; Budescu D.V.,Do Teachers Consider Advice? On the Acceptance of Computerized Expert Models,2020,57,"The literature suggests that simple expert (mathematical) models can improve the quality of decisions, but people are not always eager to accept and endorse such models. We ran three online experiments to test the receptiveness to advice from computerized expert models. Middle- and high-school teachers (N = 435) evaluated student profiles that varied in several personal and task relevant factors. They were offered (Studies I and II), or could ask for (Study III), advice from either expert models or human advisors. Overall, teachers requested and followed advice of expert models less frequently than advice from humans. Task-relevant factors (task difficulty) seem to be more salient than personal factors for teachers??willingness to receive advice. Â© 2019 by the National Council on Measurement in Education",Do Teachers Consider Advice? On the Acceptance of Computerized Expert Models,"The literature suggests that simple expert (mathematical) models can improve the quality of decisions, but people are not always eager to accept and endorse such models. We ran three online experiments to test the receptiveness to advice from computerized expert models. Middle- and high-school teachers (N = 435) evaluated student profiles that varied in several personal and task relevant factors. They were offered (Studies I and II), or could ask for (Study III), advice from either expert models or human advisors. Overall, teachers requested and followed advice of expert models less frequently than advice from humans. Task-relevant factors (task difficulty) seem to be more salient than personal factors for teachers??willingness to receive advice. Â© 2019 by the National Council on Measurement in Education","['literature', 'suggest', 'simple', 'expert', 'mathematical', 'improve', 'quality', 'decision', 'people', 'eager', 'accept', 'endorse', 'run', 'online', 'experiment', 'test', 'receptiveness', 'advice', 'computerized', 'expert', 'middle', 'highschool', 'teacher', 'N', '435', 'evaluate', 'student', 'profile', 'vary', 'personal', 'task', 'relevant', 'factor', 'offer', 'Studies', 'I', 'II', 'ask', 'Study', 'III', 'advice', 'expert', 'human', 'advisor', 'overall', 'teacher', 'request', 'follow', 'advice', 'expert', 'frequently', 'advice', 'human', 'Taskrelevant', 'factor', 'task', 'difficulty', 'salient', 'personal', 'factor', 'teacher', '??, 'willingness', 'receive', 'advice', 'Â©', '2019', 'National', 'Council']","['teacher', 'consider', 'Advice', 'Acceptance', 'Computerized', 'Expert', 'Models']",literature suggest simple expert mathematical improve quality decision people eager accept endorse run online experiment test receptiveness advice computerized expert middle highschool teacher N 435 evaluate student profile vary personal task relevant factor offer Studies I II ask Study III advice expert human advisor overall teacher request follow advice expert frequently advice human Taskrelevant factor task difficulty salient personal factor teacher ??willingness receive advice Â© 2019 National Council,teacher consider Advice Acceptance Computerized Expert Models,0.029924368,0.029830152,0.029882931,0.880449285,0.029913264,0.001275291,0.02240812,0,0.013514471,0.007769226
Wang C.; Chen P.; Jiang S.,Item Calibration Methods With Multiple Subscale Multistage Testing,2020,57,"Many large-scale educational surveys have moved from linear form design to multistage testing (MST) design. One advantage of MST is that it can provide more accurate latent trait (Î¸) estimates using fewer items than required by linear tests. However, MST generates incomplete response data by design; hence, questions remain as to how to calibrate items using the incomplete data from MST design. Further complication arises when there are multiple correlated subscales per test, and when items from different subscales need to be calibrated according to their respective score reporting metric. The current calibration-per-subscale method produced biased item parameters, and there is no available method for resolving the challenge. Deriving from the missing data principle, we showed when calibrating all items together the Rubin's ignorability assumption is satisfied such that the traditional single-group calibration is sufficient. When calibrating items per subscale, we proposed a simple modification to the current calibration-per-subscale method that helps reinstate the missing-at-random assumption and therefore corrects for the estimation bias that is otherwise existent. Three mainstream calibration methods are discussed in the context of MST, they are the marginal maximum likelihood estimation, the expectation maximization method, and the fixed parameter calibration. An extensive simulation study is conducted and a real data example from NAEP is analyzed to provide convincing empirical evidence. Â© 2019 by the National Council on Measurement in Education",Item Calibration Methods With Multiple Subscale Multistage Testing,"Many large-scale educational surveys have moved from linear form design to multistage testing (MST) design. One advantage of MST is that it can provide more accurate latent trait (Î¸) estimates using fewer items than required by linear tests. However, MST generates incomplete response data by design; hence, questions remain as to how to calibrate items using the incomplete data from MST design. Further complication arises when there are multiple correlated subscales per test, and when items from different subscales need to be calibrated according to their respective score reporting metric. The current calibration-per-subscale method produced biased item parameters, and there is no available method for resolving the challenge. Deriving from the missing data principle, we showed when calibrating all items together the Rubin's ignorability assumption is satisfied such that the traditional single-group calibration is sufficient. When calibrating items per subscale, we proposed a simple modification to the current calibration-per-subscale method that helps reinstate the missing-at-random assumption and therefore corrects for the estimation bias that is otherwise existent. Three mainstream calibration methods are discussed in the context of MST, they are the marginal maximum likelihood estimation, the expectation maximization method, and the fixed parameter calibration. An extensive simulation study is conducted and a real data example from NAEP is analyzed to provide convincing empirical evidence. Â© 2019 by the National Council on Measurement in Education","['largescale', 'educational', 'survey', 'linear', 'form', 'design', 'multistage', 'testing', 'MST', 'design', 'advantage', 'MST', 'provide', 'accurate', 'latent', 'trait', 'Î¸', 'estimate', 'item', 'require', 'linear', 'test', 'MST', 'generate', 'incomplete', 'response', 'datum', 'design', 'question', 'remain', 'calibrate', 'item', 'incomplete', 'datum', 'MST', 'design', 'complication', 'arise', 'multiple', 'correlate', 'subscale', 'test', 'item', 'different', 'subscale', 'need', 'calibrate', 'accord', 'respective', 'score', 'report', 'metric', 'current', 'calibrationpersubscale', 'method', 'produce', 'biased', 'item', 'parameter', 'available', 'method', 'resolve', 'challenge', 'derive', 'miss', 'datum', 'principle', 'calibrate', 'item', 'Rubins', 'ignorability', 'assumption', 'satisfied', 'traditional', 'singlegroup', 'calibration', 'sufficient', 'calibrate', 'item', 'subscale', 'propose', 'simple', 'modification', 'current', 'calibrationpersubscale', 'method', 'help', 'reinstate', 'missingatrandom', 'assumption', 'correct', 'estimation', 'bias', 'existent', 'mainstream', 'calibration', 'method', 'discuss', 'context', 'mst', 'marginal', 'maximum', 'likelihood', 'estimation', 'expectation', 'maximization', 'method', 'fix', 'parameter', 'calibration', 'extensive', 'simulation', 'study', 'conduct', 'real', 'datum', 'example', 'NAEP', 'analyze', 'provide', 'convince', 'empirical', 'evidence', 'Â©', '2019', 'National', 'Council']","['Item', 'Calibration', 'Methods', 'Multiple', 'Subscale', 'Multistage', 'Testing']",largescale educational survey linear form design multistage testing MST design advantage MST provide accurate latent trait Î¸ estimate item require linear test MST generate incomplete response datum design question remain calibrate item incomplete datum MST design complication arise multiple correlate subscale test item different subscale need calibrate accord respective score report metric current calibrationpersubscale method produce biased item parameter available method resolve challenge derive miss datum principle calibrate item Rubins ignorability assumption satisfied traditional singlegroup calibration sufficient calibrate item subscale propose simple modification current calibrationpersubscale method help reinstate missingatrandom assumption correct estimation bias existent mainstream calibration method discuss context mst marginal maximum likelihood estimation expectation maximization method fix parameter calibration extensive simulation study conduct real datum example NAEP analyze provide convince empirical evidence Â© 2019 National Council,Item Calibration Methods Multiple Subscale Multistage Testing,0.023571076,0.908029363,0.02251401,0.023051723,0.022833828,0.008619894,0.150176402,0.003888282,0.001052993,0
Fujimoto K.A.,A More Flexible Bayesian Multilevel Bifactor Item Response Theory Model,2020,57,"Multilevel bifactor item response theory (IRT) models are commonly used to account for features of the data that are related to the sampling and measurement processes used to gather those data. These models conventionally make assumptions about the portions of the data structure that represent these features. Unfortunately, when data violate these models' assumptions but these models are used anyway, incorrect conclusions about the cluster effects could be made and potentially relevant dimensions could go undetected. To address the limitations of these conventional models, a more flexible multilevel bifactor IRT model that does not make these assumptions is presented, and this model is based on the generalized partial credit model. Details of a simulation study demonstrating this model outperforming competing models and showing the consequences of using conventional multilevel bifactor IRT models to analyze data that violate these models' assumptions are reported. Additionally, the model's usefulness is illustrated through the analysis of the Program for International Student Assessment data related to interest inÂ science. Â© 2019 by the National Council on Measurement in Education",A More Flexible Bayesian Multilevel Bifactor Item Response Theory Model,"Multilevel bifactor item response theory (IRT) models are commonly used to account for features of the data that are related to the sampling and measurement processes used to gather those data. These models conventionally make assumptions about the portions of the data structure that represent these features. Unfortunately, when data violate these models' assumptions but these models are used anyway, incorrect conclusions about the cluster effects could be made and potentially relevant dimensions could go undetected. To address the limitations of these conventional models, a more flexible multilevel bifactor IRT model that does not make these assumptions is presented, and this model is based on the generalized partial credit model. Details of a simulation study demonstrating this model outperforming competing models and showing the consequences of using conventional multilevel bifactor IRT models to analyze data that violate these models' assumptions are reported. Additionally, the model's usefulness is illustrated through the analysis of the Program for International Student Assessment data related to interest inÂ science. Â© 2019 by the National Council on Measurement in Education","['multilevel', 'bifactor', 'item', 'response', 'theory', 'IRT', 'commonly', 'account', 'feature', 'datum', 'relate', 'sampling', 'process', 'gather', 'datum', 'conventionally', 'assumption', 'portion', 'data', 'structure', 'represent', 'feature', 'unfortunately', 'datum', 'violate', 'assumption', 'incorrect', 'conclusion', 'cluster', 'effect', 'potentially', 'relevant', 'dimension', 'undetected', 'address', 'limitation', 'conventional', 'flexible', 'multilevel', 'bifactor', 'IRT', 'assumption', 'present', 'base', 'generalized', 'partial', 'credit', 'Details', 'simulation', 'study', 'demonstrate', 'outperform', 'compete', 'consequence', 'conventional', 'multilevel', 'bifactor', 'IRT', 'analyze', 'datum', 'violate', 'assumption', 'report', 'additionally', 'usefulness', 'illustrate', 'analysis', 'Program', 'International', 'Student', 'Assessment', 'datum', 'relate', 'interest', 'science', 'Â©', '2019', 'National', 'Council']","['flexible', 'Bayesian', 'Multilevel', 'Bifactor', 'Item', 'Response', 'Theory']",multilevel bifactor item response theory IRT commonly account feature datum relate sampling process gather datum conventionally assumption portion data structure represent feature unfortunately datum violate assumption incorrect conclusion cluster effect potentially relevant dimension undetected address limitation conventional flexible multilevel bifactor IRT assumption present base generalized partial credit Details simulation study demonstrate outperform compete consequence conventional multilevel bifactor IRT analyze datum violate assumption report additionally usefulness illustrate analysis Program International Student Assessment datum relate interest science Â© 2019 National Council,flexible Bayesian Multilevel Bifactor Item Response Theory,0.02684614,0.026600526,0.026579128,0.893240526,0.026733679,0.030632918,0.018761766,0.002082733,0.035641287,0.008041686
Kim H.J.; Brennan R.L.; Lee W.-C.,A New Statistic to Assess Fitness of Cubic-Spline Postsmoothing,2020,57,"In equating, smoothing techniques are frequently used to diminish sampling error. There are typically two types of smoothing: presmoothing and postsmoothing. For polynomial log-linear presmoothing, an optimum smoothing degree can be determined statistically based on the Akaike information criterion or Chi-square difference criterion. For cubic-spline postsmoothing, visual inspection has been an important tool in choosing such optimum degrees in operational settings. This study introduces a new statistic for assessing the fitness of the cubic-spline postsmoothing method, which accommodates three conditions: (1) one standard error band, (2) deviation from unsmoothed equivalents, and (3) smoothness. A principal advantage of the new statistic proposed in this study is that an optimum degree of smoothing can be selected automatically by giving consistent amount of attention to deviation and smoothness across multiple equatings, whereas visual inspection may not beÂ consistent. Â© 2019 by the National Council on Measurement in Education",A New Statistic to Assess Fitness of Cubic-Spline Postsmoothing,"In equating, smoothing techniques are frequently used to diminish sampling error. There are typically two types of smoothing: presmoothing and postsmoothing. For polynomial log-linear presmoothing, an optimum smoothing degree can be determined statistically based on the Akaike information criterion or Chi-square difference criterion. For cubic-spline postsmoothing, visual inspection has been an important tool in choosing such optimum degrees in operational settings. This study introduces a new statistic for assessing the fitness of the cubic-spline postsmoothing method, which accommodates three conditions: (1) one standard error band, (2) deviation from unsmoothed equivalents, and (3) smoothness. A principal advantage of the new statistic proposed in this study is that an optimum degree of smoothing can be selected automatically by giving consistent amount of attention to deviation and smoothness across multiple equatings, whereas visual inspection may not beÂ consistent. Â© 2019 by the National Council on Measurement in Education","['equate', 'smooth', 'technique', 'frequently', 'diminish', 'sampling', 'error', 'typically', 'type', 'smooth', 'presmoothing', 'postsmoothe', 'polynomial', 'loglinear', 'presmoothe', 'optimum', 'smoothing', 'degree', 'determine', 'statistically', 'base', 'Akaike', 'information', 'criterion', 'Chisquare', 'difference', 'criterion', 'cubicspline', 'postsmoothe', 'visual', 'inspection', 'important', 'tool', 'choose', 'optimum', 'degree', 'operational', 'setting', 'study', 'introduce', 'new', 'statistic', 'assess', 'fitness', 'cubicspline', 'postsmoothe', 'method', 'accommodate', 'condition', '1', 'standard', 'error', 'band', '2', 'deviation', 'unsmoothed', 'equivalent', '3', 'smoothness', 'principal', 'advantage', 'new', 'statistic', 'propose', 'study', 'optimum', 'degree', 'smoothing', 'select', 'automatically', 'consistent', 'attention', 'deviation', 'smoothness', 'multiple', 'equating', 'visual', 'inspection', 'consistent', 'Â©', '2019', 'National', 'Council']","['New', 'Statistic', 'Assess', 'Fitness', 'CubicSpline', 'postsmoothe']",equate smooth technique frequently diminish sampling error typically type smooth presmoothing postsmoothe polynomial loglinear presmoothe optimum smoothing degree determine statistically base Akaike information criterion Chisquare difference criterion cubicspline postsmoothe visual inspection important tool choose optimum degree operational setting study introduce new statistic assess fitness cubicspline postsmoothe method accommodate condition 1 standard error band 2 deviation unsmoothed equivalent 3 smoothness principal advantage new statistic propose study optimum degree smoothing select automatically consistent attention deviation smoothness multiple equating visual inspection consistent Â© 2019 National Council,New Statistic Assess Fitness CubicSpline postsmoothe,0.027106338,0.893394718,0.026357134,0.02652467,0.026617141,0,0,0.132583378,0,0
Kim Y.,Partial Identification of Answer Reviewing Effects in Multiple-Choice Exams,2020,57,"Does reviewing previous answers during multiple-choice exams help examinees increase their final score? This article formalizes the question using a rigorous causal framework, the potential outcomes framework. Viewing examinees??reviewing status as a treatment and their final score as an outcome, the article first explains the challenges of identifying the causal effect of answer reviewing in regular exam-taking settings. In addition to the incapability of randomizing the treatment selection (reviewing status) and the lack of other information to make this selection process ignorable, the treatment variable itself is not fully known to researchers. Looking at examinees??answer sheet data, it is unclear whether an examinee who did not change his or her answer on a specific item reviewed it but retained the initial answer (treatment condition) or chose not to review it (control condition). Despite such challenges, however, the article develops partial identification strategies and shows that the sign of the answer reviewing effect can be reasonably inferred. By analyzing a statewide math assessment data set, the article finds that reviewing initial answers is generally beneficial for examinees. Â© 2019 by the National Council on Measurement in Education",Partial Identification of Answer Reviewing Effects in Multiple-Choice Exams,"Does reviewing previous answers during multiple-choice exams help examinees increase their final score? This article formalizes the question using a rigorous causal framework, the potential outcomes framework. Viewing examinees??reviewing status as a treatment and their final score as an outcome, the article first explains the challenges of identifying the causal effect of answer reviewing in regular exam-taking settings. In addition to the incapability of randomizing the treatment selection (reviewing status) and the lack of other information to make this selection process ignorable, the treatment variable itself is not fully known to researchers. Looking at examinees??answer sheet data, it is unclear whether an examinee who did not change his or her answer on a specific item reviewed it but retained the initial answer (treatment condition) or chose not to review it (control condition). Despite such challenges, however, the article develops partial identification strategies and shows that the sign of the answer reviewing effect can be reasonably inferred. By analyzing a statewide math assessment data set, the article finds that reviewing initial answers is generally beneficial for examinees. Â© 2019 by the National Council on Measurement in Education","['review', 'previous', 'answer', 'multiplechoice', 'exam', 'help', 'examinee', 'increase', 'final', 'score', 'article', 'formalize', 'question', 'rigorous', 'causal', 'framework', 'potential', 'outcome', 'framework', 'viewing', 'examinee', ""'"", 'review', 'status', 'treatment', 'final', 'score', 'outcome', 'article', 'explain', 'challenge', 'identify', 'causal', 'effect', 'answer', 'review', 'regular', 'examtaking', 'setting', 'addition', 'incapability', 'randomize', 'treatment', 'selection', 'review', 'status', 'lack', 'information', 'selection', 'process', 'ignorable', 'treatment', 'variable', 'fully', 'know', 'researcher', 'look', 'examinee', ""'"", 'answer', 'sheet', 'datum', 'unclear', 'examinee', 'change', 'answer', 'specific', 'item', 'review', 'retain', 'initial', 'answer', 'treatment', 'condition', 'choose', 'review', 'control', 'condition', 'despite', 'challenge', 'article', 'develop', 'partial', 'identification', 'strategy', 'sign', 'answer', 'review', 'effect', 'reasonably', 'infer', 'analyze', 'statewide', 'math', 'assessment', 'datum', 'set', 'article', 'find', 'review', 'initial', 'answer', 'generally', 'beneficial', 'examinee', 'Â©', '2019', 'National', 'Council']","['Partial', 'Identification', 'Answer', 'Reviewing', 'Effects', 'MultipleChoice', 'Exams']",review previous answer multiplechoice exam help examinee increase final score article formalize question rigorous causal framework potential outcome framework viewing examinee ' review status treatment final score outcome article explain challenge identify causal effect answer review regular examtaking setting addition incapability randomize treatment selection review status lack information selection process ignorable treatment variable fully know researcher look examinee ' answer sheet datum unclear examinee change answer specific item review retain initial answer treatment condition choose review control condition despite challenge article develop partial identification strategy sign answer review effect reasonably infer analyze statewide math assessment datum set article find review initial answer generally beneficial examinee Â© 2019 National Council,Partial Identification Answer Reviewing Effects MultipleChoice Exams,0.028077222,0.027793822,0.027762639,0.028183336,0.888182981,0.019482065,0.019405078,0,0.042154047,0
Thissen D.,Comments on Shelby Haberman's NCME Career Award Address: Statistical Theory and Assessment Practice,2020,57,"When I was a graduate student,1 Howard Wainer quoted his own graduate mentor, Harold Gulliksen, saying ""The difference between basic and applied research is that basic research has so many more applications.""2 Shelby Haberman illustrates that truth in his career award address, in which he summarizes some of the applied contributions he made to educational measurement during his years at ETS, based on his basic statistical research that continued from earlier decades. The breadth of application of Haberman's statistical research is illustrated by the range of examples in his address: the ""effects of model error in the analysis of item responses by use of latent-structure models, weighting to attempt to correct for sampling bias, and the effects of excessively complex models"" (p. 374). Those three topics do not include his seminal work on subscores (e.g., Haberman, 2008) or much of his work on item fit in IRT models (e.g., Haberman, Sinharay, and Chon, 2013); both of those lines of research were recognized with the NCME Annual Award for Exceptional Achieve-
ment in Educational Measurement in 2009 and 2015. Haberman includes pithy lines in his address that should be memorized by all students of educational measurement. Examples are: (referring to likelihood ratio tests between models) ""This kind of test typically suffices to demonstrate that any given IRT model is not valid"" (p. 375) followed a bit later by ""At this point, looking at the size of the error in a model appears more helpful than looking at the existence of error"" (p. 376). And the greatest piece of wisdom in the entire address: ""One challenge is that software produces numbers whether or not the numbers are meaningful"" (p. 381). Students told those things in classes on educational measurement might subsequently make better use of assessment data. ",Comments on Shelby Haberman's NCME Career Award Address: Statistical Theory and Assessment Practice,"When I was a graduate student,1 Howard Wainer quoted his own graduate mentor, Harold Gulliksen, saying ""The difference between basic and applied research is that basic research has so many more applications.""2 Shelby Haberman illustrates that truth in his career award address, in which he summarizes some of the applied contributions he made to educational measurement during his years at ETS, based on his basic statistical research that continued from earlier decades. The breadth of application of Haberman's statistical research is illustrated by the range of examples in his address: the ""effects of model error in the analysis of item responses by use of latent-structure models, weighting to attempt to correct for sampling bias, and the effects of excessively complex models"" (p. 374). Those three topics do not include his seminal work on subscores (e.g., Haberman, 2008) or much of his work on item fit in IRT models (e.g., Haberman, Sinharay, and Chon, 2013); both of those lines of research were recognized with the NCME Annual Award for Exceptional Achieve-
ment in Educational Measurement in 2009 and 2015. Haberman includes pithy lines in his address that should be memorized by all students of educational measurement. Examples are: (referring to likelihood ratio tests between models) ""This kind of test typically suffices to demonstrate that any given IRT model is not valid"" (p. 375) followed a bit later by ""At this point, looking at the size of the error in a model appears more helpful than looking at the existence of error"" (p. 376). And the greatest piece of wisdom in the entire address: ""One challenge is that software produces numbers whether or not the numbers are meaningful"" (p. 381). Students told those things in classes on educational measurement might subsequently make better use of assessment data. ","['I', 'graduate', 'student1', 'Howard', 'Wainer', 'quote', 'graduate', 'mentor', 'Harold', 'Gulliksen', 'difference', 'basic', 'apply', 'research', 'basic', 'research', 'applications2', 'Shelby', 'Haberman', 'illustrate', 'truth', 'career', 'award', 'address', 'summarize', 'apply', 'contribution', 'educational', 'year', 'ets', 'base', 'basic', 'statistical', 'research', 'continue', 'early', 'decade', 'breadth', 'application', 'Habermans', 'statistical', 'research', 'illustrate', 'range', 'example', 'address', 'effect', 'error', 'analysis', 'item', 'response', 'latentstructure', 'weight', 'attempt', 'correct', 'sample', 'bias', 'effect', 'excessively', 'complex', 'p', '374', 'topic', 'include', 'seminal', 'work', 'subscore', 'eg', 'Haberman', '2008', 'work', 'item', 'fit', 'IRT', 'eg', 'Haberman', 'Sinharay', 'Chon', '2013', 'line', 'research', 'recognize', 'NCME', 'Annual', 'Award', 'Exceptional', 'Achieve', 'ment', 'Educational', '2009', '2015', 'Haberman', 'include', 'pithy', 'line', 'address', 'memorize', 'student', 'educational', 'example', 'refer', 'likelihood', 'ratio', 'test', 'kind', 'test', 'typically', 'suffice', 'demonstrate', 'IRT', 'valid', 'p', '375', 'follow', 'bit', 'later', 'point', 'look', 'size', 'error', 'appear', 'helpful', 'look', 'existence', 'error', 'p', '376', 'great', 'piece', 'wisdom', 'entire', 'address', 'challenge', 'software', 'produce', 'number', 'number', 'meaningful', 'p', '381', 'Students', 'tell', 'thing', 'class', 'educational', 'subsequently', 'assessment', 'datum']","['comment', 'Shelby', 'Habermans', 'NCME', 'Career', 'Award', 'Address', 'Statistical', 'Theory', 'Assessment', 'Practice']",I graduate student1 Howard Wainer quote graduate mentor Harold Gulliksen difference basic apply research basic research applications2 Shelby Haberman illustrate truth career award address summarize apply contribution educational year ets base basic statistical research continue early decade breadth application Habermans statistical research illustrate range example address effect error analysis item response latentstructure weight attempt correct sample bias effect excessively complex p 374 topic include seminal work subscore eg Haberman 2008 work item fit IRT eg Haberman Sinharay Chon 2013 line research recognize NCME Annual Award Exceptional Achieve ment Educational 2009 2015 Haberman include pithy line address memorize student educational example refer likelihood ratio test kind test typically suffice demonstrate IRT valid p 375 follow bit later point look size error appear helpful look existence error p 376 great piece wisdom entire address challenge software produce number number meaningful p 381 Students tell thing class educational subsequently assessment datum,comment Shelby Habermans NCME Career Award Address Statistical Theory Assessment Practice,0.020060039,0.020036803,0.019971009,0.020447608,0.919484539,0.014503154,0.004301227,0.034820399,0.032003144,0.017790593
Lee S.,Logistic Regression Procedure Using Penalized Maximum Likelihood Estimation for Differential Item Functioning,2020,57,"In the logistic regression (LR) procedure for differential item functioning (DIF), the parameters of LR have often been estimated using maximum likelihood (ML) estimation. However, ML estimation suffers from the finite-sample bias. Furthermore, ML estimation for LR can be substantially biased in the presence of rare event data. The bias of ML estimation due to small samples and rare event data can degrade the performance of the LR procedure, especially when testing the DIF of difficult items in small samples. Penalized ML (PML) estimation was originally developed to reduce the finite-sample bias of conventional ML estimation and also was known to reduce the bias in the estimation of LR for the rare events data. The goal of this study is to compare the performances of the LR procedures based on the ML and PML estimation in terms of the statistical power and Type I error. In a simulation study, Swaminathan and Rogers's Wald test based on PML estimation (PSR) showed the highest statistical power in most of the simulation conditions, and LRT based on conventional PML estimation (PLRT) showed the most robust and stable Type I error. The discussion about the trade-off between bias and variance is presented in the discussionÂ section. Â© 2019 by the National Council on Measurement in Education",Logistic Regression Procedure Using Penalized Maximum Likelihood Estimation for Differential Item Functioning,"In the logistic regression (LR) procedure for differential item functioning (DIF), the parameters of LR have often been estimated using maximum likelihood (ML) estimation. However, ML estimation suffers from the finite-sample bias. Furthermore, ML estimation for LR can be substantially biased in the presence of rare event data. The bias of ML estimation due to small samples and rare event data can degrade the performance of the LR procedure, especially when testing the DIF of difficult items in small samples. Penalized ML (PML) estimation was originally developed to reduce the finite-sample bias of conventional ML estimation and also was known to reduce the bias in the estimation of LR for the rare events data. The goal of this study is to compare the performances of the LR procedures based on the ML and PML estimation in terms of the statistical power and Type I error. In a simulation study, Swaminathan and Rogers's Wald test based on PML estimation (PSR) showed the highest statistical power in most of the simulation conditions, and LRT based on conventional PML estimation (PLRT) showed the most robust and stable Type I error. The discussion about the trade-off between bias and variance is presented in the discussionÂ section. Â© 2019 by the National Council on Measurement in Education","['logistic', 'regression', 'LR', 'procedure', 'differential', 'item', 'function', 'DIF', 'parameter', 'LR', 'estimate', 'maximum', 'likelihood', 'ML', 'estimation', 'ML', 'estimation', 'suffer', 'finitesample', 'bias', 'Furthermore', 'ML', 'estimation', 'LR', 'substantially', 'bias', 'presence', 'rare', 'event', 'data', 'bias', 'ML', 'estimation', 'small', 'sample', 'rare', 'event', 'datum', 'degrade', 'performance', 'LR', 'procedure', 'especially', 'test', 'DIF', 'difficult', 'item', 'small', 'sample', 'penalize', 'ML', 'PML', 'estimation', 'originally', 'develop', 'reduce', 'finitesample', 'bias', 'conventional', 'ML', 'estimation', 'know', 'reduce', 'bias', 'estimation', 'LR', 'rare', 'event', 'data', 'goal', 'study', 'compare', 'performance', 'LR', 'procedure', 'base', 'ML', 'PML', 'estimation', 'term', 'statistical', 'power', 'Type', 'I', 'error', 'simulation', 'study', 'Swaminathan', 'Rogerss', 'Wald', 'test', 'base', 'PML', 'estimation', 'PSR', 'high', 'statistical', 'power', 'simulation', 'condition', 'LRT', 'base', 'conventional', 'PML', 'estimation', 'PLRT', 'robust', 'stable', 'type', 'I', 'error', 'discussion', 'tradeoff', 'bias', 'variance', 'present', 'discussion', 'section', 'Â©', '2019', 'National', 'Council']","['logistic', 'Regression', 'Procedure', 'Penalized', 'Maximum', 'Likelihood', 'Estimation', 'Differential', 'Item', 'Functioning']",logistic regression LR procedure differential item function DIF parameter LR estimate maximum likelihood ML estimation ML estimation suffer finitesample bias Furthermore ML estimation LR substantially bias presence rare event data bias ML estimation small sample rare event datum degrade performance LR procedure especially test DIF difficult item small sample penalize ML PML estimation originally develop reduce finitesample bias conventional ML estimation know reduce bias estimation LR rare event data goal study compare performance LR procedure base ML PML estimation term statistical power Type I error simulation study Swaminathan Rogerss Wald test base PML estimation PSR high statistical power simulation condition LRT base conventional PML estimation PLRT robust stable type I error discussion tradeoff bias variance present discussion section Â© 2019 National Council,logistic Regression Procedure Penalized Maximum Likelihood Estimation Differential Item Functioning,0.029941756,0.029482764,0.029665168,0.029869292,0.88104102,0.004115532,0.010515319,0.015739075,0.003001713,0.100641065
Castellano K.E.; McCaffrey D.F.,Comparing the Accuracy of Student Growth Measures,2020,57,"Testing programs are often interested in using a student growth measure. This article presents analytic derivations of the accuracy of common student growth measures on both the raw scale of the test and the percentile rank scale in terms of the proportional reduction in mean squared error and the squared correlation between the estimator and target. The study contrasts the accuracy of the growth measures against that of current status measures?”current test scores and their percentile ranks. Key findings include the extent that status measures are more accurate than any of the growth measures and that alternative methods to estimate growth could be more accurate than the currently used methods. Our findings highlight the importance for evaluating the statistical properties of growth measures along with other concerns for states that are debating the reporting of growth. Our results also point out that assessing the accuracy of growth measures requires the specification of quantities of interest in terms of latent achievement rather than observed test scores, which is common practice for developing status measures but essentially never done by testing programs for growthÂ measures. Â© 2019 by the National Council on Measurement in Education",Comparing the Accuracy of Student Growth Measures,"Testing programs are often interested in using a student growth measure. This article presents analytic derivations of the accuracy of common student growth measures on both the raw scale of the test and the percentile rank scale in terms of the proportional reduction in mean squared error and the squared correlation between the estimator and target. The study contrasts the accuracy of the growth measures against that of current status measures?”current test scores and their percentile ranks. Key findings include the extent that status measures are more accurate than any of the growth measures and that alternative methods to estimate growth could be more accurate than the currently used methods. Our findings highlight the importance for evaluating the statistical properties of growth measures along with other concerns for states that are debating the reporting of growth. Our results also point out that assessing the accuracy of growth measures requires the specification of quantities of interest in terms of latent achievement rather than observed test scores, which is common practice for developing status measures but essentially never done by testing programs for growthÂ measures. Â© 2019 by the National Council on Measurement in Education","['testing', 'program', 'interested', 'student', 'growth', 'measure', 'article', 'present', 'analytic', 'derivation', 'accuracy', 'common', 'student', 'growth', 'measure', 'raw', 'scale', 'test', 'percentile', 'rank', 'scale', 'term', 'proportional', 'reduction', 'mean', 'squared', 'error', 'squared', 'correlation', 'estimator', 'target', 'study', 'contrast', 'accuracy', 'growth', 'measure', 'current', 'status', 'measure', '??, 'current', 'test', 'score', 'percentile', 'rank', 'key', 'finding', 'include', 'extent', 'status', 'measure', 'accurate', 'growth', 'measure', 'alternative', 'method', 'estimate', 'growth', 'accurate', 'currently', 'method', 'finding', 'highlight', 'importance', 'evaluate', 'statistical', 'property', 'growth', 'measure', 'concern', 'state', 'debate', 'reporting', 'growth', 'result', 'point', 'assess', 'accuracy', 'growth', 'measure', 'require', 'specification', 'quantity', 'interest', 'term', 'latent', 'achievement', 'observe', 'test', 'score', 'common', 'practice', 'develop', 'status', 'measure', 'essentially', 'test', 'program', 'growth', 'measure', 'Â©', '2019', 'National', 'Council']","['compare', 'Accuracy', 'Student', 'Growth', 'Measures']",testing program interested student growth measure article present analytic derivation accuracy common student growth measure raw scale test percentile rank scale term proportional reduction mean squared error squared correlation estimator target study contrast accuracy growth measure current status measure ??current test score percentile rank key finding include extent status measure accurate growth measure alternative method estimate growth accurate currently method finding highlight importance evaluate statistical property growth measure concern state debate reporting growth result point assess accuracy growth measure require specification quantity interest term latent achievement observe test score common practice develop status measure essentially test program growth measure Â© 2019 National Council,compare Accuracy Student Growth Measures,0.02961127,0.029273133,0.029446143,0.882025723,0.029643732,0,0.115555937,0.00226489,0.00339326,0
Guo H.; Dorans N.J.,Using Weighted Sum Scores to Close the Gap Between DIF Practice and Theory,2020,57,"We make a distinction between the operational practice of using an observed score to assess differential item functioning (DIF) and the concept of departure from measurement invariance (DMI) that conditions on a latent variable. DMI and DIF indices of effect sizes, based on the Mantel-Haenszel test of common odds ratio, converge under restricted conditions if a simple sum score is used as the matching or conditioning variable in a DIF analysis. Based on theoretical results, we demonstrate analytically that matching on a weighted sum score can significantly reduce the difference between DIF and DMI measures over what can be achieved with a simple sum score. We also examine the utility of binning methods that could facilitate potential operational use of DIF with weighted sum scores. A real data application was included to show this feasibility. Â© 2019 by the National Council on Measurement in Education",Using Weighted Sum Scores to Close the Gap Between DIF Practice and Theory,"We make a distinction between the operational practice of using an observed score to assess differential item functioning (DIF) and the concept of departure from measurement invariance (DMI) that conditions on a latent variable. DMI and DIF indices of effect sizes, based on the Mantel-Haenszel test of common odds ratio, converge under restricted conditions if a simple sum score is used as the matching or conditioning variable in a DIF analysis. Based on theoretical results, we demonstrate analytically that matching on a weighted sum score can significantly reduce the difference between DIF and DMI measures over what can be achieved with a simple sum score. We also examine the utility of binning methods that could facilitate potential operational use of DIF with weighted sum scores. A real data application was included to show this feasibility. Â© 2019 by the National Council on Measurement in Education","['distinction', 'operational', 'practice', 'observed', 'score', 'assess', 'differential', 'item', 'function', 'DIF', 'concept', 'departure', 'invariance', 'dmi', 'condition', 'latent', 'variable', 'dmi', 'dif', 'index', 'effect', 'size', 'base', 'MantelHaenszel', 'test', 'common', 'odd', 'ratio', 'converge', 'restrict', 'condition', 'simple', 'sum', 'score', 'matching', 'condition', 'variable', 'DIF', 'analysis', 'base', 'theoretical', 'result', 'demonstrate', 'analytically', 'match', 'weighted', 'sum', 'score', 'significantly', 'reduce', 'difference', 'DIF', 'dmi', 'measure', 'achieve', 'simple', 'sum', 'score', 'examine', 'utility', 'bin', 'method', 'facilitate', 'potential', 'operational', 'DIF', 'weighted', 'sum', 'score', 'real', 'datum', 'application', 'include', 'feasibility', 'Â©', '2019', 'National', 'Council']","['Weighted', 'Sum', 'Scores', 'close', 'gap', 'DIF', 'Practice', 'theory']",distinction operational practice observed score assess differential item function DIF concept departure invariance dmi condition latent variable dmi dif index effect size base MantelHaenszel test common odd ratio converge restrict condition simple sum score matching condition variable DIF analysis base theoretical result demonstrate analytically match weighted sum score significantly reduce difference DIF dmi measure achieve simple sum score examine utility bin method facilitate potential operational DIF weighted sum score real datum application include feasibility Â© 2019 National Council,Weighted Sum Scores close gap DIF Practice theory,0.029187312,0.028622051,0.883364732,0.029755914,0.02906999,0,0,0,0.017298932,0.200957988
Hong S.E.; Monroe S.; Falk C.F.,Performance of Person-Fit Statistics Under Model Misspecification,2020,57,"In educational and psychological measurement, a person-fit statistic (PFS) is designed to identify aberrant response patterns. For parametric PFSs, valid inference depends on several assumptions, one of which is that the item response theory (IRT) model is correctly specified. Previous studies have used empirical data sets to explore the effects of model misspecification on PFSs. We further this line of research by using a simulation study, which allows us to explore issues that may be of interest to practitioners. Results show that, depending on the generating and analysis item models, Type I error rates at fixed values of the latent variable may be greatly inflated, even when the aggregate rates are relatively accurate. Results also show that misspecification is most likely to affect PFSs for examinees with extreme latent variable scores. Two empirical data analyses are used to illustrate the importance of model specification. Â© 2019 by the National Council on Measurement in Education",Performance of Person-Fit Statistics Under Model Misspecification,"In educational and psychological measurement, a person-fit statistic (PFS) is designed to identify aberrant response patterns. For parametric PFSs, valid inference depends on several assumptions, one of which is that the item response theory (IRT) model is correctly specified. Previous studies have used empirical data sets to explore the effects of model misspecification on PFSs. We further this line of research by using a simulation study, which allows us to explore issues that may be of interest to practitioners. Results show that, depending on the generating and analysis item models, Type I error rates at fixed values of the latent variable may be greatly inflated, even when the aggregate rates are relatively accurate. Results also show that misspecification is most likely to affect PFSs for examinees with extreme latent variable scores. Two empirical data analyses are used to illustrate the importance of model specification. Â© 2019 by the National Council on Measurement in Education","['educational', 'psychological', 'personfit', 'statistic', 'PFS', 'design', 'identify', 'aberrant', 'response', 'pattern', 'parametric', 'pfs', 'valid', 'inference', 'depend', 'assumption', 'item', 'response', 'theory', 'IRT', 'correctly', 'specify', 'previous', 'study', 'empirical', 'datum', 'set', 'explore', 'effect', 'misspecification', 'pfs', 'far', 'line', 'research', 'simulation', 'study', 'allow', 'explore', 'issue', 'interest', 'practitioner', 'result', 'depend', 'generating', 'analysis', 'item', 'Type', 'I', 'error', 'rate', 'fix', 'value', 'latent', 'variable', 'greatly', 'inflate', 'aggregate', 'rate', 'relatively', 'accurate', 'result', 'misspecification', 'likely', 'affect', 'pfs', 'examinee', 'extreme', 'latent', 'variable', 'score', 'empirical', 'datum', 'analysis', 'illustrate', 'importance', 'specification', 'Â©', '2019', 'National', 'Council']","['performance', 'PersonFit', 'Statistics', 'Misspecification']",educational psychological personfit statistic PFS design identify aberrant response pattern parametric pfs valid inference depend assumption item response theory IRT correctly specify previous study empirical datum set explore effect misspecification pfs far line research simulation study allow explore issue interest practitioner result depend generating analysis item Type I error rate fix value latent variable greatly inflate aggregate rate relatively accurate result misspecification likely affect pfs examinee extreme latent variable score empirical datum analysis illustrate importance specification Â© 2019 National Council,performance PersonFit Statistics Misspecification,0.900295427,0.024971278,0.024778288,0.025081256,0.024873752,0.049671924,0,0.040788834,0.025115901,0.03045107
Lee W.-C.; Kim S.Y.; Choi J.; Kang Y.,IRT Approaches to Modeling Scores on Mixed-Format Tests,2020,57,"This article considers psychometric properties of composite raw scores and transformed scale scores on mixed-format tests that consist of a mixture of multiple-choice and free-response items. Test scores on several mixed-format tests are evaluated with respect to conditional and overall standard errors of measurement, score reliability, and classification consistency and accuracy under three item response theory (IRT) frameworks: unidimensional IRT (UIRT), simple structure multidimensional IRT (SS-MIRT), and bifactor multidimensional IRT (BF-MIRT) models. Illustrative examples are presented using data from three mixed-format exams with various levels of format effects. In general, the two MIRT models produced similar results, while the UIRT model resulted in consistently lower estimates of reliability and classification consistency/accuracy indices compared to the MIRT models. Â© 2019 by the National Council on Measurement in Education",IRT Approaches to Modeling Scores on Mixed-Format Tests,"This article considers psychometric properties of composite raw scores and transformed scale scores on mixed-format tests that consist of a mixture of multiple-choice and free-response items. Test scores on several mixed-format tests are evaluated with respect to conditional and overall standard errors of measurement, score reliability, and classification consistency and accuracy under three item response theory (IRT) frameworks: unidimensional IRT (UIRT), simple structure multidimensional IRT (SS-MIRT), and bifactor multidimensional IRT (BF-MIRT) models. Illustrative examples are presented using data from three mixed-format exams with various levels of format effects. In general, the two MIRT models produced similar results, while the UIRT model resulted in consistently lower estimates of reliability and classification consistency/accuracy indices compared to the MIRT models. Â© 2019 by the National Council on Measurement in Education","['article', 'consider', 'psychometric', 'property', 'composite', 'raw', 'score', 'transform', 'scale', 'score', 'mixedformat', 'test', 'consist', 'mixture', 'multiplechoice', 'freeresponse', 'item', 'test', 'score', 'mixedformat', 'test', 'evaluate', 'respect', 'conditional', 'overall', 'standard', 'error', 'score', 'reliability', 'classification', 'consistency', 'accuracy', 'item', 'response', 'theory', 'IRT', 'framework', 'unidimensional', 'IRT', 'uirt', 'simple', 'structure', 'multidimensional', 'IRT', 'SSMIRT', 'bifactor', 'multidimensional', 'IRT', 'BFMIRT', 'illustrative', 'example', 'present', 'datum', 'mixedformat', 'exam', 'level', 'format', 'effect', 'general', 'MIRT', 'produce', 'similar', 'result', 'uirt', 'result', 'consistently', 'low', 'estimate', 'reliability', 'classification', 'consistencyaccuracy', 'index', 'compare', 'MIRT', 'Â©', '2019', 'National', 'Council']","['IRT', 'approach', 'modeling', 'score', 'MixedFormat', 'test']",article consider psychometric property composite raw score transform scale score mixedformat test consist mixture multiplechoice freeresponse item test score mixedformat test evaluate respect conditional overall standard error score reliability classification consistency accuracy item response theory IRT framework unidimensional IRT uirt simple structure multidimensional IRT SSMIRT bifactor multidimensional IRT BFMIRT illustrative example present datum mixedformat exam level format effect general MIRT produce similar result uirt result consistently low estimate reliability classification consistencyaccuracy index compare MIRT Â© 2019 National Council,IRT approach modeling score MixedFormat test,0.026479637,0.026289482,0.026371892,0.894534346,0.026324643,0,0.132311013,0,0.019634285,0.007781968
Chen C.-W.; Wang W.-C.; Chiu M.M.; Ro S.,Item Selection and Exposure Control Methods for Computerized Adaptive Testing with Multidimensional Ranking Items,2020,57,"The use of computerized adaptive testing algorithms for ranking items (e.g., college preferences, career choices) involves two major challenges: unacceptably high computation times (selecting from a large item pool with many dimensions) and biased results (enhanced preferences or intensified examinee responses because of repeated statements across items). To address these issues, we introduce subpool partition strategies for item selection and within-person statement exposure control procedures. Simulations showed that the multinomial method reduces computation time while maintaining measurement precision. Both the freeze and revised Sympson-Hetter online (RSHO) methods controlled the statement exposure rate; RSHO sacrificed some measurement precision but increased pool use. Furthermore, preventing a statement's repetition on consecutive items neither hindered the effectiveness of the freeze or RSHO method nor reduced measurement precision. Â© 2019 by the National Council on Measurement in Education",Item Selection and Exposure Control Methods for Computerized Adaptive Testing with Multidimensional Ranking Items,"The use of computerized adaptive testing algorithms for ranking items (e.g., college preferences, career choices) involves two major challenges: unacceptably high computation times (selecting from a large item pool with many dimensions) and biased results (enhanced preferences or intensified examinee responses because of repeated statements across items). To address these issues, we introduce subpool partition strategies for item selection and within-person statement exposure control procedures. Simulations showed that the multinomial method reduces computation time while maintaining measurement precision. Both the freeze and revised Sympson-Hetter online (RSHO) methods controlled the statement exposure rate; RSHO sacrificed some measurement precision but increased pool use. Furthermore, preventing a statement's repetition on consecutive items neither hindered the effectiveness of the freeze or RSHO method nor reduced measurement precision. Â© 2019 by the National Council on Measurement in Education","['computerized', 'adaptive', 'testing', 'algorithm', 'rank', 'item', 'eg', 'college', 'preference', 'career', 'choice', 'involve', 'major', 'challenge', 'unacceptably', 'high', 'computation', 'time', 'select', 'large', 'item', 'pool', 'dimension', 'biased', 'result', 'enhance', 'preference', 'intensify', 'examinee', 'response', 'repeat', 'statement', 'item', 'address', 'issue', 'introduce', 'subpool', 'partition', 'strategy', 'item', 'selection', 'withinperson', 'statement', 'exposure', 'control', 'procedure', 'Simulations', 'multinomial', 'method', 'reduce', 'computation', 'time', 'maintain', 'precision', 'freeze', 'revise', 'SympsonHetter', 'online', 'RSHO', 'method', 'control', 'statement', 'exposure', 'rate', 'RSHO', 'sacrifice', 'precision', 'increase', 'pool', 'furthermore', 'prevent', 'statement', 'repetition', 'consecutive', 'item', 'hinder', 'effectiveness', 'freeze', 'RSHO', 'method', 'reduce', 'precision', 'Â©', '2019', 'National', 'Council']","['Item', 'Selection', 'Exposure', 'Control', 'Methods', 'Computerized', 'Adaptive', 'Testing', 'Multidimensional', 'Ranking', 'Items']",computerized adaptive testing algorithm rank item eg college preference career choice involve major challenge unacceptably high computation time select large item pool dimension biased result enhance preference intensify examinee response repeat statement item address issue introduce subpool partition strategy item selection withinperson statement exposure control procedure Simulations multinomial method reduce computation time maintain precision freeze revise SympsonHetter online RSHO method control statement exposure rate RSHO sacrifice precision increase pool furthermore prevent statement repetition consecutive item hinder effectiveness freeze RSHO method reduce precision Â© 2019 National Council,Item Selection Exposure Control Methods Computerized Adaptive Testing Multidimensional Ranking Items,0.026215028,0.025854655,0.025940825,0.895844371,0.026145121,0.031184736,0.06369469,0.000828829,0,0
Castellano K.E.; McCaffrey D.F.,Estimating the Accuracy of Relative Growth Measures Using Empirical Data,2020,57,"The residual gain score has been of historical interest, and its percentile rank has been of interest more recently given its close correspondence to the popular Student Growth Percentile. However, these estimators suffer from low accuracy and systematic bias (bias conditional on prior latent achievement). This article explores three alternatives?”using the expected a posterior (EAP), conditioning on an additional lagged score, and correcting for measurement error bias from the prior score (Corrected-Observed)?”evaluated in terms of their systematic bias, squared correlation with their target (R2), and proportional reduction in mean squared error (PRMSE). Both analytic results (under model assumptions) and empirical results (found using item response data to calculate the growth estimators) reveal that the EAP estimators are the most accurate, whereas the Corrected-Observed removes systematic bias, but reduces overall accuracy. Adding another prior year often decreases accuracy but only slightly reduces systematic bias at realistic test reliabilities. For all estimators, R2 and PRMSE are substantially below levels that are considered necessary for reporting educational measurements with moderate to high stakes. For all but the EAP, the raw residual gain estimators have negative PRMSE, indicating that inferences about a student's latent growth would be more accurate if students were assigned the average residual rather than estimating their residual. Â© 2019 by the National Council on Measurement in Education",Estimating the Accuracy of Relative Growth Measures Using Empirical Data,"The residual gain score has been of historical interest, and its percentile rank has been of interest more recently given its close correspondence to the popular Student Growth Percentile. However, these estimators suffer from low accuracy and systematic bias (bias conditional on prior latent achievement). This article explores three alternatives?”using the expected a posterior (EAP), conditioning on an additional lagged score, and correcting for measurement error bias from the prior score (Corrected-Observed)?”evaluated in terms of their systematic bias, squared correlation with their target (R2), and proportional reduction in mean squared error (PRMSE). Both analytic results (under model assumptions) and empirical results (found using item response data to calculate the growth estimators) reveal that the EAP estimators are the most accurate, whereas the Corrected-Observed removes systematic bias, but reduces overall accuracy. Adding another prior year often decreases accuracy but only slightly reduces systematic bias at realistic test reliabilities. For all estimators, R2 and PRMSE are substantially below levels that are considered necessary for reporting educational measurements with moderate to high stakes. For all but the EAP, the raw residual gain estimators have negative PRMSE, indicating that inferences about a student's latent growth would be more accurate if students were assigned the average residual rather than estimating their residual. Â© 2019 by the National Council on Measurement in Education","['residual', 'gain', 'score', 'historical', 'interest', 'percentile', 'rank', 'interest', 'recently', 'close', 'correspondence', 'popular', 'Student', 'Growth', 'Percentile', 'estimator', 'suffer', 'low', 'accuracy', 'systematic', 'bias', 'bias', 'conditional', 'prior', 'latent', 'achievement', 'article', 'explore', 'alternative', '??, 'expect', 'posterior', 'EAP', 'conditioning', 'additional', 'lag', 'score', 'correct', 'error', 'bias', 'prior', 'score', 'CorrectedObserved', '??, 'evaluate', 'term', 'systematic', 'bias', 'square', 'correlation', 'target', 'r2', 'proportional', 'reduction', 'mean', 'squared', 'error', 'PRMSE', 'analytic', 'result', 'assumption', 'empirical', 'result', 'find', 'item', 'response', 'datum', 'calculate', 'growth', 'estimator', 'reveal', 'EAP', 'estimator', 'accurate', 'CorrectedObserved', 'remove', 'systematic', 'bias', 'reduce', 'overall', 'accuracy', 'add', 'prior', 'year', 'decrease', 'accuracy', 'slightly', 'reduce', 'systematic', 'bias', 'realistic', 'test', 'reliability', 'estimator', 'R2', 'PRMSE', 'substantially', 'level', 'consider', 'necessary', 'report', 'educational', 'moderate', 'high', 'stake', 'EAP', 'raw', 'residual', 'gain', 'estimator', 'negative', 'prmse', 'indicate', 'inference', 'student', 'latent', 'growth', 'accurate', 'student', 'assign', 'average', 'residual', 'estimate', 'residual', 'Â©', '2019', 'National', 'Council']","['estimate', 'Accuracy', 'Relative', 'Growth', 'measure', 'empirical', 'Data']",residual gain score historical interest percentile rank interest recently close correspondence popular Student Growth Percentile estimator suffer low accuracy systematic bias bias conditional prior latent achievement article explore alternative ??expect posterior EAP conditioning additional lag score correct error bias prior score CorrectedObserved ??evaluate term systematic bias square correlation target r2 proportional reduction mean squared error PRMSE analytic result assumption empirical result find item response datum calculate growth estimator reveal EAP estimator accurate CorrectedObserved remove systematic bias reduce overall accuracy add prior year decrease accuracy slightly reduce systematic bias realistic test reliability estimator R2 PRMSE substantially level consider necessary report educational moderate high stake EAP raw residual gain estimator negative prmse indicate inference student latent growth accurate student assign average residual estimate residual Â© 2019 National Council,estimate Accuracy Relative Growth measure empirical Data,0.024238804,0.023535823,0.903373277,0.024677823,0.024174272,0.010473396,0.057683136,0.030506274,0.014208958,0.012266025
Liu C.; Kolen M.J.,A New Statistic for Selecting the Smoothing Parameter for Polynomial Loglinear Equating Under the Random Groups Design,2020,57,"Smoothing is designed to yield smoother equating results that can reduce random equating error without introducing very much systematic error. The main objective of this study is to propose a new statistic and to compare its performance to the performance of the Akaike information criterion and likelihood ratio chi-square difference statistics in selecting the smoothing parameter for polynomial loglinear equating under the random groups design. These model selection statistics were compared for four sample sizes (500, 1,000, 2,000, and 3,000) and eight simulated equating conditions, including both conditions where equating is not needed and conditions where equating is needed. The results suggest that all model selection statistics tend to improve the equating accuracy by reducing the total equating error. The new statistic tended to have less overall error than the other two methods. Â© 2019 by the National Council on Measurement in Education",A New Statistic for Selecting the Smoothing Parameter for Polynomial Loglinear Equating Under the Random Groups Design,"Smoothing is designed to yield smoother equating results that can reduce random equating error without introducing very much systematic error. The main objective of this study is to propose a new statistic and to compare its performance to the performance of the Akaike information criterion and likelihood ratio chi-square difference statistics in selecting the smoothing parameter for polynomial loglinear equating under the random groups design. These model selection statistics were compared for four sample sizes (500, 1,000, 2,000, and 3,000) and eight simulated equating conditions, including both conditions where equating is not needed and conditions where equating is needed. The results suggest that all model selection statistics tend to improve the equating accuracy by reducing the total equating error. The new statistic tended to have less overall error than the other two methods. Â© 2019 by the National Council on Measurement in Education","['smoothing', 'design', 'yield', 'smoother', 'equating', 'result', 'reduce', 'random', 'equating', 'error', 'introduce', 'systematic', 'error', 'main', 'objective', 'study', 'propose', 'new', 'statistic', 'compare', 'performance', 'performance', 'Akaike', 'information', 'criterion', 'likelihood', 'ratio', 'chisquare', 'difference', 'statistic', 'select', 'smooth', 'parameter', 'polynomial', 'loglinear', 'equate', 'random', 'group', 'design', 'selection', 'statistic', 'compare', 'sample', 'size', '500', '1000', '2000', '3000', 'simulated', 'equating', 'condition', 'include', 'condition', 'equating', 'need', 'condition', 'equating', 'need', 'result', 'suggest', 'selection', 'statistic', 'tend', 'improve', 'equate', 'accuracy', 'reduce', 'total', 'equating', 'error', 'new', 'statistic', 'tend', 'overall', 'error', 'method', 'Â©', '2019', 'National', 'Council']","['New', 'Statistic', 'select', 'Smoothing', 'Parameter', 'Polynomial', 'Loglinear', 'Equating', 'Random', 'Groups', 'Design']",smoothing design yield smoother equating result reduce random equating error introduce systematic error main objective study propose new statistic compare performance performance Akaike information criterion likelihood ratio chisquare difference statistic select smooth parameter polynomial loglinear equate random group design selection statistic compare sample size 500 1000 2000 3000 simulated equating condition include condition equating need condition equating need result suggest selection statistic tend improve equate accuracy reduce total equating error new statistic tend overall error method Â© 2019 National Council,New Statistic select Smoothing Parameter Polynomial Loglinear Equating Random Groups Design,0.883920891,0.028980641,0.02894196,0.029114373,0.029042135,0,0,0.21549307,0,0
van der Linden W.J.; Choi S.W.,Improving Item-Exposure Control in Adaptive Testing,2020,57,"One of the methods of controlling test security in adaptive testing is imposing random item-ineligibility constraints on the selection of the items with probabilities automatically updated to maintain a predetermined upper bound on the exposure rates. Three major improvements of the method are presented. First, a few modifications to improve the initialization of the method and accelerate the impact of its feedback mechanism on the observed item-exposure rates are introduced. Second, the case of conditional item-exposure control given the uncertainty of examinee's ability parameter is addressed. Third, although rare for a well-designed item pool, when applied in combination with the shadow-test approach to adaptive testing the method may meet occasional infeasibility of the shadow-test model. A big M method is proposed that resolves the issue. The practical advantages of the improvements are illustrated using simulated adaptive testing from a real-world item pool under a variety ofÂ conditions. Â© 2019 by the National Council on Measurement in Education",Improving Item-Exposure Control in Adaptive Testing,"One of the methods of controlling test security in adaptive testing is imposing random item-ineligibility constraints on the selection of the items with probabilities automatically updated to maintain a predetermined upper bound on the exposure rates. Three major improvements of the method are presented. First, a few modifications to improve the initialization of the method and accelerate the impact of its feedback mechanism on the observed item-exposure rates are introduced. Second, the case of conditional item-exposure control given the uncertainty of examinee's ability parameter is addressed. Third, although rare for a well-designed item pool, when applied in combination with the shadow-test approach to adaptive testing the method may meet occasional infeasibility of the shadow-test model. A big M method is proposed that resolves the issue. The practical advantages of the improvements are illustrated using simulated adaptive testing from a real-world item pool under a variety ofÂ conditions. Â© 2019 by the National Council on Measurement in Education","['method', 'control', 'test', 'security', 'adaptive', 'testing', 'impose', 'random', 'itemineligibility', 'constraint', 'selection', 'item', 'probability', 'automatically', 'update', 'maintain', 'predetermine', 'upper', 'bind', 'exposure', 'rate', 'major', 'improvement', 'method', 'present', 'modification', 'improve', 'initialization', 'method', 'accelerate', 'impact', 'feedback', 'mechanism', 'observed', 'itemexposure', 'rate', 'introduce', 'Second', 'case', 'conditional', 'itemexposure', 'control', 'uncertainty', 'examinee', 'ability', 'parameter', 'address', 'rare', 'welldesigne', 'item', 'pool', 'apply', 'combination', 'shadowtest', 'approach', 'adaptive', 'testing', 'method', 'meet', 'occasional', 'infeasibility', 'shadowt', 'big', 'm', 'method', 'propose', 'resolve', 'issue', 'practical', 'advantage', 'improvement', 'illustrate', 'simulate', 'adaptive', 'testing', 'realworld', 'item', 'pool', 'variety', 'condition', 'Â©', '2019', 'National', 'Council']","['improve', 'ItemExposure', 'Control', 'Adaptive', 'Testing']",method control test security adaptive testing impose random itemineligibility constraint selection item probability automatically update maintain predetermine upper bind exposure rate major improvement method present modification improve initialization method accelerate impact feedback mechanism observed itemexposure rate introduce Second case conditional itemexposure control uncertainty examinee ability parameter address rare welldesigne item pool apply combination shadowtest approach adaptive testing method meet occasional infeasibility shadowt big m method propose resolve issue practical advantage improvement illustrate simulate adaptive testing realworld item pool variety condition Â© 2019 National Council,improve ItemExposure Control Adaptive Testing,0.023818438,0.023426924,0.023449095,0.023759967,0.905545576,0.003028445,0.119775423,0.004989688,0,0.001463162
Huelmann T.; Debelak R.; Strobl C.,A Comparison of Aggregation Rules for Selecting Anchor Items in Multigroup DIF Analysis,2020,57,"This study addresses the topic of how anchoring methods for differential item functioning (DIF) analysis can be used in multigroup scenarios. The direct approach would be to combine anchoring methods developed for two-group scenarios with multigroup DIF-detection methods. Alternatively, multiple tests could be carried out. The results of these tests need to be aggregated to determine the anchor for the final DIF analysis. In this study, the direct approach and three aggregation rules are investigated. All approaches are combined with a variety of anchoring methods, such as the ?œall-other purified??and ?œmean p-value threshold??methods, in two simulation studies based on the Rasch model. Our results indicate that the direct approach generally does not lead to more accurate or even to inferior results than the aggregation rules. The min rule overall shows the best trade-off between low false alarm rate and medium to high hit rate. However, it might be too sensitive when the number of groups is large. In this case, the all rule may be a good compromise. We also take a closer look at the anchor selection method ?œnext candidate,??which performed rather poorly, and suggest possibleÂ improvements. Â© 2019 by the National Council on Measurement in Education",A Comparison of Aggregation Rules for Selecting Anchor Items in Multigroup DIF Analysis,"This study addresses the topic of how anchoring methods for differential item functioning (DIF) analysis can be used in multigroup scenarios. The direct approach would be to combine anchoring methods developed for two-group scenarios with multigroup DIF-detection methods. Alternatively, multiple tests could be carried out. The results of these tests need to be aggregated to determine the anchor for the final DIF analysis. In this study, the direct approach and three aggregation rules are investigated. All approaches are combined with a variety of anchoring methods, such as the ?œall-other purified??and ?œmean p-value threshold??methods, in two simulation studies based on the Rasch model. Our results indicate that the direct approach generally does not lead to more accurate or even to inferior results than the aggregation rules. The min rule overall shows the best trade-off between low false alarm rate and medium to high hit rate. However, it might be too sensitive when the number of groups is large. In this case, the all rule may be a good compromise. We also take a closer look at the anchor selection method ?œnext candidate,??which performed rather poorly, and suggest possibleÂ improvements. Â© 2019 by the National Council on Measurement in Education","['study', 'address', 'topic', 'anchor', 'method', 'differential', 'item', 'function', 'dif', 'analysis', 'multigroup', 'scenario', 'direct', 'approach', 'combine', 'anchor', 'method', 'develop', 'twogroup', 'scenario', 'multigroup', 'DIFdetection', 'method', 'alternatively', 'multiple', 'test', 'carry', 'result', 'test', 'need', 'aggregate', 'determine', 'anchor', 'final', 'DIF', 'analysis', 'study', 'direct', 'approach', 'aggregation', 'rule', 'investigate', 'approach', 'combine', 'variety', 'anchor', 'method', '""', 'allother', 'purify', '""', '""', 'mean', 'pvalue', 'threshold', '""', 'method', 'simulation', 'study', 'base', 'Rasch', 'result', 'indicate', 'direct', 'approach', 'generally', 'lead', 'accurate', 'inferior', 'result', 'aggregation', 'rule', 'min', 'rule', 'overall', 'good', 'tradeoff', 'low', 'false', 'alarm', 'rate', 'medium', 'high', 'hit', 'rate', 'sensitive', 'number', 'group', 'large', 'case', 'rule', 'good', 'compromise', 'close', 'look', 'anchor', 'selection', 'method', '""', 'candidate', '""', 'perform', 'poorly', 'suggest', 'possible', 'improvement', 'Â©', '2019', 'National', 'Council']","['Comparison', 'Aggregation', 'Rules', 'Selecting', 'Anchor', 'Items', 'Multigroup', 'DIF', 'Analysis']","study address topic anchor method differential item function dif analysis multigroup scenario direct approach combine anchor method develop twogroup scenario multigroup DIFdetection method alternatively multiple test carry result test need aggregate determine anchor final DIF analysis study direct approach aggregation rule investigate approach combine variety anchor method "" allother purify "" "" mean pvalue threshold "" method simulation study base Rasch result indicate direct approach generally lead accurate inferior result aggregation rule min rule overall good tradeoff low false alarm rate medium high hit rate sensitive number group large case rule good compromise close look anchor selection method "" candidate "" perform poorly suggest possible improvement Â© 2019 National Council",Comparison Aggregation Rules Selecting Anchor Items Multigroup DIF Analysis,0.025464705,0.024785136,0.899052981,0.025570694,0.025126485,0,0.071879881,0.008471488,0,0.075886649
Haberman S.J.,Statistical Theory and Assessment Practice,2020,57,Examples of the impact of statistical theory on assessment practice are provided from the perspective of a statistician trained in theoretical statistics who began to work on assessments. Goodness of fit of item-response models is examined in terms of restricted likelihood-ratio tests and generalized residuals. Minimum discriminant information adjustment is used for linking with no anchors or problematic anchors and for repeater analysis. Assessment issues are examined in cases in which the number of parameters is large relative to the number ofÂ observations. Â© 2020 by the National Council on Measurement in Education,,Examples of the impact of statistical theory on assessment practice are provided from the perspective of a statistician trained in theoretical statistics who began to work on assessments. Goodness of fit of item-response models is examined in terms of restricted likelihood-ratio tests and generalized residuals. Minimum discriminant information adjustment is used for linking with no anchors or problematic anchors and for repeater analysis. Assessment issues are examined in cases in which the number of parameters is large relative to the number ofÂ observations. Â© 2020 by the National Council on Measurement in Education,"['example', 'impact', 'statistical', 'theory', 'assessment', 'practice', 'provide', 'perspective', 'statistician', 'train', 'theoretical', 'statistic', 'begin', 'work', 'assessment', 'Goodness', 'fit', 'itemresponse', 'examine', 'term', 'restricted', 'likelihoodratio', 'test', 'generalize', 'residual', 'Minimum', 'discriminant', 'information', 'adjustment', 'link', 'anchor', 'problematic', 'anchor', 'repeater', 'analysis', 'assessment', 'issue', 'examine', 'case', 'number', 'parameter', 'large', 'relative', 'number', 'observation', 'Â©', '2020', 'National', 'Council']",,example impact statistical theory assessment practice provide perspective statistician train theoretical statistic begin work assessment Goodness fit itemresponse examine term restricted likelihoodratio test generalize residual Minimum discriminant information adjustment link anchor problematic anchor repeater analysis assessment issue examine case number parameter large relative number observation Â© 2020 National Council,,0.028384087,0.028105101,0.028074031,0.887310836,0.028125944,0.003239038,0.036323513,0.027826319,0.037051861,0.005548548
Kim S.Y.; Lee W.-C.,Classification Consistency and Accuracy With Atypical Score Distributions,2020,57,"The current study aims to evaluate the performance of three non-IRT procedures (i.e., normal approximation, Livingston-Lewis, and compound multinomial) for estimating classification indices when the observed score distribution shows atypical patterns: (a) bimodality, (b) structural (i.e., systematic) bumpiness, or (c) structural zeros (i.e., no frequencies). Under a bimodal distribution, the normal approximation procedure produced substantially large bias. For a distribution with structural bumpiness, the compound multinomial procedure tended to introduce larger bias. Under a distribution with structural zeroes, the relative performance of selected estimation procedures depended on cut score location and the sample-size conditions. In general, the differences in estimation errors among the three procedures were not substantially large. Â© 2019 by the National Council on Measurement in Education",Classification Consistency and Accuracy With Atypical Score Distributions,"The current study aims to evaluate the performance of three non-IRT procedures (i.e., normal approximation, Livingston-Lewis, and compound multinomial) for estimating classification indices when the observed score distribution shows atypical patterns: (a) bimodality, (b) structural (i.e., systematic) bumpiness, or (c) structural zeros (i.e., no frequencies). Under a bimodal distribution, the normal approximation procedure produced substantially large bias. For a distribution with structural bumpiness, the compound multinomial procedure tended to introduce larger bias. Under a distribution with structural zeroes, the relative performance of selected estimation procedures depended on cut score location and the sample-size conditions. In general, the differences in estimation errors among the three procedures were not substantially large. Â© 2019 by the National Council on Measurement in Education","['current', 'study', 'aim', 'evaluate', 'performance', 'nonIRT', 'procedure', 'ie', 'normal', 'approximation', 'LivingstonLewis', 'compound', 'multinomial', 'estimate', 'classification', 'index', 'observe', 'score', 'distribution', 'atypical', 'pattern', 'bimodality', 'b', 'structural', 'ie', 'systematic', 'bumpiness', 'c', 'structural', 'zero', 'ie', 'frequency', 'bimodal', 'distribution', 'normal', 'approximation', 'procedure', 'produce', 'substantially', 'large', 'bias', 'distribution', 'structural', 'bumpiness', 'compound', 'multinomial', 'procedure', 'tend', 'introduce', 'large', 'bias', 'distribution', 'structural', 'zero', 'relative', 'performance', 'select', 'estimation', 'procedure', 'depend', 'cut', 'score', 'location', 'samplesize', 'condition', 'general', 'difference', 'estimation', 'error', 'procedure', 'substantially', 'large', 'Â©', '2019', 'National', 'Council']","['Classification', 'Consistency', 'Accuracy', 'Atypical', 'Score', 'distribution']",current study aim evaluate performance nonIRT procedure ie normal approximation LivingstonLewis compound multinomial estimate classification index observe score distribution atypical pattern bimodality b structural ie systematic bumpiness c structural zero ie frequency bimodal distribution normal approximation procedure produce substantially large bias distribution structural bumpiness compound multinomial procedure tend introduce large bias distribution structural zero relative performance select estimation procedure depend cut score location samplesize condition general difference estimation error procedure substantially large Â© 2019 National Council,Classification Consistency Accuracy Atypical Score distribution,0.02995069,0.02944425,0.029541246,0.029842204,0.881221611,0,0.026043147,0.053175965,0.005322389,0.00867681
Kim K.Y.,Two IRT Fixed Parameter Calibration Methods for the Bifactor Model,2020,57,"New items are often evaluated prior to their operational use to obtain item response theory (IRT) item parameter estimates for quality control purposes. Fixed parameter calibration is one linking method that is widely used to estimate parameters for new items and place them on the desired scale. This article provides detailed descriptions of two fixed parameter calibration methods for the bifactor model and compares their relative performance through simulation. The two methods, which were natural generalizations of their counterparts in the unidimensional context, are the one prior weights updating and multiple expectation-maximization (EM) cycles (OWU-MEM) and multiple prior weights updating and multiple EM cycles (MWU-MEM) methods. In addition, for comparison purposes, the separate calibration method with Haebara linking was included in the simulation. In general, the MWU-MEM method recovered item parameters well for both equivalent and nonequivalent groups, whereas the OWU-MEM method worked well only for equivalent groups. With a few exceptions, the MWU-MEM and Haebara methods showed comparable item parameter recovery. Â© 2019 by the National Council on Measurement in Education",Two IRT Fixed Parameter Calibration Methods for the Bifactor Model,"New items are often evaluated prior to their operational use to obtain item response theory (IRT) item parameter estimates for quality control purposes. Fixed parameter calibration is one linking method that is widely used to estimate parameters for new items and place them on the desired scale. This article provides detailed descriptions of two fixed parameter calibration methods for the bifactor model and compares their relative performance through simulation. The two methods, which were natural generalizations of their counterparts in the unidimensional context, are the one prior weights updating and multiple expectation-maximization (EM) cycles (OWU-MEM) and multiple prior weights updating and multiple EM cycles (MWU-MEM) methods. In addition, for comparison purposes, the separate calibration method with Haebara linking was included in the simulation. In general, the MWU-MEM method recovered item parameters well for both equivalent and nonequivalent groups, whereas the OWU-MEM method worked well only for equivalent groups. With a few exceptions, the MWU-MEM and Haebara methods showed comparable item parameter recovery. Â© 2019 by the National Council on Measurement in Education","['new', 'item', 'evaluate', 'prior', 'operational', 'obtain', 'item', 'response', 'theory', 'IRT', 'item', 'parameter', 'estimate', 'quality', 'control', 'purpose', 'fix', 'parameter', 'calibration', 'link', 'method', 'widely', 'estimate', 'parameter', 'new', 'item', 'place', 'desire', 'scale', 'article', 'provide', 'detailed', 'description', 'fix', 'parameter', 'calibration', 'method', 'bifactor', 'compare', 'relative', 'performance', 'simulation', 'method', 'natural', 'generalization', 'counterpart', 'unidimensional', 'context', 'prior', 'weight', 'update', 'multiple', 'expectationmaximization', 'EM', 'cycle', 'owumem', 'multiple', 'prior', 'weight', 'update', 'multiple', 'EM', 'cycle', 'MWUMEM', 'method', 'addition', 'comparison', 'purpose', 'separate', 'calibration', 'method', 'Haebara', 'link', 'include', 'simulation', 'general', 'MWUMEM', 'method', 'recover', 'item', 'parameter', 'equivalent', 'nonequivalent', 'group', 'owumem', 'method', 'work', 'equivalent', 'group', 'exception', 'MWUMEM', 'Haebara', 'method', 'comparable', 'item', 'parameter', 'recovery', 'Â©', '2019', 'National', 'Council']","['IRT', 'Fixed', 'Parameter', 'Calibration', 'Methods', 'Bifactor']",new item evaluate prior operational obtain item response theory IRT item parameter estimate quality control purpose fix parameter calibration link method widely estimate parameter new item place desire scale article provide detailed description fix parameter calibration method bifactor compare relative performance simulation method natural generalization counterpart unidimensional context prior weight update multiple expectationmaximization EM cycle owumem multiple prior weight update multiple EM cycle MWUMEM method addition comparison purpose separate calibration method Haebara link include simulation general MWUMEM method recover item parameter equivalent nonequivalent group owumem method work equivalent group exception MWUMEM Haebara method comparable item parameter recovery Â© 2019 National Council,IRT Fixed Parameter Calibration Methods Bifactor,0.893188039,0.026543314,0.026540008,0.027024213,0.026704426,0.00094983,0.148236589,0.008309738,0,0
Pokropek A.; Borgonovi F.,Linking via Pseudo-Equivalent Group Design: Methodological Considerations and an Application to the PISA and PIAAC Assessments,2020,57,This article presents the pseudo-equivalent group approach and discusses how it can enhance the quality of linking in the presence of nonequivalent groups. The pseudo-equivalent group approach allows to achieve pseudo-equivalence using propensity score reweighting techniques. We use it to perform linking to establish scale concordance between two assessments. The article presents Monte-Carlo simulations and a real data application based on data from the Survey of Adult Skills (PIAAC) and the Programme for International Student Assessment (PISA). Monte-Carlo simulations suggest that the pseudo-equivalent group design is particularly useful whenever there is a large overlap across the two groups with respect to balancing variables and when the correlation between such variables and ability is medium or high. The example based on PISA and PIAAC data indicates that the approach can provide reasonable accurate linking that can be used for group-level comparisons. Â© 2019 by the National Council on Measurement in Education,Linking via Pseudo-Equivalent Group Design: Methodological Considerations and an Application to the PISA and PIAAC Assessments,This article presents the pseudo-equivalent group approach and discusses how it can enhance the quality of linking in the presence of nonequivalent groups. The pseudo-equivalent group approach allows to achieve pseudo-equivalence using propensity score reweighting techniques. We use it to perform linking to establish scale concordance between two assessments. The article presents Monte-Carlo simulations and a real data application based on data from the Survey of Adult Skills (PIAAC) and the Programme for International Student Assessment (PISA). Monte-Carlo simulations suggest that the pseudo-equivalent group design is particularly useful whenever there is a large overlap across the two groups with respect to balancing variables and when the correlation between such variables and ability is medium or high. The example based on PISA and PIAAC data indicates that the approach can provide reasonable accurate linking that can be used for group-level comparisons. Â© 2019 by the National Council on Measurement in Education,"['article', 'present', 'pseudoequivalent', 'group', 'approach', 'discuss', 'enhance', 'quality', 'link', 'presence', 'nonequivalent', 'group', 'pseudoequivalent', 'group', 'approach', 'allow', 'achieve', 'pseudoequivalence', 'propensity', 'score', 'reweighte', 'technique', 'perform', 'link', 'establish', 'scale', 'concordance', 'assessment', 'article', 'present', 'MonteCarlo', 'simulation', 'real', 'data', 'application', 'base', 'datum', 'Survey', 'Adult', 'Skills', 'PIAAC', 'Programme', 'International', 'Student', 'Assessment', 'PISA', 'MonteCarlo', 'simulation', 'suggest', 'pseudoequivalent', 'group', 'design', 'particularly', 'useful', 'large', 'overlap', 'group', 'respect', 'balance', 'variable', 'correlation', 'variable', 'ability', 'medium', 'high', 'example', 'base', 'PISA', 'PIAAC', 'datum', 'indicate', 'approach', 'provide', 'reasonable', 'accurate', 'linking', 'grouplevel', 'comparison', 'Â©', '2019', 'National', 'Council']","['link', 'PseudoEquivalent', 'Group', 'Design', 'Methodological', 'Considerations', 'application', 'PISA', 'PIAAC', 'assessment']",article present pseudoequivalent group approach discuss enhance quality link presence nonequivalent group pseudoequivalent group approach allow achieve pseudoequivalence propensity score reweighte technique perform link establish scale concordance assessment article present MonteCarlo simulation real data application base datum Survey Adult Skills PIAAC Programme International Student Assessment PISA MonteCarlo simulation suggest pseudoequivalent group design particularly useful large overlap group respect balance variable correlation variable ability medium high example base PISA PIAAC datum indicate approach provide reasonable accurate linking grouplevel comparison Â© 2019 National Council,link PseudoEquivalent Group Design Methodological Considerations application PISA PIAAC assessment,0.895612035,0.026107486,0.026028128,0.026173331,0.02607902,0.005298502,0.070596746,0.026241748,0.027214197,0.008874913
Langenfeld T.; Thomas J.; Zhu R.; Morris C.A.,Integrating Multiple Sources of Validity Evidence for an Assessment-Based Cognitive Model,2020,57,An assessment of graphic literacy was developed by articulating and subsequently validating a skills-based cognitive model intended to substantiate the plausibility of score interpretations. Model validation involved use of multiple sources of evidence derived from large-scale field testing and cognitive labs studies. Data from large-scale field testing were evaluated using traditional psychometric methods. The psychometric analyses were augmented using eye tracking technology to perform gaze pattern and pupillometry analyses to gain better understanding of problem-solving strategies and cognitive load. Findings from the data sources were integrated to provide strong evidence supporting the model and score interpretations. Implications for using gaze pattern and pupillometry analyses to enhance learning and assessment are discussed. Â© 2019 by the National Council on Measurement in Education,Integrating Multiple Sources of Validity Evidence for an Assessment-Based Cognitive Model,An assessment of graphic literacy was developed by articulating and subsequently validating a skills-based cognitive model intended to substantiate the plausibility of score interpretations. Model validation involved use of multiple sources of evidence derived from large-scale field testing and cognitive labs studies. Data from large-scale field testing were evaluated using traditional psychometric methods. The psychometric analyses were augmented using eye tracking technology to perform gaze pattern and pupillometry analyses to gain better understanding of problem-solving strategies and cognitive load. Findings from the data sources were integrated to provide strong evidence supporting the model and score interpretations. Implications for using gaze pattern and pupillometry analyses to enhance learning and assessment are discussed. Â© 2019 by the National Council on Measurement in Education,"['assessment', 'graphic', 'literacy', 'develop', 'articulating', 'subsequently', 'validate', 'skillsbased', 'cognitive', 'intend', 'substantiate', 'plausibility', 'score', 'interpretation', 'validation', 'involve', 'multiple', 'source', 'evidence', 'derive', 'largescale', 'field', 'testing', 'cognitive', 'labs', 'study', 'Data', 'largescale', 'field', 'testing', 'evaluate', 'traditional', 'psychometric', 'method', 'psychometric', 'analysis', 'augment', 'eye', 'tracking', 'technology', 'perform', 'gaze', 'pattern', 'pupillometry', 'analysis', 'gain', 'understanding', 'problemsolving', 'strategy', 'cognitive', 'load', 'finding', 'datum', 'source', 'integrate', 'provide', 'strong', 'evidence', 'support', 'score', 'interpretation', 'implication', 'gaze', 'pattern', 'pupillometry', 'analysis', 'enhance', 'learning', 'assessment', 'discuss', 'Â©', '2019', 'National', 'Council']","['integrate', 'Multiple', 'Sources', 'Validity', 'Evidence', 'AssessmentBased', 'Cognitive']",assessment graphic literacy develop articulating subsequently validate skillsbased cognitive intend substantiate plausibility score interpretation validation involve multiple source evidence derive largescale field testing cognitive labs study Data largescale field testing evaluate traditional psychometric method psychometric analysis augment eye tracking technology perform gaze pattern pupillometry analysis gain understanding problemsolving strategy cognitive load finding datum source integrate provide strong evidence support score interpretation implication gaze pattern pupillometry analysis enhance learning assessment discuss Â© 2019 National Council,integrate Multiple Sources Validity Evidence AssessmentBased Cognitive,0.897216099,0.025647242,0.025655204,0.025752514,0.025728941,0.003953752,0.041265276,0.002636195,0.058991916,0
Mislevy R.J.,Statistical Theoreticians and Educational Assessment: Comments on Shelby Haberman's NCME Career Contributions Award,2020,57,"In his 2019 NCME Career Contributions Award address, Dr. Shelby Haberman uses examples of three kinds to illustrate how his training in theoretical statistics influenced his contributions to educational measurement. I bracket my comments on his address, and his contributions more generally, by considering two questions: Why might any theoretical statisticians receive this award? Why aren't all recipients theoretical statisticians? Through the course of the discussion emerges the answer to a third, easier, question: Why Shelby Haberman?. Â© 2020 by the National Council on Measurement in Education",Statistical Theoreticians and Educational Assessment: Comments on Shelby Haberman's NCME Career Contributions Award,"In his 2019 NCME Career Contributions Award address, Dr. Shelby Haberman uses examples of three kinds to illustrate how his training in theoretical statistics influenced his contributions to educational measurement. I bracket my comments on his address, and his contributions more generally, by considering two questions: Why might any theoretical statisticians receive this award? Why aren't all recipients theoretical statisticians? Through the course of the discussion emerges the answer to a third, easier, question: Why Shelby Haberman?. Â© 2020 by the National Council on Measurement in Education","['2019', 'NCME', 'Career', 'Contributions', 'Award', 'address', 'Dr', 'Shelby', 'Haberman', 'example', 'kind', 'illustrate', 'training', 'theoretical', 'statistic', 'influence', 'contribution', 'educational', 'I', 'bracket', 'comment', 'address', 'contribution', 'generally', 'consider', 'question', 'theoretical', 'statistician', 'receive', 'award', 'recipient', 'theoretical', 'statisticians', 'course', 'discussion', 'emerge', 'answer', 'easy', 'question', 'Shelby', 'Haberman', 'Â©', '2020', 'National', 'Council']","['statistical', 'Theoreticians', 'Educational', 'Assessment', 'Comments', 'Shelby', 'Habermans', 'NCME', 'Career', 'Contributions', 'Award']",2019 NCME Career Contributions Award address Dr Shelby Haberman example kind illustrate training theoretical statistic influence contribution educational I bracket comment address contribution generally consider question theoretical statistician receive award recipient theoretical statisticians course discussion emerge answer easy question Shelby Haberman Â© 2020 National Council,statistical Theoreticians Educational Assessment Comments Shelby Habermans NCME Career Contributions Award,0.032527858,0.032448907,0.032448575,0.869843489,0.032731171,0.000285042,0,0.03140097,0.024629518,0.000461578
Clauser B.E.; Kane M.; Clauser J.C.,Examining the Precision of Cut Scores Within a Generalizability Theory Framework: A Closer Look at the Item Effect,2020,57,"An Angoff standard setting study generally yields judgments on a number of items by a number of judges (who may or may not be nested in panels). Variability associated with judges (and possibly panels) contributes error to the resulting cut score. The variability associated with items plays a more complicated role. To the extent that the mean item judgments directly reflect empirical item difficulties, the variability in Angoff judgments over items would not add error to the cut score, but to the extent that the mean item judgments do not correspond to the empirical item difficulties, variability in mean judgments over items would add error to the cut score. In this article, we present two generalizability-theory?“based analyses of the proportion of the item variance that contributes to error in the cut score. For one approach, variance components are estimated on the probability (or proportion-correct) scale of the Angoff judgments, and for the other, the judgments are transferred to the theta scale of an item response theory model before estimating the variance components. The two analyses yield somewhat different results but both indicate that it is not appropriate to simply ignore the item variance component in estimating the error variance. Â© 2019 by the National Council on Measurement in Education",Examining the Precision of Cut Scores Within a Generalizability Theory Framework: A Closer Look at the Item Effect,"An Angoff standard setting study generally yields judgments on a number of items by a number of judges (who may or may not be nested in panels). Variability associated with judges (and possibly panels) contributes error to the resulting cut score. The variability associated with items plays a more complicated role. To the extent that the mean item judgments directly reflect empirical item difficulties, the variability in Angoff judgments over items would not add error to the cut score, but to the extent that the mean item judgments do not correspond to the empirical item difficulties, variability in mean judgments over items would add error to the cut score. In this article, we present two generalizability-theory?“based analyses of the proportion of the item variance that contributes to error in the cut score. For one approach, variance components are estimated on the probability (or proportion-correct) scale of the Angoff judgments, and for the other, the judgments are transferred to the theta scale of an item response theory model before estimating the variance components. The two analyses yield somewhat different results but both indicate that it is not appropriate to simply ignore the item variance component in estimating the error variance. Â© 2019 by the National Council on Measurement in Education","['Angoff', 'standard', 'setting', 'study', 'generally', 'yield', 'judgment', 'number', 'item', 'number', 'judge', 'nest', 'panel', 'Variability', 'associate', 'judge', 'possibly', 'panel', 'contribute', 'error', 'result', 'cut', 'score', 'variability', 'associate', 'item', 'play', 'complicated', 'role', 'extent', 'mean', 'item', 'judgment', 'directly', 'reflect', 'empirical', 'item', 'difficulty', 'variability', 'Angoff', 'judgment', 'item', 'add', 'error', 'cut', 'score', 'extent', 'mean', 'item', 'judgment', 'correspond', 'empirical', 'item', 'difficultie', 'variability', 'mean', 'judgment', 'item', 'add', 'error', 'cut', 'score', 'article', 'present', 'generalizabilitytheory', '??, 'base', 'analysis', 'proportion', 'item', 'variance', 'contribute', 'error', 'cut', 'score', 'approach', 'variance', 'component', 'estimate', 'probability', 'proportioncorrect', 'scale', 'Angoff', 'judgment', 'judgment', 'transfer', 'theta', 'scale', 'item', 'response', 'theory', 'estimate', 'variance', 'component', 'analysis', 'yield', 'somewhat', 'different', 'result', 'indicate', 'appropriate', 'simply', 'ignore', 'item', 'variance', 'component', 'estimate', 'error', 'variance', 'Â©', '2019', 'National', 'Council']","['examine', 'Precision', 'Cut', 'Scores', 'Generalizability', 'Theory', 'Framework', 'close', 'look', 'Item', 'Effect']",Angoff standard setting study generally yield judgment number item number judge nest panel Variability associate judge possibly panel contribute error result cut score variability associate item play complicated role extent mean item judgment directly reflect empirical item difficulty variability Angoff judgment item add error cut score extent mean item judgment correspond empirical item difficultie variability mean judgment item add error cut score article present generalizabilitytheory ??base analysis proportion item variance contribute error cut score approach variance component estimate probability proportioncorrect scale Angoff judgment judgment transfer theta scale item response theory estimate variance component analysis yield somewhat different result indicate appropriate simply ignore item variance component estimate error variance Â© 2019 National Council,examine Precision Cut Scores Generalizability Theory Framework close look Item Effect,0.030137573,0.029798472,0.029601426,0.880585597,0.029876933,0.011158495,0.006955487,0.106653226,0,0
Maeda H.; Zhang B.,Bayesian Extension of Biweight and Huber Weight for Robust Ability Estimation,2020,57,"When a response pattern does not fit a selected measurement model, one may resort to robust ability estimation. Two popular robust methods are biweight and Huber weight. So far, research on these methods has been quite limited. This article proposes the maximum a posteriori biweight (BMAP) and Huber weight (HMAP) estimation methods. These methods use the Bayesian prior distribution to compensate for information lost due to aberrant responses. They may also be more resistant to the detrimental effects of downweighting the nonaberrant responses. The effectiveness of BMAP and HMAP was evaluated through a Monte Carlo simulation. Results show that both methods, especially BMAP, are more effective than the original biweight and Huber weight in correcting mild forms of aberrant behavior. Â© 2019 by the National Council on Measurement in Education",Bayesian Extension of Biweight and Huber Weight for Robust Ability Estimation,"When a response pattern does not fit a selected measurement model, one may resort to robust ability estimation. Two popular robust methods are biweight and Huber weight. So far, research on these methods has been quite limited. This article proposes the maximum a posteriori biweight (BMAP) and Huber weight (HMAP) estimation methods. These methods use the Bayesian prior distribution to compensate for information lost due to aberrant responses. They may also be more resistant to the detrimental effects of downweighting the nonaberrant responses. The effectiveness of BMAP and HMAP was evaluated through a Monte Carlo simulation. Results show that both methods, especially BMAP, are more effective than the original biweight and Huber weight in correcting mild forms of aberrant behavior. Â© 2019 by the National Council on Measurement in Education","['response', 'pattern', 'fit', 'select', 'resort', 'robust', 'ability', 'estimation', 'popular', 'robust', 'method', 'biweight', 'Huber', 'weight', 'far', 'research', 'method', 'limited', 'article', 'propose', 'maximum', 'posteriori', 'biweight', 'BMAP', 'Huber', 'weight', 'HMAP', 'estimation', 'method', 'method', 'bayesian', 'prior', 'distribution', 'compensate', 'information', 'lose', 'aberrant', 'response', 'resistant', 'detrimental', 'effect', 'downweighte', 'nonaberrant', 'response', 'effectiveness', 'BMAP', 'HMAP', 'evaluate', 'Monte', 'Carlo', 'simulation', 'result', 'method', 'especially', 'BMAP', 'effective', 'original', 'biweight', 'Huber', 'weight', 'correct', 'mild', 'form', 'aberrant', 'behavior', 'Â©', '2019', 'National', 'Council']","['Bayesian', 'Extension', 'Biweight', 'Huber', 'Weight', 'robust', 'Ability', 'estimation']",response pattern fit select resort robust ability estimation popular robust method biweight Huber weight far research method limited article propose maximum posteriori biweight BMAP Huber weight HMAP estimation method method bayesian prior distribution compensate information lose aberrant response resistant detrimental effect downweighte nonaberrant response effectiveness BMAP HMAP evaluate Monte Carlo simulation result method especially BMAP effective original biweight Huber weight correct mild form aberrant behavior Â© 2019 National Council,Bayesian Extension Biweight Huber Weight robust Ability estimation,0.030879786,0.878165034,0.029789736,0.030317736,0.030847708,0.036100451,0.03297138,0.007350104,0,0.015413883
