Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
Fitzpatrick A.R.; Yen W.M.,"Fitzpatrick, Anne R. (7004620831); Yen, Wendy M. (7102684621)",7004620831; 7102684621,The Psychometric Characteristics of Choice Items,1995,Journal of Educational Measurement,32,3,,243,259,16,15,10.1111/j.1745-3984.1995.tb00465.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112838&doi=10.1111%2fj.1745-3984.1995.tb00465.x&partnerID=40&md5=b23a4877c75166e153da00d265fbf5dd,"This study investigated the psychometric characteristics of constructed‐response (CR) items referring to choice and non‐choice passages administered to students in Grades 3, 5, and 8. The items were scaled using item response theory (IRT) methodology. The results indicated no consistent differences in the difficulty and discrimination of the items referring to the two types of passages. On the average, students' scale scores on the choice and non‐choice passages were comparable. Finally, the choice passages differed in terms of overall popularity and in their attractiveness to different gender and ethnic groups Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988112838
Gessaroli M.E.; De Champlain A.F.,"Gessaroli, Marc E. (6701480991); De Champlain, André F. (7003989462)",6701480991; 7003989462,Using an approximate chi-square statistic to test the number of dimensions underlying the responses to a set of items,1996,Journal of Educational Measurement,33,2,,157,179,22,52,10.1111/j.1745-3984.1996.tb00487.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030170743&doi=10.1111%2fj.1745-3984.1996.tb00487.x&partnerID=40&md5=1f54affcb91a7ef9191af78aec338475,"An approximate x;2 statistic based on McDonald's (1967) nonlinear factor analytic representation of item response theory was proposed and investigated wah simulated data. The results were compared with Stout's T statistic (Nandakumar & Stout, 1993; Stout, 1987). Unidimensional and tWo-dimensional item response data were simulated under varying levels of sample size, test length, test reliability, and dimension dominance. The approximate X2 statistic had good control over Type I errors when unidimensional data were generated and displayed very good power in identifying the two-dimensional data The performance of the approximate X2 was at least as good as Stout's T statistic in all conditions and was better than Stout's T statistic with smaller sample sizes and shorter tests. Further implications regarding the potential use of nonlinear factor analysis and the approximate X2 in addressing current measurement issues are discussed.",,,Article,Final,,Scopus,2-s2.0-0030170743
Tate R.L.,"Tate, Richard L. (7102471183)",7102471183,Robustness of the School‐Level IRT Model,1995,Journal of Educational Measurement,32,2,,145,162,17,7,10.1111/j.1745-3984.1995.tb00460.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988077104&doi=10.1111%2fj.1745-3984.1995.tb00460.x&partnerID=40&md5=c19d54b87cf84f6db34c5356ce302a73,"The robustness of the school‐level item response theoretic (IRT) model to violations of distributional assumptions was studied in a computer simulation. Estimated precision of “expected a posteriori” (EAP) estimates of the mean school ability from BILOG 3 was compared with actual precision, varying school size, intraclass correlation, school ability, number of forms comprising the test, and item parameters. Under conditions where the school‐level precision might be possibly acceptable for real school comparisons, the EAP estimates of school ability were robust over a wide range of violations and conditions, with the estimated precision being either consistent with the actual precision or somewhat conservative. Some lack of robustness was found, however, under conditions where the precision was inherently poor and the test would presumably not be used for serious school comparisons. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988077104
Muraki E.,"Muraki, Eiji (6701455467)",6701455467,Stepwise analysis of differential item functioning based on multiple-group partial credit model,1999,Journal of Educational Measurement,36,3,,217,232,15,20,10.1111/j.1745-3984.1999.tb00555.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033246636&doi=10.1111%2fj.1745-3984.1999.tb00555.x&partnerID=40&md5=13f9db66da6ac4804dfc6f04d7087659,"Bock, Muraki, and Pfeiffenberger (1988) proposed a dichotomous item response theory (IRT) model for the detection of differential item functioning (DIF), and they estimated the IRT parameters and the means and standard deviations of the multiple latent trait distributions. This IRT DIF detection method is extended to the partial credit model (Masters, 1982; Muraki, 1993) and presented as one of the multiple-group IRT models. Uniform and non-uniform DIF items and heterogeneous latent trait distributions were used to generate polytomous responses of multiple groups. The DIF method was applied to this simulated data using a stepwise procedure. The standardized DIF measures for slope and item location parameters successfully detected the non-uniform and uniform DIF items as well as recovered the means and standard deviations of the latent trait distributions. This stepwise DIF analysis based on the multiple-group partial credit model was then applied to the National Assessment of Educational Progress (NAEP) writing trend data.",,,Article,Final,,Scopus,2-s2.0-0033246636
Allalouf A.; Hambleton R.K.; Sireci S.G.,"Allalouf, Avi (6603644062); Hambleton, Ronald K. (7006242264); Sireci, Stephen G. (6701491909)",6603644062; 7006242264; 6701491909,Identifying the causes of dif in translated verbal items,1999,Journal of Educational Measurement,36,3,,185,198,13,103,10.1111/j.1745-3984.1999.tb00553.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033246631&doi=10.1111%2fj.1745-3984.1999.tb00553.x&partnerID=40&md5=8c5c066f22dfc0ed6ddd94f5545c838d,"Translated tests are being used increasingly for assessing the knowledge and skills of individuals who speak different languages. There is little research exploring why translated items sometimes function differently across languages. If the sources of differential item functioning (DIF) across languages could be predicted, it could have important implications on test development, scoring and equating. This study focuses on two questions: ""Is DIF related to item type?"", ""What are the causes of DIF?"" The data were taken from the Israeli Psychometric Entrance Test in Hebrew (source) and Russian (translated). The results indicated that 34% of the items functioned differentially across languages. The analogy items were the most problematic with 65% showing DIF, mostly in favor of the Russian-speaking examinees. The sentence completion items were also a problem (45% DIF). The main reasons for DIF were changes in word difficulty, changes in item format, differences in cultural relevance, and changes in content.",,,Article,Final,,Scopus,2-s2.0-0033246631
Bennett R.E.; Morley M.; Quardt D.; Rock D.A.; Singley M.K.; Katz I.R.; Nhouyvanisvong A.,"Bennett, Randy Elliot (7402440584); Morley, Mary (7006568342); Quardt, Dennis (6507339710); Rock, Donald A. (7102591023); Singley, Mark K. (6603614741); Katz, Irvin R. (7101824537); Nhouyvanisvong, Adisack (6507666546)",7402440584; 7006568342; 6507339710; 7102591023; 6603614741; 7101824537; 6507666546,Psychometric and cognitive functioning of an under-determined computer-based response type for quantitative reasoning,1999,Journal of Educational Measurement,36,3,,233,252,19,19,10.1111/j.1745-3984.1999.tb00556.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033246633&doi=10.1111%2fj.1745-3984.1999.tb00556.x&partnerID=40&md5=108755f2b0e000277c4264495563e439,"We evaluated a computer-delivered response type for measuring quantitative skill. ""Generating Examples"" (GE) presents under-determined problems that can have many right answers. We administered two GE tests that differed in the manipulation of specific item features hypothesized to affect difficulty. Analyses related to internal consistency reliability, external relations, and features contributing to item difficulty, adverse impact, and examinee perceptions. Results showed that GE scores were reasonably reliable but only moderately related to the GRE quantitative section, suggesting the two tests might be tapping somewhat different skills. Item features that increased difficulty included asking examinees to supply more than one correct answer and to identify whether an item was solvable. Gender differences were similar to those found on the GRE quantitative and analytical test sections. Finally, examinees were divided on whether GE items were a fairer indicator of ability than multiple-choice items, but still overwhelmingly preferred to take the more conventional questions.",,,Article,Final,,Scopus,2-s2.0-0033246633
Veldkamp B.P.,"Veldkamp, Bernard P. (6602896542)",6602896542,Multiple objective test assembly problems,1999,Journal of Educational Measurement,36,3,,253,266,13,35,10.1111/j.1745-3984.1999.tb00557.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033246632&doi=10.1111%2fj.1745-3984.1999.tb00557.x&partnerID=40&md5=ab96756591f0e6c0736fa8d4209b71d3,"Mathematical programming techniques for optimal test assembly are discussed. Most methods optimize a single objective: for instance, the amount of information in a test, subject to a number of constraints. However, some test assembly problems have multiple objectives. A recent example in the literature is the problem of assembling test that measure multiple traits, where the amount of information in the test about each different trait has to be maximized. The present paper proposes methods appropriate for solving multiple objective test assembly problems. An overview of multiple objective optimization methods is given. The impact of the method on the optimality of the solution is shown and the appropriateness of the methods is discussed. The methods are illustrated using an empirical example of a test assembly problem for a two-dimensional mathematics item pool.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0033246632
Van Der Linden W.J.; Adema J.J.,"Van Der Linden, Wim J. (55409657500); Adema, Jos J. (6603281237)",55409657500; 6603281237,Simultaneous assembly of multiple test forms,1998,Journal of Educational Measurement,35,3,,185,198,13,43,10.1111/j.1745-3984.1998.tb00533.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032148327&doi=10.1111%2fj.1745-3984.1998.tb00533.x&partnerID=40&md5=ae424b2c03e7e4e105f044a0ee438aeb,"An algorithm for the assembly of multiple test forms is proposed in which the multiple-form problem is reduced to a series of computationally less intensive two-form problems. At each step, one form is assembled to its true specifications; the other form is a dummy assembled only to maintain a balance between the quality of the current form and the remaining forms. It is shown how the method can be implemented using the technique of 0-1 linear programming. Two empirical examples using a former item pool from the LSAT are given - one in which a set of parallel forms is assembled and another in which the targets for the information functions of the forms are shifted systematically.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0032148327
Feldt L.S.,"Feldt, Leonard S. (7003911114)",7003911114,Estimation of the Reliability of Differences Under Revised Reliabilities of Component Scores,1995,Journal of Educational Measurement,32,3,,295,301,6,2,10.1111/j.1745-3984.1995.tb00468.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988057851&doi=10.1111%2fj.1745-3984.1995.tb00468.x&partnerID=40&md5=93b5a220fed7cf4c6c540b5016c978e4,"Projecting the changes in the reliability of a difference score (d =× ‐ Y) as a consequence of changes in the reliabilities of Xand Y does not represent a straightforward application of the Spearman‐Brown formula. Formulas are developed for estimating the changes in the reliability of X‐Yunder two possible assumptions: (a) ×and Y have equal variances both before and after their reliabilities are altered, and (b) ×and Y have unequal variances before and after×and Y are modified. The second of these situations, which includes the first as a special case, is probably the more common. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988057851
Pommerich M.; Nicewander W.A.; Hanson B.A.,"Pommerich, Mary (6602421150); Nicewander, W. Alan (6601997483); Hanson, Bradley A. (7102036381)",6602421150; 6601997483; 7102036381,Estimating average domain scores,1999,Journal of Educational Measurement,36,3,,199,216,17,7,10.1111/j.1745-3984.1999.tb00554.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033196698&doi=10.1111%2fj.1745-3984.1999.tb00554.x&partnerID=40&md5=03dc02232e08d62a6b8232568673ddfb,"A simulation study was performed to determine whether a group's average percent correct in a content domain could be accurately estimated for groups taking a single test form and not the entire domain of items. Six Item Response Theory-based domain score estimation methods were evaluated, under conditions of few items per content area perform taken, small domains, and small group sizes. The methods used item responses to a single form taken to estimate examinee or group ability; domain scores were then computed using the ability estimates and domain item characteristics. The IRT-based domain score estimates typically showed greater accuracy and greater consistency across forms taken than observed performance on the form taken. For the smallest group size and least number of items taken, the accuracy of most IRT-based estimates was questionable; however, a procedure that operates on an estimated distribution of group ability showed promise under most conditions.",,,Article,Final,,Scopus,2-s2.0-0033196698
Luecht R.M.; Nungester R.J.,"Luecht, Richard M. (6602138468); Nungester, Ronald J. (6602127510)",6602138468; 6602127510,Some practical examples of computer-adaptive sequential testing,1998,Journal of Educational Measurement,35,3,,229,249,20,104,10.1111/j.1745-3984.1998.tb00537.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032148329&doi=10.1111%2fj.1745-3984.1998.tb00537.x&partnerID=40&md5=a922b468639f559b5c107fd91653942d,"Computerized testing has created new challenges for the production and administration of test forms. Many testing organizations engaged in or considering computerized testing may find themselves changing from well-established procedures for handcrafting a small number of paper-and-pencil test forms to procedures for mass producing many computerized test forms. This paper describes an integrated approach to test development and administration called computer-adaptive sequential testing, or CAST. CAST is a structured approach to test construction which incorporates both adaptive testing methods with automated test assembly to allow test developers to maintain a greater degree of control over the production, quality assurance, and administration of different types of computerized tests. CAST retains much of the efficiency of traditional computer adaptive testing (CAT) and can be modified for computer mastery testing (CMT) applications. The CAST framework is described in detail and several applications are demonstrated using a medical licensure example.",,,Article,Final,,Scopus,2-s2.0-0032148329
Stocking M.L.; Jirele T.; Lewis C.; Swanson L.,"Stocking, Martha L. (7006846128); Jirele, Thomas (6507030084); Lewis, Charles (54389552600); Swanson, Len (7201491790)",7006846128; 6507030084; 54389552600; 7201491790,Moderating possibly irrelevant multiple mean score differences on a test of mathematical reasoning,1998,Journal of Educational Measurement,35,3,,199,221,22,4,10.1111/j.1745-3984.1998.tb00534.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032148328&doi=10.1111%2fj.1745-3984.1998.tb00534.x&partnerID=40&md5=0bbb11765395c77e7f9e8df251ed460b,"A pool of items from operational tests of mathematical reasoning was constructed to investigate the feasibility of using automated test assembly (ATA) methods to simultaneously moderate possibly irrelevant differences between the performance of women and men, and African American and White test takers. None of the artificial tests exhibited substantial impact moderation, although the estimated mean scaled score differences for the relevant population indicated a modest move in the intended direction: the difference between scaled score means was reduced by about 20% for women and men and about 9% for African American and White test takers. Although many issues in the implementation of this methodology remain to be solved, the consideration of impact in ATA, along with the maintenance of the detailed test plan, appears to be a potential method of moderating possibly irrelevant mean test score differences.",,,Article,Final,,Scopus,2-s2.0-0032148328
Vispoel W.P.; Wang T.; Bleiler T.,"Vispoel, Walter P. (6603886282); Wang, Tianyou (55709761000); Bleiler, Timothy (6603213880)",6603886282; 55709761000; 6603213880,"Computerized adaptive and fixed-item testing of music listening skill: A comparison of efficiency, precision, and concurrent validity",1997,Journal of Educational Measurement,34,1,,43,63,20,13,10.1111/j.1745-3984.1997.tb00506.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031093556&doi=10.1111%2fj.1745-3984.1997.tb00506.x&partnerID=40&md5=d016ef55dddcba1f57929d2e5202e48d,"We evaluated the efficiency, precision, and concurrent validity of results obtained from adaptive and fixed-item music listening tests in three studies: (a) a computer simulation study in which each of 2,200 simulees completed a computerized adaptive tonal memory test, a computerized fixed-item tonal memory test constructed from items in the adaptive test pool, and two standardized group-administered tonal memory tests; (b) a live testing study in which each of 204 examinees took the computerized adaptive test and the standardized tests; and (c) a live testing study in which randomly equivalent groups took either the computerized adaptive test (n = 86) or the computerized fixed-item test (n = 86). The adaptive music test required 50% to 93% fewer items to match the reliability and concurrent validity of the fixed-item tests, and it yielded higher levels of reliability and concurrent validity than the fixed-item tests when test length was held constant. These findings suggest that computerized adaptive tests, which typically have been limited to visually produced items, may also be well suited for measuring skills that require aurally produced items.",,,Article,Final,,Scopus,2-s2.0-0031093556
Bock R.D.; Thissen D.; Zimowski M.F.,"Bock, R. Darrell (55436391100); Thissen, David (7003712685); Zimowski, Michele F. (6506173428)",55436391100; 7003712685; 6506173428,IRT estimation of domain scores,1997,Journal of Educational Measurement,34,3,,197,211,14,38,10.1111/j.1745-3984.1997.tb00515.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031231805&doi=10.1111%2fj.1745-3984.1997.tb00515.x&partnerID=40&md5=1cb84fae9d675325006c3ec1e52ae89f,"In classical test theory, a test is regarded as a sample of items from a domain defined by generating rules or by content, process, and format specifications. If the items are a random sample of the domain, then the percent-correct score on the test estimates the domain score, that is, the expected percent correct for all items in the domain. When the domain is represented by a large set of calibrated items, as in item banking applications, item response theory (IRT) provides an alternative estimator of the domain score by transformation of the IRT scale score on the test. This estimator has the advantage of not requiring the test items to be a random sample of the domain, and of having a simple standard error. We present here resampling results in real data demonstrating for uni- and multidimensional models that the IRT estimator is also a more accurate predictor of the domain score than is the classical percent-correct score. These results have implications for reporting outcomes of educational qualification testing and assessment.",,,Article,Final,,Scopus,2-s2.0-0031231805
Waltman K.K.,"Waltman, Kristie K. (7801509228)",7801509228,Using performance standards to link statewide achievement results to NAEP,1997,Journal of Educational Measurement,34,2,,101,121,20,2,10.1111/j.1745-3984.1997.tb00509.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287721&doi=10.1111%2fj.1745-3984.1997.tb00509.x&partnerID=40&md5=ae2d686dc264c9f629d2b304881fceea,"The purpose of this study was to investigate the comparability in score meaning of the performance regions on the ITBS and NAEP mathematics score scales that resulted from using performance standards to establish two separate links: socially moderated and statistically moderated. A socially moderated link was established by using the same achievement level descriptions in an ITBS standard-setting study that were used in a NAEP standard-setting study. A statistically moderated link was accomplished by using an equipercentile procedure. The primary findings were that (a) social moderation yielded cutscores on the ITBS scales that resulted in larger percentages of Iowa public fourth-grade students being classified within the basic, proficient, and advanced achievement regions than those reported by NAEP; (b) the equipercentile link yielded percentages on the ITBS scale that were similar to those reported by NAEP for ""type of community"" subgroups; and (c) for students taking both assessments, the corresponding achievement regions on the NAEP and ITBS scales produced low to moderate percents of agreement in student classification.",,,Article,Final,,Scopus,2-s2.0-0031287721
Mazor K.M.; Kanjee A.; Clauser B.E.,"Mazor, Kathleen M. (6701717204); Kanjee, Anil (16686355400); Clauser, Brian E. (7003595460)",6701717204; 16686355400; 7003595460,Using Logistic Regression and the Mantel‐Haenszel With Multiple Ability Estimates to Detect Differential Item Functioning,1995,Journal of Educational Measurement,32,2,,131,144,13,32,10.1111/j.1745-3984.1995.tb00459.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106723&doi=10.1111%2fj.1745-3984.1995.tb00459.x&partnerID=40&md5=097eae85cf758b9e9523a7537c786e2d,"Logistic regression has recently been advanced as a viable procedure for detecting differential item functioning (DIF). One of the advantages of this procedure is the considerable flexibility it offers in the specification of the regression equation. This article describes incorporating two ability estimates into a single regression analysis, with the result that substantially fewer items exhibit DIF. A comparable analysis is conducted using the Mantel‐Haenszel with similar results. It is argued that by simultaneously conditioning on two relevant ability estimates, more accurate matching of examinees in the reference and focal groups is obtained, and thus multidimensional item impact is not mistakenly identified as DIF. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106723
Camilli G.; Wang M.‐m.; Fesq J.,"Camilli, Gregory (7003383989); Wang, Ming‐mei (57190052600); Fesq, Jacqueline (57496051800)",7003383989; 57190052600; 57496051800,The Effects of Dimensionality on Equating the Law School Admission Test,1995,Journal of Educational Measurement,32,1,,79,96,17,32,10.1111/j.1745-3984.1995.tb00457.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21844503346&doi=10.1111%2fj.1745-3984.1995.tb00457.x&partnerID=40&md5=d669dd438dd839634d2cd94107fdd4aa,"Using factor analysis, we conducted an assessment of multidimensionality for 6 forms of the Law School Admission Test (LSAT) and found 2 subgroups of items or factors for each of the 6 forms. The main conclusion of the factor analysis component of this study was that the LSAT appears to measure 2 different reasoning abilities: inductive and deductive. The technique of N. J. Dorans & N. M. Kingston (1985) was used to examine the effect of dimensionality on equating. We began by calibrating (with item response theory [IRT] methods) all items on a form to obtain Set I of estimated IRT item parameters. Next, the test was divided into 2 homogeneous subgroups of items, each having been determined to represent a different ability (i.e., inductive or deductive reasoning). The items within these subgroups were then recalibrated separately to obtain item parameter estimates, and then combined into Set II. The estimated item parameters and true‐score equating tables for Sets I and II corresponded closely. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-21844503346
Enright M.K.; Rock D.A.; Bennett R.E.,"Enright, Mary K. (7004332498); Rock, Donald A. (7102591023); Bennett, Randy Elliot (7402440584)",7004332498; 7102591023; 7402440584,Improving measurement for graduate admissions,1998,Journal of Educational Measurement,35,3,,250,267,17,11,10.1111/j.1745-3984.1998.tb00538.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032148330&doi=10.1111%2fj.1745-3984.1998.tb00538.x&partnerID=40&md5=18fa18a4a88b625e799a558b7255254c,"In this study we examined alternative item types and section configurations for improving the discriminant and convergent validity of the GRE General Test. A computer-based test of reasoning items and a generating-explanations measure was administered to a sample of 388 examinees who previously had taken the General Test. Confirmatory factor analyses indicated that three dimensions of reasoning - verbal, analytical, and quantitative - and a fourth dimension of verbal fluency based on the generating-explanations task could be distinguished. Notably, generating explanations was as distinct from new variations of reasoning items as it was from verbal and quantitative reasoning. In the full sample, this differentiation was evident in relation to such external criteria as undergraduate grade point average (UGPA), self-reported accomplishments, and a measure of ideational fluency, with generating explanations relating uniquely to aesthetic and linguistic accomplishments and to ideational fluency. For the subset of participants with undergraduate majors in the humanities and social sciences, generating explanations added to the relationship with UGPA over that contributed by the General Test.",,,Article,Final,,Scopus,2-s2.0-0032148330
Roussos L.A.; Stout W.F.,"Roussos, Louis A. (6603805095); Stout, William F. (57207542942)",6603805095; 57207542942,Simulation studies of the effects of small sample size and studied item parameters on SIBTEST and Mantel-Haenszel type I error performance,1996,Journal of Educational Measurement,33,2,,215,230,15,171,10.1111/j.1745-3984.1996.tb00490.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030170597&doi=10.1111%2fj.1745-3984.1996.tb00490.x&partnerID=40&md5=aaaf288b629b9c2d0b6d09209d5dd877,"Two simulation studies investigated Type I error performance of two statistical procedures for detecting differential item functioning (DIF): SIBTEST and Mantel-Haenszel (MH). Because MH and SIBTEST are based on asymptotic distributions requiring ""large"" numbers of examinees, the first study examined Type I error for small sample sizes. No significant Type I error inflation occurred for either procedure. Because MH has the potential for Type I error inflation for non-Rasch models, the second study used a markedly non-Rasch test and systematically varied the shape and location of the studied item. When differences in distribution across examinee group of the measured ability were present, both procedures displayed inflated Type I error for certain items; MH displayed the greater inflation. Also, both procedures displayed statistically biased estimation of the zero DIF for certain items, though SIBTEST displayed much less than MH. When no latent distributional differences were present, both procedures performed satisfactorily under all conditions.",,,Article,Final,,Scopus,2-s2.0-0030170597
Embretson S.E.,"Embretson, Susan E. (6603914093)",6603914093,Measurement Model for Linking Individual Learning to Processes and Knowledge: Application to Mathematical Reasoning,1995,Journal of Educational Measurement,32,3,,277,294,17,31,10.1111/j.1745-3984.1995.tb00467.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073815&doi=10.1111%2fj.1745-3984.1995.tb00467.x&partnerID=40&md5=df67179693aa01019f4d62816eb0dff9,"Contemporary instructional theories increasingly emphasize the importance of linking an individual's learning to changes in cognitive processes and knowledge structures. In this article, an extension of the multidimensional Rasch model for learning and change (MRMLC) is presented so as to permit theories of processes and knowledge structures to be incorporated into the item response model. Like the MRMLC, this extension (MRMLC+) resolves some basic problems in measuring individual change and permits adaptive testing so that precise estimates of learning may be obtained. Additionally, MRMLC+ permits individual learning to be linked to substantive changes in processing and knowledge. An application to a study on the impact of short‐term instruction on mathematical problem solving shows the potential of MRMLC + for interpretations. In this study, a theoretically plausible model of knowledge structures (Mayer, Larkin, & Kadane, 1984) provides the basis of individual learning interpretations. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073815
Powers D.E.; Leung S.W.,"Powers, Donald E. (7202818498); Leung, Susan Wilson (41461609300)",7202818498; 41461609300,Answering the New SAT Reading Comprehension Questions Without the Passages,1995,Journal of Educational Measurement,32,2,,105,129,24,17,10.1111/j.1745-3984.1995.tb00458.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058839&doi=10.1111%2fj.1745-3984.1995.tb00458.x&partnerID=40&md5=fd080c8f55ef0ca475f738d40e886bb6,"It has been reasonably well established that test takers can, to varying degrees, answer some reading comprehension questions without reading the passages on which the questions are based, even for carefully constructed measures like the Scholastic Aptitude Test (SAT). The aim of this study was to determine what test‐taking strategies examinees use, and which are related to test performance, when reading passages are not available. The research focused on reading comprehension questions similar to those that will be used in the revised SAT, to be introduced in 1994. The most often cited strategies involved choosing answers on the basis of consistency with other questions and reconstructing the main theme of a missing passage from all of the questions and answers in a set. These strategies were more likely to result in successful performance on individual test items than were any of many other possible (and less constructrelevant) strategies. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988058839
Luecht R.M.,"Luecht, Richard M. (6602138468)",6602138468,"A reaction to ""moderating possibly irrelevant multiple mean score differences on a test of mathematical reasoning",1998,Journal of Educational Measurement,35,3,,223,225,2,0,10.1111/j.1745-3984.1998.tb00535.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032148331&doi=10.1111%2fj.1745-3984.1998.tb00535.x&partnerID=40&md5=921da7e3fafabb75d257a8670a03cfa0,"A pool of items from operational tests of mathematical reasoning was constructed
to investigate the feasibility of using automated test assembly (ATA) methods to
simultaneously moderate possibly irrelevant differences between the performance
of women and men, and African American and White test takers. None of the
artificial tests exhibited substantial impact moderation, although the estimated
mean scaled score differences for the relevant population indicated a modest move in the intended direction: the difference between scaled score means was reduced by about 20% for women and men and about 9% for African American and White test takers. Although many issues in the implementation of this methodology remain to be solved, the consideration of impact in ATA, along with the maintenance of the detailed test plan, appears to be a potential method of moderating possibly irrelevant mean test score differences.",,,Article,Final,,Scopus,2-s2.0-0032148331
Camilli G.; Penfield D.A.,"Camilli, Gregory (7003383989); Penfield, Douglas A. (6603331635)",7003383989; 6603331635,Variance estimation for differential test functioning based on Mantel-Haenszel statistics,1997,Journal of Educational Measurement,34,2,,123,139,16,35,10.1111/j.1745-3984.1997.tb00510.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031162007&doi=10.1111%2fj.1745-3984.1997.tb00510.x&partnerID=40&md5=14ac281322528d90e53b6e444b0ba04f,"This article concerns the simultaneous assessment of DIF for a collection of test items. Rather than an average or sum in which positive and negative DIF may cancel, we propose an index that measures the variance of DIF on a test as an indicator of the degree to which different items show DIF in different directions. It is computed from standard Mantel-Haenszel statistics (the log-odds ratio and its variance error) and may be conceptually classified as a variance component or variance effect size. Evaluated by simulation under three item response models (1PL, 2PL, and 3PL), the index is shown to be an accurate estimate of the DTF generating parameter in the case of the IPL and 2PL models with groups of equal ability. For groups of unequal ability, the index is accurate under the 1PL but not the 2PL condition; however, a weighted version of the index provides improved estimates. For the 3PL condition, the DTF generating parameter is underestimated. This latter result is due in part to a mismatch in the scales of the log-odds ratio and IRT difficulty.",,,Article,Final,,Scopus,2-s2.0-0031162007
Roussos L.A.; Stout W.F.; Marden J.I.,"Roussos, Louis A. (6603805095); Stout, William F. (57207542942); Marden, John I. (7004287772)",6603805095; 57207542942; 7004287772,Using new proximity measures with hierarchical cluster analysis to detect multidimensionality,1998,Journal of Educational Measurement,35,1,,1,30,29,61,10.1111/j.1745-3984.1998.tb00525.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032016556&doi=10.1111%2fj.1745-3984.1998.tb00525.x&partnerID=40&md5=bbbe8303933786b834291ad7d6f68912,"A new approach for partitioning test items into dimensionally distinct item clusters is introduced. The core of the approach is a new item-pair conditional-covariance-based proximity measure that can be used with hierarchical cluster analysis. An extensive simulation study designed to test the limits of the approach indicates that when approximate simple structure holds, the procedure can correctly partition the test into dimensionally homogeneous item clusters even for very high correlations between the latent dimensions. In particular, the procedure can correctly classify (on average) over 90% of the items for correlations as high as .9. The cooperative role that the procedure can play when used in conjunction with other dimensionality assessment procedures is discussed.",,,Article,Final,,Scopus,2-s2.0-0032016556
Embretson S.E.,"Embretson, Susan E. (6603914093)",6603914093,Cognitive design principles and the successful performer: A study on spatial ability,1996,Journal of Educational Measurement,33,1,,29,39,10,5,10.1111/j.1745-3984.1996.tb00477.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030102364&doi=10.1111%2fj.1745-3984.1996.tb00477.x&partnerID=40&md5=fd8f991f25f1b5f5f32ad952a0b8b721,"An important trend in educational measurement is the use of principles of cognitive psychology to design achievement and ability test items. Many studies show that manipulating the stimulus features of items influences the processes, strategies, and knowledge structures that are involved in solution. However, little is known about how cognitive design influences individual differences. That is, does applying cognitive design principles change the background skills and abilities that are associated with successful performance? This study compared the correlates of two spatial ability tests that used the same item type but different test design principles (cognitive design versus psychometric design). The results indicated differences in factorial complexity in the two tests; specifically, the impact of verbal abilities was substantially reduced by applying the cognitive design principles.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-0030102364
Oshima T.C.; Raju N.S.; Flowers C.P.,"Oshima, T.C. (55672664200); Raju, Nambury S. (7007121164); Flowers, Claudia P. (7005585699)",55672664200; 7007121164; 7005585699,Development and demonstration of multidimensional IRT-based internal measures of differential functioning of items and tests,1997,Journal of Educational Measurement,34,3,,253,272,19,45,10.1111/j.1745-3984.1997.tb00518.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287456&doi=10.1111%2fj.1745-3984.1997.tb00518.x&partnerID=40&md5=f3d9933aabf277e35e8b035a64b02deb,"This article defines and demonstrates a framework for studying differential item functioning (DIF) and differential test functioning (DTF) for tests that are intended to be multidimensional. The procedure introduced here is an extension of unidimensional differential functioning of items and tests (DFIT) recently developed by Raju, van der Linden, & Fleer (1995). To demonstrate the usefulness of these new indexes in a multidimensional IRT setting, two-dimensional data were simulated with known item parameters and known DIF and DTF. The DIF and DTF indexes were recovered reasonably well under various distributional differences of θs after multidimensional linking was applied to put the two sets of item parameters on a common scale. Further studies are suggested in the area of DIF/DTF for intentionally multidimensional tests.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-0031287456
Bridgeman B.; Morgan R.; Wang M.,"Bridgeman, Brent (7005526936); Morgan, Rick (7403330082); Wang, Ming-mei (57190052600)",7005526936; 7403330082; 57190052600,Choice among essay topics: Impact on performance and validity,1997,Journal of Educational Measurement,34,3,,273,286,13,23,10.1111/j.1745-3984.1997.tb00519.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287455&doi=10.1111%2fj.1745-3984.1997.tb00519.x&partnerID=40&md5=c85316046737b2a31690e03a9b2c1acb,"This study assessed the ability of history students to choose the essay topic on which they can get the highest score. A second, equally important question was whether the score on the chosen topic was more highly related to other indicators of proficiency in history than the score on the unchosen topic. Overall, for both U.S. and European history, scores were about one third of a standard deviation higher for the preferred topic than for the other topic. For U.S. history, about 32% of the students made the wrong choice; that is, 32% got a higher score on the other topic than on the preferred topic. In European history, 29% made the wrong choice In the U.S. history sample, the preferred essay correlated .40 with an external criterion score, compared to .34 for the other essay; in the European history sample, the preferred essay correlated .52 with the external criterion, compared to .44 for the other topic.",,,Article,Final,,Scopus,2-s2.0-0031287455
Gustafsson J.-E.,"Gustafsson, Jan-Eric (7401846675)",7401846675,Measurement characteristics of the IEA reading literacy scales for 9- and 10-year-olds at country and individual levels,1997,Journal of Educational Measurement,34,3,,233,251,18,11,10.1111/j.1745-3984.1997.tb00517.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287458&doi=10.1111%2fj.1745-3984.1997.tb00517.x&partnerID=40&md5=9f4cdb436a976326053dae2889e9b726,"This article presents an approach for studying measurement characteristics of instruments designed for comparative studies of educational achievement. It presents a reanalysis of data from 22 countries and 73,818 9- and 10-year-old students from the IEA reading literacy study. Using 2-level confirmatory factor analysis procedures derived by Muthén (1990, 1994), it is demonstrated how the overall variance in performance on items from 3 domains - Documents, Narrative Prose, and Expository Text - may be decomposed into multiple latent sources of variance at country and individual levels. For the Documents items, a 1-factor model was fitted at both levels. For Narrative Prose and Expository Text items, 2 latent variables (a general dimension of reading performance and a factor capturing high versus low performance on passages towards the end of the tests) were fitted at both country and individual levels. For the Documents dimension, estimates of country scores were close to those of the original analysis; however, for performance in the Narrative Prose and Expository Text domains, differences between the original analysis and the reanalysis were noted for many countries.",,,Article,Final,,Scopus,2-s2.0-0031287458
Livingston S.A.; Lewis C.,"Livingston, Samuel A. (35864363200); Lewis, Charles (54389552600)",35864363200; 54389552600,Estimating the Consistency and Accuracy of Classifications Based on Test Scores,1995,Journal of Educational Measurement,32,2,,179,197,18,100,10.1111/j.1745-3984.1995.tb00462.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988110382&doi=10.1111%2fj.1745-3984.1995.tb00462.x&partnerID=40&md5=fef59cd7dc1eb4a6431bc795bd31d079,"This article presents a method for estimating the accuracy and consistency of classifications based on test scores. The scores can be produced by any scoring method, including a weighted composite. The estimates use data from a single form. The reliability of the score is used to estimate effective test length in terms of discrete items. The true‐score distribution is estimated by fitting a 4‐parameter beta model. The conditional distribution of scores on an alternate form, given the true score, is estimated from a binomial distribution based on the estimated effective test length. Agreement between classifications on alternate forms is estimated by assuming conditional independence, given the true score. Evaluation of the method showed estimates to be within 1 percentage point of the actual values in most cases. Estimates of decision accuracy and decision consistency statistics were only slightly affected by changes in specified minimum and maximum possible scores. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988110382
Schnipke D.L.; Scrams D.J.,"Schnipke, Deborah L. (6506792291); Scrams, David J. (6507515074)",6506792291; 6507515074,Modeling item response times with a two-state mixture model: A new method of measuring speededness,1997,Journal of Educational Measurement,34,3,,213,232,19,176,10.1111/j.1745-3984.1997.tb00516.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287457&doi=10.1111%2fj.1745-3984.1997.tb00516.x&partnerID=40&md5=6dc561e2f635bb1806c40c0ffcb8a80a,"Speededness refers to the extent to which time limits affect examinees' test performance, and it is often measured by calculating the proportion of examinees who do not reach a certain percentage of test items. However, when tests are number-right scored (i.e., no points are subtracted for incorrect responses), examinees are likely to rapidly guess on items rather than leave them blank. Therefore, this traditional measure of speededness probably underestimates the true amount of speededness on such tests. A more accurate assessment of speededness should also reflect the tendency of examinees to rapidly guess on items as time expires. This rapid-guessing component of speededness can be estimated by modeling response times with a two-state mixture model, as demonstrated with data from a computer-administered reasoning tets. Taking into account the combined effect of unreached items and rapid guessing provides a more complete measure of speededness than has previously been available.",,,Article,Final,,Scopus,2-s2.0-0031287457
Zwick R.; Thayer D.T.; Lewis C.,"Zwick, Rebecca (7004200859); Thayer, Dorothy T. (7006657345); Lewis, Charles (54389552600)",7004200859; 7006657345; 54389552600,An empirical Bayes approach to Mantel-Haenszel DIF analysis,1999,Journal of Educational Measurement,36,1,,1,28,27,188,10.1111/j.1745-3984.1999.tb00543.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033245928&doi=10.1111%2fj.1745-3984.1999.tb00543.x&partnerID=40&md5=a4c2f2de94c7da71dc451e04466f72a2,"We developed an empirical Bayes (EB) enhancement to Mantel-Haenszel (MH) DIF analysis in which we assume that the MH statistics are normally distributed and that the prior distribution of underlying DIF parameters is also normal. We use the posterior distribution of DIF parameters to make inferences about the item's true DIF status and the posterior predictive distribution to predict the item's future observed status. DIF status is expressed in terms of the probabilities associated with each of the five DIF levels defined by the ETS classification system: C-, B-, A, B+, and C+. The EB methods yield more stable DIF estimates than do conventional methods, especially in small samples, which is advantageous in computer-adaptive testing. The EB approach may also convey information about DIF stability in a more useful way by representing the state of knowledge about an item's DIF status as probabilistic.",,,Article,Final,,Scopus,2-s2.0-0033245928
Clauser B.E.; Nungester R.J.; Mazor K.; Ripkey D.,"Clauser, Brian E. (7003595460); Nungester, Ronald J. (6602127510); Mazor, Kathleen (6701717204); Ripkey, Douglas (6602077535)",7003595460; 6602127510; 6701717204; 6602077535,A comparison of alternative matching strategies for DIF detection in tests that are multidimensional,1996,Journal of Educational Measurement,33,2,,202,214,12,32,10.1111/j.1745-3984.1996.tb00489.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030170763&doi=10.1111%2fj.1745-3984.1996.tb00489.x&partnerID=40&md5=7ec682fce9241e8c5ac172e0b3cc86c2,"Most currently accepted approaches for identifying differentially functioning test items compare performance across groups after first matching examinees on the ability of interest. The typical basis for this matching is the total test score. Previous research indicates that when the test is not approximately unidimensional, matching using the total test score may result in an inflated Type I error rate. This study compares the results of differential item functioning (DIF) analysis with matching based on the total test score, matching based on subtest scores, or multivariate matching using multiple subtest scores. Analysis of both actual and simulated data indicate that for the dimensionally complex test examined in this study, using the total test score as the matching criterion is inappropriate. The results suggest that matching on multiple subtest scores simultaneously may be superior to using either the total test score or individual relevant subtest scores.",,,Article,Final,,Scopus,2-s2.0-0030170763
Bennett R.E.; Rock D.A.,"Bennett, Randy Elliot (7402440584); Rock, Donald A. (7102591023)",7402440584; 7102591023,"Generalizability, Validity, and Examinee Perceptions of a Computer‐Delivered Formulating‐Hypotheses Test",1995,Journal of Educational Measurement,32,1,,19,36,17,24,10.1111/j.1745-3984.1995.tb00454.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099631&doi=10.1111%2fj.1745-3984.1995.tb00454.x&partnerID=40&md5=73d01157e51967fe8cbb40eab016c62b,"The Formulating‐Hypotheses (F‐H) item presents a situation and asks examinees to generate as many explanations for it as possible. This study examined the generalizability, validity, and examinee perceptions of a computer‐delivered version of the task. Eight F‐H questions were administered to 192 graduate students. Half of the items restricted examinees to 7 words per explanation, and half allowed up to 15 words. Generalizability results showed high interrater agreement, with tests of between 2 and 4 items scored by one judge achieving coefficients in the .80s. Construct validity analyses found that F‐H was only marginally related to the GRE General Test, and more strongly related than the General Test to a measure of ideational fluency. Different response limits tapped somewhat different abilities, with the 15‐word constraint appearing more useful for graduate assessment. These items added significantly to conventional measures in explaining school performance and creative expression. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988099631
Bridgeman B.; Harvey A.; Braswell J.,"Bridgeman, Brent (7005526936); Harvey, Anne (57010065200); Braswell, James (51963277900)",7005526936; 57010065200; 51963277900,Effects of Calculator Use on Scores on a Test of Mathematical Reasoning,1995,Journal of Educational Measurement,32,4,,323,340,17,23,10.1111/j.1745-3984.1995.tb00470.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122265&doi=10.1111%2fj.1745-3984.1995.tb00470.x&partnerID=40&md5=09f4edd12476e7b09b07927cab7b6d95,"A sample of college‐bound juniors from 275 high schools took a test consisting of 70 math questions from the SAT. A random half of the sample was allowed to use calculators on the test. Both genders and three ethnic groups (White, African American, and Asian American) benefitted about equally from being allowed to use calculators; Latinos benefitted slightly more than the other groups. Students who routinely used calculators on classroom mathematics tests were relatively advantaged on the calculator test. Test speededness was about the same whether or not students used calculators. Calculator effects on individual items ranged from positive through neutral to negative and could either increase or decrease the validity of an item as a measure of mathematical reasoning skills. Calculator effects could be either present or absent in both difficult and easy items Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122265
Williams V.S.L.; Pommerich M.; Thissen D.,"Williams, Valerie S. L. (35518162900); Pommerich, Mary (6602421150); Thissen, David (7003712685)",35518162900; 6602421150; 7003712685,A comparison of developmental scales based on thurstone methods and item response theory,1998,Journal of Educational Measurement,35,2,,93,107,14,28,10.1111/j.1745-3984.1998.tb00529.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220376&doi=10.1111%2fj.1745-3984.1998.tb00529.x&partnerID=40&md5=6b3c367f43261d06f2826a33684ca982,"A developmental scale for the North Carolina End-of-Grade Mathematics Tests was created using a subset of identical test forms administered to adjacent grade levels. Thurstone scaling and item response theory (IRT) techniques were employed to analyze the changes in grade distributions across these linked forms. Three variations of Thurstone scaling were examined, one based on Thurstone's 1925 procedure and two based on Thurstone's 1938 procedure. The IRT scaling was implemented using both BIMAIN and MULTILOG. All methods indicated that average mathematics performance improved from Grade 3 to Grade 8, with similar results for the two IRT analyses and one version of Thurstone's 1938 method. The standard deviations of the IRT scales did not show a consistent pattern across grades, whereas those produced by Thurstone's 1925 procedure generally decreased; one version of the 1938 method exhibited slightly increasing variation with increasing grade level, while the other version displayed inconsistent trends. This research was supported in part by the North Carolina Department of Public Instruction and a grant from the National Science Foundation (DMS-9208758). Correspondence concerning this paper should be addressed to the first author.",,,Article,Final,,Scopus,2-s2.0-0032220376
Parshall C.G.; Miller T.R.,"Parshall, Cynthia G. (6603133073); Miller, Timothy R. (7403948130)",6603133073; 7403948130,Exact Versus Asymptotic Mantel‐Haenszel DIF Statistics: A Comparison of Performance Under Small‐Sample Conditions,1995,Journal of Educational Measurement,32,3,,302,316,14,42,10.1111/j.1745-3984.1995.tb00469.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053679&doi=10.1111%2fj.1745-3984.1995.tb00469.x&partnerID=40&md5=4e9460f3db6463ef357252d01d9c9a55,"This study evaluated exact testing (Agresti, 1992) as a method for conducting Mantel‐Haenszel DIF analyses (Holland & Thayer, 1988) with relatively small samples. Sample‐size restrictions limit the standard asymptotic Mantel‐Haenszel for many practical applications; however, new developments in computing technology have made exact testing procedures feasible. The highly discrete distributions that are likely to occur in small‐sample DIF analyses could yield very different results for asymptotic versus exact methods. It is therefore important to determine under controlled conditions the extent to which the exact approach is effective in correctly identifying DIF. A series of computer simulations were conducted in which 3 levels of induced bias (IRT b‐parameter differences between groups of .25, .50, and .75) and 4 sample sizes (reference group= 500, focal group = 25, 50, 100, and 200) were investigated. Power comparisons at .01 and .05 alpha levels were carried out between the exact testing procedure and the conventional Mantel‐Haenszel Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988053679
Wainer H.; Hambleton R.K.; Meara K.,"Wainer, Howard (7006218234); Hambleton, Ronald K. (7006242264); Meara, Kevin (6508097062)",7006218234; 7006242264; 6508097062,Alternative displays for communicating naep results: A redesign and validity study,1999,Journal of Educational Measurement,36,4,,301,335,34,45,10.1111/j.1745-3984.1999.tb00559.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033235968&doi=10.1111%2fj.1745-3984.1999.tb00559.x&partnerID=40&md5=9408fb488000a42a8a6b0186be7efd30,"Five displays, chosen from the NAEP 1994 Reading: A First Look, were redesigned. The redesign was informed by the principles developed and enunciated in Wainer's 1997 popular text Visual Revelations. After the redesign was completed a survey of educational policymakers was done in which substantive questions were asked about the content of the various displays. Each redesign was paired with the original and were assigned randomly to one of two survey forms. We found that, on average, the redesigns yielded both more accurate and faster answers to the questions asked. The more difficult the question the greater the disparity between the original format and the redesigned one.",,,Article,Final,,Scopus,2-s2.0-0033235968
Tate R.L.,"Tate, Richard L. (7102471183)",7102471183,A cautionary note on irt-based linking of tests with polytomous items,1999,Journal of Educational Measurement,36,4,,336,346,10,23,10.1111/j.1745-3984.1999.tb00560.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033235967&doi=10.1111%2fj.1745-3984.1999.tb00560.x&partnerID=40&md5=4a12c3791657c8971c56089108c613a6,Published discussion of the year-to-year linking of tests comprised of polytomous items appear to suggest that the linking logic traditionally used for multiple-choice item is also appropriate for polytomous items. It is argued and illustrated that a modification of the traditional linking is necessary when tests consist of constructed-response items judged by raters and there is a possibility of year-to-year variation in the rating discrimination and severity.,,,Article,Final,,Scopus,2-s2.0-0033235967
Sawyer R.,"Sawyer, Richard (7201516630)",7201516630,Decision theory models for validating course placement tests,1996,Journal of Educational Measurement,33,3,,271,290,19,26,10.1111/j.1745-3984.1996.tb00493.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030514441&doi=10.1111%2fj.1745-3984.1996.tb00493.x&partnerID=40&md5=224adcfbe3510eefc75abab16caf43de,"Most American postsecondary institutions have course placement systems for their first-year students. Placement systems typically consist of an assessment component (to estimate students' probability of success in standard first-year courses) and an instructional component (in which academically underprepared students are taught the skills and knowledge they need to succeed in the standard courses). Validity issues related to these functions are discussed in the context of decision theory, and methods are proposed for determining appropriate cutoff scores on placement tests.",,,Article,Final,,Scopus,2-s2.0-0030514441
Feldt L.S.; Qualls A.L.,"Feldt, Leonard S. (7003911114); Qualls, Audrey L. (55848628400)",7003911114; 55848628400,Estimation of measurement error variance at specific score levels,1996,Journal of Educational Measurement,33,2,,141,156,15,22,10.1111/j.1745-3984.1996.tb00486.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030305394&doi=10.1111%2fj.1745-3984.1996.tb00486.x&partnerID=40&md5=c85a4f3df915b3751d4b23bffe77d4f8,"An improved method is derived for estimating consitional measurement error variances, that is, error variances specific to individual examinees or specific to each point on the raw score scale of the test. The method involves partitioning the test into short parallel parts, computing for each examinne the unbiased iased estimate of the variance of part-test scores, and multiplying this variance by a constant dictated by classical test theory. Empirical data are used to corroborate the principal theoretical deductions.",,,Article,Final,,Scopus,2-s2.0-0030305394
Hoskens M.; Boeck P.D.,"Hoskens, Machteld (6603649497); Boeck, Paul De (7005323510)",6603649497; 7005323510,Componential IRT Models for Polytomous Items,1995,Journal of Educational Measurement,32,4,,364,384,20,13,10.1111/j.1745-3984.1995.tb00472.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988097349&doi=10.1111%2fj.1745-3984.1995.tb00472.x&partnerID=40&md5=7e9708818c8deea25d564139cdafdbb4,"Componential IRT models for polytomous items are of particular interest in two contexts: Componential research and test development. We assume that there are basic components, such as processes and knowledge structures, involved in solving cognitive tasks. In Componential research, the subtask paradigm may be used to isolate such components in subtasks. In test development, items may be composed such that their response alternatives correspond with specific combinations of such components. In both cases the data may be modeled as polytomous items. With Bock's (1972) nominal model as a general framework, transformation matrices can be used to constrain the parameters of the response categories so as to reflect the Componential design of the response categories. In this way, both main effects and interaction effects of components can be studied. An application to a spelling task demonstrates this approach Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988097349
Kolen M.J.; Zeng L.; Hanson B.A.,"Kolen, Michael J. (6603925839); Zeng, Lingjia (7401904259); Hanson, Bradley A. (7102036381)",6603925839; 7401904259; 7102036381,Conditional standard errors of measurement for scale scores using IRT,1996,Journal of Educational Measurement,33,2,,129,140,11,47,10.1111/j.1745-3984.1996.tb00485.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030305395&doi=10.1111%2fj.1745-3984.1996.tb00485.x&partnerID=40&md5=a67077030efd902b65c4528823f39af0,"An IRT method for estimating conditional standard errors of measurement of scale scores is presented, where scale scores are nonlinear transformations of number-correct scores. The standard errors account for measurement error that is introduced due to rounding scale scores to integers. Procedures for estimating the average conditional standard error of measurement for scale scores and reliability of scale scores are also described. An illustration of the use of the methodology is presented, and the results from the IRT method are compared to the results from a previously developed method that is based on strong true-score theory.",,,Article,Final,,Scopus,2-s2.0-0030305395
De Champlain A.F.,"De Champlain, André F. (7003989462)",7003989462,The effect of multidimensionality on IRT true-score equating for subgroups of examinees,1996,Journal of Educational Measurement,33,2,,181,201,20,21,10.1111/j.1745-3984.1996.tb00488.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030305392&doi=10.1111%2fj.1745-3984.1996.tb00488.x&partnerID=40&md5=6b54c7e7adc915eaf10fe674a975c886,"The purpose of this study was to assess the dimensionality of two forms of a large-scale standardized test separately for 3 ethnic groups of examinees and to investigate whether differences in their latent trait composites have any impact on unidimensional item response theory true-score equating functions. Specifically, separate equating functions for African American and Hispanic examinees were compared to those of a Caucasian group as well as the total test taker population. On both forms, a 2-dimensional model adequately accounted for the item responses of Caucasian and African American examinees, whereas a more complex model was required for the Hispanic subgroup. The differences between equating functions for the 3 ethnic groups and the total test taker population were small and tended to be located at the low end of the score scale.",,,Article,Final,,Scopus,2-s2.0-0030305392
Allen N.L.; Donoghue J.R.,"Allen, Nancy L. (7202441192); Donoghue, John R. (7102202534)",7202441192; 7102202534,Applying the Mantel-Haenszel procedure to complex samples of items,1996,Journal of Educational Measurement,33,2,,231,251,20,23,10.1111/j.1745-3984.1996.tb00491.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030305393&doi=10.1111%2fj.1745-3984.1996.tb00491.x&partnerID=40&md5=a2e2f7959437c7d9ffbda8e67bc7beb0,"This Monte Carlo study examined the effect of complex sampling of items on the measurement of differential item functioning (DIF) using the Mantel-Haenszel procedure. Data were generated using a 3-parameter logistic item response theory model according to the balanced incomplete block (BIB) design used in the National Assessment of Educational Progress (NAEP). The length of each block of items and the number of DIF items in the matching variable were varied, as was the difficulty, discrimination, and presence of DIF in the studied item. Block, booklet, pooled booklet, and extra-information analyses were compared to a complete data analysis using the transformed log-odds on the delta scale. The pooled booklet approach is recommended for use when items are selected for examinees according to a BIB design. This study has implications for DIF analyses of other complex samples of items, such as computer administered testing or another complex assessment design.",,,Article,Final,,Scopus,2-s2.0-0030305393
Oshima T.C.,"Oshima, T.C. (55672664200)",55672664200,The Effect of Speededness on Parameter Estimation in Item Response Theory,1994,Journal of Educational Measurement,31,3,,200,219,19,66,10.1111/j.1745-3984.1994.tb00443.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988137868&doi=10.1111%2fj.1745-3984.1994.tb00443.x&partnerID=40&md5=e1e525634cb172d43b56bcdc1654d89e,"There is a paucity of research in item response theory (IRT) examining the consequences of violating the implicit assumption of nonspeededness. In this study, test data were simulated systematically under various speeded conditions. The three factors considered in relation to speededness were proportion of test not reached (5%, 10%, and 15%), response to not reached (blank vs. random response), and item ordering (random vs. easy to hard). The effects of these factors on parameter estimation were then examined by comparing the item and ability parameter estimates with the known true parameters. Results indicated that the ability estimation was least affected by speededness in terms of the correlation between true and estimated ability parameters. On the other hand, substantial effects of speededness were observed among item parameter estimates. Recommendations for minimizing the effects of speededness are discussed Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988137868
Shavelson R.J.; Ruiz-Primo M.A.; Wiley E.W.,"Shavelson, Richard J. (35613093400); Ruiz-Primo, Maria Araceli (6602793270); Wiley, Edward W. (8566632000)",35613093400; 6602793270; 8566632000,Note on sources of sampling variability in science performance assessments,1999,Journal of Educational Measurement,36,1,,61,71,10,49,10.1111/j.1745-3984.1999.tb00546.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033245936&doi=10.1111%2fj.1745-3984.1999.tb00546.x&partnerID=40&md5=407e6eafd27a6fa88bb6fd5dc1d2e593,"In 1993, we reported in Journal of Educational Measurement that task-sampling variability was the Achilles' heel of science performance assessment. To reduce measurement error, tasks needed to be stratified before sampling, sampled in large number, or possibly both. However, Cronbach, Linn, Brennan, & Haertel (1997) pointed out that a task-sampling interpretation of a large person × task variance component might be incorrect. Task and occasion sampling are confounded because tasks are typically given on only a single occasion. The person × task source of measurement error is then confounded with the pt × occasion source. If pto variability accounts for a substantial part of the commonly observed pt interaction, stratifying tasks into homogenous subsets - a cost-effective way of addressing task sampling variability - might not increase accuracy. Stratification would not address the pto source of error. Another conclusion reported in JEM was that only direct observation (DO) and notebook (NB) methods of collecting performance assessment data were exchangeable; computer simulation, short-answer, and multiple-choice methods were not. However, if Cronbach et al. were right, our exchangeability conclusion might be incorrect. After re-examining and re-analyzing data, we found support for Conbach et al. We concluded that large task-sampling variability was due to both the person × task interaction and person × task × occasion interaction. Moreover, we found that direct observation, notebook and computer simulation methods were equally exchangeable, but their exchangeability was limited by the volatility of student performances across tasks and occasions.",,,Article,Final,,Scopus,2-s2.0-0033245936
Ferrara S.; Huynh H.; Michaels H.,"Ferrara, Steven (35333949500); Huynh, Huynh (16512875100); Michaels, Hillary (7004904435)",35333949500; 16512875100; 7004904435,Contextual explanations of local dependence in item clusters in a large scale hands-on science performance assessment,1999,Journal of Educational Measurement,36,2,,119,140,21,19,10.1111/j.1745-3984.1999.tb00550.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033241616&doi=10.1111%2fj.1745-3984.1999.tb00550.x&partnerID=40&md5=4d65c17653d4cad626c102e326cda68c,"This study provides hypothesized explanations for local item dependence (LID) in a large scale hands-on science performance assessment. Items within multi-step item clusters were classified as low or high in LID using contextual analysis procedures described in this and other studies. LID was identified statistically using the average within cluster (AWC) correlation procedure described in previous studies. Levels of LID identified in contextual analyses were compared to leves of LID identified in correlation analyses. Consistent with other studies, items that appear to elicit locally dependent responses require examinees to answer and explain their answer or to use given or generated information to respond.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-0033241616
Powers D.E.; Rock D.A.,"Powers, Donald E. (7202818498); Rock, Donald A. (7102591023)",7202818498; 7102591023,Effects of coaching on SAT I: Reasoning test scores,1999,Journal of Educational Measurement,36,2,,93,118,25,94,10.1111/j.1745-3984.1999.tb00549.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033241615&doi=10.1111%2fj.1745-3984.1999.tb00549.x&partnerID=40&md5=0a21ed081aa91069e32fae6e9c4c59eb,"A College Board-sponsored survey of a nationally representative sample of 1995-96 SAT takers yielded a data base for more than 4,000 examinees, about 500 of whom had attended formal coaching programs outside their schools. Several alternative analytical methods were used to estimate the effects of coaching on SAT I: Reasoning Test scores. The various analyses produced slightly different estimates. All of the estimates, however, suggested that the effects of coaching are far less than is claimed by major commercial test preparation companies. The revised SAT does not appear to be any more coachable than its predecessor.",,,Article,Final,,Scopus,2-s2.0-0033241615
Clauser B.E.; Clyman S.G.; Swanson D.B.,"Clauser, Brian E. (7003595460); Clyman, Stephen G. (6603827946); Swanson, David B. (7201859959)",7003595460; 6603827946; 7201859959,Components of rater error in a complex performance assessment,1999,Journal of Educational Measurement,36,1,,29,45,16,23,10.1111/j.1745-3984.1999.tb00544.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033245931&doi=10.1111%2fj.1745-3984.1999.tb00544.x&partnerID=40&md5=cee1dc8bed99f7dfa3de919a20616938,"Numerous studies have examined performance assessment data using generalizability theory. Typically, these studies have treated raters as randomly sampled from a population, with each rater judging a given performance on a single occasion. This paper presents two studies that focus on aspects of the rating process that are not explicitly accounted for in this typical design. The first study makes explicit the ""committee"" facet, acknowledging that raters often work within groups. The second study makes explicit the ""rating-occasion"" facet by having each rater judge each performance on two separate occasions. The results of the first study highlight the importance of clearly specifying the relevant facets of the universe of interest. Failing to include the committee facet led to an overly optimistic estimate of the precision of the measurement procedure. By contrast, failing to include the rating-occasion facet, in the second study, had minimal impact on the estimated error variance.",,,Article,Final,,Scopus,2-s2.0-0033245931
Parshall C.G.; Houghton P.D.B.; Kromrey J.D.,"Parshall, Cynthia G. (6603133073); Houghton, Pansy Du Bose (57191233667); Kromrey, Jeffrey D. (6603843189)",6603133073; 57191233667; 6603843189,Equating Error and Statistical Bias in Small Sample Linear Equating,1995,Journal of Educational Measurement,32,1,,37,54,17,29,10.1111/j.1745-3984.1995.tb00455.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988057765&doi=10.1111%2fj.1745-3984.1995.tb00455.x&partnerID=40&md5=fc8c78a09f42e8e3163d95193fe4d746,"A resampling study was conducted to compare the statistical bias and standard errors of nonequivalent‐groups linear test equating in small samples of examinees. Sample sizes of 15, 25, 50, and 100 were examined. One thousand samples of each size were drawn with replacement from each of 5 archival data files from teacher subject area tests. For each test, data files from 2 parallel forms were used. Results suggest trivial levels of equating bias even with small samples, but substantial increases in standard errors as sample size decreases. Results were interpreted in terms of applications to testing situations in which small numbers of examinees are available. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988057765
Bennett R.E.; Sebrechts M.M.,"Bennett, Randy Elliot (7402440584); Sebrechts, Marc M. (6701858562)",7402440584; 6701858562,A computer-based task for measuring the representational component of quantitative proficiency,1997,Journal of Educational Measurement,34,1,,64,77,13,9,10.1111/j.1745-3984.1997.tb00507.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031092675&doi=10.1111%2fj.1745-3984.1997.tb00507.x&partnerID=40&md5=54282d7d19f8973b2ef16bff8ba406b7,"In this study, we created a computer-delivered problem-solving task based on the cognitive research literature and investigated its validity for graduate admissions assessment. The task asked examinees to sort mathematical Word problem stems according to prototypes. Data analyses focused on the meaning of sorting scores and examinee perceptions of the task. Results showed that those who sorted well tended to have higher GRE General Test scores and college grades than did examinees who sorted less proficiently. Examinees generally preferred this task to multiple-choice items like those found on the General Test's Quantitative section and felt the task was a fairer measure of their ability to succeed in graduate school. Adaptations of the task might be used in admissions tests, as well as for instructional assessments to help lower-scoring examinees localize and remediate problem-solving difficulties.",,,Article,Final,,Scopus,2-s2.0-0031092675
Ercikan K.; Schwarz R.D.; Julian M.W.; Burket G.R.; Weber M.M.; Link V.,"Ercikan, Kadriye (6603174172); Schwarz, Richard D. (7401845696); Julian, Marc W. (7005695883); Burket, George R. (6602105926); Weber, Melba M. (7404139914); Link, Valerie (6603696006)",6603174172; 7401845696; 7005695883; 6602105926; 7404139914; 6603696006,Calibration and scoring of tests with multiple-choice and constructed-response item types,1998,Journal of Educational Measurement,35,2,,137,154,17,55,10.1111/j.1745-3984.1998.tb00531.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220380&doi=10.1111%2fj.1745-3984.1998.tb00531.x&partnerID=40&md5=8c1ac78086c200349d36a8b2df20975a,"This article discusses and demonstrates combining scores from multiple-choice (MC) and constructed-response (CR) items to create a common scale using item response theory methodology. Two specific issues addressed are (a) whether MC and CR items can be calibrated together and (b) whether simultaneous calibration of the two item types leads to loss of information. Procedures are discussed and empirical results are provided using a set of tests in the areas of reading, language, mathematics, and science in three grades.",,,Article,Final,,Scopus,2-s2.0-0032220380
De Gruijter D.N.M.,"De Gruijter, Dato N. M. (6507363394)",6507363394,On information of percentile ranks,1997,Journal of Educational Measurement,34,2,,177,178,1,1,10.1111/j.1745-3984.1997.tb00513.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287719&doi=10.1111%2fj.1745-3984.1997.tb00513.x&partnerID=40&md5=8430bb5e2ae6ab46887ca8d797dcd81a,Recently May and Nicewander concluded that percentile ranks are inferior to raw scores as indicators of latent ability. Here it is argued that their arguments are faulty.,,,Article,Final,,Scopus,2-s2.0-0031287719
Vispoel W.P.,"Vispoel, Walter P. (6603886282)",6603886282,Psychometric characteristics of computer-adaptive and self-adaptive vocabulary tests: The role of answer feedback and test anxiety,1998,Journal of Educational Measurement,35,2,,155,167,12,21,10.1111/j.1745-3984.1998.tb00532.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220383&doi=10.1111%2fj.1745-3984.1998.tb00532.x&partnerID=40&md5=eb6da97e01da862b171dcd0192bd6809,"This study focused on the effects of administration mode (computer-adaptive test [CAT] versus self-adaptive test [SAT]), item-by-item answer feedback (present versus absent), and test anxiety on results obtained from computerized vocabulary tests. Examinees were assigned at random to four testing conditions (CAT with feedback, CAT without feedback, SAT with feedback, SAT without feedback). Examinees completed the Test Anxiety Inventory (Spielberger, 1980) before taking their assigned computerized tests. Results showed that the CATs were more reliable and took less time to complete than the SATs. Administration time for both the CATs and SATs was shorter when feedback was provided than when it was not, and this difference was most pronounced for examinees at medium to high levels of test anxiety. These results replicate prior findings regarding the precision and administrative efficiency of CATs and SATs but point to new possible benefits of including answer feedback on such tests.",,,Article,Final,,Scopus,2-s2.0-0032220383
Wang T.; Vispoel W.P.,"Wang, Tianyou (55709761000); Vispoel, Walter P. (6603886282)",55709761000; 6603886282,Properties of ability estimation methods in computerized adaptive testing,1998,Journal of Educational Measurement,35,2,,109,135,26,59,10.1111/j.1745-3984.1998.tb00530.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220384&doi=10.1111%2fj.1745-3984.1998.tb00530.x&partnerID=40&md5=987a1f26fe50484b579fd0f01ded314d,"Simulations of computerized adaptive tests (CATs) were used to evaluate results yielded by four commonly used ability estimation methods: maximum likelihood estimation (MLE) and three Bayesian approaches-Owen's method, expected a posteriori (EAP), and maximum a posteriori. In line with the theoretical nature of the ability estimates and previous empirical research, the results showed clear distinctions between MLE and the Bayesian methods, with MLE yielding lower bias, higher standard errors, higher root mean square errors, lower fidelity, and lower administrative efficiency. Standard errors for MLE based on test information underestimated actual standard errors, whereas standard errors for the Bayesian methods based on posterior distribution standard deviations accurately estimated actual standard errors. Among the Bayesian methods, Owen's provided the worst overall results, and EAP provided the best. Using a variable starting rule in which examinees were initially classified into three broad ability groups greatly reduced the bias for the Bayesian methods, but had little effect on the results for MLE. On the basis of these results, guidelines are offered for selecting appropriate CAT ability estimation methods in different decision contexts. This article is based on a doctoral dissertation (Wang, 1995) that was directed by Walter P. Vispoel. We thank Roberto de la Torre for his assistance in developing software for the computer simulation analyses and Rebecca Zwick and the anonymous reviewers for their helpful comments on the original manuscripts. Correspondence concerning this article should be addressed to the first author.",,,Article,Final,,Scopus,2-s2.0-0032220384
Bennett R.E.; Steffen M.; Singley M.K.; Morley M.; Jacquemin D.,"Bennett, Randy Elliot (7402440584); Steffen, Manfred (7006154079); Singley, Mark Kevin (6603614741); Morley, Mary (7006568342); Jacquemin, Daniel (7006430415)",7402440584; 7006154079; 6603614741; 7006568342; 7006430415,"Evaluating an automatically scorable, open-ended response type for measuring mathematical reasoning in computer-adaptive tests",1997,Journal of Educational Measurement,34,2,,162,176,14,46,10.1111/j.1745-3984.1997.tb00512.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031165297&doi=10.1111%2fj.1745-3984.1997.tb00512.x&partnerID=40&md5=e52fcb9e99b843a688cd358d15c5bb2e,"The first generation of computer-based texts depends largely on multiple-choice items and constructed-response questions that can be scored through literal matches with a key. This study evaluated scoring accuracy and item functioning for an open-ended response type where correct answers, posed as mathematical expressions, can take many different surface forms. Items were administered to 1,864 participants in field trials of a new admissions test for quantitatively oriented graduate programs. Results showed automatic scoring to approximate the accuracy of multiple-choice scanning, with all processing errors stemming from examinees improperly entering responses. In addition, the items functioned similarly in difficulty, item-total relations, and male-female performance differences to other response types being considered for the measure.",,,Article,Final,,Scopus,2-s2.0-0031165297
Williamson D.M.; Bejar I.I.; Hone A.S.,"Williamson, David M. (7402435734); Bejar, Isaac I. (6602577229); Hone, Anne S. (7003427385)",7402435734; 6602577229; 7003427385,'Mental model' comparison of automated and human scoring,1999,Journal of Educational Measurement,36,2,,158,184,26,37,10.1111/j.1745-3984.1999.tb00552.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033147856&doi=10.1111%2fj.1745-3984.1999.tb00552.x&partnerID=40&md5=fff49f296f0403f77d9894c702bc5c93,"'Mental models' used by automated scoring for the simulation divisions of the computerized Architect Registration Examination are contrasted with those used by experienced human graders. Candidate solutions (N = 3613) received both automated and human holistic scores. Quantitative analyses suggest high correspondence between automated and human scores; thereby suggesting similar mental models are implemented. Solutions with discrepancies between automated and human scores were selected for qualitative analysis. The human graders were reconvened to review the human scores and to investigate the source of score discrepancies in light of rationales provided by the automated scoring process. After review, slightly more than half of the score discrepancies were reduced or eliminated. Six sources of discrepancy between original human scores and automated scores were identified: subjective criteria; objective criteria; tolerances/ weighting; details; examinee task interpretation; and unjustified. The tendency of the human graders to be compelled by automated score rationales varied by the nature of original score discrepancy. We determine that, while the automated scores are based on a mental model consistent with that of expert graders, there remain some important differences, both intentional and incidental, which distinguish between human and automated scoring. We conclude that automated scoring has the potential to enhance the validity evidence of scores in addition to improving efficiency.",,,Article,Final,,Scopus,2-s2.0-0033147856
Béland A.; Mislevy R.J.,"Béland, Anne (8154538400); Mislevy, Robert J. (6701800690)",8154538400; 6701800690,Probability-based inference in a domain of proportional reasoning tasks,1996,Journal of Educational Measurement,33,1,,3,27,24,8,10.1111/j.1745-3984.1996.tb00476.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306431&doi=10.1111%2fj.1745-3984.1996.tb00476.x&partnerID=40&md5=9e4faa37b3a8e1146c0a9329dbb31e4d,"Educators and psychologists are increasingly interested in modeling the processes and knowledge structures by which people learn and solve problems. Meaningful progress has been made in developing cognitive models in several domains and in devising observational settings that provide clues about subjects' cognition from this perspective. Less attention has been paid to drawing inferences or making decisions with such data, even though observations provide only imperfect information about cognition, and complex interrelationships can exist among observations and theoretical variables. This article discusses the use of probability-based reasoning to explicate hypothesized and empirical relationships and to structure inference in this context. Ideas are illustrated with an example concerning proportional reasoning.",,,Article,Final,,Scopus,2-s2.0-0030306431
Lane S.; Liu M.; Ankenmann R.D.; Stone C.A.,"Lane, Suzanne (7202106909); Liu, Mei (55743448100); Ankenmann, Robert D. (6603355304); Stone, Clement A. (7201720784)",7202106909; 55743448100; 6603355304; 7201720784,Generalizability and validity of a mathematics performance assessment,1996,Journal of Educational Measurement,33,1,,71,92,21,52,10.1111/j.1745-3984.1996.tb00480.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306430&doi=10.1111%2fj.1745-3984.1996.tb00480.x&partnerID=40&md5=c0f87719ae742b3600bc9eba8711a42a,"The QUASAR Cognitive Assessment Instrument (QCAI) is designed to measure program outcomes and growth in mathematics. It consists of a relatively large set of open-ended tasks that assess mathematical problem solving, reasoning, and communication at the middle-school grade levels. This study provides some evidence for the generalizability and validity of the assessment. The results from the generalizability studies indicate that the error due to raters is minimal, whereas there is considerable differential student performance across tasks. The dependability of grade level scores for absolute decision making is encouraging; when the number of students is equal to 350, the coefficients are between .80 and .97 depending on the form and grade level. As expected, there tended to be a higher relationship between the QCAI scores and both the problem solving and conceptual subtest scores from a mathematics achievement multiple-choice test than between the QCAI scores and the mathematics computation subtest scores.",,,Article,Final,,Scopus,2-s2.0-0030306430
Tatsuoka K.K.; Tatsuoka M.M.,"Tatsuoka, Kikumi K. (6603447775); Tatsuoka, Maurice M. (6506443458)",6603447775; 6506443458,Computerized cognitive diagnostic adaptive testing: Effect on remedial instruction as empirical validation,1997,Journal of Educational Measurement,34,1,,3,20,17,53,10.1111/j.1745-3984.1997.tb00504.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287480&doi=10.1111%2fj.1745-3984.1997.tb00504.x&partnerID=40&md5=b7ea4ed045d9944da150294b6c68e418,"The purpose of this study is to show the usefulness of cognitive diagnoses for remedial instruction. Cognitive diagnoses were done by an adaptive testing system using the rule-space methodology, which was developed by K. K. Tatsuoka and her associates (K. K. Tatsuoka, 1983, 1990; K. K. Tatsuoka & M. M. Tatsuoka, 1987; M. M. Tatsuoka & K. K. Tatsuoka, 1989). The results of the study strongly indicate that knowing students' knowledge states prior to remediation is very effective and that the rule-space method can effectively diagnose students' knowledge states and can point out ways for remediating their errors quickly with minimum effort. It is also found that the design of instructional units for remediation can be effectively guided by the rule-space model, because the determination of all possible knowledge states in a domain of interest, given an incidence matrix, is based on a partially ordered tree structure of knowledge states, which is equivalent to item-score patterns determined logically from the incidence matrix.",,,Article,Final,,Scopus,2-s2.0-0031287480
Brennan R.L.,"Brennan, Robert L. (34975092300)",34975092300,The Conventional Wisdom About Group Mean Scores,1995,Journal of Educational Measurement,32,4,,385,396,11,33,10.1111/j.1745-3984.1995.tb00473.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988115628&doi=10.1111%2fj.1745-3984.1995.tb00473.x&partnerID=40&md5=ad3366584970d536b1ec0569962c4b42,"Not infrequently, investigators assume that reliability for groups is greater than reliability for persons, and/or that error variance for groups is less than error variance for persons. Using generalizability theory, it is shown that this conventional wisdom is not necessarily true. Examples are provided from the course evaluation literature and the performance testing literature Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988115628
Powers D.E.; Fowles M.E.; Farnum M.; Ramsey P.,"Powers, Donald E. (7202818498); Fowles, Mary E. (6602483278); Farnum, Marisa (57065735800); Ramsey, Paul (57191232030)",7202818498; 6602483278; 57065735800; 57191232030,They Think Less of My Handwritten Essay If Others Word Process Theirs? Effects on Essay Scores of Intermingling Handwritten and Word‐Processed Essays,1994,Journal of Educational Measurement,31,3,,220,233,13,84,10.1111/j.1745-3984.1994.tb00444.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988137814&doi=10.1111%2fj.1745-3984.1994.tb00444.x&partnerID=40&md5=cc4f7307ed023e04eae323a69f69c76d,"A study was undertaken to determine the effects on essay scores of intermingling handwritten and word‐processed versions of student essays. A sample of examinees, each of whom had produced both a handwritten and a word‐processed essay, was drawn from a larger sample of students who had participated in a pilot study of a new academic skills assessment battery. Students’original handwritten essays were converted to word‐processed versions, and their original word‐processed essays were converted to handwritten versions. Analyses revealed higher average scores for essays scored in the handwritten mode than for essays scored as word processed, regardless of the mode in which essays were originally produced. Several hypotheses were advanced to explain the discrepancies between scores on handwritten and word‐processed essays. The training of essay readers was subsequently modified on the basis of these hypotheses, and the experiment was repeated using the modified training with a new set of readers. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988137814
Otter M.E.; Mellenbergh G.J.; Glopper K.d.,"Otter, Martha E. (7006018093); Mellenbergh, Gideon J. (7003739438); Glopper, Kees de (57191231174)",7006018093; 7003739438; 57191231174,The Relation Between Information‐Processing Variables and Test‐Retest Stability for Questionnaire Items,1995,Journal of Educational Measurement,32,2,,199,216,17,18,10.1111/j.1745-3984.1995.tb00463.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072536&doi=10.1111%2fj.1745-3984.1995.tb00463.x&partnerID=40&md5=699a024de73b31201558410658f3fd96,"Recently developed cognitive theories explain why some questionnaire items can be answered in a reliable and valid manner and others cannot. Those theories distinguish two components: (a) the interpretation or understanding of a question and (b) the role of memory. The present study investigates the ability of these two components to forecast the test‐retest association coefficients of 207 pilot questionnaire items used in an international study of reading literacy in which 2 populations were involved: Grade 5 and Grade 2 of primary and secondary education, respectively. The analysis of the data showed that for both populations, both components forecast the relative sizes of the test‐retest correlation coefficients. The results strongly suggest that if one wishes to use questionnaire items in research about relationships, then the items should be as unambiguous as possible. Moreover, the information needed to formulate an answer must be easily accessible in the respondent's memory. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988072536
Engelhard Jr. G.,"Engelhard Jr., George (7003970969)",7003970969,"Clarification to ""examining rater errors in the assessment of written composition with a many-faceted rasch model""",1996,Journal of Educational Measurement,33,1,,115,116,1,5,10.1111/j.1745-3984.1996.tb00483.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306433&doi=10.1111%2fj.1745-3984.1996.tb00483.x&partnerID=40&md5=5c8cd4388a26d9e2d56620f389b911af,"In the article ""Examining Rater Errors in the Assessment of Written Composition With a Many-Faceted Rasch Model"" (JEM, Volume 31, Number 2, Summer 1994), the data presented in Figure 3 may be misleading. The ""four clear spikes"" (p. 106) that appear in Figure 3 were highlighted by the automatic scaling procedure used by the computer program that generated this histogram; as is well known, the use of different scaling units would yield histograms with different shapes (Moore & McCabe, 1993). For example, when the same data
are presented as a bar chart (see Figure 1 below) rather than as a histogram, the four spikes are not evident. As graphical procedures become more readily available to measurement researchers, additional research and discussion are needed regarding standards for evaluating data displays that do not simply reproduce the actual data values.",,,Article,Final,,Scopus,2-s2.0-0030306433
Janssen R.; De Boeck P.,"Janssen, Rianne (7202677512); De Boeck, Paul (7005323510)",7202677512; 7005323510,The contribution of a response-production component to a free-response synonym task,1996,Journal of Educational Measurement,33,4,,417,432,15,4,10.1111/j.1745-3984.1996.tb00499.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030300450&doi=10.1111%2fj.1745-3984.1996.tb00499.x&partnerID=40&md5=71b9e3efb20c458e615fd4b1fc9631b8,"A cognitive approach to the study of format differences is illustrated using synonym tasks. By means of a multiple regression analysis with latent variables, it is shown that both a response-production component and an evaluation component are involved in answering a free-response synonym task. Given the results of Janssen, De Boeck, and Vander Steene (1996), the format differences between the multiple-choice evaluation task and the free-response synonym task can be explained in terms of the kinds of verbal abilities measured. The evaluation task is a pure measure of verbal comprehension, while the free-response synonym task is affected by verbal comprehension and verbal fluency, as well. The design used to study format differences controls both for content effects and for the effects of repeating item stems across formats.",,,Article,Final,,Scopus,2-s2.0-0030300450
Feldt L.S.,"Feldt, Leonard S. (7003911114)",7003911114,Confidence intervals for the proportion of mastery in criterion-referenced measurement,1996,Journal of Educational Measurement,33,1,,106,114,8,2,10.1111/j.1745-3984.1996.tb00482.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306432&doi=10.1111%2fj.1745-3984.1996.tb00482.x&partnerID=40&md5=07346d89f18b49f4c488a9e75feee18f,"A relatively simple method is developed to obtain confidence intervals for a student's proportion of domain mastery in criterion-referenced or mastery measurement situations. The method uses the binomial distribution as a model for the student's scores under hypothetically repeated assessments. Though the use of this model is not a new idea, the method of implementation has not been proposed previously. The technique makes use of widely available F tables and hence does not require elaborate computer equipment or proprietary computer programs.",,,Article,Final,,Scopus,2-s2.0-0030306432
Impara J.C.; Plake B.S.,"Impara, James C. (6602233011); Plake, Barbara S. (6603689848)",6602233011; 6603689848,Standard setting: An alternative approach,1997,Journal of Educational Measurement,34,4,,353,366,13,102,10.1111/j.1745-3984.1997.tb00523.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031284018&doi=10.1111%2fj.1745-3984.1997.tb00523.x&partnerID=40&md5=4750d7a4fb665ddc92756f8264fb2030,"Since 1971 there have been a number of studies in which a cut score has been set using a method proposed by Angoff (1971). In this method, each member of a panel of judges estimates for each test question the proportion correct for a specific target group of examinees. Prior and contemporary research suggests that this is a difficult task for judges. Angoff also proposed that judges simply indicate whether or not an examinee from the target group will be able to answer each question correctly (the yes/no method). We report on the results of two studies that compare a yes/no estimation with a proportion correct estimation. The two studies demonstrate that both methods produce essentially equal cut scores and that judges find the yes/no method more comfortable to use than the estimated proportion correct method.",,,Article,Final,,Scopus,2-s2.0-0031284018
Schulz E.M.; Nicewander W.A.,"Schulz, E. Matthew (57213119158); Nicewander, W. Alan (6601997483)",57213119158; 6601997483,Grade equivalent and IRT representations of growth,1997,Journal of Educational Measurement,34,4,,315,331,16,15,10.1111/j.1745-3984.1997.tb00521.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031284019&doi=10.1111%2fj.1745-3984.1997.tb00521.x&partnerID=40&md5=d9ca5145ca0efa7bf02fee7f912cbeeb,"It has long been a part of psychometric lore that the variance of children's scores on cognitive tests increases with age. This increasing-variance phenomenon was first observed on Binet's intelligence measures in the early 1900s. An important detail in this matter is the fact that developmental scales based on age or grade have served as the medium for demonstrating the increasing-variance phenomenon. Recently, developmental scales based on item response theory (IRT) have shown constant or decreasing variance of measures of achievement with increasing age. This discrepancy is of practical and theoretical importance. Conclusions about the effects of variables on growth in achievement will depend on the metric chosen. In this study, growth in the mean of a latent educational achievement variable is assumed to be a negatively accelerated function of grade; within-grade variance is assumed to be constant across grade, and observed test scores are assumed to follow an IRT model. Under these assumptions, the variance of grade equivalent scores increases markedly. Perspective on this phenomenon is gained by examining longitudinal trends in centimeter and age equivalent measures of height.",,,Article,Final,,Scopus,2-s2.0-0031284019
Fitzpatrick A.R.; Link V.B.; Yen W.M.; Burket G.R.; Ito K.; Sykes R.C.,"Fitzpatrick, Anne R. (7004620831); Link, Valerie B. (6603696006); Yen, Wendy M. (7102684621); Burket, George R. (6602105926); Ito, Kyoko (55478907400); Sykes, Robert C. (7101991229)",7004620831; 6603696006; 7102684621; 6602105926; 55478907400; 7101991229,Scaling performance assessments: A comparison of one-parameter and two-parameter partial credit models,1996,Journal of Educational Measurement,33,3,,291,314,23,17,10.1111/j.1745-3984.1996.tb00494.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030505472&doi=10.1111%2fj.1745-3984.1996.tb00494.x&partnerID=40&md5=d739a40f13576a53f3893171da2e138f,"In one study, parameters were estimated for constructed-response (CR) items in 8 tests from 4 operational testing programs using the 1-parameter and 2-parameter partial credit (1PPC and 2PPC) models. Where multiple-choice (MC) items were present, these models were combined with the 1-parameter and 3-parameter logistic (1PL and 3PL) models, respectively. We found that item fit was better when the 2PPC model was used alone or with the 3PL model. Also, the slopes of the CR and MC items were found to differ substantially. In a second study, item parameter estimates produced using the 1PL-1PPC and 3PL-2PPC model combinations were evaluated for fit to simulated data generated using true parameters known to fit one model combination or the other. The results suggested that the more flexible 3PL-2PPC model combination would produce better item fit than the 1PL-1PPC combination.",,,Article,Final,,Scopus,2-s2.0-0030505472
Douglas J.A.; Roussos L.A.; Stout W.F.,"Douglas, Jeffrey A. (7403213236); Roussos, Louis A. (6603805095); Stout, William F. (57207542942)",7403213236; 6603805095; 57207542942,Item-bundle DIF hypothesis testing: Identifying suspect bundles and assessing their differential functioning,1996,Journal of Educational Measurement,33,4,,465,484,19,108,10.1111/j.1745-3984.1996.tb00502.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030300457&doi=10.1111%2fj.1745-3984.1996.tb00502.x&partnerID=40&md5=d7826f2e5ca7edcfb89df925f2403856,"This article proposes two multidimensional IRT model-based methods of selecting item bundles (clusters of not necessarily adjacent items chosen according to some organizational principle) suspected of displaying DIF amplfication. The approach embodied in these two methods is inspired by Shealy and Stout's (1993a, 1993b) multidimensional model for DIF. Each bundle selected by these methods constitutes a DIF amplification hypothesis. When SIBTEST (Shealy & Stout, 1993b) confirms DIF amplification in selected bundles, differential bundle functioning (DBF) is said to occur. Three real data examples illustrate the two methods for suspect bundle selection. The effectiveness of the methods is argued on statistical grounds. A distinction between benign and adverse DIF is made. The decision whether flagged DIF items or DBF bundles display benign or adverse DIP/DBF must depend in part on nonstatistical construct validity arguments. Conducting DBF analyses using these methods should help in the identification of the causes of DIF/DBF.",,,Article,Final,,Scopus,2-s2.0-0030300457
Bridgeman B.; Lewis C.,"Bridgeman, Brent (7005526936); Lewis, Charles (54389552600)",7005526936; 54389552600,Gender differences in college mathematics grades and SAT-M scores: A reanalysis of Wainer and Steinberg,1996,Journal of Educational Measurement,33,3,,257,270,13,26,10.1111/j.1745-3984.1996.tb00492.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030488931&doi=10.1111%2fj.1745-3984.1996.tb00492.x&partnerID=40&md5=31cf48e5e7f4ceeff9671d9316d0f69f,"Wainer and Steinberg (1992) showed that within broad categories of first-year college mathematics courses (e.g., calculus) men had substantially higher average scores on the mathematics section of the SAT (SAT-M) than women who earned the same letter grade. However, three aspects of their analyses may lead to unwarranted conclusions. First, they focused primarily on differences in SAT-M scores given course grades when the more important question for admissions officers is the difference in course grades given scores on the predictor. Second, they failed to account for differences among calculus courses (e.g., calculus for engineers versus calculus for liberal arts students). Most importantly, Wainer and Steinberg focused on the use of SAT-M as a single predictor. A reanalysis presented here indicated that a more appropriate composite indicator made up of both SAT-M and high school grade point average demonstrated minuscule gender differences for both calculus and precalculus courses.",,,Article,Final,,Scopus,2-s2.0-0030488931
Vispoel W.P.; Rocklin T.R.; Wang T.; Bleiler T.,"Vispoel, Walter P. (6603886282); Rocklin, Thomas R. (6602382313); Wang, Tianyou (55709761000); Bleiler, Timothy (6603213880)",6603886282; 6602382313; 55709761000; 6603213880,Can examinees use a review option to obtain positively biased ability estimates on a computerized adaptive test?,1999,Journal of Educational Measurement,36,2,,141,157,16,12,10.1111/j.1745-3984.1999.tb00551.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033147646&doi=10.1111%2fj.1745-3984.1999.tb00551.x&partnerID=40&md5=4167d09df17efb155af7f028874a7b41,"Part of the controversy about allowing examinees to review and change answers to previous items on computerized adaptive tests (CATs) centers on a strategy for obtaining positively biased ability estimates attributed to Wainer (1993) in which examinees intentionally answer items incorrectly before review and to the best of their abilities upon review. Our results, based on both simulated and live testing data, showed that there were instances in which the Wainer strategy yielded inflated ability estimates as well as instances in which it yielded deflated ability estimates. The success of the strategy in inflating ability estimates depended on the ability estimation method used (maximum likelihood versus Bayesian), the examinee's true ability level, the standard error of the ability estimate, the examinee's ability to implement the strategy, and the type of decision made from the ability estimate. We discuss approaches to dealing with the Wainer strategy in operational CAT settings.",,,Article,Final,,Scopus,2-s2.0-0033147646
Hosenfeld B.; Van Den Boom D.C.; Resing W.C.M.,"Hosenfeld, Bettina (6506045572); Van Den Boom, Dymphna C. (7003705565); Resing, Wilma C. M. (6602627839)",6506045572; 7003705565; 6602627839,Constructing geometric analogies for the longitudinal testing of elementary school children,1997,Journal of Educational Measurement,34,4,,367,372,5,29,10.1111/j.1745-3984.1997.tb00524.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031284016&doi=10.1111%2fj.1745-3984.1997.tb00524.x&partnerID=40&md5=803771855599404be1f5b4072bd2ac20,"This article reports the development of a new geometric analogies test for elementary school children and the examination of its utility for future longitudinal investigations. Analogies are intelligence test problems of the form A : B :: C : ?  with the constraint that the relation between A and B be equivalent to the relation between C and the answer term. An example of a simple geometric analogy is ""small circle : large circle :: small square : ?,"" where the correct answer is ""large square."" Geometric analogies can be assumed to measure analogical reasoning more purely than the often-used verbal analogies, because little vocabulary or specific domain knowledge is needed for their solution. In order to create a large item pool we used a facet-design matrix including six elements---circles, squares, triangles,  entagons, hexagons, and ellipses--and five transformations adding an element, changing size, halving, doubling, and changing position. By combining one or two of the six elements with one, two, or three of the five transformations, we constructed a theoretical item pool containing
12,150 items. From this item pool we selected items of several classes of difficulty so that every element and every transformation were represented with approximately the same frequency in the final test. In order to define the theoretical difficulty level of each item we used the following equation: Difficulty = 0.5 x Elements + 1 x Transformations, a simplified version of the equation obtained by Mulholland, Pellegrino, and Glaser (1980). Items with two or more equally correct solutions as described by Bethell-Fox, Lohman, and Snow (1984) were excluded.",,,Article,Final,,Scopus,2-s2.0-0031284016
Yen W.M.; Burket G.R.,"Yen, Wendy M. (7102684621); Burket, George R. (6602105926)",7102684621; 6602105926,Comparison of item response theory and Thurstone methods of vertical scaling,1997,Journal of Educational Measurement,34,4,,293,313,20,19,10.1111/j.1745-3984.1997.tb00520.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031284017&doi=10.1111%2fj.1745-3984.1997.tb00520.x&partnerID=40&md5=0428cd43bd2a7a3fcea03f61b3b54f81,"Vertical achievement scales, which range from the lower elementary grades to high school, are used pervasively in educational assessment. Using simulated data modeled after real tests, the present article examines two procedures available for vertical scaling: a Thurstone method and three-parameter item response theory. Neither procedure produced artifactual scale shrinkage; both procedures produced modest scale expansion for one simulated condition.",,,Article,Final,,Scopus,2-s2.0-0031284017
Chang H.-H.; Mazzeo J.; Roussos L.,"Chang, Hua-Hua (7407524642); Mazzeo, John (57225690331); Roussos, Louis (6603805095)",7407524642; 57225690331; 6603805095,Detecting DIF for polytomously scored items: An adaptation of the SIBTEST procedure,1996,Journal of Educational Measurement,33,3,,333,353,20,134,10.1111/j.1745-3984.1996.tb00496.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030488932&doi=10.1111%2fj.1745-3984.1996.tb00496.x&partnerID=40&md5=91a431380db465c761363bf77f3b2c4e,"Shealy and Stout (1993) proposed a DIF detection procedure called SIBTEST and demonstrated its utility with both simulated and real data sets. Current versions of SIBTEST can be used only for dichotomous items. In this article, an extension to handle polytomous items is developed. Two simulation studies are presented which compare the modified SIBTESTprocedure with the Mantel and standardized mean difference (SMD) procedures. The first study compares the procedures under conditions in which the Mantel and SMD procedures have been shown to perform well (Zwick, Donoghue, & Grima. 1993). Results of Study 1 suggest that SIBTEST performed reasonably well, but that the Mantel and SMD procedures performed slightly better. The second study uses data simulated under conditions in which observed-score DIF methods for dichotomous items have not performed well. The results of Study 2 indicate that under these conditions the modified SIBTEST procedure provides better control of impact-induced Type I error inflation than the other procedures.",,,Article,Final,,Scopus,2-s2.0-0030488932
Schnipke D.L.; Green B.F.,"Schnipke, Deborah L. (6506792291); Green, Bert F. (7401825047)",6506792291; 7401825047,A Comparison of Item Selection Routines in Linear and Adaptive Tests,1995,Journal of Educational Measurement,32,3,,227,242,15,5,10.1111/j.1745-3984.1995.tb00464.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099623&doi=10.1111%2fj.1745-3984.1995.tb00464.x&partnerID=40&md5=a3138e799f9dbe11dd38e85ad580db78,"Two item selection algorithms were compared in simulated linear and adaptive tests of cognitive ability. One algorithm selected items that maximally differentiated between examinees. The other used item response theory (IRT) to select items having maximum information for each examinee. Normally distributed populations of 1,000 cases were simulated, using test lengths of 4, 5, 6, and 7 items. Overall, adaptive tests based on maximum information provided the most information over the widest range of ability values and, in general, differentiated among examinees slightly better than the other tests. Although the maximum differentiation technique may be adequate in some circumstances, adaptive tests based on maximum information are clearly superior. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988099623
Sheehan K.M.,"Sheehan, Kathleen M. (7005727437)",7005727437,A tree-based approach to proficiency scaling and diagnostic assessment,1997,Journal of Educational Measurement,34,4,,333,352,19,52,10.1111/j.1745-3984.1997.tb00522.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031284020&doi=10.1111%2fj.1745-3984.1997.tb00522.x&partnerID=40&md5=7e6a907d68539c50f787244a5a960697,"A new procedure for generating instructionally relevant diagnostic feedback is proposed. The approach involves first constructing a strong model of student proficiency and then testing whether individual students' observed item response vectors are consistent with that model. Diagnoses are specified in terms of the combinations of skills needed to score at increasingly higher levels on a test's reported score scale. The approach is applied to the problem of developing diagnostic feedback for the SAT 1 Verbal Reasoning test. Using a variation of Wright's (1977) person-fit statistic, it is shown that the estimated proficiency model accounts for 91% of the ""explainable"" variation in students' observed item response vectors.",,,Article,Final,,Scopus,2-s2.0-0031284020
Engelhard Jr. G.,"Engelhard Jr., George (7003970969)",7003970969,Evaluating rater accuracy in performance assessments,1996,Journal of Educational Measurement,33,1,,56,70,14,70,10.1111/j.1745-3984.1996.tb00479.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306428&doi=10.1111%2fj.1745-3984.1996.tb00479.x&partnerID=40&md5=0843ffaa6c0332a98c55c88e6600ab19,"A new method for evaluating rater accuracy within the context of performance assessments is described. Accuracy is defined as the match between ratings obtained from operational raters and those obtained from an expert panel on a set of benchmark, exemplar, or anchor performances. An extended Rasch measurement model called the FACETS model is presented for examining rater accuracy. The FACETS model is illustrated with 373 benchmark papers rated by 20 operational raters and an expert panel. The data are from the 1993 field test of the High School Graduation Writing Test in Georgia. The data suggest that there are statistically significant differences in rater accuracy; the data also suggest that it is easier to be accurate on some benchmark papers than on others. A small example is presented to illustrate how the accuracy ordering of raters may not be invariant over different subsets of benchmarks used to evaluate accuracy.",,,Article,Final,,Scopus,2-s2.0-0030306428
Kalohn J.C.; Spray J.A.,"Kalohn, John C. (6508043760); Spray, Judith A. (7006830106)",6508043760; 7006830106,The effect of model misspecification on classification decisions made using a computerized test,1999,Journal of Educational Measurement,36,1,,47,59,12,8,10.1111/j.1745-3984.1999.tb00545.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033096078&doi=10.1111%2fj.1745-3984.1999.tb00545.x&partnerID=40&md5=508e9c53bae0b09af1db5d2c29f76acb,"Many computerized testing algorithms require the fitting of some item response theory (IRT) model to examinees' responses to facilitate item selection, the determination of test stopping rules, and classification decisions. Some IRT models are thought to be particularly useful for small volume certification programs that wish to make the transition to computerized adaptive testing (CAT). The one-parameter logistic model (1-PLM) is usually assumed to require a smaller sample size than the three-parameter logistic model (3-PLM) for item parameter calibrations. This study examined the effects of model misspecification on the precision of the decisions made using the sequential probability ratio test (SPRT). For this comparison, the 1-PLM was used to estimate item parameters, even though the items' characteristics were represented by a 3-PLM. Results demonstrated that the 1-PLM produced considerably more decision errors under simulation conditions similar to a real testing environment, compared to the true model and to a fixed-form standard reference set of items.",,,Article,Final,,Scopus,2-s2.0-0033096078
Welch C.J.; Miller T.R.,"Welch, Catherine J. (57064765000); Miller, Timothy R. (7403948130)",57064765000; 7403948130,Assessing Differential Item Functioning in Direct Writing Assessments: Problems and an Example,1995,Journal of Educational Measurement,32,2,,163,178,15,22,10.1111/j.1745-3984.1995.tb00461.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988091745&doi=10.1111%2fj.1745-3984.1995.tb00461.x&partnerID=40&md5=b3aa7125003f1f6c4afae46e4c1b72b5,"The recent emphasis on various types of performance assessments raises questions concerning the differential effects of such assessments on population subgroups. Procedures for detecting differential item functioning (DIF) in data from performance assessments are available but may be hindered by problems that stem from this mode of assessment. Foremost among these are problems related to finding an appropriate matching variable. These problems are discussed and results are presented for three methods for DIF detection in polytomous items using data from a direct writing assessment. The purpose of the study is to examine the effects of using different combinations of internal and external matching variables. The procedures included a generalized Mantel‐Haenszel statistic, a technique based on meta‐analysis methodology, and logistic discriminant function analysis. In general, the results did not support the use of an external matching criterion and indicated that continued problems may be expected in attempts to assess DIF in performance assessments. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988091745
Kim S.‐H.; Cohen A.S.; Park T.‐H.,"Kim, Seock‐Ho (7601601135); Cohen, Allan S. (55465451100); Park, Tae‐Hak (57191233681)",7601601135; 55465451100; 57191233681,Detection of Differential Item Functioning in Multiple Groups,1995,Journal of Educational Measurement,32,3,,261,276,15,66,10.1111/j.1745-3984.1995.tb00466.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053865&doi=10.1111%2fj.1745-3984.1995.tb00466.x&partnerID=40&md5=33ca7ff0870bbb20a56b353b35cf5fcf,"Detection of differential item functioning (DIF) is most often done between two groups of examinees under item response theory. It is sometimes important, however, to determine whether DIF is present in more than two groups. In this article we present a method for detection of DIF in multiple groups. The method is closely related to Lard's chi‐square for comparing vectors of item parameters estimated in two groups. An example using real data is provided. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988053865
Clauser B.E.; Nungester R.J.; Swaminathan H.,"Clauser, Brian E. (7003595460); Nungester, Ronald J. (6602127510); Swaminathan, Hariharan (6602382602)",7003595460; 6602127510; 6602382602,Improving the matching for DIF analysis by conditioning on both test score and an educational background variable,1996,Journal of Educational Measurement,33,4,,453,464,11,25,10.1111/j.1745-3984.1996.tb00501.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030300449&doi=10.1111%2fj.1745-3984.1996.tb00501.x&partnerID=40&md5=0105b6851dea489f03589a3f7e8a9fae,"When tests are designed to measure dimensionally complex material, DIF analysis with matching based on the total test score may be inappropriate. Previous research has demonstrated that matching can be improved by using multiple internal or both internal and external measures to more completely account for the latent ability space. The present article extends this line of research by examining the potential to improve matching by conditioning simultaneously on test score and a categorical variable representing the educational background of the examinees. The responses of male and female examinees from a test of medical competence were analyzed using a logistic regression procedure. Results show a substantial reduction in the number of items identified as displaying significant DIF when conditioning is based on total test score and a variable representing educational background as opposed to total test score only.",,,Article,Final,,Scopus,2-s2.0-0030300449
Mislevy R.J.,"Mislevy, Robert J. (6701800690)",6701800690,Test theory reconceived,1996,Journal of Educational Measurement,33,4,,379,416,37,140,10.1111/j.1745-3984.1996.tb00498.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030300445&doi=10.1111%2fj.1745-3984.1996.tb00498.x&partnerID=40&md5=e2767705f0e1684de6a26c373e912b6e,"Educational test theory consists of statistical and methodological tools to support inference about examinees' knowledge, skills, and accomplishments. Its evolution has been shaped by the nature of users' inferences, which have been framed almost exclusively in terms of trait and behavioral psychology, and focused on students' tendency to act in prespecified ways in prespecified domains of tasks. Progress in the methodology of test theory enabled users to extend the range of inference and ground interpretations more solidly within these psychological paradigms. Developments in cognitive and developmental psychology have broadened the range of inferences we wish to make about students' learning to encompass conjectures about the nature and acquisition of their knowledge. The same underlying principles of inference that led to standard test theory can support inference in this broader universe of discourse. Familiar models and methods - sometimes extended, sometimes reinterpreted, sometimes applied to problems wholly different from those for which they were first devised - can play a useful role to this end.",,,Article,Final,,Scopus,2-s2.0-0030300445
Attali Y.; Goldschmidt C.,"Attali, Yigal (7801686463); Goldschmidt, Chanan (56932337100)",7801686463; 56932337100,The effects of component variables on performance in graph comprehension tests,1996,Journal of Educational Measurement,33,1,,93,105,12,4,10.1111/j.1745-3984.1996.tb00481.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306429&doi=10.1111%2fj.1745-3984.1996.tb00481.x&partnerID=40&md5=ba94fbb5ebf518aaf8df41f087f539c1,"Some cognitive characteristics of graph comprehension items were studied, and a model comprised of several variables was developed. 132 graph items of the Psychometric Entrance Test were included in the study. By analyzing the actual difficulty of the items, an evaluation of the impact of the cognitive variables on item difficulties could be made. Results indicate that successful prediction of item difficulty can be calculated on the basis of a wide range of item characteristics and task demands. This suggests that items can be screened for processing difficulty prior to being administered to examinees. However, the results also have implications for test validity in that the various processing variables identified involve distinct ability dimensions.",,,Article,Final,,Scopus,2-s2.0-0030306429
Powers D.E.; Fowles M.E.,"Powers, Donald E. (7202818498); Fowles, Mary E. (6602483278)",7202818498; 6602483278,Effects of applying different time limits to a proposed GRE writing test,1996,Journal of Educational Measurement,33,4,,433,452,19,22,10.1111/j.1745-3984.1996.tb00500.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030300448&doi=10.1111%2fj.1745-3984.1996.tb00500.x&partnerID=40&md5=2e7e497e43c338cf952574b96dae3186,"In order to determine the role of time limits on both test performance and test validity, we asked approximately 300 volunteers - prospective graduate students - to each write two essays - one in a 40-minute time period and the other in 60 minutes. Analyses revealed that, on average, test performance was significantly better when examinees were given 60 minutes instead of 40. However, there was no interaction between test-taking style (fast vs. slow) and time limits. That is, examinees who described themselves as slow writers/test takers did not benefit any more (or any less) from generous time limits than did their quicker counterparts. In addition, there was no detectable effect of different time limits on the meaning of essay scores, as suggested by their relationship to several nontest indicators of writing ability.",,,Article,Final,,Scopus,2-s2.0-0030300448
Osterlind S.J.; Friedman S.J.,"Osterlind, Steven J. (6507846637); Friedman, Stephen J. (57032773600)",6507846637; 57032773600,"Constructing test items: Multiple-choice, constructed-response, performance, and other formats (2nd edition)",1999,Journal of Educational Measurement,36,3,,267,270,3,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033196483&partnerID=40&md5=fbd7d64595dab0ed12b3b6da176c1488,"""The primary goal for this book is to contribute to the improvement of tests and measurement by aiding good test-item construction"" (p. 9). To accomplish this goal, the author articulates four key issues that must be addressed: 1. Description of the characteristics and functions of test items; 2. Presentation of editorial guidelines for writing test items; 3. Presentation of methods for determining the quality of test items; 4. Presentation of a compendium of important issues about test items.  Following an introduction (Chapter 1), the author devotes succeeding chapters to the purpose and characteristics of test items, the content of items from a validity perspective, practical considerations when writing items, guidelines for multiple-choice items, guidelines for constructed-response and performance items, guide-lines for other formats (i.e., true-false, matching, etc.), the use of item analysis to judge the quality of items, and ethical and legal considerations.",,,Article,Final,,Scopus,2-s2.0-0033196483
Gibson W.M.; Weiner J.A.,"Gibson, Wade M. (35355427300); Weiner, John A. (36146521500)",35355427300; 36146521500,Generating random parallel test forms using CTT in a computer-based environment,1998,Journal of Educational Measurement,35,4,,297,310,13,20,10.1111/j.1745-3984.1998.tb00540.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032216875&doi=10.1111%2fj.1745-3984.1998.tb00540.x&partnerID=40&md5=ffd34b787df979a9cee017648f83058e,"This paper describes a procedure for automated test forms assembly based on Classical Test Theory (CTT). The procedure uses stratified random content sampling and test form pre-equating to ensure both content and psychometric equivalence in generating virtually unlimited parallel forms. The procedure extends the usefulness of CTT in automated test form construction, yielding classical item statistics based on representative sample distributions and pre-equated test forms with known psychometric characteristics. A rationale for the procedure is presented followed by an example application and discussion of psychometric considerations related to its use.",,,Article,Final,,Scopus,2-s2.0-0032216875
Frisbie D.A.; Cantor N.K.,"Frisbie, David A. (7003704007); Cantor, Nancy K. (57191232940)",7003704007; 57191232940,The Validity of Scores From Alternative Methods of Assessing Spelling Achievement,1995,Journal of Educational Measurement,32,1,,55,78,23,7,10.1111/j.1745-3984.1995.tb00456.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937292959&doi=10.1111%2fj.1745-3984.1995.tb00456.x&partnerID=40&md5=e2dec85eefc9e21c440248f9f265e9a8,"The purpose of this study was to gather evidence to examine the validity of alternative methods for assessing the spelling achievements of students in Grades 2–7. Students in Grades 2, 3, 5, and 7 took both a dictation spelling test and an objective spelling test. Scores from the Iowa Tests of Basic Skills were available for nearly all students. Comparisons among item formats centered on difficulty, content coverage and efficiency, reliability, relationship with dictation scores, the influence of context, and relationship to overall achievement. Overall, no single objective format stood out above the others, but some demonstrated superiority to the dictation format on several dimensions. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84937292959
Nandakumar R.; Yu F.,"Nandakumar, Ratna (7005999386); Yu, Feng (37032391300)",7005999386; 37032391300,Empirical validation of DIMTEST on nonnormal ability distributions,1996,Journal of Educational Measurement,33,3,,355,368,13,14,10.1111/j.1745-3984.1996.tb00497.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030515333&doi=10.1111%2fj.1745-3984.1996.tb00497.x&partnerID=40&md5=b33125eb754a60e98ce7e1959571b169,"DIMTEST is a nonparametric statistical test procedure for assessing unidimensionality of binary item response data. The development of Stout's statistic, T, used in the DIMTEST procedure, does not require the assumption of a particular parametric form for the ability distributions or the item response functions. The purpose of the present study was to empirically investigate the performance of the statistic T with respect to different shapes of ability distributions. Several nonnormal distributions, both symmetric and nonsymmetric, were considered for this purpose. Other factors varied in the study were test length, sample size, and the level of correlation between abilities. The results of Type I error and power studies showed that the test statistic T exhibited consistently similar performance for all different shapes of ability distributions investigated in the study, which confirmed the nonparametric nature of the statistic T.",,,Article,Final,,Scopus,2-s2.0-0030515333
Revuelta J.; Ponsoda V.,"Revuelta, Javier (7004923637); Ponsoda, Vicente (6602100295)",7004923637; 6602100295,A comparison of item exposure control methods in computerized adaptive testing,1998,Journal of Educational Measurement,35,4,,311,327,16,104,10.1111/j.1745-3984.1998.tb00541.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032216877&doi=10.1111%2fj.1745-3984.1998.tb00541.x&partnerID=40&md5=eebea301a3e690ea578c6a6f4d864509,"Two new methods for item exposure control were proposed. In the Progressive method, as the test progresses, the influence of a random component on item selection is reduced and the importance of item information is increasingly more prominent. In the Restricted Maximum Information method, no item is allowed to be exposed in more than a predetermined proportion of tests. Both methods were compared with six other item-selection methods (Maximum Information, One Parameter, McBride and Martin, Randomesque, Sympson and Hetter, and Random Item Selection) with regard to test precision and item exposure variables. Results showed that the Restricted method was useful to reduce maximum exposure rates and that the Progressive method reduced the number of unused items. Both did well regarding precision. Thus, a combined Progressive-Restricted method may be useful to control item exposure without a serious decrease in test precision.",,,Article,Final,,Scopus,2-s2.0-0032216877
Impara J.C.; Plake B.S.,"Impara, James C. (6602233011); Plake, Barbara S. (6603689848)",6602233011; 6603689848,Teachers' ability to estimate item difficulty: A test of the assumptions in the Angoff standard setting method,1998,Journal of Educational Measurement,35,1,,69,81,12,121,10.1111/j.1745-3984.1998.tb00528.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220624&doi=10.1111%2fj.1745-3984.1998.tb00528.x&partnerID=40&md5=5553040c13642ac5b775f11cb0de1902,"The Angoff (1971) standard setting method requires expert panelists to (a) conceptualize candidates who possess the qualifications of interest (e.g., the minimally qualified) and (b) estimate actual item performance for these candidates. Past and current research (Bejar, 1983; Shepard, 1994) suggests that estimating item performance is difficult for panelists. If panelists cannot perform this task, the validity of the standard based on these estimates is in question. This study tested the ability of 26 classroom teachers to estimate item performance for two groups of their students on a locally developed district-wide science test. Teachers were more accurate in estimating the performance of the total group than of the ""borderline group,"" but in neither case was their accuracy level high. Implications of this finding for the validity of item performance estimates by panelists using the Angoff standard setting method are discussed.",,,Article,Final,,Scopus,2-s2.0-0032220624
Lu C.‐h.; Suen H.K.,"Lu, Chin‐hsieh (57189411041); Suen, Hoi K. (7004818780)",57189411041; 7004818780,Assessment Approaches and Cognitive Styles,1995,Journal of Educational Measurement,32,1,,1,17,16,18,10.1111/j.1745-3984.1995.tb00453.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988110403&doi=10.1111%2fj.1745-3984.1995.tb00453.x&partnerID=40&md5=7caf27f925369a8526b24c5713130e8b,"The outcomes on multiple‐choice tests and performance‐based assessments for field‐independent and field‐dependent students were examined. A substantial interaction between cognitive style and assessment approach was found. Results suggested that performance‐based assessment tended to favor field‐independent subjects. Dependent on the purpose and intended use of assessment, this finding may raise concerns for validity based on either fairness or curriculum relevance. Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988110403
Allalouf A.; Ben-Shakhar G.,"Allalouf, Avi (6603644062); Ben-Shakhar, Gershon (7004002344)",6603644062; 7004002344,The effect of coaching on the predictive validity of scholastic aptitude tests,1998,Journal of Educational Measurement,35,1,,31,47,16,37,10.1111/j.1745-3984.1998.tb00526.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220620&doi=10.1111%2fj.1745-3984.1998.tb00526.x&partnerID=40&md5=e8f263f65eda612d676c92f1008a0010,"The present study was designed to examine whether coaching affects the predictive validity and fairness of scholastic aptitude tests. Two randomly allocated groups, coached and uncoached, were compared, and the results revealed that although coaching enhanced scores on the Israeli Psychometric Entrance Test by about 25% of a standard deviation, it did not affect predictive validity and did not create a prediction bias. These results refute claims that coaching reduces predictive validity and creates a bias against the uncoached examinees in predicting the criterion. The results are consistent with the idea that score improvement due to coaching does not result strictly from learning specific skills that are irrelevant to the criterion.",,,Article,Final,,Scopus,2-s2.0-0032220620
Stocking M.L.; Ward W.C.; Potenza M.T.,"Stocking, Martha L. (7006846128); Ward, William C. (7202733418); Potenza, Maria T. (24303642700)",7006846128; 7202733418; 24303642700,Simulating the use of disclosed items in computerized adaptive testing,1998,Journal of Educational Measurement,35,1,,48,68,20,5,10.1111/j.1745-3984.1998.tb00527.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032220621&doi=10.1111%2fj.1745-3984.1998.tb00527.x&partnerID=40&md5=3fd7201cad5740ee384304e741f135c2,"Regular use of questions previously made available to the public (i.e., disclosed items) may provide one way to meet the requirement for large numbers of questions in a continuous testing environment, that is, an environment in which testing is offered at test taker convenience throughout the year rather than on a few prespecified test dates. First it must be shown that such use has effects on test scores small enough to be acceptable. In this study simulations are used to explore the use of disclosed items under a worst-case scenario which assumes that disclosed items are always answered correctly. Some item pool and test designs were identified in which the use of disclosed items produces effects on test scores that may be viewed as negligible.",,,Article,Final,,Scopus,2-s2.0-0032220621
Wainer H.,"Wainer, Howard (7006218234)",7006218234,Using trilinear plots for NAEP state data,1996,Journal of Educational Measurement,33,1,,41,55,14,7,10.1111/j.1745-3984.1996.tb00478.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030306445&doi=10.1111%2fj.1745-3984.1996.tb00478.x&partnerID=40&md5=e0e47e7c7f205f327adf37c3dfee8d58,"Understanding the distribution of achievement levels of students' performance in the National Assessment of Educational Progress (NAEP) is aided through the use of the trilinear chart. In this article, this chart is described and its use illustrated with data from the 1992 state NAEP mathematics assessment. It is shown that one can see readily the trends in performance for different demographic groups for all of the 44 participating jurisdictions simultaneously. It is suggested that this graphical form may be useful in other contexts, as well.",,,Article,Final,,Scopus,2-s2.0-0030306445
Zwick R.; Thayer D.T.; Wingersky M.,"Zwick, Rebecca (7004200859); Thayer, Dorothy T. (7006657345); Wingersky, Marilyn (6507024898)",7004200859; 7006657345; 6507024898,Effect of Rasch Calibration on Ability and DIF Estimation in Computer‐Adaptive Tests,1995,Journal of Educational Measurement,32,4,,341,363,22,23,10.1111/j.1745-3984.1995.tb00471.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073756&doi=10.1111%2fj.1745-3984.1995.tb00471.x&partnerID=40&md5=465fd5bef5f0697d640f2925f2238ffd,"In a previous simulation study of methods for assessing differential item functioning (DIF) in computer‐adaptive tests (Zwick, Thayer, & Wingersky, 1993, 1994), modified versions of the Mantel‐Haenszel and standardization methods were found to perform well. In that study, data were generated using the 3‐parameter logistic (3PL) model and this same model was assumed in obtaining item parameter estimates. In the current study, the 3PL data were used but the Rasch model was assumed in obtaining the item parameter estimates, which determined the information table used for item selection. Although the obtained DIF statistics were highly correlated with the generating DIF values, they tended to be smaller in magnitude than in the 3PL analysis, resulting in a lower probability of DIF detection. This reduced sensitivity appeared to be related to a degradation in the accuracy of matching. Expected true scores from the Rasch‐based computer‐adaptive test tended to be biased downward, particularly for lower‐ability examinees Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073756
Davey T.; Godwin J.; Mittelholtz D.,"Davey, Tim (36785089100); Godwin, Janet (7102623892); Mittelholtz, David (6505976269)",36785089100; 7102623892; 6505976269,Developing and scoring an innovative computerized writing assessment,1997,Journal of Educational Measurement,34,1,,21,41,20,19,10.1111/j.1745-3984.1997.tb00505.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287478&doi=10.1111%2fj.1745-3984.1997.tb00505.x&partnerID=40&md5=6bd55d667edd6b353c35c4b5957da6f9,"We describe the development and administration of a recently introduced computer-based test of writing skills. This test asks the examinee to edit a writing passage presented on a computer screen. To do this, the examinee moves a cursor to a suspect section of the passage and chooses from a list of alternative ways of rewriting that section. Any or all parts of the passage can be changed, as often as the examinee likes. An able examinee identifies and fixes errors in grammar, organization, and style, whereas a less able examinee may leave errors untouched, replace an error with another error, or even introduce errors where none existed previously. All these response alternatives contrive to present both obvious and subtle scoring difficulties. These difficulties were attacked through the combined use of option weighting and the sequential probability ratio test, the result of which is to classify examinees into several discrete ability groups. Item calibration was enabled by augmenting sparse pretest samples through data meiosis, in which response vectors were randomly recombined to produce offspring that retained much of the character of their parents. These procedures are described, and operational examples are offered.",,,Article,Final,,Scopus,2-s2.0-0031287478
Clauser B.E.; Subhiyah R.G.; Nungester R.J.; Ripkey D.R.; Clyman S.G.; McKinley D.,"Clauser, Brian E. (7003595460); Subhiyah, Raja G. (6602766392); Nungester, Ronald J. (6602127510); Ripkey, Douglas R. (6602077535); Clyman, Stephen G. (6603827946); McKinley, Danette (7005955087)",7003595460; 6602766392; 6602127510; 6602077535; 6603827946; 7005955087,Scoring a Performance‐Based Assessment by Modeling the Judgments of Experts,1995,Journal of Educational Measurement,32,4,,397,415,18,50,10.1111/j.1745-3984.1995.tb00474.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099041&doi=10.1111%2fj.1745-3984.1995.tb00474.x&partnerID=40&md5=7b2bebae397eca055010ed43573852d3,"Performance assessments typically require expert judges to individually rate each performance. This results in a limitation in the use of such assessments because the rating process may be extremely time consuming. This article describes a scoring algorithm that is based on expert judgments but requires the rating of only a sample of performances. A regression‐based policy capturing procedure was implemented to model the judgment policies of experts. The data set was a seven‐case performance assessment of physician patient management skills. The assessment used a computer‐based simulation of the patient care environment. The results showed a substantial improvement in correspondence between scores produced using the algorithm and actual ratings, when compared to raw scores. Scores based on the algorithm were also shown to be superior to raw scores and equal to expert ratings for making pass/fail decisions which agreed with those made by an independent committee of experts Copyright © 1995, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988099041
Potenza M.T.; Stocking M.L.,"Potenza, Maria T. (24303642700); Stocking, Martha L. (7006846128)",24303642700; 7006846128,Flawed items in computerized adaptive testing,1997,Journal of Educational Measurement,34,1,,79,96,17,4,10.1111/j.1745-3984.1997.tb00508.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287479&doi=10.1111%2fj.1745-3984.1997.tb00508.x&partnerID=40&md5=f03fbd633ed19eea31e5cd3eb03b6f0f,"A multiple-choice test item is identified as flawed if it has no single best answer. In spite of extensive quality control procedures, the administration of flawed items to test takers is inevitable. A limited set of common strategies for dealing with flawed items in conventional testing, grounded in the principle of fairness to examinees, is reexamined in the context of adaptive testing. An additional strategy, available for adaptive testing, of retesting from a pool cleansed of flawed items, is compared to the existing strategies. Retesting was found to be no practical improvement over current strategies.",,,Article,Final,,Scopus,2-s2.0-0031287479
French A.W.; Miller T.R.,"French, Ann W. (7202095071); Miller, Timothy R. (7403948130)",7202095071; 7403948130,Logistic regression and its use in detecting differential item functioning in polytomous items,1996,Journal of Educational Measurement,33,3,,315,332,17,90,10.1111/j.1745-3984.1996.tb00495.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030486106&doi=10.1111%2fj.1745-3984.1996.tb00495.x&partnerID=40&md5=0d2ba6e89afdf07fdf5c8cb6d153e362,"A computer simulation study was conducted to determine the feasibility of using logistic regression procedures to detect differential item functioning (DIF) in polytomous items. One item in a simulated test of 25 items contained DIF; parameters for that item were varied to create three conditions of nonuniform DIF and one of uniform DIF. Item scores were generated using a generalized partial credit model, and the data were recoded into multiple dichotomies in order to use logistic regression procedures. Results indicate that logistic regression is powerful in detecting most forms of DIF; however, it required large amounts of data manipulation, and interpretation of the results was sometimes difficult. Some logistic regression procedures may be useful in the post hoc analysis of DIF for polytomous items.",,,Article,Final,,Scopus,2-s2.0-0030486106
Tate R.L.; King F.,"Tate, Richard L. (7102471183); King, FJ (57191231964)",7102471183; 57191231964,Factors Which Influence Precision of School‐Level IRT Ability Estimates,1994,Journal of Educational Measurement,31,1,,1,15,14,9,10.1111/j.1745-3984.1994.tb00431.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072850&doi=10.1111%2fj.1745-3984.1994.tb00431.x&partnerID=40&md5=94ae727f15aadf9fea6bc0a54df8cad9,"The precision of the group‐level IRT model applied to school ability estimation is described, assuming use of Bayesian estimation with precision represented by the standard deviation of the posterior distribution. Similarities and differences between the school‐level model and the familiar individual‐level IRT model are considered. School size and between‐school variability, two factors not relevant at the student level, are dominant determinants of school‐level precision. Under the multiple‐matrix sampling design required for the school‐level IRT, the number of items associated with a scale does not influence the precision at the school level. Also, the effects of school ability and item quality on school‐level precision are often relatively weak. It was found that the use of Bayesian estimation could result in a systematic distortion of the true ranking of schools based on ability because of an estimation bias which is a function of school size. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988072850
Scheuneman J.D.; Gerritz K.,"Scheuneman, Janice Dowd (6602613561); Gerritz, Kalle (57191232395)",6602613561; 57191232395,Using Differential Item Functioning Procedures to Explore Sources of Item Difficulty and Group Performance Characteristics,1990,Journal of Educational Measurement,27,2,,109,131,22,56,10.1111/j.1745-3984.1990.tb00737.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106714&doi=10.1111%2fj.1745-3984.1990.tb00737.x&partnerID=40&md5=367d07d54ce227a43787c4b320057add,"Statistics used to detect differential item functioning can also reflect differential strengths and weaknesses in the performance characteristics of population subgroups. In turn, item features associated with the differential performance patterns are likely to reflect some facet of the item task and hence its difficulty, that might previously have been overlooked. In this study, several item features were identified and coded for a large number of reading comprehension items from the two admissions testing programs. Item features included subject matter content, various properties of item structure, cognitive demand indicators, and semantic content (propositional analysis). Differential item functioning was evaluated for males and females and for White and Black examinees. Results showed a number of significant relationships between item features and indicators of differential item functioning—many of which were consistent across testing programs. Implications of the results for related areas of research are discussed. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106714
Wise S.L.; Plake B.S.; Johnson P.L.; Roos L.L.,"Wise, Steven L. (7202622563); Plake, Barbara S. (6603689848); Johnson, Phillip L. (57191234251); Roos, Linda L. (7006475431)",7202622563; 6603689848; 57191234251; 7006475431,A Comparison of Self‐Adapted and Computerized Adaptive Tests,1992,Journal of Educational Measurement,29,4,,329,339,10,37,10.1111/j.1745-3984.1992.tb00381.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988104019&doi=10.1111%2fj.1745-3984.1992.tb00381.x&partnerID=40&md5=4eafb9bcb987ea6bd18c44df689554f0,"According to item response theory (IRT), examinee ability estimation is independent of the particular set of test items administered from a calibrated pool. Although the most popular application of this feature of IRT is computerized adaptive (CA) testing, a recently proposed alternative is self‐adapted (SA) testing, in which examinees choose the difficulty level of each of their test items. This study compared examinee performance under SA and CA tests, finding that examinees taking the SA test (a) obtained significantly higher ability scores and (b) reported significantly lower posttest state anxiety. The results of this study suggest that SA testing is a desirable format for computer‐based testing. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988104019
Ackerman T.A.,"Ackerman, Terry A. (16404476400)",16404476400,"A Didactic Explanation of Item Bias, Item Impact, and Item Validity From a Multidimensional Perspective",1992,Journal of Educational Measurement,29,1,,67,91,24,245,10.1111/j.1745-3984.1992.tb00368.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072859&doi=10.1111%2fj.1745-3984.1992.tb00368.x&partnerID=40&md5=cde9825004b3295520c849ef2793be97,"Many researchers have suggested that the main cause of item bias is the misspecification of the latent ability space, where items that measure multiple abilities are scored as though they are measuring a single ability. If two different groups of examinees have different underlying multidimensional ability distributions and the test items are capable of discriminating among levels of abilities on these multiple dimensions, then any unidimensional scoring scheme has the potential to produce item bias. It is the purpose of this article to provide the testing practitioner with insight about the difference between item bias and item impact and how they relate to item validity. These concepts will be explained from a multidimensional item response theory (MIRT) perspective. Two detection procedures, the Mantel‐Haenszel (as modified by Holland and Thayer, 1988) and Shealy and Stout's Simultaneous Item Bias (SIB; 1991) strategies, will be used to illustrate how practitioners can detect item bias. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988072859
Schwarz S.P.; McMorris R.F.; DeMers L.P.,"Schwarz, Shirley P. (8971962600); McMorris, Robert F. (6603270899); DeMers, Lawrence P. (57191235318)",8971962600; 6603270899; 57191235318,Reasons for Changing Answers: An Evaluation Using Personal Interviews,1991,Journal of Educational Measurement,28,2,,163,171,8,27,10.1111/j.1745-3984.1991.tb00351.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128987&doi=10.1111%2fj.1745-3984.1991.tb00351.x&partnerID=40&md5=6bec42ac320fab3a4a2cf54970e3c99a,"Researchers investigating answer changing have consistently found the preponderance of changes on objective items to be from wrong to right, but little is understood about the mechanisms involved in this phenomenon. In this study, personal interviews were combined with instruction in answer‐changing research to investigate further the processes involved in answer changing. Students changed answers and gained from changing, with those in the upper two thirds of the classes gaining the most. Each test‐taking strategy produced a mean gain, but particular strategies were not significantly correlated with percentage of gain or percentage of change. Most students reported changing answers for thoughtful reasons such as rereading, rethinking, or remembering more information; very few changes were due to clerical errors. For each reason, most changes were wrong‐to‐right. We conclude that reconsideration of test items is probably underestimated in answer‐changing studies. The role of memory should be considered in why people change and in how successful they judge their changing to have been. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988128987
Freedle R.; Kostin I.,"Freedle, Roy (6506851813); Kostin, Irene (57207506545)",6506851813; 57207506545,Item Difficulty of Four Verbal Item Types and an Index of Differential Item Functioning for Black and White Examinees,1990,Journal of Educational Measurement,27,4,,329,343,14,19,10.1111/j.1745-3984.1990.tb00752.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106394&doi=10.1111%2fj.1745-3984.1990.tb00752.x&partnerID=40&md5=7dd346435917557dbeb884a3f38ffd4e,"In this study, the authors explored the importance of item difficulty (equated delta) as a predictor of differential item functioning (DIF) of Black versus matched White examinees for four verbal item types (analogies, antonyms, sentence completions, reading comprehension) using 13 GRE‐disclosed forms (988 verbal items) and 11 SAT‐disclosed forms (935 verbal items). The average correlation across test forms for each item type (and often the correlation for each individual test form as well) revealed a significant relationship between item difficulty and DIF value for both GRE and SAT. The most important finding indicates that for hard items, Black examinees perform differentially better than matched ability White examinees for each of the four item types and for both the GRE and SAT tests! The results further suggest that the amount of verbal context is an important determinant of the magnitude of the relationship between item difficulty and differential performance of Black versus matched White examinees. Several hypotheses accounting for this result were explored. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106394
Nandakumar R.,"Nandakumar, Ratna (7005999386)",7005999386,Traditional Dimensionality Versus Essential Dimensionality,1991,Journal of Educational Measurement,28,2,,99,117,18,55,10.1111/j.1745-3984.1991.tb00347.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988087275&doi=10.1111%2fj.1745-3984.1991.tb00347.x&partnerID=40&md5=7c93d966fc9178be724b087bf99afdce,"This article addresses testing the hypothesis of one versus more than one dominant (essential) dimension in the possible presence of minor dimensions. The method used is Stout's statistical test of essential unidimensionality, which is based on the theory of essential unidimensionality. Differences between the traditional definition of dimensionality provided by item response theory, which counts all dimensions present, and essential dimensionality, which counts only dominant dimensions, are discussed. As Monte Carlo studies demonstrate, Stout's test of essential unidimensionality tends to indicate essential unidimensionality in the presence of one dominant dimension and one or more minor dimensions that have a relatively small influence on item scores. As the influence of the minor dimensions increases, Stout's test is more likely to reject the hypothesis of essential unidimensionality. To assist in interpreting these studies, a rough index of the deviation from essential unidimensionality is proposed. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988087275
Johnson E.G.,"Johnson, Eugene G. (57191232117)",57191232117,The Design of the National Assessment of Educational Progress,1992,Journal of Educational Measurement,29,2,,95,110,15,57,10.1111/j.1745-3984.1992.tb00369.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988080244&doi=10.1111%2fj.1745-3984.1992.tb00369.x&partnerID=40&md5=e586e6a51e111e4c9101da512f00d01e,"The key features of the design of the National Assessment of Educational Progress (NAEP) are discussed with particular emphasis on the design to be used for the 1992 assessment. An overview of the design and its philosophy are given with a description of the multicomponent solution to the twin requirements of reliably measuring trends in achievement while responding to changing educational priorities and advances in measurement technology. The student sample designs for the National Assessment and the Trial State Assessment are described. The focused‐balanced incomplete block (focused‐BIB) spiraling method of item sampling is discussed and compared with simpler matrix sampling designs. The impact of the NAEP design on the analysis of assessment data is discussed. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988080244
Plake B.S.; Kane M.T.,"Plake, Barbara S. (6603689848); Kane, Michael T. (36088969800)",6603689848; 36088969800,Comparison of Methods for Combining the Minimum Passing Levels for Individual Items into a Passing Score for a Test,1991,Journal of Educational Measurement,28,3,,249,256,7,8,10.1111/j.1745-3984.1991.tb00357.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988120988&doi=10.1111%2fj.1745-3984.1991.tb00357.x&partnerID=40&md5=a281391adda9a326fb75700852ea14ea,"The purpose of this study was to compare several methods for determining a passing score on an examination from the individual raters' estimates of minimal pass levels for the items. The methods investigated differ in the weighting that the estimates for each item receive in the aggregation process. An IRT‐based simulation method was used to model a variety of error components of minimum pass levels. The results indicate little difference in estimated passing scores across the three methods. Less error was present when the ability level of the minimally competent candidates matched the expected difficulty level of the test. No meaningful improvement in passing score estimation was achieved for a 50‐item test as opposed to a 25‐item test; however, the RMSE values for estimates with 10 raters were smaller than those for 5 raters. The results suggest that the simplest method for aggregating minimum pass levels across the items in a test–adding them up–is the preferred method. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988120988
Sireci S.G.; Thissen D.; Wainer H.,"Sireci, Stephen G. (6701491909); Thissen, David (7003712685); Wainer, Howard (7006218234)",6701491909; 7003712685; 7006218234,On the Reliability of Testlet‐Based Tests,1991,Journal of Educational Measurement,28,3,,237,247,10,199,10.1111/j.1745-3984.1991.tb00356.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128749&doi=10.1111%2fj.1745-3984.1991.tb00356.x&partnerID=40&md5=38078ab5b4f4f98a1af2af48ed56ad47,"If a test is constructed of testlets, one must take into account the within‐testlet structure in the calculation of test statistics. Failing to do so may yield serious biases in the estimation of such statistics as reliability. We demonstrate how to calculate the reliability of a testlet‐based test. We show that traditional reliabilities calculated on two reading comprehension tests constructed of four testlets are substantial overestimates. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988128749
Harris D.J.,"Harris, Deborah J. (7403921256)",7403921256,A Comparison of Angoff's Design I and Design II for Vertical Equating Using Traditional and IRT Methodology,1991,Journal of Educational Measurement,28,3,,221,235,14,12,10.1111/j.1745-3984.1991.tb00355.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073260&doi=10.1111%2fj.1745-3984.1991.tb00355.x&partnerID=40&md5=288982bc901db8232dcf0fe7a80395b8,"Practical considerations in conducting an equating study often require a trade‐off between testing time and sample size. A counterbalanced design (Angoff's Design II) is often selected because, as each examinee is administered both test forms and therefore the errors are correlated, sample sizes can be dramatically reduced over those required by a spiraling design (Angoff's Design I), where each examinee is administered only one test form. However, the counterbalanced design may be subject to fatigue, practice, or context effects. This article investigated these two data collection designs (for a given sample size) with equipercentile and IRT equating methodology in the vertical equating of two mathematics achievement tests. Both designs and both methodologies were judged to adequately meet an equivalent expected score criterion; Design II was found to exhibit more stability over different samples. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073260
Swaminathan H.; Rogers H.J.,"Swaminathan, Hariharan (6602382602); Rogers, H. Jane (7202667648)",6602382602; 7202667648,Detecting Differential Item Functioning Using Logistic Regression Procedures,1990,Journal of Educational Measurement,27,4,,361,370,9,733,10.1111/j.1745-3984.1990.tb00754.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988120749&doi=10.1111%2fj.1745-3984.1990.tb00754.x&partnerID=40&md5=0aefe0f223cc98d6424242aaf0ceadd8,"A logistic regression model for characterizing differential item functioning (DIF) between two groups is presented. A distinction is drawn between uniform and nonuniform DIF in terms of the parameters of the model. A statistic for testing the hypothesis of no DIF is developed. Through simulation studies, it is shown that the logistic regression procedure is more powerful than the Mantel‐Haenszel procedure for detecting nonuniform DIF and as powerful in detecting uniform DIF. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988120749
Donoghue J.R.,"Donoghue, John R. (7102202534)",7102202534,An Empirical Examination of the IRT Information of Polytomously Scored Reading Items Under the Generalized Partial Credit Model,1994,Journal of Educational Measurement,31,4,,295,311,16,41,10.1111/j.1745-3984.1994.tb00448.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073777&doi=10.1111%2fj.1745-3984.1994.tb00448.x&partnerID=40&md5=c14486620f79257bd4502ac19858ea6b,"Using Muraki's (1992) generalized partial credit IRT model, polytomous items (responses to which can be scored as ordered categories) from the 1991 field test of the NAEP Reading Assessment were calibrated simultaneously with multiple‐choice and short open‐ended items. Expected information of each type of item was computed. On average, four‐category polytomous items yielded 2.1 to 3.1 times as much IRT information as dichotomous items. These results provide limited support for the ad hoc rule of weighting k‐category polytomous items the same as k ‐ 1 dichotomous items for computing total scores. Polytomous items provided the most information about examinees of moderately high proficiency; the information function peaked at 1.0 to 1.5, and the population distribution mean was 0. When scored dichotomously, information in polytomous items sharply decreased, but they still provided more expected information than did the other response formats. For reference, a derivation of the information function for the generalized partial credit model is included. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073777
Nandakumar R.,"Nandakumar, Ratna (7005999386)",7005999386,Assessing Dimensionality of a Set of Item Responses‐Comparison of Different Approaches,1994,Journal of Educational Measurement,31,1,,17,35,18,42,10.1111/j.1745-3984.1994.tb00432.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988123041&doi=10.1111%2fj.1745-3984.1994.tb00432.x&partnerID=40&md5=baa7182bb73ef2093ec0c00834754eef,"This study compares the performance of three methodologies for assessing unidi‐mensionality: DIMTEST, Holland and Rosenbaum's approach, and nonlinear factor analysis. Each method is examined and compared with other methods on simulated and real data sets. Seven data sets, all with 2,000 examinees, were generated: three unidimensional and four two‐dimensional data sets. Two levels of correlation between abilities were considered:ρ=3 andρ=. 7. Eight different real data sets were used: Four of them were expected to be unidimensional, and the other four were expected to be two‐dimensional. Findings suggest that all three methods correctly confirmed unidimensionality but differed in their ability to detect lack of unidimensionality. DIMTEST showed excellent power in detecting lack of unidimensionality; Holland and Rosenbaum's and nonlinear factor analysis approaches showed good power, provided the correlation between abilities was low. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988123041
Lane S.,"Lane, Suzanne (7202106909)",7202106909,Use of Restricted Item Response Models for Examining Item Difficulty Ordering and Slope Uniformity,1991,Journal of Educational Measurement,28,4,,295,309,14,7,10.1111/j.1745-3984.1991.tb00360.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128976&doi=10.1111%2fj.1745-3984.1991.tb00360.x&partnerID=40&md5=a9325cb7a42ac5b4fcd20359c954b923,"This article demonstrates the utility of restricted item response models for examining item difficulty ordering and slope uniformity for an item set that reflects varying cognitive processes. Twelve sets of paired algebra word problems were developed to systematically reflect various types of cognitive processes required for successful performance. This resulted in a total of 24 items. They reflected distance‐rate–time (DRT), interest, and area problems. Hypotheses concerning difficulty ordering and slope uniformity for the items were tested by constraining item difficulty and discrimination parameters in hierarchical item response models. The first set of model comparisons tested the equality of the discrimination and difficulty parameters for each set of paired items. The second set of model comparisons examined slope uniformity within the complex DRT problems. The third set of model comparisons examined whether the familiarity of the story context affected item difficulty for two types of complex DRT problems. The last set of model comparisons tested the hypothesized difficulty ordering of the items. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988128976
Sehmitt A.P.; Dorans N.J.,"Sehmitt, Alieia P. (57191231584); Dorans, Nell J. (6602289148)",57191231584; 6602289148,Differential Item Functioning for Minority Examinees on the SAT,1990,Journal of Educational Measurement,27,1,,67,81,14,58,10.1111/j.1745-3984.1990.tb00735.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119917&doi=10.1111%2fj.1745-3984.1990.tb00735.x&partnerID=40&md5=f4851b721663df9f7beff64d15c26c2d,"The standardization approach to assessing differential item functioning (DIF), including standardized distractor analysis, is described. The results of studies conducted on Asian Americans, Hispanics (Mexican Americans and Puerto Ricans), and Blacks on the Scholastic Aptitude Test (SAT) are described and then synthesized across studies. Where the groups were limited to include only examinees who spoke English as their best language, very few items across forms and ethnic groups exhibited large DIF. Major findings include evidence of differential speededness (where minority examinees did not complete SAT‐Verbal sections at the same rate as White students with comparable SAT‐Verbal scores) for Blacks and Hispanics and, when the item content is of special interest, advantages for the relevant ethnic group. In addition, homographs tend to disadvantage all three ethnic groups, but the effect of vertical relationships in analogy items are not as consistent. Although these findings are important in understanding DIF, they do not seem to account for all differences. Other variables related to DIF still need to be identified. Furthermore, these findings are seen as tentative until corroborated by studies using controlled data collection designs. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988119917
Kelderman H.; Macready G.B.,"Kelderman, Henk (6601958064); Macready, George B. (6507246634)",6601958064; 6507246634,The Use of Loglinear Models for Assessing Differential Item Functioning Across Manifest and Latent Examinee Groups,1990,Journal of Educational Measurement,27,4,,307,327,20,64,10.1111/j.1745-3984.1990.tb00751.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988087846&doi=10.1111%2fj.1745-3984.1990.tb00751.x&partnerID=40&md5=63a2443e7fafc6fc528d75e47521cf02,"Loglinear latent class models are used to detect differential item functioning (DIF). These models are formulated in such a manner that the attribute to be assessed may be continuous, as in a Rasch model, or categorical, as in Latent Class Mastery models. Further, an item may exhibit DIF with respect to a manifest grouping variable, a latent grouping variable, or both. Likelihood‐ratio tests for assessing the presence of various types of DIF are described, and these methods are illustrated through the analysis of a “real world” data set. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84988087846
Becker D.F.; Forsyth R.A.,"Becker, Douglas F. (58388690500); Forsyth, Robert A. (57029929200)",58388690500; 57029929200,An Empirical Investigation of Thurstone and IRT Methods of Scaling Achievement Tests,1992,Journal of Educational Measurement,29,4,,341,354,13,10,10.1111/j.1745-3984.1992.tb00382.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058555&doi=10.1111%2fj.1745-3984.1992.tb00382.x&partnerID=40&md5=89f0e881cb5025e5152f44292625c889,"The purpose of this study was to investigate the nature and characteristics of the measurement scales developed using Thurstone and item response theory (IRT) methods of scaling achievement tests for the same set of data. Expanded standard score scales were created using Thurstone, one‐parameter IRT, and three‐parameter IRT models, and descriptive information on achievement growth and variability was obtained for examinees in Grades 9 through 12 in the subject areas of vocabulary, reading, and mathematics. The results indicated increasing variability in all three test areas for all three scaling methods as grade level increased. In addition, greater average growth across grades was observed at the 90th percentile as compared to the 10th percentile, even with the IRT‐based scales. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988058555
Beaton A.E.; Johnson E.G.,"Beaton, Albert E. (7007130983); Johnson, Eugene G. (57191232117)",7007130983; 57191232117,Overview of the Scaling Methodology Used in the National Assessment,1992,Journal of Educational Measurement,29,2,,163,175,12,19,10.1111/j.1745-3984.1992.tb00372.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053782&doi=10.1111%2fj.1745-3984.1992.tb00372.x&partnerID=40&md5=1fca328c71340fd0c374556b2c780bc7,"The National Assessment of Educational Progress (NAEP) uses item response theory (IRT)–based scaling methods to summarize the information in complex data sets. Scale scores are presented as tools for illuminating patterns in the data and for exploiting regularities across patterns of responses to tasks requiring similar skills. In this way, the dominant features of the data are captured. Discussed are the necessity of global scores or more detailed subscores, the creation of developmental scales spanning different age levels, and the use of scale anchoring as a way of interpreting the scales. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988053782
Samsa G.P.,"Samsa, Gregory P. (57000637000)",57000637000,Resolution of a Regression Paradox in Pretest‐Posttest Designs,1992,Journal of Educational Measurement,29,4,,321,328,7,3,10.1111/j.1745-3984.1992.tb00380.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073808&doi=10.1111%2fj.1745-3984.1992.tb00380.x&partnerID=40&md5=236db111c8882908edaf570b87c159b1,"Regression to the mean (RTM) of individuals is the tendency for extreme individuals to become less extreme on remeasurement; RTM of group means describes this same tendency among group means. Under the classical test model, in pretest‐posttest designs where subjects are selected on the basis of extreme values at the pretest, RTM of group means will always occur for the attribute used to select extreme subjects. For other attributes, however, RTM of group means requires a positive correlation between that attribute's measurement error and the measurement error of the attribute used in the selection. Thus, while all attributes will evidence RTM of individuals, extreme groups do not always regress to the mean. RTM depends most fundamentally on the magnitude of the pretest measurement error. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073808
Hambleton R.K.; Jones R.W.; Rogers H.J.,"Hambleton, Ronald K. (7006242264); Jones, Russell W. (57064502000); Rogers, H. Jane (7202667648)",7006242264; 57064502000; 7202667648,Influence of Item Parameter Estimation Errors in Test Development,1993,Journal of Educational Measurement,30,2,,143,155,12,33,10.1111/j.1745-3984.1993.tb01071.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988084575&doi=10.1111%2fj.1745-3984.1993.tb01071.x&partnerID=40&md5=015b4d71dc097e1f34e9e6d59cb99f4a,"Item response models are finding increasing use in achievement and aptitude test development. Item response theory (IRT) test development involves the selection of test items based on a consideration of their item information functions. But a problem arises because item information functions are determined by their item parameter estimates, which contain error. When the “best” items are selected on the basis of their statistical characteristics, there is a tendency to capitalize on chance due to errors in the item parameter estimates. The resulting test, therefore, falls short of the test that was desired or expected. The purposes of this article are (a) to highlight the problem of item parameter estimation errors in the test development process, (b) to demonstrate the seriousness of the problem with several simulated data sets, and (c) to offer a conservative solution for addressing the problem in IRT‐based test development. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988084575
Bennett R.E.; Rock D.A.; Wang M.,"Bennett, Randy Elliot (7402440584); Rock, Donald A. (7102591023); Wang, Minhwei (57191233018)",7402440584; 7102591023; 57191233018,Equivalence of Free‐Response and Multiple‐Choice Items,1991,Journal of Educational Measurement,28,1,,77,92,15,99,10.1111/j.1745-3984.1991.tb00345.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122998&doi=10.1111%2fj.1745-3984.1991.tb00345.x&partnerID=40&md5=086d20e29cc0620762af754b4b72dfbe,"This study examined the relationship of multiple‐choice and free‐response items contained on the College Board's Advanced Placement Computer Science (APCS) examination. Confirmatory factor analysis was used to test the fit of a two‐factor model where each item format marked its own factor. Results showed a single‐factor solution to provide the most parsimonious fit in each of two random‐half samples. This finding might be accounted for by several mechanisms, including overlap in the specific processes assessed by the multiple‐choice and free‐response items and the limited opportunity for skill differentiation afforded by the year‐long APCS course. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122998
Norris S.P.,"Norris, Stephen P. (7103213615)",7103213615,Effect of Eliciting Verbal Reports of Thinking on Critical Thinking Test Performance,1990,Journal of Educational Measurement,27,1,,41,58,17,42,10.1111/j.1745-3984.1990.tb00733.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988137841&doi=10.1111%2fj.1745-3984.1990.tb00733.x&partnerID=40&md5=a37243a5b9306451f96b8c09f10ec1d3,"Verbal reports of examinees' thinking on multiple‐choice critical thinking test items can provide useful validation data only if the verbal reporting does not change the course of examinees' thinking and performance. Using a completely randomized factorial design, 343 senior high school students were divided into five groups. In four of the groups, different procedures were used to elicit students' thinking as they worked through Part A of a critical thinking test of observation appraisal (Norris & King, 1983). In the control group, students took the same test in paper‐and‐pencil format. There were no significant differences in test performance among the five groups nor in the quality of thinking among the four groups from whom verbal reports of thinking were elicited. These results are evidence that verbal reports of thinking can meet one of the necessary conditions of useful validation data—namely, that collecting the data does not alter examinees' thinking and performance. Some analyses found significant interviewer main effects and sex‐by‐interviewer and elicitation‐level‐by‐interviewer‐by‐sex‐by‐grade interaction effects. Analysis of these interactions suggested that the role of the interviewer might limit the generality of the technique. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988137841
Kolen M.J.; Hanson B.A.; Brennan R.L.,"Kolen, Michael J. (6603925839); Hanson, Bradley A. (7102036381); Brennan, Robert L. (34975092300)",6603925839; 7102036381; 34975092300,Conditional Standard Errors of Measurement for Scale Scores,1992,Journal of Educational Measurement,29,4,,285,307,22,59,10.1111/j.1745-3984.1992.tb00378.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099663&doi=10.1111%2fj.1745-3984.1992.tb00378.x&partnerID=40&md5=98f7c098b573d511f0cc16697ff0c920,"Standard errors of measurement of scale scores by score level (conditional standard errors of measurement) can be valuable to users of test results. In addition, the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 1985) recommends that conditional standard errors be reported by test developers. Although a variety of procedures are available for estimating conditional standard errors of measurement for raw scores, few procedures exist for estimating conditional standard errors of measurement for scale scores from a single test administration. In this article, a procedure is described for estimating the reliability and conditional standard errors of measurement of scale scores. This method is illustrated using a strong true score model. Practical applications of this methodology are given. These applications include a procedure for constructing score scales that equalize standard errors of measurement along the score scale. Also included are examples of the effects of various nonlinear raw‐to‐scale score transformations on scale score reliability and conditional standard errors of measurement. These illustrations examine the effects on scale score reliability and conditional standard errors of measurement of (a) the different types of raw‐to‐scale score transformations (e.g., normalizing scores), (b) the number of scale score points used, and (c) the transformation used to equate alternate forms of a test. All the illustrations use data from the ACT Assessment testing program. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988099663
Reise S.P.; Yu J.,"Reise, Steve P. (7004178887); Yu, Jiayuan (57191235568)",7004178887; 57191235568,Parameter Recovery in the Graded Response Model Using MULTILOG,1990,Journal of Educational Measurement,27,2,,133,144,11,184,10.1111/j.1745-3984.1990.tb00738.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119475&doi=10.1111%2fj.1745-3984.1990.tb00738.x&partnerID=40&md5=a80f17f7549469cbd318f4cfd718d383,"The graded response model can be used to describe test‐taking behavior when item responses are classified into ordered categories. In this study, parameter recovery in the graded response model was investigated using the MULTILOG computer program under default conditions. Based on items having five response categories, 36 simulated data sets were generated that varied on true θ distribution, true item discrimination distribution, and calibration sample size. The findings suggest, first, the correlations between the true and estimated parameters were consistently greater than 0.85 with sample sizes of at least 500. Second, the root mean square error differences between true and estimated parameters were comparable with results from binary data parameter recovery studies. Of special note was the finding that the calibration sample size had little influence on the recovery of the true ability parameter but did influence item‐parameter recovery. Therefore, it appeared that item‐parameter estimation error, due to small calibration samples, did not result in poor person‐parameter estimation. It was concluded that at least 500 examinees are needed to achieve an adequate calibration under the graded model. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988119475
Yen W.M.,"Yen, Wendy M. (7102684621)",7102684621,Scaling Performance Assessments: Strategies for Managing Local Item Dependence,1993,Journal of Educational Measurement,30,3,,187,213,26,484,10.1111/j.1745-3984.1993.tb00423.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988115553&doi=10.1111%2fj.1745-3984.1993.tb00423.x&partnerID=40&md5=3add2ce4952675c53fba5e55f6a0085b,"Performance assessments appear on a priori grounds to be likely to produce far more local item dependence (LID) than that produced in the use of traditional multiple‐choice tests. This article (a) defines local item independence, (b) presents a compendium of causes of LID, (c) discusses some of LID's practical measurement implications, (d) details some empirical results for both performance assessments and multiple‐choice tests, and (e) suggests some strategies for managing LID in order to avoid negative measurement consequences. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988115553
Busch J.C.; Jaeger R.M.,"Busch, John Christian (7102117876); Jaeger, Richard M. (7201646840)",7102117876; 7201646840,"Influence of Type of Judge, Normative Information, and Discussion on Standards Recommended for the National Teacher Examinations",1990,Journal of Educational Measurement,27,2,,145,163,18,57,10.1111/j.1745-3984.1990.tb00739.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988123074&doi=10.1111%2fj.1745-3984.1990.tb00739.x&partnerID=40&md5=3189e2bfffef130aa965fa0e4dbbee5c,"There are few empirical investigations of the consequences of using widely recommended data collection procedures in conjunction with a specific standardsetting method such as the Angoff (1971) procedure. Such recommendations include the use of several types of judges, the provision of normative information on examinees' test performance, and the opportunity to discuss and reconsider initial recommendations in an iterative standard‐setting procedure. This study of 236 expert judges investigated the effects of using these recommended procedures on (a) average recommended test standards, (b) the variability of recommended test standards, and (c) the reliability of recommended standards for seven subtests of the National Teacher Examinations Communication Skills and General Knowledge Tests. Small, but sometimes statistically significant, changes in mean recommended test standards were observed when judges were allowed to reconsider their initial recommendations following review of normative information and discussion. Means for public school judges changed more than did those for college or university judges. In addition, there was a significant reduction in the within‐group variability of standards recommended for several subtests. Methods for estimating the reliability of recommended test standards proposed by Kane and Wilson (1984) were applied, and their hypothesis of positive covariation between empirical item difficulties and mean recommended standards was confirmed. The data collection procedures examined in this study resulted in substantial increases in the reliability of recommended test standards. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988123074
Baxter G.P.; Shavelson R.J.; Goldman S.R.; Pine J.,"Baxter, Gail P. (7202105744); Shavelson, Richard J. (35613093400); Goldman, Susan R. (24378233700); Pine, Jerry (7006572849)",7202105744; 35613093400; 24378233700; 7006572849,Evaluation of Procedure‐Based Scoring for Hands‐On Science Assessment,1992,Journal of Educational Measurement,29,1,,1,17,16,76,10.1111/j.1745-3984.1992.tb00364.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988091750&doi=10.1111%2fj.1745-3984.1992.tb00364.x&partnerID=40&md5=44624826e8f51661db58141a32327b09,"This article evaluates a procedure‐based scoring system for a performance assessment (an observed paper towels investigation) and a notebook surrogate completed by fifth‐grade students varying in hands‐on science experience. Results suggested interrater reliability of scores for observed performance and notebooks was adequate (>.80) with the reliability of the former higher. In contrast, interrater agreement on procedures was higher for observed hands‐on performance (.92) than for notebooks (.66). Moreover, for the notebooks, the reliability of scores and agreement on procedures varied by student experience, but this was not so for observed performance. Both the observed‐performance and notebook measures correlated less with traditional ability than did a multiple‐choice science achievement test. The correlation between the two performance assessments and the multiple‐choice test was only moderate (mean = .46), suggesting that different aspects of science achievement have been measured. Finally, the correlation between the observed‐performance scores and the notebook scores was .83, suggesting that notebooks may provide a reasonable, albeit less reliable, surrogate for the observed hands‐on performance of students. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988091750
de Gruijter D.N.M.,"de Gruijter, Dato N. M. (6507363394)",6507363394,A Note on the Bias of UCON Item Parameter Estimation in the Rasch Model,1990,Journal of Educational Measurement,27,3,,285,288,3,3,10.1111/j.1745-3984.1990.tb00749.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074893&doi=10.1111%2fj.1745-3984.1990.tb00749.x&partnerID=40&md5=c1ef7a45380b1d2ce27688c1a20f98e9,"Divgi (1986) demonstrated that the bias of UCON item parameter estimates is not removed by the factor (n − 1)/n. Andrich (1989) argued in this journal that the demonstration was faulty. In this note a complete proof of Divgfs conclusion is presented. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988074893
Breland H.M.; Danos D.O.; Kahn H.D.; Kubota M.Y.; Bonner M.W.,"Breland, Hunter M. (6506538277); Danos, Despina O. (57191232556); Kahn, Helen D. (55435265700); Kubota, Melvin Y. (57191231519); Bonner, Marilyn W. (57191233251)",6506538277; 57191232556; 55435265700; 57191231519; 57191233251,Performance Versus Objective Testing and Gender: An Exploratory Study of an Advanced Placement History Examination,1994,Journal of Educational Measurement,31,4,,275,293,18,39,10.1111/j.1745-3984.1994.tb00447.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122266&doi=10.1111%2fj.1745-3984.1994.tb00447.x&partnerID=40&md5=a184e969fd489bc867e7bb0530615ed3,"To explore a phenomenon of gender differences in Advanced Placement examinations, random samples of free‐response test booklets were taken from the 1986 examination in U.S. History. These examinations were chosen because they consistently show significant gender differences in objective scores but no gender differences in free‐response scores. A rescoring of the free responses was conducted that focused on their historical content. This rescoring was conducted by readers other than those who conducted the original scoring and involved tallies of specific historical points made, supporting evidence given, and factual errors. Ratings were also made of handwriting quality, neatness, and English composition quality of the free responses. Analyses conducted indicate that free‐response tasks of the type examined may have inherent characteristics that reward English composition abilities, and that some females may compensate for inferior historical knowledge with superior English composition abilities. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122266
Miller T.R.; Spray J.A.,"Miller, Timothy R. (7403948130); Spray, Judith A. (7006830106)",7403948130; 7006830106,Logistic Discriminant Function Analysis for DIF Identification of Polytomously Scored Items,1993,Journal of Educational Measurement,30,2,,107,122,15,87,10.1111/j.1745-3984.1993.tb01069.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090421&doi=10.1111%2fj.1745-3984.1993.tb01069.x&partnerID=40&md5=d23e1cd574763060c6c492b98a646e81,"The purpose of this article is to present logistic discriminant function analysis as a means of differential item functioning (DIF) identification of items that are polytomously scored. The procedure is presented with examples of a DIF analysis using items from a 27‐item mathematics test which includes six open‐ended response items scored polytomously. The results show that the logistic discriminant function procedure is ideally suited for DIF identification on nondichotomously scored test items. It is simpler and more practical than polytomous extensions of the logistic regression DIF procedure and appears to fee more powerful than a generalized Mantel‐Haenszelprocedure. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988090421
Livingston S.A.,"Livingston, Samuel A. (35864363200)",35864363200,Small‐Sample Equating With Log‐Linear Smoothing,1993,Journal of Educational Measurement,30,1,,23,39,16,68,10.1111/j.1745-3984.1993.tb00420.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122543&doi=10.1111%2fj.1745-3984.1993.tb00420.x&partnerID=40&md5=e315662e60f0e70aab2d7494ff4d1ffa,"This study investigated the extent to which log‐linear smoothing could improve the accuracy of common‐item equating by the chained equipercentile method in small samples of examinees. Examinee response data from a 100‐item test were used to create two overlapping forms of 58 items each, with 24 items in common. The criterion equating was a direct equipercentile equating of the two forms in the full population of 93,283 examinees. Anchor equatings were performed in samples of 25, 50, 100, and 200 examinees, with 50 pairs of samples at each size level. Four equatings were performed with each pair of samples: one based on unsmoothed distributions and three based on varying degrees of smoothing. Smoothing reduced, by at least half, the sample size required for a given degree of accuracy. Smoothing that preserved only two moments of the marginal distributions resulted in equatings that failed to capture the curvilinearity in the population equating. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122543
Sheehan K.; Mislevy R.J.,"Sheehan, Kathleen (7005727437); Mislevy, Robert J. (6701800690)",7005727437; 6701800690,Integrating Cognitive and Psychometric Models to Measure Document Literacy,1990,Journal of Educational Measurement,27,3,,255,272,17,42,10.1111/j.1745-3984.1990.tb00747.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988128763&doi=10.1111%2fj.1745-3984.1990.tb00747.x&partnerID=40&md5=0136991dba7cfda9ea660041d09a8bfd,"The Survey of Young Adult Literacy conducted in 1985 by the National Assessment of Educational Progress included 63 items that elicited skills in acquiring and using information from written documents. These items were analyzed using two different models: (1) a qualitative cognitive model, which characterized items in terms of the processing tasks they required, and (2) an item response theory (IRT) model, which characterized items difficulties and respondents' proficiencies simply by tendencies toward correct response. This paper demonstrates how a generalization of Fischer and Seheibleehner's Linear Logistic Test Model can be used to integrate information from the cognitive analysis into the IRT analysis, providing a foundation for subsequent item construction, test development, and diagnosis of individuals skill deficiencies. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988128763
Raymond M.R.; Viswesvaran C.,"Raymond, Mark R. (35611207000); Viswesvaran, Chockalingam (7003281345)",35611207000; 7003281345,Least Squares Models to Correct for Rater Effects in Performance Assessment,1993,Journal of Educational Measurement,30,3,,253,268,15,29,10.1111/j.1745-3984.1993.tb00426.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988101449&doi=10.1111%2fj.1745-3984.1993.tb00426.x&partnerID=40&md5=aacfc67fc32d430a0fb9da54b873b831,"This study describes three least squares models to control for rater effects in performance evaluation: ordinary least squares (OLS); weighted least squares (WLS); and ordinary least squares, subsequent to applying a logistic transformation to observed ratings (LOG‐OLS). The models were applied to ratings obtained from four administrations of an oral examination required for certification in a medical specialty. For any single administration, there were 40 raters and approximately 115 candidates, and each candidate was rated by four raters. The results indicated that raters exhibited significant amounts of leniency error and that application of the least squares models would change the pass‐fail status of approximately 7% to 9% of the candidates. Ratings adjusted by the models demonstrated higher reliability and correlated slightly higher than observed ratings with the scores on a written examination. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988101449
Skaggs G.; Lissitz R.W.,"Skaggs, Gary (20436934200); Lissitz, Robert W. (6602902644)",20436934200; 6602902644,The Consistency of Detecting Item Bias Across Different Test Administrations: Implications of Another Failure,1992,Journal of Educational Measurement,29,3,,227,242,15,14,10.1111/j.1745-3984.1992.tb00375.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988101327&doi=10.1111%2fj.1745-3984.1992.tb00375.x&partnerID=40&md5=8db280ad336adb562080be1bba73d91b,"Several item bias detection methods were applied to the analysis of bias among males and females for items from a curriculum‐based mathematics test. The focus of this analysis was the consistency of the methods across different test administrations of the same items. The results indicated that, of the methods studied, the Mantel–Haenszel (M–H) and IRT‐based sum‐of‐squares methods were the most consistent. However, the degree of reliability and agreement for these methods was modest at best. As with most prior research, no reasonable explanation could be found for the most consistently flagged items. A likely reason for this lies in the confusion of visible genetic group characteristics with their instructional backgrounds. A multidimensional perspective of item bias is proposed for future research that will take such confounding into account. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988101327
Wainer H.; Kaplan B.; Lewis C.,"Wainer, Howard (7006218234); Kaplan, Bruce (55422892100); Lewis, Charles (54389552600)",7006218234; 55422892100; 54389552600,A Comparison of the Performance of Simulated Hierarchical and Linear Testlets,1992,Journal of Educational Measurement,29,3,,243,251,8,13,10.1111/j.1745-3984.1992.tb00376.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988080222&doi=10.1111%2fj.1745-3984.1992.tb00376.x&partnerID=40&md5=11154850e6a9a14cb2376ac826f2f489,"A series of computer simulations were run to measure the relationship between testlet validity and the factors of item pool size and testlet length for both adaptive and linearly constructed testlets. We confirmed the generality of earlier empirical findings (Wainer, Lewis, Kaplan, & Braswell, 1991) that making a testlet adaptive yields only modest increases in aggregate validity because of the peakedness of the typical proficiency distribution. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988080222
Lunz M.E.; Bergstrom B.A.,"Lunz, Mary E. (7003385546); Bergstrom, Betty A. (7006173143)",7003385546; 7006173143,An Empirical Study of Computerized Adaptive Test Administration Conditions,1994,Journal of Educational Measurement,31,3,,251,263,12,26,10.1111/j.1745-3984.1994.tb00446.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112456&doi=10.1111%2fj.1745-3984.1994.tb00446.x&partnerID=40&md5=c5ac5d06a9bd032aff7bba6694cd5135,"This empirical study was designed to determine the impact of computerized adap‐ tive test (CAT) administration formats on student performance. Students in medical technology programs took a paper‐and‐pencil and an individualized, computerized adaptive test. Students were randomly assigned to adaptive test administration for‐ mats to ascertain the effect on student performance of altering: (a) the difficulty of the first item, (b) the targeted level of test difficulty, (c) minimum test length, and (d) the opportunity to control the test. Computerized adaptive test data were analyzed with ANCO VA. The paper‐and.pencil test was used as a covariate to equalize abil‐ ity variance among cells. The only significant main effect was for opportunity to control the test. There were no significant interactions among test administration formats. This study provides evidence concerning adjusting traditional computer‐ ized adaptive testing to more familiar testing modalities. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988112456
Brookhart S.M.,"Brookhart, Susan M. (6602194945)",6602194945,Teachers' Grading Practices: Meaning and Values,1993,Journal of Educational Measurement,30,2,,123,142,19,122,10.1111/j.1745-3984.1993.tb01070.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003218341&doi=10.1111%2fj.1745-3984.1993.tb01070.x&partnerID=40&md5=2e4733b16675c113f420bed23870e4e6,"Classroom teachers do not always follow recommended grading practices. Why not? It is possible to conceptualize this question as a validity issue and ask whether teachers' concerns over the many uses of grades outweigh concerns about the interpretation of grades. The purpose of this study was to investigate the meaning classroom teachers associate with grades, the value judgments they make when considering grades, and whether the meaning or values associated with grades differed by whether teachers had measurement instruction. A sample of 84 teachers, 40 with and 44 without measurement instruction, responded to classroom grading scenarios in two ways–with multiple‐choice responses indicating what they would do and with written responses to the question, “Why did you make this choice?” A coding scheme based on Messick's (1989a, 1989b) progressive matrix of facets of validity was used for quantitative and qualitative analyses of written responses. The meaning of grades is closely related to the idea of student work; grades are pay students earn for activities they perform. The relationship of this notion to classroom management should be investigated. Teachers do make value judgments when assigning grades and are especially concerned about being fair. Teachers also are concerned about the consequences of grade use, especially for developing student self‐esteem and good attitudes toward future school work. Measurement instruction made very little difference, although it did reduce the amount of self‐referenced grading reported. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-0003218341
Adema J.J.,"Adema, Jos J. (6603281237)",6603281237,The Construction of Customized Two‐Stage Tests,1990,Journal of Educational Measurement,27,3,,241,253,12,15,10.1111/j.1745-3984.1990.tb00746.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119487&doi=10.1111%2fj.1745-3984.1990.tb00746.x&partnerID=40&md5=39e564040e9a28cb02dd6e705dd4d64c,"In this paper mixed integer linear programming models for customizing two‐stage tests are given. Model constraints are imposed with respect to test composition, administration time, inter‐item dependencies, and other practical considerations. It is not difficult to modify the models to make them useful in constructing multistage tests. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84988119487
May K.; Nicewander W.A.,"May, Kim (7101955482); Nicewander, W. Alan (6601997483)",7101955482; 6601997483,Reliability and Information Functions for Percentile Ranks,1994,Journal of Educational Measurement,31,4,,313,325,12,13,10.1111/j.1745-3984.1994.tb00449.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074645&doi=10.1111%2fj.1745-3984.1994.tb00449.x&partnerID=40&md5=a6d43671f67065066a391ec7c7438465,"Reliabilities and information functions for percentile ranks and number‐right scores were compared in the context of item response theory. The basic results were: (a) The percentile rank is always less informative and reliable than the number‐right score; and (b)for easy or difficult tests composed of highly discriminating items, the percentile rank often yields unacceptably low reliability and information relative to the number‐right score. These results suggest that standardized scores that are linear transformations of the number‐right score (e.g., z scores) are much more reliable and informative indicators of the relative standing of a test score than are percentile ranks. The findings reported here demonstrate that there exist situations in which the percent of items known by examinees can be accurately estimated, but that the percent of persons falling below a given score cannot. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988074645
Mislevy R.J.; Sheehan K.M.; Wingersky M.,"Mislevy, Robert J. (6701800690); Sheehan, Kathleen M. (7005727437); Wingersky, Marilyn (6507024898)",6701800690; 7005727437; 6507024898,How to Equate Tests With Little or No Data,1993,Journal of Educational Measurement,30,1,,55,78,23,58,10.1111/j.1745-3984.1993.tb00422.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074887&doi=10.1111%2fj.1745-3984.1993.tb00422.x&partnerID=40&md5=d7997b09e2f9971bc6dda87d289afe98,"Standard procedures for equating tests, including those based on item response theory (IRT), require item responses from large numbers of examinees. Such data may not be forthcoming for reasons theoretical, political, or practical. Information about items' operating characteristics may be available from other sources, however, such as content and format specifications, expert opinion, or psychological theories about the skills and strategies required to solve them. This article shows how, in the IRT framework, collateral information about items can be exploited to augment or even replace examinee responses when linking or equating new tests to established scales. The procedures are illustrated with data from the Pre‐Professional Skills Test (PPST). Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988074887
Muthén B.O.; Kao C.‐F.; Burstein L.,"Muthén, Bengt O. (7003555242); Kao, Chih‐Fen (56325772500); Burstein, Leigh (24392337600)",7003555242; 56325772500; 24392337600,Instructionally Sensitive Psychometrics: Application of a New IRT‐Based Detection Technique to Mathematics Achievement Test Items,1991,Journal of Educational Measurement,28,1,,1,22,21,97,10.1111/j.1745-3984.1991.tb00340.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099646&doi=10.1111%2fj.1745-3984.1991.tb00340.x&partnerID=40&md5=df834bfa16a6eced18074fe94f6f2ea1,"Achievement modeling is carried out in groups of students characterized by heterogeneous instructional background. Extensions of item response theory models incorporate variables reflecting different amounts of opportunity‐to‐leam (OTL). The effects of these OTL variables are studied with respect to their influence on both the latent trait and the item performance directly. Such direct effects may reflect instructionally sensitive items. U.S. eighth‐grade mathematics data from the Second International Mathematics Study are analyzed. Here, the same test is taken by students enrolled in typical instruction and students enrolled in elementary algebra classes. It is shown that the new analysis provides a more detailed way to examine the influence of instruction on responses to test items than does conventional item response theory. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988099646
Oshima T.C.; Miller M.D.,"Oshima, Takako C. (55672664200); Miller, M. David (55757783023)",55672664200; 55757783023,Multidimensionality and IRT‐Based Item lnvariance Indexes: The Effect of Between‐Group Variation in Trait Correlation,1990,Journal of Educational Measurement,27,3,,273,283,10,8,10.1111/j.1745-3984.1990.tb00748.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090435&doi=10.1111%2fj.1745-3984.1990.tb00748.x&partnerID=40&md5=65e69dfc21732e9be13ad35a38a1e634,"Using a bidimensional two‐parameter logistic model, the authors generated data for two groups on a 40‐item test. The item parameters were the same for the two groups, but the correlation between the two traits varied between groups. The difference in the trait correlation was directly related to the number of items judged not to be invariant using traditional unidimensional IRT‐based unsigned item invariance indexes; the higher trait correlation leads to higher discrimination parameter estimates when a unidimensional IRT model is fit to the multidimensional data. In the most extreme case, when rθ1 θ2= Ofor one group and r θ1 θ2= 1.0 for the other group, 33 out of 40 items were identified as not invariant. When using signed indexes, the effect was much smaller. The authors, therefore, suggest a cautious use of IRT‐based item invariance indexes when data are potentially multidimensional and groups may vary in the strength of the correlations among traits. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988090435
Woodruff D.,"Woodruff, David (16470978300)",16470978300,Stepping Up Test Score Conditional Variances,1991,Journal of Educational Measurement,28,3,,191,196,5,4,10.1111/j.1745-3984.1991.tb00353.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073717&doi=10.1111%2fj.1745-3984.1991.tb00353.x&partnerID=40&md5=543b4dcb654c1c1f4c11f0570384fac2,"In Woodruff (1990), I derived estimates for the conditional standard error of measurement in prediction (CSEMP), the conditional standard error of estimation (CSEE), and the conditional standard error of prediction (CSEP). My original estimates assume that the conditional residual error score variances and the conditional residual true score variances, obtained from the regression of an observed score onto a parallel observed score, obey the same step‐up rules as do the marginal error score variance and the marginal true score variance. The present article derives alternative estimates for the various test score conditional variances that do not depend on these assumptions. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073717
Hassmén P.; Hunt D.P.,"Hassmén, Peter (7004745996); Hunt, Darwin P. (7402464442)",7004745996; 7402464442,Human Self‐Assessment in Multiple‐Choice Testing,1994,Journal of Educational Measurement,31,2,,149,160,11,44,10.1111/j.1745-3984.1994.tb00440.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988087269&doi=10.1111%2fj.1745-3984.1994.tb00440.x&partnerID=40&md5=8620c660bf522aba71ac1abad3d0492c,"Research indicates that the multiple‐choice format in itself often seems to favor males over females. The present study utilizes a method that enables test takers to assess the correctness of their answers. Applying this self‐assessment method may not only make the multiple‐choice tests less biased but also provide a more comprehensive measurement of usable knowledge‐that is, the kind of knowledge about which a person is sufficiently sure so that he or she will use the knowledge to make decisions and take actions. The performance of male and female undergraduates on a conventional multiple‐choice test was compared with their performance on a multiple‐choice self‐assessment test. Results show that the difference between test scores of males and those of females was reduced when subjects were allowed to make self‐assessments. This may be explained in terms of the alleged difference in cognitive style between the genders. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988087269
Huynh H.; Ferrara S.,"Huynh, Huynh (16512875100); Ferrara, Steven (35333949500)",16512875100; 35333949500,A Comparison of Equal Percentile and Partial Credit Equatings for Performance‐Based Assessments Composed of Free‐Response Items,1994,Journal of Educational Measurement,31,2,,125,141,16,13,10.1111/j.1745-3984.1994.tb00438.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122259&doi=10.1111%2fj.1745-3984.1994.tb00438.x&partnerID=40&md5=6bb1e9870a9c0e1086ca65c302948242,"This study compares the equal percentile (EP) and partial credit (PC) equatings for raw scores derived from performance‐based assessments composed of free‐response (open‐ended) items clustered around long reading selections or multistep mathematics problems. Data are from the Maryland School Performance Assessment Program. The results suggest that Masters (1982; Wright & Masters, 1982) partial credit model may be useful for equating examinations composed of moderately easy (or not too difficult)items sharing a first principal component with at least 25% of the total variance. This conclusion appears to hold even in the presence of some level of response dependency for the items within each cluster. Although visible discrepancies were found between PC and EP equated scores in the skewed tail of the score distributions, the direction of these discrepancies is unpredictable. Therefore, it cannot be concluded from the study that the two methods give equivalent results when the distributions are markedly skewed. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122259
Norcini J.J.; Shea J.A.,"Norcini, John J. (7006095771); Shea, Judy A. (35572343300)",7006095771; 35572343300,Equivalent Estimates of Borderline Group Performance in Standard Setting,1992,Journal of Educational Measurement,29,1,,19,24,5,7,10.1111/j.1745-3984.1992.tb00365.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122536&doi=10.1111%2fj.1745-3984.1992.tb00365.x&partnerID=40&md5=1fd79217aead22112d4f5f24ecade05f,"The purpose of this study was to determine if a linear procedure, typically applied to an entire examination when equating scores and reseating judges' standards, could be used with individual item data gathered through Angoffs standard‐setting method (1971). Specifically, experts estimates of borderline group performance on one form of a test were transformed to be on the same scale as experts' estimates of borderline group performance on another form of the test. The transformations were based on examinees' responses to the items and on judges' estimates of borderline group performance. The transformed values were compared to the actual estimates provided by a group of judges. The equated and reseated values were reasonably close to those actually assigned by the experts. Bias in the estimates was also relatively small. In general, the reseating procedure was more accurate than the equating procedure, especially when the examinee sample size for equating was small. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122536
Thissen D.; Wainer H.; Wang X.‐B.,"Thissen, David (7003712685); Wainer, Howard (7006218234); Wang, Xiang‐Bo (57064540800)",7003712685; 7006218234; 57064540800,Are Tests Comprising Both Multiple‐Choice and Free‐Response Items Necessarily Less Unidimensional Than Multiple‐Choice Tests?An Analysis of Two Tests,1994,Journal of Educational Measurement,31,2,,113,123,10,80,10.1111/j.1745-3984.1994.tb00437.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058293&doi=10.1111%2fj.1745-3984.1994.tb00437.x&partnerID=40&md5=65baadd8abaf31fec004ff69a2518cc2,"We consider the relationship between the multiple‐choice and free‐response sections on the Computer Science and Chemistry tests of the College Board's Advanced Placement program. Restricted factor analysis shows that the free‐response sections measure the same underlying proficiency as the multiple‐choice sections for the most part. However, there is also a significant, if relatively small, amount of local dependence among the free‐response items that produces a small degree of multidimensionauty for each test Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988058293
Stricker L.J.,"Stricker, Lawrence J. (7003613990)",7003613990,Current Validity of 1975 and 1985 SATs: Implications for Validity Trends Since the Mid‐1970s,1991,Journal of Educational Measurement,28,2,,93,98,5,4,10.1111/j.1745-3984.1991.tb00346.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988113039&doi=10.1111%2fj.1745-3984.1991.tb00346.x&partnerID=40&md5=94c058043d4628aa3b95c42be27e38de,"The aim of this study was to appraise whether different forms of the SA T used since the mid‐1970s varied in their correlations with academic performance criteria in the same cohort of examinees. A 1975 form and a 1985 form were administered to two random samples of high school juniors, and self‐reported grade point average and high school rank were obtained. The SAT‐Verbal and SAT‐Mathematical scores generally had similar correlations with the grade criteria in the two samples. The principal conclusion is that the 1975 form of the SAT does not have greater validity than the 1985 form in assessing academic performance, at least at the high school level. This outcome offers no support for the hypothesis that the decline in the SAT's ability to predict college grades since the mid‐1970s, observed in recent research, is attributable to changes in the test. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988113039
Homan S.; Hewitt M.; Linder J.,"Homan, Susan (7003531964); Hewitt, Margaret (7005077837); Linder, Jean (57191233219)",7003531964; 7005077837; 57191233219,The Development and Validation of a Formula for Measuring Single‐Sentence Test Item Readability,1994,Journal of Educational Measurement,31,4,,349,358,9,28,10.1111/j.1745-3984.1994.tb00452.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988080235&doi=10.1111%2fj.1745-3984.1994.tb00452.x&partnerID=40&md5=0c504944bd650ed7e4529b2d0f961154,"This study describes the development and validation of the Homan‐Hewitt Readability Formula. This formula estimates the readability level of single‐sentence test items. Its initial development is based on the assumption that differences in readability level will affect item difficulty. The validation of the formula is achieved by (a) estimating the readability levels of sets of test items predicted to be written at 2nd‐ through 8th‐grade levels; (b) administering the tests to 782 students in grades 2 through 5; (3) using the class means as the unit of analyses and subjecting the data to a two‐factor repeated measures ANOVA. Significant differences were found on class mean performance scores across the levels of readability. These results indicated that a relationship exists between students’reading grade levels and their responses to test items written at higher readability levels. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988080235
Kolen M.J.,"Kolen, Michael J. (6603925839)",6603925839,Smoothing Methods for Estimating Test Score Distributions,1991,Journal of Educational Measurement,28,3,,257,282,25,33,10.1111/j.1745-3984.1991.tb00358.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119890&doi=10.1111%2fj.1745-3984.1991.tb00358.x&partnerID=40&md5=21e9da44968d92fba8341a8821b472d8,"Frequency distributions of test scores may appear irregular and, as estimates of a population distribution, contain a substantial amount of sampling error. Techniques for smoothing score distributions are available that have the capacity to improve estimation. In this article, estimation/smoothing methods that are flexible enough to fit a wide variety of test score distributions are reviewed. The methods are a kernel method, a strong true–score model–based method, and a method that uses polynomial log–linear models. The use of these methods is then reviewed, and applications of the methods are presented that include describing and comparing test score distributions, estimating norms, and estimating equipercentile equivalents in test score equating. Suggestions for further research are also provided. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988119890
Ben‐Shakhar G.; Sinai Y.,"Ben‐Shakhar, Gershon (7004002344); Sinai, Yakov (57191233863)",7004002344; 57191233863,Gender Differences in Multiple‐Choice Tests: The Role of Differential Guessing Tendencies,1991,Journal of Educational Measurement,28,1,,23,35,12,125,10.1111/j.1745-3984.1991.tb00341.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-39549119406&doi=10.1111%2fj.1745-3984.1991.tb00341.x&partnerID=40&md5=5b15c0b4e446a4a7e92ba2e7d2ddfa86,"The present study focused on gender differences in the tendency to omit items and to guess in multiple‐choice tests. It was hypothesized that males would show greater guessing tendencies than females and that the use of formula scoring rather than the use of number of correct answers would result in a relative advantage for females. Two samples were examined: ninth graders and applicants to Israeli universities. The teenagers took a battery of five or six aptitude tests used to place them in various high schools, and the adults took a battery of five tests designed to select candidates to the various faculties of the Israeli universities. The results revealed a clear male advantage in most subtests of both batteries. Four measures of item‐omission tendencies were computed for each subtest, and a consistent pattern of greater omission rates among females was revealed by all measures in most subtests of the two batteries. This pattern was observed even in the few subtests that did not show male superiority and even when permissive instructions were used. Correcting the raw scores for guessing reduced the male advantage in all cases (and in the few subtests that showed female advantage the difference increased as a result of this correction), but this effect was small. It was concluded that although gender differences in guessing tendencies are robust they account for only a small fraction of the observed gender differences in multiple‐choice tests. The results were discussed, focusing on practical implications. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-39549119406
Roberts D.M.,"Roberts, Dennis M. (55465556800)",55465556800,An Empirical Study on the Nature of Trick Test Questions,1993,Journal of Educational Measurement,30,4,,331,344,13,8,10.1111/j.1745-3984.1993.tb00430.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988115627&doi=10.1111%2fj.1745-3984.1993.tb00430.x&partnerID=40&md5=05fd46d3cf72ec960e082294014a9cc5,"This study attempted to better define trick questions and see if students could differentiate between trick and not–trick questions. Phase 1 elicited definitions of trick questions so as to identify essential characteristics. Seven components were found. Phase 2 obtained ratings to see which components of trick questions were considered to be most crucial. The intention of the item constructor and the fact that the questions had multiple correct answers received highest ratings from students. Phase 3 presented a collection of statistics items, some of which were labeled on an a priori basis as being trick or not–trick. The analysis indicated that examinees were able to statistically differentiate between trick and not–trick items, but the difference compared to chance was small. Not–trick items were more successfully sorted than trick items, and trick items that were classified as intentional were sorted about as well as nonintentional items. Evidence seems to suggest that the concept of trickiness is not as clear as some test construction textbook authors suggest. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988115627
Wainer H.,"Wainer, Howard (7006218234)",7006218234,Measurement Problems,1993,Journal of Educational Measurement,30,1,,1,21,20,45,10.1111/j.1745-3984.1993.tb00419.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072890&doi=10.1111%2fj.1745-3984.1993.tb00419.x&partnerID=40&md5=0e45d36bcb4b86d317b49184262f1969,"History teaches the continuity of science; the developments of tomorrow have their genesis in the problems of today. Thus any attempt to look forward is well begun with an examination of unsettled questions. Since a clearer idea of where we are going smoothes the path into the unknown future, a periodic review of such questions is prudent. The present day, lying near the juncture of the centuries, is well suited for such a review. This article reports 16 unsolved problems in educational measurement and points toward what seem to be promising avenues of solution. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988072890
Tan E.S.; Imbos Tj.; Does R.J.M.M.,"Tan, E.S. (36919912800); Imbos, Tj. (57224280097); Does, R.J.M.M. (6701550268)",36919912800; 57224280097; 6701550268,A Distribution‐Free Approach for Comparing Growth of Knowledge,1994,Journal of Educational Measurement,31,1,,51,65,14,8,10.1111/j.1745-3984.1994.tb00434.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988090485&doi=10.1111%2fj.1745-3984.1994.tb00434.x&partnerID=40&md5=587581a3dac5c37bb133e59a3313318f,"The longitudinal testing of student achievement requires the solution of several new problem areas. In this article, several small groups of medical students at the University of Limburg Medical School in Maastricht, The Netherlands, are compared with respect to their performances. The results indicate, that, despite the possession of more knowledge at entrance, students with a low rate of growth of knowledge in the first year demonstrate a lower level of knowledge after the second academic year and continue to do so throughout the academic program when compared to students who show a higher rate of growth of knowledge in the first year. The analysis has been carried out using a distribution‐free version of a longitudinal IRT‐model suggested by Albers, Does, Imbos, and Janssen (1989). Furthermore, growth of knowledge has been described by means of a general regression model. Statistical inferences are possible using a randomization design extended to the situation where the observations are time‐dependent proportions of correct answers. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84988090485
"Engelhard G., Jr","Engelhard, George (7003970969)",7003970969,Examining Rater Errors in the Assessment of Written Composition With a Many‐Faceted Rasch Model,1994,Journal of Educational Measurement,31,2,,93,112,19,205,10.1111/j.1745-3984.1994.tb00436.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122960&doi=10.1111%2fj.1745-3984.1994.tb00436.x&partnerID=40&md5=64f31813ba7cf54a45fbba5604b4bfc5,"This study describes several categories of rater errors (rater severity, halo effect, central tendency, and restriction of range). Criteria are presented for evaluating the quality of ratings based on a many‐faceted Rasch measurement (FACETS) model for analyzing judgments. A random sample of 264 compositions rated by 15 raters and a validity committee from the 1990 administration of the Eighth Grade Writing Test in Georgia is used to illustrate the model. The data suggest that there are significant differences in rater severity. Evidence of a halo effect is found for two raters who appear to be rating the compositions holistically rather than analytically. Approximately 80% of the ratings are in the two middle categories of the rating scale, indicating that the error of central tendency is present. Restriction of range is evident when the unadjusted raw score distribution is examined, although this rater error is less evident when adjusted estimates of writing competence are used Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122960
Ryan K.E.,"Ryan, Katherine E. (16176259100)",16176259100,The Performance of the Mantel‐Haenszel Procedure Across Samples and Matching Criteria,1991,Journal of Educational Measurement,28,4,,325,337,12,9,10.1111/j.1745-3984.1991.tb00362.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988103809&doi=10.1111%2fj.1745-3984.1991.tb00362.x&partnerID=40&md5=3667ac16f235eb052a2d3d29d91eb622,"This study examined the reliability of the Mantel‐Haenszel indexes across different samples of test takers as well as across sample sizes and investigated whether these indexes are robust to item context effects. Mathematics data from the Second International Mathematics Study (SIMS; 1985) for U.S. eighth‐grade students were analyzed. The results suggest that the MH D‐DIF is robust to item context effects. However, larger sample sizes than those used in this investigation (N = 141‐167 for the focal group) may be necessary to obtain stable estimates from the Mantel‐Haenszel procedure. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988103809
Baker F.B.; Al‐Karni A.,"Baker, Frank B. (7202513736); Al‐Karni, Ali (58449126700)",7202513736; 58449126700,A Comparison of Two Procedures for Computing IRT Equating Coefficients,1991,Journal of Educational Measurement,28,2,,147,162,15,59,10.1111/j.1745-3984.1991.tb00350.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058863&doi=10.1111%2fj.1745-3984.1991.tb00350.x&partnerID=40&md5=281a34e19628ef78dd06e83bfd2a684c,"In order to equate tests under Item Response Theory (IRT), one must obtain the slope and intercept coefficients of the appropriate linear transformation. This article compares two methods for computing such equating coefficients–Loyd and Hoover (1980) and Stocking and Lord (1983). The former is based upon summary statistics of the test calibrations; the latter is based upon matching test characteristic curves by minimizing a quadratic loss function. Three types of equating situations: horizontal, vertical, and that inherent in IRT parameter recovery studies–were investigated. The results showed that the two computing procedures generally yielded similar equating coefficients in all three situations. In addition, two sets of SAT data were equated via the two procedures, and little difference in the obtained results was observed. Overall, the results suggest that the Loyd and Hoover procedure usually yields acceptable equating coefficients. The Stocking and Lord procedure improves upon the Loyd and Hoover values and appears to be less sensitive to atypical test characteristics. When the user has reason to suspect that the test calibrations may be associated with data sets that are typically troublesome to calibrate, the Stocking and Lord procedure is to be preferred. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988058863
Gitomer D.H.; Yamamoto K.,"Gitomer, Drew H. (6506661181); Yamamoto, Kentaro (56130477200)",6506661181; 56130477200,Performance Modeling That Integrates Latent Trait and Class Theory,1991,Journal of Educational Measurement,28,2,,173,189,16,22,10.1111/j.1745-3984.1991.tb00352.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988103888&doi=10.1111%2fj.1745-3984.1991.tb00352.x&partnerID=40&md5=98c4e4df20660cd73a19e4ebee5036fd,"The qualitative characterization of individual performance that is central to modem psychological theory is not adequately modeled by traditional psychometric theory that assumes, among other things, unidimensionality. In the present study, data are presented that are more adequately modeled by HYBRID, a model that incorporates both latent trait and latent class components. The latent classes were defined by a cognitive analysis of the understanding that individuals have for a circumscribed domain. In addition to providing a better statistical fit, the analysis also improves the amount of diagnostic information available for a given individual. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988103888
Young J.W.,"Young, John W. (7408528464)",7408528464,Gender Bias in Predicting College Academic Performance: A New Approach Using Item Response Theory,1991,Journal of Educational Measurement,28,1,,37,47,10,37,10.1111/j.1745-3984.1991.tb00342.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988103803&doi=10.1111%2fj.1745-3984.1991.tb00342.x&partnerID=40&md5=46bd40c1ca5a9a525efef2832b18792e,"The possibility of differential prediction of college academic performance for men and women is, at present, an issue of concern both to measurement specialists and to the general public. In prior studies, the use of a single equation to predict grade point average (GPA) from preadmissions measures has generally led to the systematic underprediction of women's college grades. In the following study, Item Response Theory (IRT) was used to develop a form of adjusted cumulative GPA, called the IRT‐based GPA. Significant underprediction for women occurred using a version of the cumulative GPA as the outcome measure. In contrast, the use of the IRT‐based GPA indicated no significant underprediction for men or women. A single regression equation worked best in predicting both men's and women's IRT‐based GPA. In addition, the IRT‐based GPA was substantially more predictable from preadmissions measures than the cumulative GPA. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988103803
Clauser B.; Mazor K.M.; Hambleton R.K.,"Clauser, Brian (7003595460); Mazor, Kathleen M. (6701717204); Hambleton, Ronald K. (7006242264)",7003595460; 6701717204; 7006242264,The Effects of Score Group Width on the Mantel‐Haenszel Procedure,1994,Journal of Educational Measurement,31,1,,67,78,11,30,10.1111/j.1745-3984.1994.tb00435.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21344493699&doi=10.1111%2fj.1745-3984.1994.tb00435.x&partnerID=40&md5=e535ce710b87ecc0b694bd87698524cd,"Previous research examining the effects of reducing the number of score groups used in the matching criterion of the Mantel‐Haenszel procedure, when screening for DIF, has produced ambiguous results. The goal of this study was to resolve the ambiguity by examining the problem with a simulated data set. The main results from this study call into question the preliminary recommendations of several other researchers that four or more score groups are sufficient and produce stable results. Although considerable stability and very little Type I error was noted with equal ability distribution comparisons, with unequal ability distributions, the Type I error rate was substantially inflated. These results argue against the appropriateness of implementing the procedure by collapsing score groups. The current data suggest that more than modest reductions in the number of score groups cannot be recommended when the ability distributions of the reference and focal groups differ Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-21344493699
Colton D.A.; Kane M.T.; Kingsbury C.; Estes C.A.,"Colton, Dean A. (7005083059); Kane, Michael T. (36088969800); Kingsbury, Carole (57512413100); Estes, Carmen A. (7005765539)",7005083059; 36088969800; 57512413100; 7005765539,A Strategy for Examining the Validity of Job Analysis Data,1991,Journal of Educational Measurement,28,4,,283,294,11,12,10.1111/j.1745-3984.1991.tb00359.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988087914&doi=10.1111%2fj.1745-3984.1991.tb00359.x&partnerID=40&md5=ca74522bc5145508ff2083158aeeb915,"A strategy for examining the validity of job analysis inventory data is introduced. This strategy involves the testing of hypotheses regarding expected consistencies within the data, and between the data and independent sources of information. Although the purpose of the job analysis is to obtain an empirical description of work patterns, some relationships among the data can be predicted with a high degree of confidence, and these predictions can then be used to test the validity of the job analysis data. An example investigates the validity of data collected as part of a job analysis for nurses. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988087914
Lukhele R.; Thissen D.; Wainer H.,"Lukhele, Robert (6507441639); Thissen, David (7003712685); Wainer, Howard (7006218234)",6507441639; 7003712685; 7006218234,"On the Relative Value of Multiple‐Choice, Constructed Response, and Examinee‐Selected Items on Two Achievement Tests",1994,Journal of Educational Measurement,31,3,,234,250,16,116,10.1111/j.1745-3984.1994.tb00445.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073215&doi=10.1111%2fj.1745-3984.1994.tb00445.x&partnerID=40&md5=98a8e7b709b15de9799ba35a85959e2f,"Using analyses based on fitting item response models to data from the College Board's Advanced Placement exams in chemistry and United States history, we found that the constructed response portion of the tests yielded little information over and above that provided by the multiple‐choice sections. These tests also allow examinees to select subsets of the constructed response items; we found that scoring on the basis of the selections themselves provided almost as much information as did scoring on the basis of the answers Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073215
Stevenson J.C.; Evans G.T.,"Stevenson, John C. (57200830724); Evans, Glen T. (16440209000)",57200830724; 16440209000,Conceptualization and Measurement of Cognitive Holding Power,1994,Journal of Educational Measurement,31,2,,161,181,20,8,10.1111/j.1745-3984.1994.tb00441.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106402&doi=10.1111%2fj.1745-3984.1994.tb00441.x&partnerID=40&md5=ddeafd3a0610ff41280e2ca1da349a28,"The concept of cognitive holding power is synthesized from theories of settings and of cognitive structures and is conceptualized as a characteristic of a learning setting that presses students into different kinds of cognitive activity. Settings which press students into using first‐ or second‐order cognitive procedures are regarded as having first‐ or second‐order cognitive holding power. The development of an instrument to measure these two dimensions of cognitive holding power is outlined. The independence of the dimensions, their reliabilities and validity, and factor structures are examined. Each dimension was found to have high reliability across vocational education and high school settings, and each was correlated as predicted with other classroom variables. The potential contribution of this research to understanding the relationship between different approaches to the teaching of problem solving and the ability to undertake problem‐solving transfer tasks is outlined. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106402
Wainer H.; Lewis C.,"Wainer, Howard (7006218234); Lewis, Charles (54389552600)",7006218234; 54389552600,Toward a Psychometrics for Testlets,1990,Journal of Educational Measurement,27,1,,1,14,13,54,10.1111/j.1745-3984.1990.tb00730.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988129067&doi=10.1111%2fj.1745-3984.1990.tb00730.x&partnerID=40&md5=a22a42b17708fd571b2fc61577573dde,"In 1987 the testlet was introduced as one way of dealing with a variety of problems that might occur with the algorithmic construction of tests. In the short time since its introduction, its range of plausible usages has been broadened considerably through the work of other researchers. In this paper we examine three different applications of the testlet concept and describe the psychometric models which seem most suitable for each application. In each case, the testlet concept gracefully solves a problem that would have been awkward with other, more traditional approaches. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988129067
Prinsell C.P.; Ramsey P.H.; Ramsey P.P.,"Prinsell, Catherine P. (57191235122); Ramsey, Philip H. (7102658299); Ramsey, Patricia P. (7102659007)",57191235122; 7102658299; 7102659007,"Score Gains, Attitudes, and Behavior Changes Due to Answer‐Changing Instruction",1994,Journal of Educational Measurement,31,4,,327,337,10,12,10.1111/j.1745-3984.1994.tb00450.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106764&doi=10.1111%2fj.1745-3984.1994.tb00450.x&partnerID=40&md5=c93215a04aac0e9b13aba16d0cad74b1,"Six undergraduate and three graduate classes were given multiple‐choice tests with subsequent evaluation of answer changes. The 300 students were tested twice, once before and once after instruction on answer changing. After each test, students were asked to complete two forms. The forms evaluated attitude toward answer changing, reasons for changing, and confidence in final answers. Students showed a significant increase in favorability toward answer changing after instruction. No significant change was found in number of answers changed. Psychology students were found to change significantly more items than were business students. Mean gain score did not change significantly after instruction. It was concluded that although instruction does lead to a change in attitude in answer changing, the number of changes and overall gain due to answer changing do not change. It was also determined that students continue to make significant gains even when their confidence in the final answer is less than 50 on a 100‐point scale. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106764
Dorans N.J.; Schmitt A.P.; Bleistein C.A.,"Dorans, Neil J. (6602289148); Schmitt, Alicia P. (57032797900); Bleistein, Carole A. (57189565384)",6602289148; 57032797900; 57189565384,The Standardization Approach to Assessing Comprehensive Differential Item Functioning,1992,Journal of Educational Measurement,29,4,,309,319,10,67,10.1111/j.1745-3984.1992.tb00379.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988123085&doi=10.1111%2fj.1745-3984.1992.tb00379.x&partnerID=40&md5=8c1509d85f58b0d1d3685d399a423565,"The standardization approach to comprehensive differential item functioning (Cdif) is described and contrasted with the log‐linear approach to differential distractor functioning explicated by Green, Crone, and Folk (1989) and with the IRT‐based approach to differential alternative functioning explicated by Thissen, Steinberg, and Wainer (1992). This descriptive approach is used routinely as an adjunct to Mantel‐Haenszel differential item functioning (DIP) detection (Dorans & Holland, 1992) in many operational testing programs at the Educational Testing Service. Data from an edition of the SAT are used to illustrate how the standardization approach to Cdif could be used to uncover differential speededness. Speculations about the sources of differential speededness for Black examinees and Hispanic examinees are offered, and some implications of the existence of differential speededness for DIP detection are mentioned. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988123085
Plake B.S.; Impara J.C.; Potenza M.T.,"Plake, Barbara S. (6603689848); Impara, James C. (6602233011); Potenza, Maria T. (24303642700)",6603689848; 6602233011; 24303642700,Content Specificity of Expert Judgments in a Standard‐Setting Study,1994,Journal of Educational Measurement,31,4,,339,347,8,24,10.1111/j.1745-3984.1994.tb00451.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988058917&doi=10.1111%2fj.1745-3984.1994.tb00451.x&partnerID=40&md5=e717bd88fa0efc9f0fe6b8a45924c2f6,"This study investigated the comparability of Angoff‐based item ratings on a general education test battery made by judges from within‐content specialties and across content domains. Judges were from English, mathematics, science, and social studies specialties in teacher education programs in a midwestem state. Cutscores established from the judges’ratings of out‐of‐content items differed little from the cutscores set using the ratings made by the content specialists. Further, out‐of‐content ratings by judges were not more influenced by performance data than were the ratings provided by judges rating items within their content specialty. The degree to ‐which these results generalize to other content specialties needs to be investigated. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988058917
Cohen A.S.; Kim S.‐H.; Subkoviak M.J.,"Cohen, Allan S. (55465451100); Kim, Seock‐Ho (7601601135); Subkoviak, Michael J. (6506489478)",55465451100; 7601601135; 6506489478,Influence of Prior Distributions on Detection of DIF,1991,Journal of Educational Measurement,28,1,,49,59,10,6,10.1111/j.1745-3984.1991.tb00343.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988073297&doi=10.1111%2fj.1745-3984.1991.tb00343.x&partnerID=40&md5=27a1ec95ba71b1252280d3deb6c70855,"Detection of differential item functioning (DIF) on items intentionally constructed to favor one group over another was investigated on item parameter estimates obtained from two item response theory‐based computer programs, LOGIST and BILOG. Signed‐ and unsigned‐area measures based on joint maximum likelihood estimation, marginal maximum likelihood estimation, and two marginal maximum a posteriori estimation procedures were compared with each other to determine whether detection of DIF could be improved using prior distributions. Results indicated that item parameter estimates obtained using either prior condition were less deviant than when priors were not used. Differences in detection of DIF appeared to be related to item parameter estimation condition and to some extent to sample size. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988073297
Spray J.A.; Welch C.J.,"Spray, Judith A. (7006830106); Welch, Catherine J. (57064765000)",7006830106; 57064765000,Estimation of Classification Consistency When the Probability of a Correct Response Varies,1990,Journal of Educational Measurement,27,1,,15,25,10,1,10.1111/j.1745-3984.1990.tb00731.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032069804&doi=10.1111%2fj.1745-3984.1990.tb00731.x&partnerID=40&md5=7251950d4e3d458d865d3a4d59ff292e,"The purpose of this study was to examine the effect that large, within‐examinee item difficulty variability had on estimates of the proportion of consistent classification of examinees into mastery categories over two test administrations. The classification consistency estimate was based on a single test administration from an estimation procedure suggested by Subkoviak (1976). Analyses of simulated data revealed that the use of a single estimate for an examinee's probability of a correct response, even when that probability varied greatly within a test for an examinee, did not affect the estimation of the proportion of consistent classifications. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-85032069804
Wainer H.; Lewis C.; Kaplan B.; Braswell J.,"Wainer, Howard (7006218234); Lewis, Charles (54389552600); Kaplan, Bruce (55422892100); Braswell, James (51963277900)",7006218234; 54389552600; 55422892100; 51963277900,Building Algebra Testlets: A Comparison of Hierarchical and Linear Structures,1991,Journal of Educational Measurement,28,4,,311,323,12,11,10.1111/j.1745-3984.1991.tb00361.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988091748&doi=10.1111%2fj.1745-3984.1991.tb00361.x&partnerID=40&md5=6f0960c01ede3de26db0791dbe117324,"Earlier (Wainer & Lewis, 1990), we reported the initial development of a testlet‐based algebra test. In this account, we provide the details of this excursion into the use of testlets. A pretest of two 15–item algebra tests was carried out in which examinees' performance on a 4‐item subset of each test (a 4–item testlet) was used to predict performance on the entire test. Two models for constructing the testlets were considered: hierarchical (adaptive) and linear (fixed format). These models are compared with each other. It was found on cross–validation that, although an adaptive testlet is superior to a fixed format testlet, this superiority is modest, whereas the potential cost of that superiority is considerable. It was concluded that in circumstances similar to those we report a fixed format testlet that uses the best items in a pool can do almost as well as the optimal adaptive testlet of equal length from that same pool. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988091748
Mullis I.V.S.,"Mullis, Ina V. S. (6508053212)",6508053212,"Developing the NAEP Content‐Area Frameworks and Innovative Assessment Methods in the 1992 Assessments of Mathematics, Reading, and Writing",1992,Journal of Educational Measurement,29,2,,111,131,20,6,10.1111/j.1745-3984.1992.tb00370.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112481&doi=10.1111%2fj.1745-3984.1992.tb00370.x&partnerID=40&md5=9b16a0f8cd7b56fbcfbc27548d53950d,"This article provides an overview of the consensus processes for the development of the frameworks underlying the NAEP assessments, with emphasis on those for the 1990 and 1992 assessments of mathematics, the 1992 assessment of reading, and the 1994 assessment of science. In addition, innovative assessment techniques included in the 1992 assessments of mathematics, reading, and writing are described, including use of mathematics tools, oral interviews, and portfolio assessment. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988112481
Kim S.‐H.; Cohen A.S.,"Kim, Seock‐Ho (7601601135); Cohen, Allan S. (55465451100)",7601601135; 55465451100,Effects of Linking Methods on Detection of DIF,1992,Journal of Educational Measurement,29,1,,51,66,15,52,10.1111/j.1745-3984.1992.tb00367.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122556&doi=10.1111%2fj.1745-3984.1992.tb00367.x&partnerID=40&md5=60fdd7bddc649f681f8c9ec155f0657d,"Studies of differential item functioning under item response theory require that item parameter estimates be placed on the same metric before comparisons can be made. The present study compared the effects of three methods for linking metrics: a weighted mean and sigma method (WMS); the test characteristic curve method (TCC); and the minimum chi‐square method (MCS), on detection of differential item functioning. Both iterative and noniterative linking procedures were compared for each method. Results indicated that detection of differentially functioning items following linking via the test characteristic curve method gave the most accurate results when the sample size was small. When the sample size was large, results for the three linking methods were essentially the same. Iterative linking provided an improvement in detection of differentially functioning items over noniterative linking particularly with the .05 alpha level. The weighted mean and sigma method showed greater improvement with iterative linking than either the test characteristic curve or minimum chi‐square method. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122556
Sykes R.C.; Fitzpatrick A.R.,"Sykes, Robert C. (7101991229); Fitzpatrick, Anne R. (7004620831)",7101991229; 7004620831,The Stability of IRT b Values,1992,Journal of Educational Measurement,29,3,,201,211,10,10,10.1111/j.1745-3984.1992.tb00373.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988080210&doi=10.1111%2fj.1745-3984.1992.tb00373.x&partnerID=40&md5=088941d5c28fdf7517d5b16640e48120,"This study investigated possible explanations for an observed change in Rasch item parameters (b values) obtained from consecutive administrations of a professional licensure examination. Considered in this investigation were variables related to item position, item type, item content, and elapsed time between administrations of the item. An analysis of covariance methodology was used to assess the relations between these variables and change in item b values, with the elapsed time index serving to control for differences that could be attributed to average or pool changes in b values over time. A series of analysis of covariance models were fitted to the data in an attempt to identify item characteristics that were significantly related to the change in b values after the time elapsed between item administrations had been controlled. The findings indicated that the change in item b values was not related either to item position or to item type. A small, positive relationship between this change and elapsed time indicated that the pool b values were increasing over time. A test of simple effects suggested the presence of greater change for one of the content categories analyzed. These findings are interpreted, and suggestions for future research are provided. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988080210
Norcini J.J.,"Norcini, John J. (7006095771)",7006095771,Equivalent Pass/Fail Decisions,1990,Journal of Educational Measurement,27,1,,59,66,7,8,10.1111/j.1745-3984.1990.tb00734.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988087883&doi=10.1111%2fj.1745-3984.1990.tb00734.x&partnerID=40&md5=22e3fd0854af66b91aba7887723e29d0,"In competency testing, it is sometimes difficult to properly equate scores of different forms of a test and thereby assure equivalent cutting scores. Under such circumstances, it is possible to set standards separately for each test form and then scale the judgments of the standard setters to achieve equivalent pass/fail decisions. Data from standard setters and examinees for a medical certifying examination were reanalyzed. Cutting score equivalents were derived by applying a linear procedure to the standard‐setting results. These were compared against criteria along with the cutting score equivalents derived from typical examination equating procedures. Results indicated that the cutting score equivalents produced by the experts were closer to the criteria than standards derived from examinee performance, especially when the number of examinees used in equating was small. The root mean square error estimate was about 1 item on a 189‐item test. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988087883
Mislevy R.J.; Beaton A.E.; Kaplan B.; Sheehan K.M.,"Mislevy, Robert J. (6701800690); Beaton, Albert E. (7007130983); Kaplan, Bruce (55422892100); Sheehan, Kathleen M. (7005727437)",6701800690; 7007130983; 55422892100; 7005727437,Estimating Population Characteristics From Sparse Matrix Samples of Item Responses,1992,Journal of Educational Measurement,29,2,,133,161,28,294,10.1111/j.1745-3984.1992.tb00371.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106408&doi=10.1111%2fj.1745-3984.1992.tb00371.x&partnerID=40&md5=9b5abb78740156918508bc6eb1671b85,"The multiple‐matrix item sampling designs that provide information about population characteristics most efficiently administer too few responses to students to estimate their proficiencies individually. Marginal estimation procedures, which estimate population characteristics directly from item responses, must be employed to realize the benefits of such a sampling design. Numerical approximations of the appropriate marginal estimation procedures for a broad variety of analyses can be obtained by constructing, from the results of a comprehensive extensive marginal solution, files of plausible values of student proficiencies. This article develops the concepts behind plausible values in a simplified setting, sketches their use in the National Assessment of Educational Progress (NAEP), and illustrates the approach with data from the Scholastic Aptitude Test (SA T). Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106408
Bridgeman B.; Rock D.A.,"Bridgeman, Brent (7005526936); Rock, Donald A. (7102591023)",7005526936; 7102591023,Relationships Among Multiple–Choice and Open–Ended Analytical Questions,1993,Journal of Educational Measurement,30,4,,313,329,16,36,10.1111/j.1745-3984.1993.tb00429.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988060668&doi=10.1111%2fj.1745-3984.1993.tb00429.x&partnerID=40&md5=91cee2c97b49e5d82675ddd0b8e55a22,"Exploratory and confirmatory factor analyses were used to explore relationships among existing item types and three new computer–administered item types for the analytical scale of the Graduate Record Examination General Test. One new item type was an open–ended version of the current multiple–choice analytical reasoning item type. The other new item types had no counterparts on the existing test. The computer tests were administered at four sites to a sample of students who had previously taken the GRE General Test. Scores from the regular GRE and the special computer administration were matched for a sample of 349 students. Factor analyses suggested that the new item types with no counterparts in the existing GRE were reliably assessing unique constructs but the open–ended analytical reasoning items were not measuring anything beyond what is measured by the current multiple–choice version of these items. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988060668
Kolen M.J.; Harris D.J.,"Kolen, Michael J. (6603925839); Harris, Deborah J. (7403921256)",6603925839; 7403921256,Comparison of Item Preequating and Random Groups Equating Using IRT and Equipercentile Methods,1990,Journal of Educational Measurement,27,1,,27,39,12,24,10.1111/j.1745-3984.1990.tb00732.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988084019&doi=10.1111%2fj.1745-3984.1990.tb00732.x&partnerID=40&md5=0db35cde95d7e6bbf9c0d8c9317fcac8,"An item‐preequating design and a random groups design were used to equate forms of the American College Testing (ACT) Assessment Mathematics Test. Equipercentile and 3‐parameter logistic model item‐response theory (IRT) procedures were used for both designs. Both pretest methods produced inadequate equating results, and the IRT item preequating method resulted in more equating error than had no equating been conducted. Although neither of the item preequating methods performed well, the results from the equipercentile preequating method were more consistent with those from the random groups method than were the results from the IRT item pretest method. Item context and position effects were likely responsible, at least in part, for the inadequate results for item preequating. Such effects need to be either controlled or modeled, and the design further researched before the item preequating design can be recommended for operational use. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988084019
Muthén B.O.,"Muthén, Bengt O. (7003555242)",7003555242,Multilevel Factor Analysis of Class and Student Achievement Components,1991,Journal of Educational Measurement,28,4,,338,354,16,341,10.1111/j.1745-3984.1991.tb00363.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988141818&doi=10.1111%2fj.1745-3984.1991.tb00363.x&partnerID=40&md5=f6f0f7c9feec75e02b83c27104c741d7,"This article analyzes mathematics achievement data from the Second International Mathematics Study (SIMS; Crosswhite, Dossey, Swafford, McKnight, & Cooney, 1985) in which U.S. students are measured at the beginning and end of eighth grade. The aim of the article is to address some substantive analysis questions in the SIMS data and show the potential of multilevel factor analysis methodology. Issues related to between‐ and within‐class decomposition of achievement variance and the change of this decomposition over the course of the eighth grade are studied. As a starting point, random effects ANOVA is considered for each achievement score. Each score contains a large amount of measurement error. The effects of unreliability on variance decomposition are shown with the help of a multilevel factor analysis model. Unreliability has severely distorting effects on this type of ANOVA while multilevel factor analysis gives results corresponding to what would be obtained with perfectly reliable scores. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84988141818
Hanson B.A.; Brennan R.L.,"Hanson, Bradley A. (7102036381); Brennan, Robert L. (34975092300)",7102036381; 34975092300,An Investigation of Classification Consistency Indexes Estimated Under Alternative Strong True Score Models,1990,Journal of Educational Measurement,27,4,,345,359,14,43,10.1111/j.1745-3984.1990.tb00753.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112815&doi=10.1111%2fj.1745-3984.1990.tb00753.x&partnerID=40&md5=40fc343331ede572873e6621e3772059,"Using several data sets, the authors examine the relative performance of the beta binomial model and two other more general strong true score models in estimating several indexes of classification consistency. It is shown that the beta binomial model can provide inadequate fits to raw score distributions compared to more general models. This lack of fit is reflected in differences in decision consistency indexes computed using the beta binomial model and the other models. It is recommended that the adequacy of a model in fitting the data be assessed before the model is used to estimate decision consistency indexes. When the beta binomial model does not fit the data, the more general models discussed here may provide an adequate fit and, in such cases, would be more appropriate for computing decision consistency indexes. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988112815
Marsh H.W.,"Marsh, Herbert W. (7201585638)",7201585638,Stability of Individual Differences in Multiwave Panel Studies: Comparison of Simplex Models and One‐Factor Models,1993,Journal of Educational Measurement,30,2,,157,183,26,89,10.1111/j.1745-3984.1993.tb01072.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000739285&doi=10.1111%2fj.1745-3984.1993.tb01072.x&partnerID=40&md5=438a9b1684df897f6083f4bbd0a06a89,"The purpose of this investigation is to evaluate structural equation models (SEMs) for measures of the same construct collected on multiple occasions (one‐variable, multiwave panel studies). Simplex models hypothesize that a measure at any one wave is substantially influenced by the measure at the 0immediately preceding wave; correlations between the same construct measured on different occasions are predicted to decline systematically as the number of intervening occasions increases. Alternatively, a one‐factor model posits that a person's score at any one time is a function of some underlying “true” score and a random disturbance that is idiosyncratic to the time; no temporal ordering of correlations is assumed. Both the simplex and one‐factor models can befit when there is only a single indicator of each construct at each wave (e.g., scale scores), but there are serious limitations to such models. Stronger models are possible when the same set of multiple indicators (e.g., the items that make up the scale) is measured at each wave. In Study 1, based on students' evaluations of teaching effectiveness collected over an 8‐year period, one‐factor models fit the data well, whereas simplex models did not. In Study 2, based on personality variables collected over a 4‐year period during adolescence, one‐factor models again provided an excellent fit to the data, whereas the simplex model did marginally poorer. The results challenge an overreliance on simplex models and demonstrate that a one‐factor model is a potentially useful alternative that should be considered in multiwave studies. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-0000739285
Martinez M.E.,"Martinez, Michael E. (55613233189)",55613233189,A Comparison of Multiple‐Choice and Constructed Figural Response Items,1991,Journal of Educational Measurement,28,2,,131,145,14,33,10.1111/j.1745-3984.1991.tb00349.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988129012&doi=10.1111%2fj.1745-3984.1991.tb00349.x&partnerID=40&md5=845ff1cf7a1a65afbce6d597726f9a99,"In contrast to multiple‐choice test questions, figural response items call for constructed responses and rely upon figural material, such as illustrations and graphs, as the response medium. Figural response questions in various science domains were created and administered to a sample of 4th‐, 8th‐, and 12th‐grade students. Item and test statistics from parallel sets of figural response and multiple‐choice questions were compared. Figural response items were generally more difficult, especially for questions that were difficult (p < .5) in their constructed‐response forms. Figural response questions were also slightly more discriminating and reliable than their multiple‐choice counterparts, but they had higher omit rates. This article addresses the relevance of guessing to figural response items and the diagnostic value of the item type. Plans for future research on figural response items are discussed. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988129012
Wainer H.; Sireci S.G.; Thissen D.,"Wainer, Howard (7006218234); Sireci, Stephen G. (6701491909); Thissen, David (7003712685)",7006218234; 6701491909; 7003712685,Differential Testlet Functioning: Definitions and Detection,1991,Journal of Educational Measurement,28,3,,197,219,22,97,10.1111/j.1745-3984.1991.tb00354.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988074695&doi=10.1111%2fj.1745-3984.1991.tb00354.x&partnerID=40&md5=9ce1cc83d77e696417f469ae31371588,"It is sometimes sensible to think of the fundamental unit of test construction as being larger than an individual item. This unit, dubbed the testlet, must pass muster in the same way that items do. One criterion of a good item is the absence of DIF–the item must function in the same way in all important subpopulations of examinees. In this article, we define what we mean by testlet DIF and provide a statistical methodology to detect it. This methodology parallels the IRT‐based likelihood ratio procedures explored previously by Thissen, Steinberg, and Wainer (1988, in press). We illustrate this methodology with analyses of data from a testlet‐based experimental version of the Scholastic Aptitude Test (SAT). Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988074695
Quails‐Payne A.L.,"Quails‐Payne, Audrey L. (57191231309)",57191231309,A Comparison of Score Level Estimates of the Standard Error of Measurement,1992,Journal of Educational Measurement,29,3,,213,225,12,25,10.1111/j.1745-3984.1992.tb00374.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072558&doi=10.1111%2fj.1745-3984.1992.tb00374.x&partnerID=40&md5=0874cc1a8aee244d232b6f986664c6e3,"Numerous methods have been proposed and investigated for estimating · the standard error of measurement (SEM) at specific score levels. Consensus on the preferred method has not been obtained, in part because there is no standard criterion. The criterion procedure in previous investigations has been a single test occasion procedure. This study compares six estimation techniques. Two criteria were calculated by using test results obtained from a test‐retest or parallel forms design. The relationship between estimated score level standard errors and the score scale was similar for the six procedures. These relationships were also congruent to findings from previous investigations. Similarity between estimates and criteria varied over methods and criteria. For test‐retest conditions, the estimation techniques are interchangeable. The user's selection could be based on personal preference. However, for parallel forms conditions, the procedures resulted in estimates that were meaningfully different. The preferred estimation technique would be Feldt's method (cited in Gupta, 1965; Feldt, 1984). Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988072558
Farr R.; Pritchard R.; Smitten B.,"Farr, Roger (24786322400); Pritchard, Robert (57213336992); Smitten, Brian (57191232359)",24786322400; 57213336992; 57191232359,A Description of What Happens When an Examinee Takes a Multiple‐Choice Reading Comprehension Test,1990,Journal of Educational Measurement,27,3,,209,226,17,99,10.1111/j.1745-3984.1990.tb00744.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988121013&doi=10.1111%2fj.1745-3984.1990.tb00744.x&partnerID=40&md5=ca072a863cf2e626c37c297237698324,"This study investigated the strategies a group of college students used to complete a portion of a standardized reading comprehension test. Twenty‐six students were randomly assigned to either an introspective interview, in which the subjects explained to a researcher what they were doing and thinking as they read the test passages and answered the multiple‐choice questions, or a retrospective interview, in which the students completed the test without interruption and then recounted for the researcher how they had gone about the task. Data analysis resulted in the identification of three broad categories of processing behavior: an overall approach to the test task, reading strategies, and test‐taking strategies. In addition, difficulties encountered by the subjects were identified. Results indicate that the common element in each subject's approach to the test was a focus on getting to the questions as quickly as possible and then using the questions to direct a search of the passage to locate the best possible information to answer the questions. The implications of these results for better understanding the relationship between test‐taking behavior and reading are discussed. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988121013
Embretson S.E.,"Embretson, Susan E. (6603914093)",6603914093,Measuring and Validating Cognitive Modifiability as an Ability: A Study in the Spatial Domain,1992,Journal of Educational Measurement,29,1,,25,50,25,45,10.1111/j.1745-3984.1992.tb00366.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112938&doi=10.1111%2fj.1745-3984.1992.tb00366.x&partnerID=40&md5=4e68a0c54b8d817db5261ff0627f4557,"Measuring cognitive modifiability from the responsiveness of an individual's performance to intervention has long been viewed (e.g., Dearborne, 1921) as an alternative to traditional (static) ability measurement. Currently, dynamic testing, in which cues or instruction are presented with ability test items, is a popular method for assessing cognitive modifiability. Despite the long‐standing interest, however, little data exists to support the validity of cognitive modifiability measures in any ability domain. Several special methodological difficulties have limited validity studies, including psychometric problems in measuring modifiability (i.e., as change), lack of appropriate validation criteria, and difficulty in linking modifiability to cognitive theory. In this article, relatively new developments for solving the validation problems are applied to measuring and validating spatial modifiability. Criterion‐related validity for predicting learning in an applied knowledge domain, as well as construct validity, is supported. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84988112938
Woodruff D.,"Woodruff, David (16470978300)",16470978300,Conditional Standard Error of Measurement in Prediction,1990,Journal of Educational Measurement,27,3,,191,208,17,9,10.1111/j.1745-3984.1990.tb00743.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988121011&doi=10.1111%2fj.1745-3984.1990.tb00743.x&partnerID=40&md5=470600df2e4fbbc2338db28e5959f339,"Previous methods for estimating the conditional standard error of measurement (CSEM) at specific score or ability levels are critically discussed, and a brief summary of prior empirical results is given. A new method is developed that avoids theoretical problems inherent in some prior methods, is easy to implement, and estimates not only a quantity analogous to the CSEM at each score but also the conditional standard error of prediction (CSEP) at each score and the conditional true score standard deviation (CTSSD) at each score, The new method differs from previous methods in that previous methods have concentrated on attempting to estimate error variance conditional on a fixed value of true score, whereas the new method considers the variance of observed scores conditional on a fixed value of an observed parallel measurement and decomposes these conditional observed score variances into true and error parts. The new method and several older methods are applied to a variety of tests, and representative results are graphically displayed. The CSEM‐Iike estimates produced by the new method are called conditional standard error of measurement in prediction (CSEMP) estimates and are similar to those produced by older methods, but the CSEP estimates produced by the new method offer an alternative interpretation of the accuracy of a test at different scores. Finally, evidence is presented that shows that previous methods can produce dissimilar results and that the shape of the score distribution may influence the way in which the CSEM varies across the score scale. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988121011
Wainer H.; Wang X.‐B.; Thissen D.,"Wainer, Howard (7006218234); Wang, Xiang‐Bo (57064540800); Thissen, David (7003712685)",7006218234; 57064540800; 7003712685,How Well Can We Compare Scores on Test Forms That Are Constructed by Examinees Choice?,1994,Journal of Educational Measurement,31,3,,183,199,16,32,10.1111/j.1745-3984.1994.tb00442.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988123039&doi=10.1111%2fj.1745-3984.1994.tb00442.x&partnerID=40&md5=e2fe2a827e78dae5cfcab15149437377,"When an exam consists, in whole or in part, of constructed‐response items, it is a common practice to allow the examinee to choose a subset of the questions to answer. This procedure is usually adopted so that the limited number of items that can be completed in the allotted time does not unfairly affect the examinee. This results in the de facto administration of several different test forms, where the exact structure of any particular form is determined by the examinee. However, when different forms are administered, a canon of good testing practice requires that those forms be equated to adjust for differences in their difficulty. When the items are chosen by the examinee, traditional equating procedures do not strictly apply due to the nonignorable nature of the missing responses. In this article, we examine the comparability of scores on such tests within an IRT framework. We illustrate the approach with data from the College Board's Advanced Placement Test in Chemistry Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988123039
Zimmerman D.W.,"Zimmerman, Donald W. (7202940240)",7202940240,A Note on Interpretation of Formulas for the Reliability of Differences,1994,Journal of Educational Measurement,31,2,,143,147,4,17,10.1111/j.1745-3984.1994.tb00439.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988119845&doi=10.1111%2fj.1745-3984.1994.tb00439.x&partnerID=40&md5=d973608326c7f662b9ea4ceb806b1335,"It is widely recognized that the reliability of a difference score depends on the reliabilities of the constituent scores and their intercorrelation. Authors often use a well‐known identity to express the reliability of a difference as a function of the reliabilities of the components, assuming that the intercorrelation remains constant. This approach is misleading, because the familiar formula is a composite function in which the correlation between components is a function of reliability. An alternative formula, containing the correlation between true scores instead of the correlation between observed scores, provides more useful information and yields values that are not quite as anomalous as the ones usually obtained Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988119845
Bridgeman B.,"Bridgeman, Brent (7005526936)",7005526936,A Comparison of Quantitative Questions in Open‐Ended and Multiple‐Choice Formats,1992,Journal of Educational Measurement,29,3,,253,271,18,102,10.1111/j.1745-3984.1992.tb00377.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988099671&doi=10.1111%2fj.1745-3984.1992.tb00377.x&partnerID=40&md5=6fa3ca5b4a3c02c30a37979faa3bcdb0,"Open–ended counterparts to a set of items from the quantitative section of the Graduate Record Examination (GRE–Q) were developed. Examinees responded to these items by gridding a numerical answer on a machine‐readable answer sheet or by typing on a computer. The test section with the special answer sheets was administered at the end of a regular GRE administration. Test forms were spiraled so that random groups received either the grid‐in questions or the same questions in a multiple‐choice format. In a separate data collection effort, 364 paid volunteers who had recently taken the GRE used a computer keyboard to enter answers to the same set of questions. Despite substantial format differences noted for individual items, total scores for the multiple‐choice and open‐ended tests demonstrated remarkably similar correlational patterns. There were no significant interactions of test format with either gender or ethnicity. Copyright © 1992, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988099671
Williamson G.L.; Appelbaum M.; Epanchin A.,"Williamson, Gary L. (55628926900); Appelbaum, Mark (7004466835); Epanchin, Alex (57191231479)",55628926900; 7004466835; 57191231479,Longitudinal Analyses of Academic Achievement,1991,Journal of Educational Measurement,28,1,,61,76,15,66,10.1111/j.1745-3984.1991.tb00344.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988112896&doi=10.1111%2fj.1745-3984.1991.tb00344.x&partnerID=40&md5=9f045294447a0fc597697b0205841385,"Mathematical models of individual growth are the basis for analyzing academic achievement over time. This study demonstrates that much can be learned about academic growth from the analysis of individual growth curves. In addition, we illustrate the aggregation of individual responses to provide descriptions of institutional growth. Although more efficient statistical methods are available, the simple approach taken here serves to demonstrate the logic and approach to analyzing longitudinal data. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988112896
Zwick R.; Donoghue J.R.; Grima A.,"Zwick, Rebecca (7004200859); Donoghue, John R. (7102202534); Grima, Angela (6602153606)",7004200859; 7102202534; 6602153606,Assessment of Differential Item Functioning for Performance Tasks,1993,Journal of Educational Measurement,30,3,,233,251,18,143,10.1111/j.1745-3984.1993.tb00425.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988106388&doi=10.1111%2fj.1745-3984.1993.tb00425.x&partnerID=40&md5=6a16dd0e34049655894481c6703967d6,"Although the belief has been expressed that performance assessments are intrinsically more fair than multiple‐choice measures, some forms of performance assessment may in fact be more likely than conventional tests to tap construct‐irrelevant factors. The assessment of differential item functioning (DIF) can be helpful in investigating the effect on subpopulations of the introduction of performance tasks. In this study, two extensions of the Mantel‐Haenszel (MH; 1959) procedure that may be useful in assessing DIP in performance measures were explored. The test of conditional association proposed by Mantel (1963) seems promising as a test of DIF for pofytomous items when the primary interest is in the between‐group difference in item means, conditional on some measure of ability. The generalized statistic proposed by Mantel and Haenszel may be more useful than Mantel's test when the entire response distributions of the groups are of interest. Simulation results showed that, for both inferential procedures, the studied item should be included in the matching variable, as in the dichotomous case. Descriptive statistics that index the magnitude of DIP were also investigated. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988106388
Gressard R.P.; Loyd B.H.,"Gressard, Risa P. (6506955010); Loyd, Brenda H. (6601961198)",6506955010; 6601961198,A Comparison of Item Sampling Plans in the Application of Multiple Matrix Sampling,1991,Journal of Educational Measurement,28,2,,119,130,11,3,10.1111/j.1745-3984.1991.tb00348.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988110404&doi=10.1111%2fj.1745-3984.1991.tb00348.x&partnerID=40&md5=9cc4a5c1e304a48ba1f7b742df4a383c,"This study used a Monte Carlo approach to investigate the effect of item sampling by item stratification on parameter estimation when applying multiple matrix sampling to achievement data. From the results of this study it was concluded that the item sampling method and sampling plan which is a practical compromise in terms of precision and sample size is one based on item stratification by item discrimination and a sampling plan with a moderate number of subtests. This sampling condition provides reasonable precision of the mean and variance estimates but requires only a moderately sized sample. Copyright © 1991, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988110404
Cizek G.J.,"Cizek, Gregory J. (8215228500)",8215228500,Reconsidering Standards and Criteria,1993,Journal of Educational Measurement,30,2,,93,106,13,56,10.1111/j.1745-3984.1993.tb01068.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039134006&doi=10.1111%2fj.1745-3984.1993.tb01068.x&partnerID=40&md5=c5d73a27ad43b95f9ca7b70410105735,"An early debate about the nature of setting standards on educational achievement tests centered on the extent to which resulting standards were arbitrary. Subsequent research in the area has advanced solutions to many practical standard setting problems, but the more fundamental issue regarding the empirical grounding of judgmental standard setting procedures has remained unresolved and largely unaddressed. This article reviews some of the salient elements of the debate about the nature of standard setting on educational assessments and suggests that the dispute can never be satisfactorily resolved within the current paradigm. A reconcep‐tualization of the nature of standard setting is proposed, and suggestions for future research are provided. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-0039134006
Budescu D.; Bar‐Hillel M.,"Budescu, David (7003588697); Bar‐Hillel, Maya (6602143279)",7003588697; 6602143279,To Guess or Not to Guess: A Decision‐Theoretic View of Formula Scoring,1993,Journal of Educational Measurement,30,4,,277,291,14,81,10.1111/j.1745-3984.1993.tb00427.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122332&doi=10.1111%2fj.1745-3984.1993.tb00427.x&partnerID=40&md5=66d260283b3beb185e450a75195d9e60,"Multiple‐choice tests are often scored by formulas under which the respondent's expected score for an item is the same whether he or she omits it or guesses at random. Typically, these formulas are accompanied by instructions that discourage guessing. In this article, we look at test taking from the normative and descriptive perspectives of judgment and decision theory. We show that for a rational test taker, whose goal is the maximization of expected score, answering is either superior or equivalent to omitting–a fact which follows from the scoring formula. For test takers who are not fully rational, or have goals other than the maximization of expected score, it is very hard to give adequate formula scoring instructions, and even the recommendation to answer under partial knowledge is problematic (though generally beneficial). Our analysis derives from a critical look at standard assumptions about the epistemic states, response strategies, and strategic motivations of test takers. In conclusion, we endorse the number‐right scoring rule, which discourages omissions and is robust against variability in respondent motivations, limitations in judgments of uncertainty, and item vagaries. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122332
Shavelson R.J.; Baxter G.P.; Gao X.,"Shavelson, Richard J. (35613093400); Baxter, Gail P. (7202105744); Gao, Xiaohong (55712081600)",35613093400; 7202105744; 55712081600,Sampling Variability of Performance Assessments,1993,Journal of Educational Measurement,30,3,,215,232,17,232,10.1111/j.1745-3984.1993.tb00424.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988122571&doi=10.1111%2fj.1745-3984.1993.tb00424.x&partnerID=40&md5=45ffe4f6d017ab82b169f3cb3eda927d,"In this article, performance assessments are cast within a sampling framework. More specifically, a performance assessment is viewed as a sample of student performance drawn from a complex universe defined by a combination of all possible tasks, occasions, raters, and measurement methods. Using generalizability theory, we present evidence bearing on the generalizability and convergent validity of performance assessments sampled from a range of measurement facets and measurement methods. Results at both the individual and school level indicate that task‐sampling variability is the major source ofmeasurment error. Large numbers of tasks are needed to get a reliable measure of mathematics and science achievement at the elementary level. With respect to convergent validity, results suggest that methods do not converge. Students' performance scores, then, are dependent on both the task and method sampled. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988122571
Ruiz‐Primo M.A.; Baxter G.P.; Shavelson R.J.,"Ruiz‐Primo, Maria Araceli (6602793270); Baxter, Gail P. (7202105744); Shavelson, Richard J. (35613093400)",6602793270; 7202105744; 35613093400,On the Stability of Performance Assessments,1993,Journal of Educational Measurement,30,1,,41,53,12,50,10.1111/j.1745-3984.1993.tb00421.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-21144460806&doi=10.1111%2fj.1745-3984.1993.tb00421.x&partnerID=40&md5=064f3710b2537c943ff7af14a8a2fc75,"This study examined the stability of scores on two types of performance assessments, an observed hands‐on investigation and a notebook surrogate. Twenty‐nine sixth‐grade students in a hands‐on inquiry‐based science curriculum completed three investigations on two occasions separated by 5 months. Results indicated that: (a) the generalizability across occasions for relative decisions was, on average, moderate for the observed investigations (.52) and the notebooks (.50); (b) the generalizability for absolute decisions was only slightly lower; (c) the major source of measurement error was the person by occasion (residual) interaction; and (d) the procedures students used to carry out the investigations tended to change from one occasion to the other. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-21144460806
Bridgeman B.; Lewis C.,"Bridgeman, Brent (7005526936); Lewis, Charles (54389552600)",7005526936; 54389552600,The Relationship of Essay and Multiple‐Choice Scores With Grades in College Courses,1994,Journal of Educational Measurement,31,1,,37,50,13,58,10.1111/j.1745-3984.1994.tb00433.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988087862&doi=10.1111%2fj.1745-3984.1994.tb00433.x&partnerID=40&md5=9e06e37f5a7ae56e70b36fe9ca2829de,"Essay and multiple‐choice scores from Advanced Placement (AP) examinations in American History, European History, English Language and Composition, and Biology were matched with freshman grades in a sample of 32 colleges. Multiple‐choice scores from the American History and Biology examinations were more highly correlated with freshman grade point averages than were essay scores from the same examinations, but essay scores were essentially equivalent to multiple‐choice scores in correlations with course grades in history, English, and biology. In history courses, men and women received comparable grades and had nearly equal scores on the AP essays, but the multiple‐choice scores of men were nearly one half of a standard deviation higher than the scores of women. Copyright © 1994, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988087862
Nandakumar R.,"Nandakumar, Ratna (7005999386)",7005999386,Simultaneous DIF Amplification and Cancellation: Shealy‐Stout's Test for DIF,1993,Journal of Educational Measurement,30,4,,293,311,18,49,10.1111/j.1745-3984.1993.tb00428.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988098075&doi=10.1111%2fj.1745-3984.1993.tb00428.x&partnerID=40&md5=c51eeb54cfb341f600e8e26e3cc5f683,"The present study investigates the phenomena of simultaneous DIF amplification and cancellation and SIBTEST's role in detecting such. A variety of simulated test data were generated for this purpose. In addition, real test data from various sources were analyzed. The results from both simulated and real test data, as Sheafy and Stout's theory (1993a, 1993b) suggests, show that the SIBTEST is effective in assessing DIF amplification and cancellation (partially or fully) at the test score level. Finally, methodological and substantive implications of DIF amplification and cancellation are discussed. Copyright © 1993, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988098075
Angoff W.H.; Johnson E.G.,"Angoff, William H. (16648034100); Johnson, Eugene G. (57191232117)",16648034100; 57191232117,The Differential Impact of Curriculum on Aptitude Test Scores,1990,Journal of Educational Measurement,27,4,,291,305,14,15,10.1111/j.1745-3984.1990.tb00750.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988097382&doi=10.1111%2fj.1745-3984.1990.tb00750.x&partnerID=40&md5=91fb943a8c506208e535759edb0e9bb7,"A sample of 22, 923 students who had taken the SAT and the GRE General Test was classified by the four general undergraduate fields of study and by sex. The authors performed several analyses to determine the degree of differential impact that sex and field of study might have on GRE‐Verbal, GRE‐Quantitative, and GRE‐Analytical scores after controlling on SAT‐Verbal and SAT‐Mathematical scores. They found, first, that the correlations of SAT‐Verbal with GRE‐Verbal scores and SAT‐Mathematical with GRE‐Quantitative scores were extremely high, .86 in the total sample and ranging from the low to middle .80s in the eight subgroups. The impact of curriculum and sex, after controlling on SAT scores, was found to be low on GRE‐ Verbal scores but relatively high on GRE‐Quantitative scores, with students in heavily quantitative fields enjoying an advantage over their peers in less quantitative fields of study. The impact was moderate on GRE‐Analytical scores. Further studies designed to “purify” the fields of study and include only clearly verbal fields and clearly mathematical fields showed small additional impact. An additional study indicated a generally slight effect of the institution attended on GRE‐Quantitative scores after controlling for sex, major field of study, and initial ability. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988097382
De Ayala R.J.; Dodd B.G.; Koch W.R.,"De Ayala, R.J. (6602251247); Dodd, Barbara G. (57189017793); Koch, William R. (7401681218)",6602251247; 57189017793; 7401681218,A Simulation and Comparison of Flexilevel and Bayesian Computerized Adaptive Testing,1990,Journal of Educational Measurement,27,3,,227,239,12,9,10.1111/j.1745-3984.1990.tb00745.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053680&doi=10.1111%2fj.1745-3984.1990.tb00745.x&partnerID=40&md5=2913525f77166cb8fbc6f33f33148543,"Computerized adaptive testing (CAT) is a testing procedure that adapts an examination to an examinee's ability by administering only items of appropriate difficulty for the examinee. In this study, the authors compared Lord's flexilevel testing procedure (flexilevel CAT) with an item response theory‐based CAT using Bayesian estimation of ability (Bayesian CAT). Three flexilevel CATs, which differed in test length (36, 18, and 11 items), and three Bayesian CATs were simulated; the Bayesian CATs differed from one another in the standard error of estimate (SEE) used for terminating the test (0.25, 0.10, and 0.05). Results showed that the flexilevel 36‐ and 18‐item CATs produced ability estimates that may be considered as accurate as those of the Bayesian CAT with SEE = 0.10 and comparable to the Bayesian CAT with SEE = 0.05. The authors discuss the implications for classroom testing and for item response theory‐based CAT. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988053680
Braun H.I.; Bennett R.E.; Frye D.; Soloway E.,"Braun, Henry I. (7202368777); Bennett, Randy Elliot (7402440584); Frye, Douglas (7006220550); Soloway, Elliot (7005901938)",7202368777; 7402440584; 7006220550; 7005901938,Scoring Constructed Responses Using Expert Systems,1990,Journal of Educational Measurement,27,2,,93,108,15,44,10.1111/j.1745-3984.1990.tb00736.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002256644&doi=10.1111%2fj.1745-3984.1990.tb00736.x&partnerID=40&md5=3d562a678815d392608b7a4ea9ceeade,"The use of constructed‐response items in large scale standardized testing has been hampered by the costs and difficulties associated with obtaining reliable scores. The advent of expert systems may signal the eventual removal of this impediment. This study investigated the accuracy with which expert systems could score a new, nonmultiple‐choice item type. The item type presents a faulty solution to a computer programming problem and asks the student to correct the solution. This item type was administered to a sample of high school seniors enrolled in an Advanced Placement course in Computer Science who also took the Advanced Placement Computer Science (APCS) examination. Results indicated that the expert systems were able to produce scores for between 82% and 95% of the solutions encountered and to display high agreement with a human reader on the correctness of the solutions. Diagnoses of the specific errors produced by students were less accurate. Correlations with scores on the objective and free‐response sections of the APCS examination were moderate. Implications for additional research and for testing practice are offered. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-0002256644
Young J.W.,"Young, John W. (7408528464)",7408528464,Adjusting the Cumulative GPA Using Item Response Theory,1990,Journal of Educational Measurement,27,2,,175,186,11,36,10.1111/j.1745-3984.1990.tb00741.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988053720&doi=10.1111%2fj.1745-3984.1990.tb00741.x&partnerID=40&md5=e6af85b6f14fd8f6a17edb7d9aa5dc54,"In college admissions, the predictive validity of preadmissions measures such as standardized test scores and high school grades is of wide interest. These measures are most often validated against the criterion of the first‐year grade point average (GPA). However, neither the first‐year GPA nor the four‐year cumulative GPA is an adequate indicator of academic performance through four years of college. In this study, Item Response Theory (IRT) is used to develop a more reliable measure of performance, called an IRT‐based GPA, which is used to estimate the validity of traditional preadmissions information. The data are preadmissions information and course grades for the Class of 1986 at Stanford University (N = 1564). Principal factor analysis is used as a precursor to determine the dimensionality of the course data and to partition courses into approximately unidimensional subsets, each of which is scaled independently. Results show a substantial increase in predictability when the IRT‐based GPA is used instead of the usual GPA. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988053720
Alexander R.A.,"Alexander, Ralph A. (7401479684)",7401479684,Correction Formulas for Correlations Restricted by Selection on an Unmeasured Variable,1990,Journal of Educational Measurement,27,2,,187,189,2,12,10.1111/j.1745-3984.1990.tb00742.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988057749&doi=10.1111%2fj.1745-3984.1990.tb00742.x&partnerID=40&md5=2113907abfbac81dd425a38928a29885,"In discussions of range-restricted correlation coefficients, two situations are most often addressed. The first is the case where restriction has occurred directly on one of the two variables of interest. The other is termed indirect range restriction to designate the situation where direct restriction has occurred on some third variable. The Thorndike (1947) Case 3 formula provides a means of correcting correlations that have arisen in this latter case when information on the third variable is available. Bryant and Gokhale (1972) gave a formula for
correcting such indirectly restricted correlations when no information is available on the third (directly restricted) variable. This note shows that the Bryant and Gokhale (1972) formula is accurate only in one special instance of indirect range restriction. The more general correction formula is given and demonstrated. Throughout the discussion, r' and S' will refer to the range-restricted correlation and standard deviation, and r and S to their unrestricted counterparts.",,,Article,Final,,Scopus,2-s2.0-84988057749
Bolger N.; Kellaghan T.,"Bolger, Niall (6701425490); Kellaghan, Thomas (6603187171)",6701425490; 6603187171,Method of Measurement and Gender Differences in Scholastic Achievement,1990,Journal of Educational Measurement,27,2,,165,174,9,100,10.1111/j.1745-3984.1990.tb00740.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988072888&doi=10.1111%2fj.1745-3984.1990.tb00740.x&partnerID=40&md5=5a5b1d19715c0629a84d8ac4cbcb111f,"Gender differences in scholastic achievement as a function of method of measurement were examined by comparing the performance of 15‐year‐old boys (N = 739) and girls (N = 758) in Irish schools on multiple‐choice tests and free‐response tests (requiring short written answers) of mathematics, Irish, and English achievement. Males performed significantly better than females on multiple‐choice tests compared to their performance on free‐response examinations. An expectation that the gender difference would be larger for the languages and smaller for mathematics because of the superior verbal skills attributed to females was not fulfilled. Copyright © 1990, Wiley Blackwell. All rights reserved",,,Article,Final,,Scopus,2-s2.0-84988072888
