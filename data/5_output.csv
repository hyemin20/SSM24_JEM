Authors,Title,Year,Volume,Abstract,Title_spell,Abstract_spell,A_tokens,T_tokens,A_tokens_join,T_tokens_join,lda_0,lda_1,lda_2,lda_3,lda_4,nmf_0,nmf_1,nmf_2,nmf_3,nmf_4
Güler N.; Penfield R.D.,A comparison of the logistic regression and contingency table methods for simultaneous detection of uniform and nonuniform dif,2009,46,"In this study, we investigate the logistic regression (LR), Mantel-Haenszel (MH), and Breslow-Day (BD) procedures for the simultaneous detection of both uniform and nonuniform differential item functioning (DIF). A simulation study was used to assess and compare the Type I error rate and power of a combined decision rule (CDR), which assesses DIF using a combination of the decisions made with BD and MH to those of LR. The results revealed that while the Type I error rate of CDR was consistently below the nominal alpha level, the Type I error rate of LR was high for the conditions having unequal ability distributions. In addition, the power of CDR was consistently higher than that of LR across all forms of DIF. © 2009 by the National Council on Measurement in Education.",A comparison of the logistic regression and contingency table methods for simultaneous detection of uniform and nonuniform dif,"In this study, we investigate the logistic regression (LR), Mantel-Haenszel (MH), and Breslow-Day (BD) procedures for the simultaneous detection of both uniform and nonuniform differential item functioning (DIF). A simulation study was used to assess and compare the Type I error rate and power of a combined decision rule (CDR), which assesses DIF using a combination of the decisions made with BD and MH to those of LR. The results revealed that while the Type I error rate of CDR was consistently below the nominal alpha level, the Type I error rate of LR was high for the conditions having unequal ability distributions. In addition, the power of CDR was consistently higher than that of LR across all forms of DIF. © 2009 by the National Council on Measurement in Education.","['study', 'investigate', 'logistic', 'regression', 'LR', 'MantelHaenszel', 'MH', 'BreslowDay', 'BD', 'procedure', 'simultaneous', 'detection', 'uniform', 'nonuniform', 'differential', 'item', 'function', 'DIF', 'simulation', 'study', 'assess', 'compare', 'Type', 'I', 'error', 'rate', 'power', 'combine', 'decision', 'rule', 'CDR', 'assess', 'DIF', 'combination', 'decision', 'BD', 'MH', 'LR', 'result', 'reveal', 'type', 'I', 'error', 'rate', 'CDR', 'consistently', 'nominal', 'alpha', 'level', 'type', 'I', 'error', 'rate', 'LR', 'high', 'condition', 'unequal', 'ability', 'distribution', 'addition', 'power', 'CDR', 'consistently', 'high', 'LR', 'form', 'dif', '©', '2009', 'National', 'Council']","['comparison', 'logistic', 'regression', 'contingency', 'table', 'method', 'simultaneous', 'detection', 'uniform', 'nonuniform', 'dif']",study investigate logistic regression LR MantelHaenszel MH BreslowDay BD procedure simultaneous detection uniform nonuniform differential item function DIF simulation study assess compare Type I error rate power combine decision rule CDR assess DIF combination decision BD MH LR result reveal type I error rate CDR consistently nominal alpha level type I error rate LR high condition unequal ability distribution addition power CDR consistently high LR form dif © 2009 National Council,comparison logistic regression contingency table method simultaneous detection uniform nonuniform dif,0.030266312745270214,0.030267200934851912,0.030290860452470186,0.37371034358151,0.5354652822858977,0.10877022297009874,0.0,0.0,0.0,0.0
Kim S.,A comparative study of IRT fixed parameter calibration methods,2006,43,"This article provides technical descriptions of five fixed parameter calibration (FPC) methods, which were based on marginal maximum likelihood estimation via the EM algorithm, and evaluates them through simulation. The five FPC methods described are distinguished from each other by how many times they update the prior ability distribution and by how many EM cycles they use. Specifically, the five FPC methods included no prior weights updating and one EM cycle (NWU-OEM) or multiple EM cycles (NWU-MEM), one prior weights updating and one EM cycle (OWU-OEM) or multiple EM cycles (OWU-MEM), and multiple weights updating and multiple EM cycles (MWU-MEM) methods. All the five FPC methods were evaluated in terms of recovery of the underlying ability distribution and item parameters. An important factor in the simulation was three different ability (normal) distributions - N(0, 1), N(0.5, 1.2 2), and N(1, 1.4 2)-for FPC groups, with the fixed item parameters obtained with a reference N(0, 1) group. Only the MWU-MEM method appeared to perform properly under all the three distributions. Under the N(0, 1) distribution, the NWU-MEM and OWU-MEM methods also appeared to perform properly. Under the N(0.5, 1.2 2), and N(1, 1.4 2) distributions, however, the four methods other than the MWU-MEM method resulted in some or severe under-estimation in the recovery.",A comparative study of IRT fixed parameter calibration methods,"This article provides technical descriptions of five fixed parameter calibration (FPC) methods, which were based on marginal maximum likelihood estimation via the EM algorithm, and evaluates them through simulation. The five FPC methods described are distinguished from each other by how many times they update the prior ability distribution and by how many EM cycles they use. Specifically, the five FPC methods included no prior weights updating and one EM cycle (NWU-OEM) or multiple EM cycles (NWU-MEM), one prior weights updating and one EM cycle (OWU-OEM) or multiple EM cycles (OWU-MEM), and multiple weights updating and multiple EM cycles (MWU-MEM) methods. All the five FPC methods were evaluated in terms of recovery of the underlying ability distribution and item parameters. An important factor in the simulation was three different ability (normal) distributions - N(0, 1), N(0.5, 1.2 2), and N(1, 1.4 2)-for FPC groups, with the fixed item parameters obtained with a reference N(0, 1) group. Only the MWU-MEM method appeared to perform properly under all the three distributions. Under the N(0, 1) distribution, the NWU-MEM and OWU-MEM methods also appeared to perform properly. Under the N(0.5, 1.2 2), and N(1, 1.4 2) distributions, however, the four methods other than the MWU-MEM method resulted in some or severe under-estimation in the recovery.","['article', 'provide', 'technical', 'description', 'fix', 'parameter', 'calibration', 'FPC', 'method', 'base', 'marginal', 'maximum', 'likelihood', 'estimation', 'EM', 'algorithm', 'evaluate', 'simulation', 'FPC', 'method', 'describe', 'distinguish', 'time', 'update', 'prior', 'ability', 'distribution', 'EM', 'cycle', 'specifically', 'FPC', 'method', 'include', 'prior', 'weight', 'update', 'EM', 'cycle', 'NWUOEM', 'multiple', 'EM', 'cycle', 'NWUMEM', 'prior', 'weight', 'update', 'EM', 'cycle', 'OWUOEM', 'multiple', 'EM', 'cycle', 'owumem', 'multiple', 'weight', 'update', 'multiple', 'EM', 'cycle', 'MWUMEM', 'method', 'FPC', 'method', 'evaluate', 'term', 'recovery', 'underlying', 'ability', 'distribution', 'item', 'parameter', 'important', 'factor', 'simulation', 'different', 'ability', 'normal', 'distribution', 'N0', '1', 'N05', '12', '2', 'N1', '14', '2for', 'FPC', 'group', 'fix', 'item', 'parameter', 'obtain', 'reference', 'N0', '1', 'group', 'MWUMEM', 'method', 'appear', 'perform', 'properly', 'distribution', 'N0', '1', 'distribution', 'NWUMEM', 'OWUMEM', 'method', 'appear', 'perform', 'properly', 'N05', '12', '2', 'N1', '14', '2', 'distribution', 'method', 'MWUMEM', 'method', 'result', 'severe', 'underestimation', 'recovery']","['comparative', 'study', 'IRT', 'fix', 'parameter', 'calibration', 'method']",article provide technical description fix parameter calibration FPC method base marginal maximum likelihood estimation EM algorithm evaluate simulation FPC method describe distinguish time update prior ability distribution EM cycle specifically FPC method include prior weight update EM cycle NWUOEM multiple EM cycle NWUMEM prior weight update EM cycle OWUOEM multiple EM cycle owumem multiple weight update multiple EM cycle MWUMEM method FPC method evaluate term recovery underlying ability distribution item parameter important factor simulation different ability normal distribution N0 1 N05 12 2 N1 14 2for FPC group fix item parameter obtain reference N0 1 group MWUMEM method appear perform properly distribution N0 1 distribution NWUMEM OWUMEM method appear perform properly N05 12 2 N1 14 2 distribution method MWUMEM method result severe underestimation recovery,comparative study IRT fix parameter calibration method,0.02860137815727887,0.028602915079335677,0.5501691320313135,0.36386448242362357,0.028762092308448402,0.0,0.0,0.011737775195600297,0.062454357989263895,0.0
Dorans N.J.,Using subpopulation invariance to assess test score equity,2004,41,"Score equity assessment (SEA) is introduced, and placed within a fair assessment context that includes differential prediction or fair selection and differential item functioning. The notion of subpopulation invariance of linking functions is central to the assessment of score equity, just as it has been for differential item functioning and differential prediction. Advanced Placement (AP) data are used for illustrative purposes. The use of multiple-choice and constructed response items in AP provides an opportunity to observe a case where subpopulation invariance of linking functions does not hold (U.S. History), and a case in which it does hold (Calculus AB). The lack of invariance for U.S. History might be attributed to several sources. The role of SEA in assessing the fairness of test assembly processes is discussed.",Using subpopulation invariance to assess test score equity,"Score equity assessment (SEA) is introduced, and placed within a fair assessment context that includes differential prediction or fair selection and differential item functioning. The notion of subpopulation invariance of linking functions is central to the assessment of score equity, just as it has been for differential item functioning and differential prediction. Advanced Placement (AP) data are used for illustrative purposes. The use of multiple-choice and constructed response items in AP provides an opportunity to observe a case where subpopulation invariance of linking functions does not hold (U.S. History), and a case in which it does hold (Calculus AB). The lack of invariance for U.S. History might be attributed to several sources. The role of SEA in assessing the fairness of test assembly processes is discussed.","['score', 'equity', 'assessment', 'SEA', 'introduce', 'place', 'fair', 'assessment', 'context', 'include', 'differential', 'prediction', 'fair', 'selection', 'differential', 'item', 'function', 'notion', 'subpopulation', 'invariance', 'link', 'function', 'central', 'assessment', 'score', 'equity', 'differential', 'item', 'function', 'differential', 'prediction', 'Advanced', 'Placement', 'AP', 'datum', 'illustrative', 'purpose', 'multiplechoice', 'construct', 'response', 'item', 'AP', 'provide', 'opportunity', 'observe', 'case', 'subpopulation', 'invariance', 'link', 'function', 'hold', 'US', 'history', 'case', 'hold', 'Calculus', 'AB', 'lack', 'invariance', 'US', 'history', 'attribute', 'source', 'role', 'SEA', 'assess', 'fairness', 'test', 'assembly', 'process', 'discuss']","['subpopulation', 'invariance', 'assess', 'test', 'score', 'equity']",score equity assessment SEA introduce place fair assessment context include differential prediction fair selection differential item function notion subpopulation invariance link function central assessment score equity differential item function differential prediction Advanced Placement AP datum illustrative purpose multiplechoice construct response item AP provide opportunity observe case subpopulation invariance link function hold US history case hold Calculus AB lack invariance US history attribute source role SEA assess fairness test assembly process discuss,subpopulation invariance assess test score equity,0.02873243699476349,0.028641448987710472,0.02906520870196495,0.185473499440682,0.7280874058748791,0.0339149808009569,0.014786700745309403,0.02919570871531077,0.009248149052275731,0.023785097891758616
Finch H.,Comparison of the performance of varimax and promax rotations: Factor structure recovery for dichotomous items,2006,43,"Nonlinear factor analysis is a tool commonly used by measurement specialists to identify both the presence and nature of multidimensionality in a set of test items, an important issue given that standard Item Response Theory models assume a unidimensional latent structure. Results from most factor-analytic algorithms include loading matrices, which are used to link items with factors. Interpretation of the loadings typically occurs after they have been rotated in order to amplify the presence of simple structure. The purpose of this simulation study is to compare the ability of two commonly used methods of rotation, Varimax and Promax, in terms of their ability to correctly link items to factors and to identify the presence of simple structure. Results suggest that the two approaches are equally able to recover the underlying factor structure, regardless of the correlations among the factors, though the oblique method is better able to identify the presence of a ""simple structure."" These results suggest that for identifying which items are associated with which factors, either approach is effective, but that for identifying simple structure when it is present, the oblique method is preferable. © Copyright 2006 by the National Council on Measurement in Education.",Comparison of the performance of varimax and promax rotations: Factor structure recovery for dichotomous items,"Nonlinear factor analysis is a tool commonly used by measurement specialists to identify both the presence and nature of multidimensionality in a set of test items, an important issue given that standard Item Response Theory models assume a unidimensional latent structure. Results from most factor-analytic algorithms include loading matrices, which are used to link items with factors. Interpretation of the loadings typically occurs after they have been rotated in order to amplify the presence of simple structure. The purpose of this simulation study is to compare the ability of two commonly used methods of rotation, Varimax and Promax, in terms of their ability to correctly link items to factors and to identify the presence of simple structure. Results suggest that the two approaches are equally able to recover the underlying factor structure, regardless of the correlations among the factors, though the oblique method is better able to identify the presence of a ""simple structure."" These results suggest that for identifying which items are associated with which factors, either approach is effective, but that for identifying simple structure when it is present, the oblique method is preferable. © Copyright 2006 by the National Council on Measurement in Education.","['nonlinear', 'factor', 'analysis', 'tool', 'commonly', 'specialist', 'identify', 'presence', 'nature', 'multidimensionality', 'set', 'test', 'item', 'important', 'issue', 'standard', 'Item', 'Response', 'Theory', 'assume', 'unidimensional', 'latent', 'structure', 'result', 'factoranalytic', 'algorithm', 'include', 'loading', 'matrix', 'link', 'item', 'factor', 'Interpretation', 'loading', 'typically', 'occur', 'rotate', 'order', 'amplify', 'presence', 'simple', 'structure', 'purpose', 'simulation', 'study', 'compare', 'ability', 'commonly', 'method', 'rotation', 'Varimax', 'Promax', 'term', 'ability', 'correctly', 'link', 'item', 'factor', 'identify', 'presence', 'simple', 'structure', 'result', 'suggest', 'approach', 'equally', 'able', 'recover', 'underlie', 'factor', 'structure', 'regardless', 'correlation', 'factor', 'oblique', 'method', 'able', 'identify', 'presence', 'simple', 'structure', 'result', 'suggest', 'identifying', 'item', 'associate', 'factor', 'approach', 'effective', 'identify', 'simple', 'structure', 'present', 'oblique', 'method', 'preferable', '©', 'copyright', '2006', 'National', 'Council']","['comparison', 'performance', 'varimax', 'promax', 'rotation', 'Factor', 'structure', 'recovery', 'dichotomous', 'item']",nonlinear factor analysis tool commonly specialist identify presence nature multidimensionality set test item important issue standard Item Response Theory assume unidimensional latent structure result factoranalytic algorithm include loading matrix link item factor Interpretation loading typically occur rotate order amplify presence simple structure purpose simulation study compare ability commonly method rotation Varimax Promax term ability correctly link item factor identify presence simple structure result suggest approach equally able recover underlie factor structure regardless correlation factor oblique method able identify presence simple structure result suggest identifying item associate factor approach effective identify simple structure present oblique method preferable © copyright 2006 National Council,comparison performance varimax promax rotation Factor structure recovery dichotomous item,0.026896397181837124,0.026897415171586742,0.026989848904460614,0.8922334199324071,0.026982918809708413,0.007656663340623415,0.019212151402503645,0.0009427076731224193,0.07186253353072189,0.0
Meyer J.P.; Setzer J.C.,A comparison of bridging methods in the analysis of NAEP trends with the new race subgroup definitions,2009,46,"Recent changes to federal guidelines for the collection of data on race and ethnicity allow respondents to select multiple race categories. Redefining race subgroups in this manner poses problems for research spanning both sets of definitions. NAEP long-term trends have used the single-race subgroup definitions for over thirty years. Little is known about the effects of redefining race subgroups on these trends. Bridging methods for reconciling the single and multiple race definitions have been developed. These methods treat single-race subgroup membership as unknown or missing. A simulation study was conducted to determine the effectiveness of four bridging methods: multiple imputation logistic regression, multiple imputation probabilistic whole assignment, deterministic whole assignment - smallest group, and deterministic whole assignment - largest group. Only the first of these methods incorporates covariate information about examinees into the bridging procedure. The other three methods only use information contained in the race item response. The simulation took into account the percentage of biracial examinees and the missing data mechanism. Results indicated that the multiple imputation logistic regression was often the best performing method. Given that all K-12 and higher education institutions will be required to use the multiple-race definitions by 2009, implications for No Child Left Behind and other federally mandated reporting are discussed. © 2009 by the National Council on Measurement in Education.",A comparison of bridging methods in the analysis of NAEP trends with the new race subgroup definitions,"Recent changes to federal guidelines for the collection of data on race and ethnicity allow respondents to select multiple race categories. Redefining race subgroups in this manner poses problems for research spanning both sets of definitions. NAEP long-term trends have used the single-race subgroup definitions for over thirty years. Little is known about the effects of redefining race subgroups on these trends. Bridging methods for reconciling the single and multiple race definitions have been developed. These methods treat single-race subgroup membership as unknown or missing. A simulation study was conducted to determine the effectiveness of four bridging methods: multiple imputation logistic regression, multiple imputation probabilistic whole assignment, deterministic whole assignment - smallest group, and deterministic whole assignment - largest group. Only the first of these methods incorporates covariate information about examinees into the bridging procedure. The other three methods only use information contained in the race item response. The simulation took into account the percentage of biracial examinees and the missing data mechanism. Results indicated that the multiple imputation logistic regression was often the best performing method. Given that all K-12 and higher education institutions will be required to use the multiple-race definitions by 2009, implications for No Child Left Behind and other federally mandated reporting are discussed. © 2009 by the National Council on Measurement in Education.","['recent', 'change', 'federal', 'guideline', 'collection', 'datum', 'race', 'ethnicity', 'allow', 'respondent', 'select', 'multiple', 'race', 'category', 'redefine', 'race', 'subgroup', 'manner', 'pose', 'problem', 'research', 'span', 'set', 'definition', 'naep', 'longterm', 'trend', 'singlerace', 'subgroup', 'definition', 'thirty', 'year', 'little', 'know', 'effect', 'redefine', 'race', 'subgroup', 'trend', 'bridging', 'method', 'reconcile', 'single', 'multiple', 'race', 'definition', 'develop', 'method', 'treat', 'singlerace', 'subgroup', 'membership', 'unknown', 'miss', 'simulation', 'study', 'conduct', 'determine', 'effectiveness', 'bridging', 'method', 'multiple', 'imputation', 'logistic', 'regression', 'multiple', 'imputation', 'probabilistic', 'assignment', 'deterministic', 'assignment', 'small', 'group', 'deterministic', 'assignment', 'large', 'group', 'method', 'incorporate', 'covariate', 'information', 'examine', 'bridging', 'procedure', 'method', 'information', 'contain', 'race', 'item', 'response', 'simulation', 'account', 'percentage', 'biracial', 'examinee', 'miss', 'datum', 'mechanism', 'result', 'indicate', 'multiple', 'imputation', 'logistic', 'regression', 'perform', 'method', 'K12', 'high', 'institution', 'require', 'multiplerace', 'definition', '2009', 'implication', 'child', 'leave', 'federally', 'mandate', 'reporting', 'discuss', '©', '2009', 'National', 'Council']","['comparison', 'bridge', 'method', 'analysis', 'naep', 'trend', 'new', 'race', 'subgroup', 'definition']",recent change federal guideline collection datum race ethnicity allow respondent select multiple race category redefine race subgroup manner pose problem research span set definition naep longterm trend singlerace subgroup definition thirty year little know effect redefine race subgroup trend bridging method reconcile single multiple race definition develop method treat singlerace subgroup membership unknown miss simulation study conduct determine effectiveness bridging method multiple imputation logistic regression multiple imputation probabilistic assignment deterministic assignment small group deterministic assignment large group method incorporate covariate information examine bridging procedure method information contain race item response simulation account percentage biracial examinee miss datum mechanism result indicate multiple imputation logistic regression perform method K12 high institution require multiplerace definition 2009 implication child leave federally mandate reporting discuss © 2009 National Council,comparison bridge method analysis naep trend new race subgroup definition,0.023923610844109156,0.023925044805335416,0.0239390266969333,0.9039728778794633,0.024239439774158773,2.8503847478633048e-05,0.012393306256031588,0.009942779329839765,0.038489155829843566,0.033180721178722165
Gierl M.J.; Leighton J.P.; Tan X.,Evaluating DETECT classification accuracy and consistency when data display complex structure,2006,43,"DETECT, the acronym for Dimensionality Evaluation To Enumerate Contributing Traits, is an innovative and relatively new nonparametric dimensionality assessment procedure used to identify mutually exclusive, dimensionally homogeneous clusters of items using a genetic algorithm (Zhang & Stout, 1999). Because the clusters of items are mutually exclusive, this procedure is most useful when the data display approximate simple structure. In many testing situations, however, data display a complex multidimensional structure. The purpose of the current study was to evaluate DETECT item classification accuracy and consistency when the data display different degrees of complex structure using both simulated and real data. Three variables were manipulated in the simulation study: The percentage of items displaying complex structure (10%, 30%, and 50%), the correlation between dimensions (.00, .30, .60, .75, and .90), and the sample size (500, 1,000, and 1,500). The results from the simulation study reveal that DETECT can accurately and consistently cluster items according to their true underlying dimension when as many as 30% of the items display complex structure, if the correlation between dimensions is less than or equal to .75 and the sample size is at least 1,000 examinees. If 50% of the items display complex structure, then the correlation between dimensions should be less than or equal to .60 and the sample size be, at least, 1,000 examinees. When the correlation between dimensions is .90, DETECT does not work well with any complex dimensional structure or sample size. Implications for practice and directions for future research are discussed.",Evaluating DETECT classification accuracy and consistency when data display complex structure,"DETECT, the acronym for Dimensionality Evaluation To Enumerate Contributing Traits, is an innovative and relatively new nonparametric dimensionality assessment procedure used to identify mutually exclusive, dimensionally homogeneous clusters of items using a genetic algorithm (Zhang & Stout, 1999). Because the clusters of items are mutually exclusive, this procedure is most useful when the data display approximate simple structure. In many testing situations, however, data display a complex multidimensional structure. The purpose of the current study was to evaluate DETECT item classification accuracy and consistency when the data display different degrees of complex structure using both simulated and real data. Three variables were manipulated in the simulation study: The percentage of items displaying complex structure (10%, 30%, and 50%), the correlation between dimensions (.00, .30, .60, .75, and .90), and the sample size (500, 1,000, and 1,500). The results from the simulation study reveal that DETECT can accurately and consistently cluster items according to their true underlying dimension when as many as 30% of the items display complex structure, if the correlation between dimensions is less than or equal to .75 and the sample size is at least 1,000 examinees. If 50% of the items display complex structure, then the correlation between dimensions should be less than or equal to .60 and the sample size be, at least, 1,000 examinees. When the correlation between dimensions is .90, DETECT does not work well with any complex dimensional structure or sample size. Implications for practice and directions for future research are discussed.","['detect', 'acronym', 'Dimensionality', 'Evaluation', 'Enumerate', 'Contributing', 'Traits', 'innovative', 'relatively', 'new', 'nonparametric', 'dimensionality', 'assessment', 'procedure', 'identify', 'mutually', 'exclusive', 'dimensionally', 'homogeneous', 'cluster', 'item', 'genetic', 'algorithm', 'Zhang', 'Stout', '1999', 'cluster', 'item', 'mutually', 'exclusive', 'procedure', 'useful', 'datum', 'display', 'approximate', 'simple', 'structure', 'testing', 'situation', 'datum', 'display', 'complex', 'multidimensional', 'structure', 'purpose', 'current', 'study', 'evaluate', 'detect', 'item', 'classification', 'accuracy', 'consistency', 'datum', 'display', 'different', 'degree', 'complex', 'structure', 'simulated', 'real', 'datum', 'variable', 'manipulate', 'simulation', 'study', 'percentage', 'item', 'display', 'complex', 'structure', '10', '30', '50', 'correlation', 'dimension', '00', '30', '60', '75', '90', 'sample', 'size', '500', '1000', '1500', 'result', 'simulation', 'study', 'reveal', 'detect', 'accurately', 'consistently', 'cluster', 'item', 'accord', 'true', 'underlying', 'dimension', '30', 'item', 'display', 'complex', 'structure', 'correlation', 'dimension', 'equal', '75', 'sample', 'size', '1000', 'examine', '50', 'item', 'display', 'complex', 'structure', 'correlation', 'dimension', 'equal', '60', 'sample', 'size', '1000', 'examine', 'correlation', 'dimension', '90', 'detect', 'work', 'complex', 'dimensional', 'structure', 'sample', 'size', 'Implications', 'practice', 'direction', 'future', 'research', 'discuss']","['evaluate', 'detect', 'classification', 'accuracy', 'consistency', 'datum', 'display', 'complex', 'structure']",detect acronym Dimensionality Evaluation Enumerate Contributing Traits innovative relatively new nonparametric dimensionality assessment procedure identify mutually exclusive dimensionally homogeneous cluster item genetic algorithm Zhang Stout 1999 cluster item mutually exclusive procedure useful datum display approximate simple structure testing situation datum display complex multidimensional structure purpose current study evaluate detect item classification accuracy consistency datum display different degree complex structure simulated real datum variable manipulate simulation study percentage item display complex structure 10 30 50 correlation dimension 00 30 60 75 90 sample size 500 1000 1500 result simulation study reveal detect accurately consistently cluster item accord true underlying dimension 30 item display complex structure correlation dimension equal 75 sample size 1000 examine 50 item display complex structure correlation dimension equal 60 sample size 1000 examine correlation dimension 90 detect work complex dimensional structure sample size Implications practice direction future research discuss,evaluate detect classification accuracy consistency datum display complex structure,0.02481023439572871,0.024822809197235323,0.0951406306821217,0.830000770857942,0.025225554866972266,0.015486800101836892,0.05132840447574982,0.0,0.036619769055300885,0.0
Prowker A.; Camilli G.,Looking beyond the overall scores of NAEP assessments: Applications of generalized linear mixed modeling for exploring value-added item difficulty effects,2007,44,"The central idea of differential item functioning (DIF) is to examine differences between two groups at the item level while controlling for overall proficiency. This approach is useful for examining hypotheses at a finer-grain level than are permitted by a total test score. The methodology proposed in this paper is also aimed at estimating differences at the item rather than the overall score level, yet with the innovation where item-level differences for many groups simultaneously are the focus. This is a straightforward generalization of DIF as variance rather than one or several group differences; conceptually, this can be referred to as item difficulty variation (IDV). When instruction is of interest, and ""groups"" is a unit at which instruction is determined or delivered, then IDV signals value-added effects that can be influenced by either demographic or instructional variables. © 2007 by the National Council on Measurement in Education.",Looking beyond the overall scores of NAEP assessments: Applications of generalized linear mixed modeling for exploring value-added item difficulty effects,"The central idea of differential item functioning (DIF) is to examine differences between two groups at the item level while controlling for overall proficiency. This approach is useful for examining hypotheses at a finer-grain level than are permitted by a total test score. The methodology proposed in this paper is also aimed at estimating differences at the item rather than the overall score level, yet with the innovation where item-level differences for many groups simultaneously are the focus. This is a straightforward generalization of DIF as variance rather than one or several group differences; conceptually, this can be referred to as item difficulty variation (IDV). When instruction is of interest, and ""groups"" is a unit at which instruction is determined or delivered, then IDV signals value-added effects that can be influenced by either demographic or instructional variables. © 2007 by the National Council on Measurement in Education.","['central', 'idea', 'differential', 'item', 'function', 'DIF', 'examine', 'difference', 'group', 'item', 'level', 'control', 'overall', 'proficiency', 'approach', 'useful', 'examine', 'hypothesis', 'finergrain', 'level', 'permit', 'total', 'test', 'score', 'methodology', 'propose', 'paper', 'aim', 'estimate', 'difference', 'item', 'overall', 'score', 'level', 'innovation', 'itemlevel', 'difference', 'group', 'simultaneously', 'focus', 'straightforward', 'generalization', 'DIF', 'variance', 'group', 'difference', 'conceptually', 'refer', 'item', 'difficulty', 'variation', 'IDV', 'instruction', 'interest', 'group', 'unit', 'instruction', 'determine', 'deliver', 'IDV', 'signal', 'valueadde', 'effect', 'influence', 'demographic', 'instructional', 'variable', '©', '2007', 'National', 'Council']","['look', 'overall', 'score', 'naep', 'assessment', 'application', 'generalized', 'linear', 'mixed', 'modeling', 'explore', 'valueadde', 'item', 'difficulty', 'effect']",central idea differential item function DIF examine difference group item level control overall proficiency approach useful examine hypothesis finergrain level permit total test score methodology propose paper aim estimate difference item overall score level innovation itemlevel difference group simultaneously focus straightforward generalization DIF variance group difference conceptually refer item difficulty variation IDV instruction interest group unit instruction determine deliver IDV signal valueadde effect influence demographic instructional variable © 2007 National Council,look overall score naep assessment application generalized linear mixed modeling explore valueadde item difficulty effect,0.026178628960903238,0.026159427774787886,0.2459221615860367,0.6748733934536622,0.02686638822461011,0.07842937960134522,0.020930775112213706,0.0,0.009635163717322119,0.03839729256343422
Monahan P.O.; Lee W.-C.; Ankenmann R.D.,Generating dichotomous item scores with the four-parameter beta compound binomial model,2007,44,"A Monte Carlo simulation technique for generating dichotomous item scores is presented that implements (a) a psychometric model with different explicit assumptions than traditional parametric item response theory (IRT) models, and (b) item characteristic curves without restrictive assumptions concerning mathematical form. The four-parameter beta compound-binomial (4PBCB) strong true score model (with two-term approximation to the compound binomial) is used to estimate and generate the true score distribution. The nonparametric item-true score step functions are estimated by classical item difficulties conditional on proportion-correct total score. The technique performed very well in replicating inter-item correlations, item statistics (point-biserial correlation coefficients and item proportion-correct difficulties), first four moments of total score distribution, and coefficient alpha of three real data sets consisting of educational achievement test scores. The technique replicated real data (including subsamples of differing proficiency) as well as the three-parameter logistic (3PL) IRT model (and much better than the 1PL model) and is therefore a promising alternative simulation technique. This 4PBCB technique may be particularly useful as a more neutral simulation procedure for comparing methods that use different IRT models. © 2007 by the National Council on Measurement in Education.",Generating dichotomous item scores with the four-parameter beta compound binomial model,"A Monte Carlo simulation technique for generating dichotomous item scores is presented that implements (a) a psychometric model with different explicit assumptions than traditional parametric item response theory (IRT) models, and (b) item characteristic curves without restrictive assumptions concerning mathematical form. The four-parameter beta compound-binomial (4PBCB) strong true score model (with two-term approximation to the compound binomial) is used to estimate and generate the true score distribution. The nonparametric item-true score step functions are estimated by classical item difficulties conditional on proportion-correct total score. The technique performed very well in replicating inter-item correlations, item statistics (point-biserial correlation coefficients and item proportion-correct difficulties), first four moments of total score distribution, and coefficient alpha of three real data sets consisting of educational achievement test scores. The technique replicated real data (including subsamples of differing proficiency) as well as the three-parameter logistic (3PL) IRT model (and much better than the 1PL model) and is therefore a promising alternative simulation technique. This 4PBCB technique may be particularly useful as a more neutral simulation procedure for comparing methods that use different IRT models. © 2007 by the National Council on Measurement in Education.","['Monte', 'Carlo', 'simulation', 'technique', 'generate', 'dichotomous', 'item', 'score', 'present', 'implement', 'psychometric', 'different', 'explicit', 'assumption', 'traditional', 'parametric', 'item', 'response', 'theory', 'IRT', 'b', 'item', 'characteristic', 'curve', 'restrictive', 'assumption', 'concern', 'mathematical', 'form', 'fourparameter', 'beta', 'compoundbinomial', '4PBCB', 'strong', 'true', 'score', 'twoterm', 'approximation', 'compound', 'binomial', 'estimate', 'generate', 'true', 'score', 'distribution', 'nonparametric', 'itemtrue', 'score', 'step', 'function', 'estimate', 'classical', 'item', 'difficulty', 'conditional', 'proportioncorrect', 'total', 'score', 'technique', 'perform', 'replicate', 'interitem', 'correlation', 'item', 'statistic', 'pointbiserial', 'correlation', 'coefficient', 'item', 'proportioncorrect', 'difficulty', 'moment', 'total', 'score', 'distribution', 'coefficient', 'alpha', 'real', 'datum', 'set', 'consist', 'educational', 'achievement', 'test', 'score', 'technique', 'replicate', 'real', 'datum', 'include', 'subsample', 'differ', 'proficiency', 'threeparameter', 'logistic', '3pl', 'IRT', '1pl', 'promising', 'alternative', 'simulation', 'technique', '4PBCB', 'technique', 'particularly', 'useful', 'neutral', 'simulation', 'procedure', 'compare', 'method', 'different', 'IRT', '©', '2007', 'National', 'Council']","['generate', 'dichotomous', 'item', 'score', 'fourparameter', 'beta', 'compound', 'binomial']",Monte Carlo simulation technique generate dichotomous item score present implement psychometric different explicit assumption traditional parametric item response theory IRT b item characteristic curve restrictive assumption concern mathematical form fourparameter beta compoundbinomial 4PBCB strong true score twoterm approximation compound binomial estimate generate true score distribution nonparametric itemtrue score step function estimate classical item difficulty conditional proportioncorrect total score technique perform replicate interitem correlation item statistic pointbiserial correlation coefficient item proportioncorrect difficulty moment total score distribution coefficient alpha real datum set consist educational achievement test score technique replicate real datum include subsample differ proficiency threeparameter logistic 3pl IRT 1pl promising alternative simulation technique 4PBCB technique particularly useful neutral simulation procedure compare method different IRT © 2007 National Council,generate dichotomous item score fourparameter beta compound binomial,0.022516652453464837,0.022515480021981758,0.022560558448485042,0.9097367357590266,0.022670573317041785,0.006894319730964927,0.08109453088974193,0.01846191867474611,0.04222686409380554,0.011682883237798163
Lockwood J.R.; McCaffrey D.F.; Hamilton L.S.; Stecher B.; Le V.-N.; Martinez J.F.,The sensitivity of value-added teacher effect estimates to different mathematics achievement measures,2007,44,"Using longitudinal data from a cohort of middle school students from a large school district, we estimate separate ""value-added"" teacher effects for two subscales of a mathematics assessment under a variety of statistical models varying in form and degree of control for student background characteristics. We find that the variation in estimated effects resulting from the different mathematics achievement measures is large relative to variation resulting from choices about model specification, and that the variation within teachers across achievement measures is larger than the variation across teachers. These results suggest that conclusions about individual teachers' performance based on value-added models can be sensitive to the ways in which student achievement is measured. © 2007 by the National Council on Measurement in Education.",The sensitivity of value-added teacher effect estimates to different mathematics achievement measures,"Using longitudinal data from a cohort of middle school students from a large school district, we estimate separate ""value-added"" teacher effects for two subscales of a mathematics assessment under a variety of statistical models varying in form and degree of control for student background characteristics. We find that the variation in estimated effects resulting from the different mathematics achievement measures is large relative to variation resulting from choices about model specification, and that the variation within teachers across achievement measures is larger than the variation across teachers. These results suggest that conclusions about individual teachers' performance based on value-added models can be sensitive to the ways in which student achievement is measured. © 2007 by the National Council on Measurement in Education.","['longitudinal', 'datum', 'cohort', 'middle', 'school', 'student', 'large', 'school', 'district', 'estimate', 'separate', 'valueadde', 'teacher', 'effect', 'subscale', 'mathematic', 'assessment', 'variety', 'statistical', 'vary', 'form', 'degree', 'control', 'student', 'background', 'characteristic', 'find', 'variation', 'estimate', 'effect', 'result', 'different', 'mathematics', 'achievement', 'measure', 'large', 'relative', 'variation', 'result', 'choice', 'specification', 'variation', 'teacher', 'achievement', 'measure', 'large', 'variation', 'teacher', 'result', 'suggest', 'conclusion', 'individual', 'teacher', 'performance', 'base', 'valueadde', 'sensitive', 'way', 'student', 'achievement', 'measure', '©', '2007', 'National', 'Council']","['sensitivity', 'valueadde', 'teacher', 'effect', 'estimate', 'different', 'mathematic', 'achievement', 'measure']",longitudinal datum cohort middle school student large school district estimate separate valueadde teacher effect subscale mathematic assessment variety statistical vary form degree control student background characteristic find variation estimate effect result different mathematics achievement measure large relative variation result choice specification variation teacher achievement measure large variation teacher result suggest conclusion individual teacher performance base valueadde sensitive way student achievement measure © 2007 National Council,sensitivity valueadde teacher effect estimate different mathematic achievement measure,0.03016592420722174,0.030166450838765277,0.03025527126108452,0.2753952661579927,0.6340170875349357,0.0,0.0,0.0008275671233295625,3.627610209002119e-05,0.17269329883413806
Wainer H.; Wang X.A.; Skorupski W.P.; Bradlow E.T.,A Bayesian method for evaluating passing scores: The PPoP curve,2005,42,"In this note, we demonstrate an interesting use of the posterior distributions (and corresponding posterior samples of proficiency) that are yielded by fitting a fully Bayesian test scoring model to a complex assessment. Specifically, we examine the efficacy of the test in combination with the specific passing score that was chosen through expert judgment, or, in general, any external a priori criterion. In addition, we study the robustness of the test's efficacy with respect to choice of the passing score.",A Bayesian method for evaluating passing scores: The PPoP curve,"In this note, we demonstrate an interesting use of the posterior distributions (and corresponding posterior samples of proficiency) that are yielded by fitting a fully Bayesian test scoring model to a complex assessment. Specifically, we examine the efficacy of the test in combination with the specific passing score that was chosen through expert judgment, or, in general, any external a priori criterion. In addition, we study the robustness of the test's efficacy with respect to choice of the passing score.","['note', 'demonstrate', 'interesting', 'posterior', 'distribution', 'correspond', 'posterior', 'sample', 'proficiency', 'yield', 'fit', 'fully', 'bayesian', 'test', 'scoring', 'complex', 'assessment', 'specifically', 'examine', 'efficacy', 'test', 'combination', 'specific', 'pass', 'score', 'choose', 'expert', 'judgment', 'general', 'external', 'priori', 'criterion', 'addition', 'study', 'robustness', 'test', 'efficacy', 'respect', 'choice', 'pass', 'score']","['bayesian', 'method', 'evaluate', 'pass', 'score', 'PPoP', 'curve']",note demonstrate interesting posterior distribution correspond posterior sample proficiency yield fit fully bayesian test scoring complex assessment specifically examine efficacy test combination specific pass score choose expert judgment general external priori criterion addition study robustness test efficacy respect choice pass score,bayesian method evaluate pass score PPoP curve,0.030865133572059757,0.030858981341566196,0.1637570158263572,0.7433978903249104,0.031120978935106423,0.006695552967003912,0.07633769833261361,0.02367627548862577,0.0,0.0
Van Der Linden W.J.,A comparison of item-selection methods for adaptive tests with content constraints,2005,42,"In test assembly, a fundamental difference exists between algorithms that select a test sequentially or simultaneously. Sequential assembly allows us to optimize an objective function at the examinee's ability estimate, such as the test information function in computerized adaptive testing. But it leads to the non-trivial problem of how to realize a set of content constraints on the test - a problem more naturally solved by a simultaneous item-selection method. Three main item-selection methods in adaptive testing offer solutions to this dilemma. The spiraling method moves item selection across categories of items in the pool proportionally to the numbers needed from them. Item selection by the weighted-deviations method (WDM) and the shadow test approach (STA) is based on projections of the future consequences of selecting an item. These two methods differ in that the former calculates a projection of a weighted sum of the attributes of the eventual test and the latter a projection of the test itself. The pros and cons of these methods are analyzed. An empirical comparison between the WDM and STA was conducted for an adaptive version of the Law School Admission Test (LSAT), which showed equally good item-exposure rates but violations of some of the constraints and larger bias and inaccuracy of the ability estimator for the WDM.",A comparison of item-selection methods for adaptive tests with content constraints,"In test assembly, a fundamental difference exists between algorithms that select a test sequentially or simultaneously. Sequential assembly allows us to optimize an objective function at the examinee's ability estimate, such as the test information function in computerized adaptive testing. But it leads to the non-trivial problem of how to realize a set of content constraints on the test - a problem more naturally solved by a simultaneous item-selection method. Three main item-selection methods in adaptive testing offer solutions to this dilemma. The spiraling method moves item selection across categories of items in the pool proportionally to the numbers needed from them. Item selection by the weighted-deviations method (WDM) and the shadow test approach (STA) is based on projections of the future consequences of selecting an item. These two methods differ in that the former calculates a projection of a weighted sum of the attributes of the eventual test and the latter a projection of the test itself. The pros and cons of these methods are analyzed. An empirical comparison between the WDM and STA was conducted for an adaptive version of the Law School Admission Test (LSAT), which showed equally good item-exposure rates but violations of some of the constraints and larger bias and inaccuracy of the ability estimator for the WDM.","['test', 'assembly', 'fundamental', 'difference', 'exist', 'algorithm', 'select', 'test', 'sequentially', 'simultaneously', 'sequential', 'assembly', 'allow', 'optimize', 'objective', 'function', 'examinees', 'ability', 'estimate', 'test', 'information', 'function', 'computerized', 'adaptive', 'testing', 'lead', 'nontrivial', 'problem', 'realize', 'set', 'content', 'constraint', 'test', 'problem', 'naturally', 'solve', 'simultaneous', 'itemselection', 'method', 'main', 'itemselection', 'method', 'adaptive', 'testing', 'offer', 'solution', 'dilemma', 'spiral', 'method', 'item', 'selection', 'category', 'item', 'pool', 'proportionally', 'number', 'need', 'Item', 'selection', 'weighteddeviation', 'method', 'WDM', 'shadow', 'test', 'approach', 'STA', 'base', 'projection', 'future', 'consequence', 'select', 'item', 'method', 'differ', 'calculate', 'projection', 'weighted', 'sum', 'attribute', 'eventual', 'test', 'projection', 'test', 'pro', 'con', 'method', 'analyze', 'empirical', 'comparison', 'WDM', 'STA', 'conduct', 'adaptive', 'version', 'Law', 'School', 'Admission', 'test', 'LSAT', 'equally', 'good', 'itemexposure', 'rate', 'violation', 'constraint', 'large', 'bias', 'inaccuracy', 'ability', 'estimator', 'WDM']","['comparison', 'itemselection', 'method', 'adaptive', 'test', 'content', 'constraint']",test assembly fundamental difference exist algorithm select test sequentially simultaneously sequential assembly allow optimize objective function examinees ability estimate test information function computerized adaptive testing lead nontrivial problem realize set content constraint test problem naturally solve simultaneous itemselection method main itemselection method adaptive testing offer solution dilemma spiral method item selection category item pool proportionally number need Item selection weighteddeviation method WDM shadow test approach STA base projection future consequence select item method differ calculate projection weighted sum attribute eventual test projection test pro con method analyze empirical comparison WDM STA conduct adaptive version Law School Admission test LSAT equally good itemexposure rate violation constraint large bias inaccuracy ability estimator WDM,comparison itemselection method adaptive test content constraint,0.02250895851589922,0.022510550011397498,0.3515258153627393,0.5803289923809783,0.023125683728985564,0.007432636771894607,0.0,0.011887967837035109,0.08769510244316958,0.007153984138058852
Henson R.; Templin J.; Douglas J.,Using efficient model based sum-scores for conducting skills diagnoses,2007,44,"Consider test data, a specified set of dichotomous skills measured by the test, and an IRT cognitive diagnosis model (ICDM). Statistical estimation of the data set using the ICDM can provide examinee estimates of mastery for these skills, referred to generally as attributes. With such detailed information about each examinee, future instruction can be tailored specifically for each student, often referred to as formative assessment. However, use of such cognitive diagnosis models to estimate skills in classrooms can require computationally intensive and complicated statistical estimation algorithms, which can diminish the breadth of applications of attribute level diagnosis. We explore the use of sum-scores (each attribute measured by a sum-score) combined with estimated model-based sum-score mastery/nonmastery cutoffs as an easy-to-use and intuitive method to estimate attribute mastery in classrooms and other settings where simple skills diagnostic approaches are desirable. Using a simulation study of skills diagnosis test settings and assuming a test consisting of a model-based calibrated set of items, correct classification rates (CCRs) are compared among four model-based approaches for estimating attribute mastery, namely using full model-based estimation and three different methods of computing sum-scores (simple sum-scores, complex sum-scores, and weighted complex sum-scores) combined with model-based mastery sum-score cutoffs. In summary, the results suggest that model-based sum-scores and mastery cutoffs can be used to estimate examinee attribute mastery with only moderate reductions in CCRs in comparison with the full model-based estimation approach. Certain topics are mentioned that are currently being investigated, especially applications in classroom and textbook settings. © 2007 by the National Council on Measurement in Education.",Using efficient model based sum-scores for conducting skills diagnoses,"Consider test data, a specified set of dichotomous skills measured by the test, and an IRT cognitive diagnosis model (ICDM). Statistical estimation of the data set using the ICDM can provide examinee estimates of mastery for these skills, referred to generally as attributes. With such detailed information about each examinee, future instruction can be tailored specifically for each student, often referred to as formative assessment. However, use of such cognitive diagnosis models to estimate skills in classrooms can require computationally intensive and complicated statistical estimation algorithms, which can diminish the breadth of applications of attribute level diagnosis. We explore the use of sum-scores (each attribute measured by a sum-score) combined with estimated model-based sum-score mastery/nonmastery cutoffs as an easy-to-use and intuitive method to estimate attribute mastery in classrooms and other settings where simple skills diagnostic approaches are desirable. Using a simulation study of skills diagnosis test settings and assuming a test consisting of a model-based calibrated set of items, correct classification rates (CCRs) are compared among four model-based approaches for estimating attribute mastery, namely using full model-based estimation and three different methods of computing sum-scores (simple sum-scores, complex sum-scores, and weighted complex sum-scores) combined with model-based mastery sum-score cutoffs. In summary, the results suggest that model-based sum-scores and mastery cutoffs can be used to estimate examinee attribute mastery with only moderate reductions in CCRs in comparison with the full model-based estimation approach. Certain topics are mentioned that are currently being investigated, especially applications in classroom and textbook settings. © 2007 by the National Council on Measurement in Education.","['consider', 'test', 'datum', 'specify', 'set', 'dichotomous', 'skill', 'measure', 'test', 'IRT', 'cognitive', 'diagnosis', 'ICDM', 'statistical', 'estimation', 'datum', 'set', 'ICDM', 'provide', 'examinee', 'estimate', 'mastery', 'skill', 'refer', 'generally', 'attribute', 'detailed', 'information', 'examinee', 'future', 'instruction', 'tailor', 'specifically', 'student', 'refer', 'formative', 'assessment', 'cognitive', 'diagnosis', 'estimate', 'skill', 'classroom', 'require', 'computationally', 'intensive', 'complicated', 'statistical', 'estimation', 'algorithm', 'diminish', 'breadth', 'application', 'attribute', 'level', 'diagnosis', 'explore', 'sumscore', 'attribute', 'measure', 'sumscore', 'combine', 'estimate', 'modelbase', 'sumscore', 'masterynonmastery', 'cutoff', 'easytouse', 'intuitive', 'method', 'estimate', 'attribute', 'mastery', 'classroom', 'setting', 'simple', 'skill', 'diagnostic', 'approach', 'desirable', 'simulation', 'study', 'skill', 'diagnosis', 'test', 'setting', 'assume', 'test', 'consist', 'modelbase', 'calibrate', 'set', 'item', 'correct', 'classification', 'rate', 'ccr', 'compare', 'modelbase', 'approach', 'estimate', 'attribute', 'mastery', 'modelbase', 'estimation', 'different', 'method', 'computing', 'sumscore', 'simple', 'sumscore', 'complex', 'sumscore', 'weight', 'complex', 'sumscore', 'combine', 'modelbase', 'mastery', 'sumscore', 'cutoff', 'summary', 'result', 'suggest', 'modelbase', 'sumscore', 'mastery', 'cutoff', 'estimate', 'examinee', 'attribute', 'mastery', 'moderate', 'reduction', 'CCRs', 'comparison', 'modelbase', 'estimation', 'approach', 'certain', 'topic', 'mention', 'currently', 'investigate', 'especially', 'application', 'classroom', 'textbook', 'setting', '©', '2007', 'National', 'Council']","['efficient', 'base', 'sumscore', 'conduct', 'skill', 'diagnosis']",consider test datum specify set dichotomous skill measure test IRT cognitive diagnosis ICDM statistical estimation datum set ICDM provide examinee estimate mastery skill refer generally attribute detailed information examinee future instruction tailor specifically student refer formative assessment cognitive diagnosis estimate skill classroom require computationally intensive complicated statistical estimation algorithm diminish breadth application attribute level diagnosis explore sumscore attribute measure sumscore combine estimate modelbase sumscore masterynonmastery cutoff easytouse intuitive method estimate attribute mastery classroom setting simple skill diagnostic approach desirable simulation study skill diagnosis test setting assume test consist modelbase calibrate set item correct classification rate ccr compare modelbase approach estimate attribute mastery modelbase estimation different method computing sumscore simple sumscore complex sumscore weight complex sumscore combine modelbase mastery sumscore cutoff summary result suggest modelbase sumscore mastery cutoff estimate examinee attribute mastery moderate reduction CCRs comparison modelbase estimation approach certain topic mention currently investigate especially application classroom textbook setting © 2007 National Council,efficient base sumscore conduct skill diagnosis,0.027491377617644072,0.02749283552545349,0.027755358686691066,0.8896721380536112,0.027588290116600106,0.0013561361195348698,0.10565944985921,0.0,0.011899675703849604,0.0
Gorin J.S.,Manipulating processing difficulty of reading comprehension questions: The feasibility of verbal item generation,2005,42,"Based on a previously validated cognitive processing model of reading comprehension, this study experimentally examines potential generative components of text-based multiple-choice reading comprehension test questions. Previous research (Embretson & Wetzel, 1987; Gorin & Embretson, 2005; Sheehan & Ginther, 2001) shows text encoding and decision processes account for significant proportions of variance in item difficulties. In the current study, Linear Logistic Latent Trait Model (LLTM; Fischer, 1973) parameter estimates of experimentally manipulated items are examined to further verify the impact of encoding and decision processes on item difficulty. Results show that manipulation of some passage features, such as increased use of negative wording, significantly increases item difficulty in some cases, whereas others, such as altering the order of information presentation in a passage, did not significantly affect item difficulty, but did affect reaction time. These results suggest that reliable changes in difficulty and response time through algorithmic manipulation of certain task features is feasible. However, non-significant results for several manipulations highlight potential challenges to item generation in establishing direct links between theoretically relevant item features and individual item processing. Further examination of these relationships will be informative to item writers as well as test developers interested in the feasibility of item generation as an assessment tool.",Manipulating processing difficulty of reading comprehension questions: The feasibility of verbal item generation,"Based on a previously validated cognitive processing model of reading comprehension, this study experimentally examines potential generative components of text-based multiple-choice reading comprehension test questions. Previous research (Embretson & Wetzel, 1987; Gorin & Embretson, 2005; Sheehan & Ginther, 2001) shows text encoding and decision processes account for significant proportions of variance in item difficulties. In the current study, Linear Logistic Latent Trait Model (LLTM; Fischer, 1973) parameter estimates of experimentally manipulated items are examined to further verify the impact of encoding and decision processes on item difficulty. Results show that manipulation of some passage features, such as increased use of negative wording, significantly increases item difficulty in some cases, whereas others, such as altering the order of information presentation in a passage, did not significantly affect item difficulty, but did affect reaction time. These results suggest that reliable changes in difficulty and response time through algorithmic manipulation of certain task features is feasible. However, non-significant results for several manipulations highlight potential challenges to item generation in establishing direct links between theoretically relevant item features and individual item processing. Further examination of these relationships will be informative to item writers as well as test developers interested in the feasibility of item generation as an assessment tool.","['base', 'previously', 'validate', 'cognitive', 'processing', 'read', 'comprehension', 'study', 'experimentally', 'examine', 'potential', 'generative', 'component', 'textbase', 'multiplechoice', 'reading', 'comprehension', 'test', 'question', 'previous', 'research', 'Embretson', 'Wetzel', '1987', 'Gorin', 'Embretson', '2005', 'Sheehan', 'Ginther', '2001', 'text', 'encoding', 'decision', 'process', 'account', 'significant', 'proportion', 'variance', 'item', 'difficulty', 'current', 'study', 'Linear', 'Logistic', 'Latent', 'Trait', 'lltm', 'Fischer', '1973', 'parameter', 'estimate', 'experimentally', 'manipulate', 'item', 'examine', 'far', 'verify', 'impact', 'encoding', 'decision', 'process', 'item', 'difficulty', 'result', 'manipulation', 'passage', 'feature', 'increase', 'negative', 'wording', 'significantly', 'increase', 'item', 'difficulty', 'case', 'alter', 'order', 'information', 'presentation', 'passage', 'significantly', 'affect', 'item', 'difficulty', 'affect', 'reaction', 'time', 'result', 'suggest', 'reliable', 'change', 'difficulty', 'response', 'time', 'algorithmic', 'manipulation', 'certain', 'task', 'feature', 'feasible', 'nonsignificant', 'result', 'manipulation', 'highlight', 'potential', 'challenge', 'item', 'generation', 'establish', 'direct', 'link', 'theoretically', 'relevant', 'item', 'feature', 'individual', 'item', 'process', 'examination', 'relationship', 'informative', 'item', 'writer', 'test', 'developer', 'interested', 'feasibility', 'item', 'generation', 'assessment', 'tool']","['manipulate', 'processing', 'difficulty', 'read', 'comprehension', 'question', 'feasibility', 'verbal', 'item', 'generation']",base previously validate cognitive processing read comprehension study experimentally examine potential generative component textbase multiplechoice reading comprehension test question previous research Embretson Wetzel 1987 Gorin Embretson 2005 Sheehan Ginther 2001 text encoding decision process account significant proportion variance item difficulty current study Linear Logistic Latent Trait lltm Fischer 1973 parameter estimate experimentally manipulate item examine far verify impact encoding decision process item difficulty result manipulation passage feature increase negative wording significantly increase item difficulty case alter order information presentation passage significantly affect item difficulty affect reaction time result suggest reliable change difficulty response time algorithmic manipulation certain task feature feasible nonsignificant result manipulation highlight potential challenge item generation establish direct link theoretically relevant item feature individual item process examination relationship informative item writer test developer interested feasibility item generation assessment tool,manipulate processing difficulty read comprehension question feasibility verbal item generation,0.020822213949399503,0.020825556450126726,0.02123296136615119,0.9147947713636418,0.022324496870680728,0.001235217610552591,0.013147094408500743,0.0,0.08230229685592581,0.008512735801344103
Lamprianou I.; Boyle B.,Accuracy of measurement in the context of mathematics National Curriculum tests in England for ethnic minority pupils and pupils who speak English as an additional language,2004,41,"Research has suggested that inappropriate or misfitting response patterns may have detrimental effects on the quality and validity of measurement. It has been suggested that factors like language and ethnic background are related to the generation of misfitting response patterns, but the empirical research on this is rather poor. This research analyzes data from three testing cycles of the National Curriculum tests in mathematics in England using the Rasch model. It was found that pupils having English as an additional language and pupils belonging to ethnic minorities are significantly more likely to generate aberrant response patterns. However, within the groups of pupils belonging to ethnic minorities, those who speak English as an additional language are not significantly more likely to generate misfitting response patterns. This may indicate that the ethnic background effect is more significant than the effect of the first language spoken. The results suggest that pupils having English as an additional language and pupils belonging to ethnic minorities are mismeasured significantly more than the remainder of pupils by taking the mathematics National Curriculum tests. More research is needed to generalize the results to other subjects and contexts.",Accuracy of measurement in the context of mathematics National Curriculum tests in England for ethnic minority pupils and pupils who speak English as an additional language,"Research has suggested that inappropriate or misfitting response patterns may have detrimental effects on the quality and validity of measurement. It has been suggested that factors like language and ethnic background are related to the generation of misfitting response patterns, but the empirical research on this is rather poor. This research analyzes data from three testing cycles of the National Curriculum tests in mathematics in England using the Rasch model. It was found that pupils having English as an additional language and pupils belonging to ethnic minorities are significantly more likely to generate aberrant response patterns. However, within the groups of pupils belonging to ethnic minorities, those who speak English as an additional language are not significantly more likely to generate misfitting response patterns. This may indicate that the ethnic background effect is more significant than the effect of the first language spoken. The results suggest that pupils having English as an additional language and pupils belonging to ethnic minorities are mismeasured significantly more than the remainder of pupils by taking the mathematics National Curriculum tests. More research is needed to generalize the results to other subjects and contexts.","['research', 'suggest', 'inappropriate', 'misfit', 'response', 'pattern', 'detrimental', 'effect', 'quality', 'validity', 'suggest', 'factor', 'like', 'language', 'ethnic', 'background', 'relate', 'generation', 'misfit', 'response', 'pattern', 'empirical', 'research', 'poor', 'research', 'analyze', 'datum', 'testing', 'cycle', 'National', 'Curriculum', 'test', 'mathematic', 'England', 'Rasch', 'find', 'pupil', 'English', 'additional', 'language', 'pupil', 'belong', 'ethnic', 'minority', 'significantly', 'likely', 'generate', 'aberrant', 'response', 'pattern', 'group', 'pupil', 'belong', 'ethnic', 'minority', 'speak', 'English', 'additional', 'language', 'significantly', 'likely', 'generate', 'misfitting', 'response', 'pattern', 'indicate', 'ethnic', 'background', 'effect', 'significant', 'effect', 'language', 'speak', 'result', 'suggest', 'pupil', 'English', 'additional', 'language', 'pupil', 'belong', 'ethnic', 'minority', 'mismeasure', 'significantly', 'remainder', 'pupil', 'mathematic', 'National', 'Curriculum', 'test', 'More', 'research', 'need', 'generalize', 'result', 'subject', 'context']","['accuracy', 'context', 'mathematics', 'National', 'Curriculum', 'test', 'England', 'ethnic', 'minority', 'pupil', 'pupil', 'speak', 'English', 'additional', 'language']",research suggest inappropriate misfit response pattern detrimental effect quality validity suggest factor like language ethnic background relate generation misfit response pattern empirical research poor research analyze datum testing cycle National Curriculum test mathematic England Rasch find pupil English additional language pupil belong ethnic minority significantly likely generate aberrant response pattern group pupil belong ethnic minority speak English additional language significantly likely generate misfitting response pattern indicate ethnic background effect significant effect language speak result suggest pupil English additional language pupil belong ethnic minority mismeasure significantly remainder pupil mathematic National Curriculum test More research need generalize result subject context,accuracy context mathematics National Curriculum test England ethnic minority pupil pupil speak English additional language,0.030398234869858057,0.03039949224103368,0.030833297672051767,0.877814551848543,0.030554423368513557,0.008994345784954606,0.019469629688275317,0.0,0.01222631264772124,0.028000934753162062
Yao L.; Boughton K.,Multidimensional linking for tests with mixed item types,2009,46,"Numerous assessments contain a mixture of multiple choice (MC) and constructed response (CR) item types and many have been found to measure more than one trait. Thus, there is a need for multidimensional dichotomous and polytomous item response theory (IRT) modeling solutions, including multidimensional linking software. For example, multidimensional item response theory (MIRT) may have a promising future in subscale score proficiency estimation, leading toward a more diagnostic orientation, which requires the linking of these subscale scores across different forms and populations. Several multidimensional linking studies can be found in the literature; however, none have used a combination of MC and CR item types. Thus, this research explores multidimensional linking accuracy for tests composed of both MC and CR items using a matching test characteristic/response function approach. The two-dimensional simulation study presented here used real data-derived parameters from a large-scale statewide assessment with two subscale scores for diagnostic profiling purposes, under varying conditions of anchor set lengths (6, 8, 16, 32, 60), across 10 population distributions, with a mixture of simple versus complex structured items, using a sample size of 3,000. It was found that for a well chosen anchor set, the parameters recovered well after equating across all populations, even for anchor sets composed of as few as six items. © 2009 by the National Council on Measurement in Education.",Multidimensional linking for tests with mixed item types,"Numerous assessments contain a mixture of multiple choice (MC) and constructed response (CR) item types and many have been found to measure more than one trait. Thus, there is a need for multidimensional dichotomous and polytomous item response theory (IRT) modeling solutions, including multidimensional linking software. For example, multidimensional item response theory (MIRT) may have a promising future in subscale score proficiency estimation, leading toward a more diagnostic orientation, which requires the linking of these subscale scores across different forms and populations. Several multidimensional linking studies can be found in the literature; however, none have used a combination of MC and CR item types. Thus, this research explores multidimensional linking accuracy for tests composed of both MC and CR items using a matching test characteristic/response function approach. The two-dimensional simulation study presented here used real data-derived parameters from a large-scale statewide assessment with two subscale scores for diagnostic profiling purposes, under varying conditions of anchor set lengths (6, 8, 16, 32, 60), across 10 population distributions, with a mixture of simple versus complex structured items, using a sample size of 3,000. It was found that for a well chosen anchor set, the parameters recovered well after equating across all populations, even for anchor sets composed of as few as six items. © 2009 by the National Council on Measurement in Education.","['numerous', 'assessment', 'contain', 'mixture', 'multiple', 'choice', 'MC', 'construct', 'response', 'CR', 'item', 'type', 'find', 'measure', 'trait', 'need', 'multidimensional', 'dichotomous', 'polytomous', 'item', 'response', 'theory', 'IRT', 'modeling', 'solution', 'include', 'multidimensional', 'link', 'software', 'example', 'multidimensional', 'item', 'response', 'theory', 'MIRT', 'promising', 'future', 'subscale', 'score', 'proficiency', 'estimation', 'lead', 'diagnostic', 'orientation', 'require', 'linking', 'subscale', 'score', 'different', 'form', 'population', 'multidimensional', 'link', 'study', 'find', 'literature', 'combination', 'MC', 'CR', 'item', 'type', 'research', 'explore', 'multidimensional', 'link', 'accuracy', 'test', 'compose', 'MC', 'CR', 'item', 'match', 'test', 'characteristicresponse', 'function', 'approach', 'twodimensional', 'simulation', 'study', 'present', 'real', 'dataderived', 'parameter', 'largescale', 'statewide', 'assessment', 'subscale', 'score', 'diagnostic', 'profiling', 'purpose', 'vary', 'condition', 'anchor', 'set', 'length', '6', '8', '16', '32', '60', '10', 'population', 'distribution', 'mixture', 'simple', 'versus', 'complex', 'structured', 'item', 'sample', 'size', '3000', 'find', 'choose', 'anchor', 'set', 'parameter', 'recover', 'equate', 'population', 'anchor', 'set', 'compose', 'item', '©', '2009', 'National', 'Council']","['multidimensional', 'link', 'test', 'mixed', 'item', 'type']",numerous assessment contain mixture multiple choice MC construct response CR item type find measure trait need multidimensional dichotomous polytomous item response theory IRT modeling solution include multidimensional link software example multidimensional item response theory MIRT promising future subscale score proficiency estimation lead diagnostic orientation require linking subscale score different form population multidimensional link study find literature combination MC CR item type research explore multidimensional link accuracy test compose MC CR item match test characteristicresponse function approach twodimensional simulation study present real dataderived parameter largescale statewide assessment subscale score diagnostic profiling purpose vary condition anchor set length 6 8 16 32 60 10 population distribution mixture simple versus complex structured item sample size 3000 find choose anchor set parameter recover equate population anchor set compose item © 2009 National Council,multidimensional link test mixed item type,0.022160803289304074,0.02216191346676246,0.022546233622949885,0.8436035162968363,0.08952753332414715,0.005508762689832875,0.03943225929738724,0.04881821637408311,0.06538097611745894,0.0
Gierl M.J.; Cui Y.; Zhou J.,Reliability and attribute-based scoring in cognitive diagnostic assessment,2009,46,"The attribute hierarchy method (AHM) is a psychometric procedure for classifying examinees' test item responses into a set of structured attribute patterns associated with different components from a cognitive model of task performance. Results from an AHM analysis yield information on examinees' cognitive strengths and weaknesses. Hence, the AHM can be used for cognitive diagnostic assessment. The purpose of this study is to introduce and evaluate a new concept for assessing attribute reliability using the ratio of true score variance to observed score variance on items that probe specific cognitive attributes. This reliability procedure is evaluated and illustrated using both simulated data and student response data from a sample of algebra items taken from the March 2005 administration of the SAT. The reliability of diagnostic scores and the implications for practice are also discussed. © 2009 by the National Council on Measurement in Education.",Reliability and attribute-based scoring in cognitive diagnostic assessment,"The attribute hierarchy method (AHM) is a psychometric procedure for classifying examinees' test item responses into a set of structured attribute patterns associated with different components from a cognitive model of task performance. Results from an AHM analysis yield information on examinees' cognitive strengths and weaknesses. Hence, the AHM can be used for cognitive diagnostic assessment. The purpose of this study is to introduce and evaluate a new concept for assessing attribute reliability using the ratio of true score variance to observed score variance on items that probe specific cognitive attributes. This reliability procedure is evaluated and illustrated using both simulated data and student response data from a sample of algebra items taken from the March 2005 administration of the SAT. The reliability of diagnostic scores and the implications for practice are also discussed. © 2009 by the National Council on Measurement in Education.","['attribute', 'hierarchy', 'method', 'AHM', 'psychometric', 'procedure', 'classify', 'examine', 'test', 'item', 'response', 'set', 'structured', 'attribute', 'pattern', 'associate', 'different', 'component', 'cognitive', 'task', 'performance', 'result', 'AHM', 'analysis', 'yield', 'information', 'examinee', 'cognitive', 'strength', 'weakness', 'AHM', 'cognitive', 'diagnostic', 'assessment', 'purpose', 'study', 'introduce', 'evaluate', 'new', 'concept', 'assess', 'attribute', 'reliability', 'ratio', 'true', 'score', 'variance', 'observed', 'score', 'variance', 'item', 'probe', 'specific', 'cognitive', 'attribute', 'reliability', 'procedure', 'evaluate', 'illustrate', 'simulate', 'datum', 'student', 'response', 'datum', 'sample', 'algebra', 'item', 'March', '2005', 'administration', 'SAT', 'reliability', 'diagnostic', 'score', 'implication', 'practice', 'discuss', '©', '2009', 'National', 'Council']","['reliability', 'attributebase', 'scoring', 'cognitive', 'diagnostic', 'assessment']",attribute hierarchy method AHM psychometric procedure classify examine test item response set structured attribute pattern associate different component cognitive task performance result AHM analysis yield information examinee cognitive strength weakness AHM cognitive diagnostic assessment purpose study introduce evaluate new concept assess attribute reliability ratio true score variance observed score variance item probe specific cognitive attribute reliability procedure evaluate illustrate simulate datum student response datum sample algebra item March 2005 administration SAT reliability diagnostic score implication practice discuss © 2009 National Council,reliability attributebase scoring cognitive diagnostic assessment,0.0265921081008054,0.026585937888203685,0.026584523627291197,0.893531644699461,0.026705785684238714,0.0,0.2514957932430877,0.0,0.0,0.0
Betebenner D.W.; Shang Y.; Xiang Y.; Zhao Y.; Yue X.,The impact of performance level misclassification on the accuracy and precision of percent at performance level measures,2008,45,"No Child Left Behind (NCLB) performance mandates, embedded within state accountability systems, focus school AYP (adequate yearly progress) compliance squarely on the percentage of students at or above proficient. The singular importance of this quantity for decision-making purposes has initiated extensive research into percent proficient as a measure of school quality. In particular, technical discussions have scrutinized the impact of sampling, measurement, and other sources of error on percent proficient statistics. In this article, we challenge the received orthodoxy that measurement error associated with individual students' scores is inconsequential for aggregate percent proficient statistics. Synthesizing current classification accuracy research with techniques from randomized response designs, we establish results which specify the extent to which measurement error - manifest as performance level misclassifications - produces bias and increases error variability for percent at performance level statistics. The results have direct relevance for the design of coherent and fair accountability systems based upon assessment outcomes. © 2008 by the National Council on Measurement in Education.",The impact of performance level misclassification on the accuracy and precision of percent at performance level measures,"No Child Left Behind (NCLB) performance mandates, embedded within state accountability systems, focus school AYP (adequate yearly progress) compliance squarely on the percentage of students at or above proficient. The singular importance of this quantity for decision-making purposes has initiated extensive research into percent proficient as a measure of school quality. In particular, technical discussions have scrutinized the impact of sampling, measurement, and other sources of error on percent proficient statistics. In this article, we challenge the received orthodoxy that measurement error associated with individual students' scores is inconsequential for aggregate percent proficient statistics. Synthesizing current classification accuracy research with techniques from randomized response designs, we establish results which specify the extent to which measurement error - manifest as performance level misclassifications - produces bias and increases error variability for percent at performance level statistics. The results have direct relevance for the design of coherent and fair accountability systems based upon assessment outcomes. © 2008 by the National Council on Measurement in Education.","['child', 'leave', 'nclb', 'performance', 'mandate', 'embed', 'state', 'accountability', 'system', 'focus', 'school', 'AYP', 'adequate', 'yearly', 'progress', 'compliance', 'squarely', 'percentage', 'student', 'proficient', 'singular', 'importance', 'quantity', 'decisionmake', 'purpose', 'initiate', 'extensive', 'research', 'percent', 'proficient', 'measure', 'school', 'quality', 'particular', 'technical', 'discussion', 'scrutinize', 'impact', 'sample', 'source', 'error', 'percent', 'proficient', 'statistic', 'article', 'challenge', 'receive', 'orthodoxy', 'error', 'associate', 'individual', 'student', 'score', 'inconsequential', 'aggregate', 'percent', 'proficient', 'statistic', 'synthesize', 'current', 'classification', 'accuracy', 'research', 'technique', 'randomize', 'response', 'design', 'establish', 'result', 'specify', 'extent', 'error', 'manifest', 'performance', 'level', 'misclassification', 'produce', 'bias', 'increase', 'error', 'variability', 'percent', 'performance', 'level', 'statistic', 'result', 'direct', 'relevance', 'design', 'coherent', 'fair', 'accountability', 'system', 'base', 'assessment', 'outcome', '©', '2008', 'National', 'Council']","['impact', 'performance', 'level', 'misclassification', 'accuracy', 'precision', 'percent', 'performance', 'level', 'measure']",child leave nclb performance mandate embed state accountability system focus school AYP adequate yearly progress compliance squarely percentage student proficient singular importance quantity decisionmake purpose initiate extensive research percent proficient measure school quality particular technical discussion scrutinize impact sample source error percent proficient statistic article challenge receive orthodoxy error associate individual student score inconsequential aggregate percent proficient statistic synthesize current classification accuracy research technique randomize response design establish result specify extent error manifest performance level misclassification produce bias increase error variability percent performance level statistic result direct relevance design coherent fair accountability system base assessment outcome © 2008 National Council,impact performance level misclassification accuracy precision percent performance level measure,0.023121962518782804,0.023123304892152054,0.02314372835752096,0.9071971686075408,0.02341383562400336,0.0077928433826098655,0.038337229445230325,0.0035309457780367034,0.0012542170939177889,0.06371592181918684
Muckle T.J.; Karabatsos G.,Hierarchical generalized linear models for the analysis of judge ratings,2009,46,"It is known that the Rasch model is a special two-level hierarchical generalized linear model (HGLM). This article demonstrates that the many-faceted Rasch model (MFRM) is also a special case of the two-level HGLM, with a random intercept representing examinee ability on a test, and fixed effects for the test items, judges, and possibly other facets. This perspective suggests useful modeling extensions of the MFRM. For example, in the HGLM framework it is possible to model random effects for items and judges in order to assess their stability across examinees. The MFRM can also be extended so that item difficulty and judge severity are modeled as functions of examinee characteristics (covariates), for the purposes of detecting differential item functioning and differential rater functioning. Practical illustrations of the HGLM are presented through the analysis of simulated and real judge-mediated data sets involving ordinal responses. © 2009 by the National Council on Measurement in Education.",Hierarchical generalized linear models for the analysis of judge ratings,"It is known that the Rasch model is a special two-level hierarchical generalized linear model (HGLM). This article demonstrates that the many-faceted Rasch model (MFRM) is also a special case of the two-level HGLM, with a random intercept representing examinee ability on a test, and fixed effects for the test items, judges, and possibly other facets. This perspective suggests useful modeling extensions of the MFRM. For example, in the HGLM framework it is possible to model random effects for items and judges in order to assess their stability across examinees. The MFRM can also be extended so that item difficulty and judge severity are modeled as functions of examinee characteristics (covariates), for the purposes of detecting differential item functioning and differential rater functioning. Practical illustrations of the HGLM are presented through the analysis of simulated and real judge-mediated data sets involving ordinal responses. © 2009 by the National Council on Measurement in Education.","['know', 'Rasch', 'special', 'twolevel', 'hierarchical', 'generalized', 'linear', 'HGLM', 'article', 'demonstrate', 'manyfaceted', 'Rasch', 'MFRM', 'special', 'case', 'twolevel', 'HGLM', 'random', 'intercept', 'represent', 'examinee', 'ability', 'test', 'fix', 'effect', 'test', 'item', 'judge', 'possibly', 'facet', 'perspective', 'suggest', 'useful', 'modeling', 'extension', 'MFRM', 'example', 'HGLM', 'framework', 'possible', 'random', 'effect', 'item', 'judge', 'order', 'assess', 'stability', 'examine', 'MFRM', 'extend', 'item', 'difficulty', 'judge', 'severity', 'function', 'examinee', 'characteristic', 'covariate', 'purpose', 'detect', 'differential', 'item', 'function', 'differential', 'rater', 'function', 'practical', 'illustration', 'HGLM', 'present', 'analysis', 'simulated', 'real', 'judgemediate', 'data', 'set', 'involve', 'ordinal', 'response', '©', '2009', 'National', 'Council']","['hierarchical', 'generalize', 'linear', 'analysis', 'judge', 'rating']",know Rasch special twolevel hierarchical generalized linear HGLM article demonstrate manyfaceted Rasch MFRM special case twolevel HGLM random intercept represent examinee ability test fix effect test item judge possibly facet perspective suggest useful modeling extension MFRM example HGLM framework possible random effect item judge order assess stability examine MFRM extend item difficulty judge severity function examinee characteristic covariate purpose detect differential item function differential rater function practical illustration HGLM present analysis simulated real judgemediate data set involve ordinal response © 2009 National Council,hierarchical generalize linear analysis judge rating,0.026445548128585503,0.026446417134783417,0.026511657999253743,0.45479195146718904,0.46580442527018834,0.027387732388863592,0.009554793953199086,0.003063889710919751,0.039062585969900386,0.031524319965148345
Li Y.H.; Lissitz R.W.,Applications of the analytically derived asymptotic standard errors of item response theory item parameter estimates,2004,41,"The analytically derived asymptotic standard errors (SEs) of maximum likelihood (ML) item estimates can be approximated by a mathematical function without examinees' responses to test items, and the empirically determined SEs of marginal maximum likelihood estimation (MMLE)/Bayesian item estimates can be obtained when the same set of items is repeatedly estimated from the simulation (or resampling) test data. The latter method will result in rather stable and accurate SE estimates as the number of replications increases, but requires cumbersome and time-consuming calculations. Instead of using the empirically determined method, the adequacy of using the analytical-based method in predicting the SEs for item parameter estimates was examined by comparing results produced from both approaches. The results indicated that the SEs yielded from both approaches were, in most cases, very similar, especially when they were applied to a generalized partial credit model. This finding encourages test practitioners and researchers to apply the analytically asymptotic SEs of item estimates to the context of item-linking studies, as well as to the method of quantifying the SEs of equating scores for the item response theory (IRT) true-score method. Three-dimensional graphical presentation for the analytical SEs of item estimates as the bivariate function of item difficulty together with item discrimination was also provided for a better understanding of several frequently used IRT models.",Applications of the analytically derived asymptotic standard errors of item response theory item parameter estimates,"The analytically derived asymptotic standard errors (SEs) of maximum likelihood (ML) item estimates can be approximated by a mathematical function without examinees' responses to test items, and the empirically determined SEs of marginal maximum likelihood estimation (MMLE)/Bayesian item estimates can be obtained when the same set of items is repeatedly estimated from the simulation (or resampling) test data. The latter method will result in rather stable and accurate SE estimates as the number of replications increases, but requires cumbersome and time-consuming calculations. Instead of using the empirically determined method, the adequacy of using the analytical-based method in predicting the SEs for item parameter estimates was examined by comparing results produced from both approaches. The results indicated that the SEs yielded from both approaches were, in most cases, very similar, especially when they were applied to a generalized partial credit model. This finding encourages test practitioners and researchers to apply the analytically asymptotic SEs of item estimates to the context of item-linking studies, as well as to the method of quantifying the SEs of equating scores for the item response theory (IRT) true-score method. Three-dimensional graphical presentation for the analytical SEs of item estimates as the bivariate function of item difficulty together with item discrimination was also provided for a better understanding of several frequently used IRT models.","['analytically', 'derive', 'asymptotic', 'standard', 'error', 'se', 'maximum', 'likelihood', 'ML', 'item', 'estimate', 'approximate', 'mathematical', 'function', 'examine', 'response', 'test', 'item', 'empirically', 'determined', 'se', 'marginal', 'maximum', 'likelihood', 'estimation', 'mmlebayesian', 'item', 'estimate', 'obtain', 'set', 'item', 'repeatedly', 'estimate', 'simulation', 'resampling', 'test', 'datum', 'method', 'result', 'stable', 'accurate', 'SE', 'estimate', 'number', 'replication', 'increase', 'require', 'cumbersome', 'timeconsuming', 'calculation', 'instead', 'empirically', 'determined', 'method', 'adequacy', 'analyticalbase', 'method', 'predict', 'se', 'item', 'parameter', 'estimate', 'examine', 'compare', 'result', 'produce', 'approach', 'result', 'indicate', 'se', 'yield', 'approach', 'case', 'similar', 'especially', 'apply', 'generalized', 'partial', 'credit', 'finding', 'encourage', 'test', 'practitioner', 'researcher', 'apply', 'analytically', 'asymptotic', 'se', 'item', 'estimate', 'context', 'itemlinke', 'study', 'method', 'quantify', 'se', 'equate', 'score', 'item', 'response', 'theory', 'IRT', 'truescore', 'method', 'threedimensional', 'graphical', 'presentation', 'analytical', 'se', 'item', 'estimate', 'bivariate', 'function', 'item', 'difficulty', 'item', 'discrimination', 'provide', 'understanding', 'frequently', 'IRT']","['application', 'analytically', 'derive', 'asymptotic', 'standard', 'error', 'item', 'response', 'theory', 'item', 'parameter', 'estimate']",analytically derive asymptotic standard error se maximum likelihood ML item estimate approximate mathematical function examine response test item empirically determined se marginal maximum likelihood estimation mmlebayesian item estimate obtain set item repeatedly estimate simulation resampling test datum method result stable accurate SE estimate number replication increase require cumbersome timeconsuming calculation instead empirically determined method adequacy analyticalbase method predict se item parameter estimate examine compare result produce approach result indicate se yield approach case similar especially apply generalized partial credit finding encourage test practitioner researcher apply analytically asymptotic se item estimate context itemlinke study method quantify se equate score item response theory IRT truescore method threedimensional graphical presentation analytical se item estimate bivariate function item difficulty item discrimination provide understanding frequently IRT,application analytically derive asymptotic standard error item response theory item parameter estimate,0.026906492689348905,0.026907951273870512,0.026926296224870205,0.8921363958428347,0.02712286396907566,0.0,0.0067476651955813,0.01928407128841791,0.09924229530720988,0.0
Armstrong R.D.; Shi M.,Model-free CUSUM methods for person fit,2009,46,This article demonstrates the use of a new class of model-free cumulative sum (CUSUM) statistics to detect person fit given the responses to a linear test. The fundamental statistic being accumulated is the likelihood ratio of two probabilities. The detection performance of this CUSUM scheme is compared to other model-free person-fit statistics found in the literature as well as an adaptation of another CUSUM approach. The study used both simulated responses and real response data from a large-scale standardized admission test. © 2009 by the National Council on Measurement in Education.,,This article demonstrates the use of a new class of model-free cumulative sum (CUSUM) statistics to detect person fit given the responses to a linear test. The fundamental statistic being accumulated is the likelihood ratio of two probabilities. The detection performance of this CUSUM scheme is compared to other model-free person-fit statistics found in the literature as well as an adaptation of another CUSUM approach. The study used both simulated responses and real response data from a large-scale standardized admission test. © 2009 by the National Council on Measurement in Education.,"['article', 'demonstrate', 'new', 'class', 'modelfree', 'cumulative', 'sum', 'CUSUM', 'statistic', 'detect', 'person', 'fit', 'response', 'linear', 'test', 'fundamental', 'statistic', 'accumulate', 'likelihood', 'ratio', 'probability', 'detection', 'performance', 'CUSUM', 'scheme', 'compare', 'modelfree', 'personfit', 'statistic', 'find', 'literature', 'adaptation', 'CUSUM', 'approach', 'study', 'simulate', 'response', 'real', 'response', 'datum', 'largescale', 'standardized', 'admission', 'test', '©', '2009', 'National', 'Council']",,article demonstrate new class modelfree cumulative sum CUSUM statistic detect person fit response linear test fundamental statistic accumulate likelihood ratio probability detection performance CUSUM scheme compare modelfree personfit statistic find literature adaptation CUSUM approach study simulate response real response datum largescale standardized admission test © 2009 National Council,,0.03172554696184088,0.031726213043546975,0.03176069448202777,0.8729118940345127,0.03187565147807163,0.026153566903138997,0.0306015350831599,0.0,0.026140117237656017,0.00020338287225552968
Almond R.G.; DiBello L.V.; Moulder B.; Zapata-Rivera J.-D.,Modeling diagnostic assessments with bayesian networks,2007,44,"This paper defines Bayesian network models and examines their applications to IRT-based cognitive diagnostic modeling. These models are especially suited to building inference engines designed to be synchronous with the finer grained student models that arise in skills diagnostic assessment. Aspects of the theory and use of Bayesian network models are reviewed, as they affect applications to diagnostic assessment. The paper discusses how Bayesian network models are set up with expert information, improved and calibrated from data, and deployed as evidence-based inference engines. Aimed at a general educational measurement audience, the paper illustrates the flexibility and capabilities of Bayesian networks through a series of concrete examples, and without extensive technical detail. Examples are provided of proficiency spaces with direct dependencies among proficiency nodes, and of customized evidence models for complex tasks. This paper is intended to motivate educational measurement practitioners to learn more about Bayesian networks from the research literature, to acquire readily available Bayesian network software, to perform studies with real and simulated data sets, and to look for opportunities in educational settings that may benefit from diagnostic assessment fueled by Bayesian network modeling. © 2007 by the National Council on Measurement in Education.",Modeling diagnostic assessments with bayesian networks,"This paper defines Bayesian network models and examines their applications to IRT-based cognitive diagnostic modeling. These models are especially suited to building inference engines designed to be synchronous with the finer grained student models that arise in skills diagnostic assessment. Aspects of the theory and use of Bayesian network models are reviewed, as they affect applications to diagnostic assessment. The paper discusses how Bayesian network models are set up with expert information, improved and calibrated from data, and deployed as evidence-based inference engines. Aimed at a general educational measurement audience, the paper illustrates the flexibility and capabilities of Bayesian networks through a series of concrete examples, and without extensive technical detail. Examples are provided of proficiency spaces with direct dependencies among proficiency nodes, and of customized evidence models for complex tasks. This paper is intended to motivate educational measurement practitioners to learn more about Bayesian networks from the research literature, to acquire readily available Bayesian network software, to perform studies with real and simulated data sets, and to look for opportunities in educational settings that may benefit from diagnostic assessment fueled by Bayesian network modeling. © 2007 by the National Council on Measurement in Education.","['paper', 'define', 'bayesian', 'network', 'examine', 'application', 'irtbase', 'cognitive', 'diagnostic', 'modeling', 'especially', 'suit', 'build', 'inference', 'engine', 'design', 'synchronous', 'finer', 'grain', 'student', 'arise', 'skill', 'diagnostic', 'assessment', 'Aspects', 'theory', 'bayesian', 'network', 'review', 'affect', 'application', 'diagnostic', 'assessment', 'paper', 'discuss', 'bayesian', 'network', 'set', 'expert', 'information', 'improve', 'calibrate', 'datum', 'deploy', 'evidencebase', 'inference', 'engine', 'aim', 'general', 'educational', 'audience', 'paper', 'illustrate', 'flexibility', 'capability', 'bayesian', 'network', 'series', 'concrete', 'example', 'extensive', 'technical', 'detail', 'example', 'provide', 'proficiency', 'space', 'direct', 'dependency', 'proficiency', 'node', 'customize', 'evidence', 'complex', 'task', 'paper', 'intend', 'motivate', 'educational', 'practitioner', 'learn', 'bayesian', 'network', 'research', 'literature', 'acquire', 'readily', 'available', 'bayesian', 'network', 'software', 'perform', 'study', 'real', 'simulated', 'data', 'set', 'look', 'opportunity', 'educational', 'setting', 'benefit', 'diagnostic', 'assessment', 'fuel', 'bayesian', 'network', '©', '2007', 'National', 'Council']","['diagnostic', 'assessment', 'bayesian', 'network']",paper define bayesian network examine application irtbase cognitive diagnostic modeling especially suit build inference engine design synchronous finer grain student arise skill diagnostic assessment Aspects theory bayesian network review affect application diagnostic assessment paper discuss bayesian network set expert information improve calibrate datum deploy evidencebase inference engine aim general educational audience paper illustrate flexibility capability bayesian network series concrete example extensive technical detail example provide proficiency space direct dependency proficiency node customize evidence complex task paper intend motivate educational practitioner learn bayesian network research literature acquire readily available bayesian network software perform study real simulated data set look opportunity educational setting benefit diagnostic assessment fuel bayesian network © 2007 National Council,diagnostic assessment bayesian network,0.025947875462872177,0.025942038778127157,0.02598918893593643,0.8961415880652387,0.02597930875782555,0.0,0.10448627841471285,0.0,0.0,0.0
Randall J.; Engelhard Jr. G.,Examining teacher grades using rasch measurement theory,2009,46,"In this study, we present an approach to questionnaire design within educational research based on Guttman's mapping sentences and Many-Facet Rasch Measurement Theory. We designed a 54-item questionnaire using Guttman's mapping sentences to examine the grading practices of teachers. Each item in the questionnaire represented a unique student scenario that was graded by teachers. Three focus groups of elementary (N = 5), middle (N = 4), and high school (N = 2) teachers examined the scenarios for clarity, comprehensiveness, and ease of understanding. Based on the suggestions of the focus groups, the revised questionnaires were completed by 516 public school teachers located in a major metropolitan county in the Southeast. The grades assigned by the teachers to the scenarios were analyzed using the FACETS computer program. The results of the analyses suggest that teachers primarily assign grades on the basis of student achievement as expected, although for some teachers other facets (ability, behavior, and effort) may play a role in final grade assignment. © 2009 by the National Council on Measurement in Education.",Examining teacher grades using rasch measurement theory,"In this study, we present an approach to questionnaire design within educational research based on Guttman's mapping sentences and Many-Facet Rasch Measurement Theory. We designed a 54-item questionnaire using Guttman's mapping sentences to examine the grading practices of teachers. Each item in the questionnaire represented a unique student scenario that was graded by teachers. Three focus groups of elementary (N = 5), middle (N = 4), and high school (N = 2) teachers examined the scenarios for clarity, comprehensiveness, and ease of understanding. Based on the suggestions of the focus groups, the revised questionnaires were completed by 516 public school teachers located in a major metropolitan county in the Southeast. The grades assigned by the teachers to the scenarios were analyzed using the FACETS computer program. The results of the analyses suggest that teachers primarily assign grades on the basis of student achievement as expected, although for some teachers other facets (ability, behavior, and effort) may play a role in final grade assignment. © 2009 by the National Council on Measurement in Education.","['study', 'present', 'approach', 'questionnaire', 'design', 'educational', 'research', 'base', 'Guttmans', 'mapping', 'sentence', 'ManyFacet', 'Rasch', 'Theory', 'design', '54item', 'questionnaire', 'Guttmans', 'mapping', 'sentence', 'examine', 'grade', 'practice', 'teacher', 'item', 'questionnaire', 'represent', 'unique', 'student', 'scenario', 'grade', 'teacher', 'focus', 'group', 'elementary', 'N', '5', 'middle', 'N', '4', 'high', 'school', 'N', '2', 'teacher', 'examine', 'scenario', 'clarity', 'comprehensiveness', 'ease', 'understanding', 'base', 'suggestion', 'focus', 'group', 'revise', 'questionnaire', 'complete', '516', 'public', 'school', 'teacher', 'locate', 'major', 'metropolitan', 'county', 'Southeast', 'grade', 'assign', 'teacher', 'scenario', 'analyze', 'FACETS', 'computer', 'program', 'result', 'analysis', 'suggest', 'teacher', 'primarily', 'assign', 'grade', 'basis', 'student', 'achievement', 'expect', 'teacher', 'facet', 'ability', 'behavior', 'effort', 'play', 'role', 'final', 'grade', 'assignment', '©', '2009', 'National', 'Council']","['examine', 'teacher', 'grade', 'rasch', 'theory']",study present approach questionnaire design educational research base Guttmans mapping sentence ManyFacet Rasch Theory design 54item questionnaire Guttmans mapping sentence examine grade practice teacher item questionnaire represent unique student scenario grade teacher focus group elementary N 5 middle N 4 high school N 2 teacher examine scenario clarity comprehensiveness ease understanding base suggestion focus group revise questionnaire complete 516 public school teacher locate major metropolitan county Southeast grade assign teacher scenario analyze FACETS computer program result analysis suggest teacher primarily assign grade basis student achievement expect teacher facet ability behavior effort play role final grade assignment © 2009 National Council,examine teacher grade rasch theory,0.026464936186605226,0.0264510789968151,0.026480655622123525,0.028604894466435354,0.8919984347280208,0.0,0.0,0.0,0.0,0.16542702037467014
De La Torre J.,An empirically based method of Q-matrix validation for the DINA model: Development and applications,2008,45,"Most model fit analyses in cognitive diagnosis assume that a Q matrix is correct after it has been constructed, without verifying its appropriateness. Consequently, any model misfit attributable to the Q matrix cannot be addressed and remedied. To address this concern, this paper proposes an empirically based method of validating a Q matrix used in conjunction with the DINA model. The proposed method can be implemented with other considerations such as substantive information about the items, or expert knowledge about the domain, to produce a more integrative framework of Q-matrix validation. The paper presents the theoretical foundation for the proposed method, develops an algorithm for its practical implementation, and provides real and simulated data applications to examine its viability. Relevant issues regarding the implementation of the method are discussed. © 2008 by the National Council on Measurement in Education.",An empirically based method of Q-matrix validation for the DINA model: Development and applications,"Most model fit analyses in cognitive diagnosis assume that a Q matrix is correct after it has been constructed, without verifying its appropriateness. Consequently, any model misfit attributable to the Q matrix cannot be addressed and remedied. To address this concern, this paper proposes an empirically based method of validating a Q matrix used in conjunction with the DINA model. The proposed method can be implemented with other considerations such as substantive information about the items, or expert knowledge about the domain, to produce a more integrative framework of Q-matrix validation. The paper presents the theoretical foundation for the proposed method, develops an algorithm for its practical implementation, and provides real and simulated data applications to examine its viability. Relevant issues regarding the implementation of the method are discussed. © 2008 by the National Council on Measurement in Education.","['Most', 'fit', 'analysis', 'cognitive', 'diagnosis', 'assume', 'Q', 'matrix', 'correct', 'construct', 'verify', 'appropriateness', 'consequently', 'misfit', 'attributable', 'Q', 'matrix', 'address', 'remedied', 'address', 'concern', 'paper', 'propose', 'empirically', 'base', 'method', 'validate', 'Q', 'matrix', 'conjunction', 'DINA', 'propose', 'method', 'implement', 'consideration', 'substantive', 'information', 'item', 'expert', 'knowledge', 'domain', 'produce', 'integrative', 'framework', 'qmatrix', 'validation', 'paper', 'present', 'theoretical', 'foundation', 'propose', 'method', 'develop', 'algorithm', 'practical', 'implementation', 'provide', 'real', 'simulated', 'data', 'application', 'examine', 'viability', 'relevant', 'issue', 'regard', 'implementation', 'method', 'discuss', '©', '2008', 'National', 'Council']","['empirically', 'base', 'method', 'qmatrix', 'validation', 'DINA', 'Development', 'application']",Most fit analysis cognitive diagnosis assume Q matrix correct construct verify appropriateness consequently misfit attributable Q matrix address remedied address concern paper propose empirically base method validate Q matrix conjunction DINA propose method implement consideration substantive information item expert knowledge domain produce integrative framework qmatrix validation paper present theoretical foundation propose method develop algorithm practical implementation provide real simulated data application examine viability relevant issue regard implementation method discuss © 2008 National Council,empirically base method qmatrix validation DINA Development application,0.025768397579919913,0.025768862604547103,0.02576802592049405,0.8968650912987032,0.025829622596335797,0.0,0.06709079742968298,0.004911658868813688,0.03096842920086858,0.0
De La Torre J.; Karelitz T.M.,Impact of diagnosticity on the adequacy of models for cognitive diagnosis under a linear attribute structure: A simulation study,2009,46,"Compared to unidimensional item response models (IRMs), cognitive diagnostic models (CDMs) based on latent classes represent examinees' knowledge and item requirements using discrete structures. This study systematically examines the viability of retrofitting CDMs to IRM-based data with a linear attribute structure. The study utilizes a procedure to make the IRM and CDM frameworks comparable and investigates how estimation accuracy is affected by test diagnosticity and the match between the true and fitted models. The study shows that comparable results can be obtained when highly diagnostic IRM data are retrofitted with CDM, and vice versa, retrofitting CDMs to IRM-based data in some conditions can result in considerable examinee misclassification, and model fit indices provide limited indication of the accuracy of item parameter estimation and attribute classification. © 2009 by the National Council on Measurement in Education.",Impact of diagnosticity on the adequacy of models for cognitive diagnosis under a linear attribute structure: A simulation study,"Compared to unidimensional item response models (IRMs), cognitive diagnostic models (CDMs) based on latent classes represent examinees' knowledge and item requirements using discrete structures. This study systematically examines the viability of retrofitting CDMs to IRM-based data with a linear attribute structure. The study utilizes a procedure to make the IRM and CDM frameworks comparable and investigates how estimation accuracy is affected by test diagnosticity and the match between the true and fitted models. The study shows that comparable results can be obtained when highly diagnostic IRM data are retrofitted with CDM, and vice versa, retrofitting CDMs to IRM-based data in some conditions can result in considerable examinee misclassification, and model fit indices provide limited indication of the accuracy of item parameter estimation and attribute classification. © 2009 by the National Council on Measurement in Education.","['compare', 'unidimensional', 'item', 'response', 'IRMs', 'cognitive', 'diagnostic', 'CDMs', 'base', 'latent', 'class', 'represent', 'examine', 'knowledge', 'item', 'requirement', 'discrete', 'structure', 'study', 'systematically', 'examine', 'viability', 'retrofit', 'cdm', 'irmbase', 'datum', 'linear', 'attribute', 'structure', 'study', 'utilize', 'procedure', 'IRM', 'cdm', 'framework', 'comparable', 'investigate', 'estimation', 'accuracy', 'affect', 'test', 'diagnosticity', 'match', 'true', 'fit', 'study', 'comparable', 'result', 'obtain', 'highly', 'diagnostic', 'IRM', 'datum', 'retrofit', 'CDM', 'vice', 'versa', 'retrofit', 'CDMs', 'irmbase', 'datum', 'condition', 'result', 'considerable', 'examinee', 'misclassification', 'fit', 'index', 'provide', 'limited', 'indication', 'accuracy', 'item', 'parameter', 'estimation', 'attribute', 'classification', '©', '2009', 'National', 'Council']","['impact', 'diagnosticity', 'adequacy', 'cognitive', 'diagnosis', 'linear', 'attribute', 'structure', 'simulation', 'study']",compare unidimensional item response IRMs cognitive diagnostic CDMs base latent class represent examine knowledge item requirement discrete structure study systematically examine viability retrofit cdm irmbase datum linear attribute structure study utilize procedure IRM cdm framework comparable investigate estimation accuracy affect test diagnosticity match true fit study comparable result obtain highly diagnostic IRM datum retrofit CDM vice versa retrofit CDMs irmbase datum condition result considerable examinee misclassification fit index provide limited indication accuracy item parameter estimation attribute classification © 2009 National Council,impact diagnosticity adequacy cognitive diagnosis linear attribute structure simulation study,0.027083802267600713,0.02708475629485781,0.3345875568108934,0.5840942045809829,0.027149680045665158,0.0009777273865289244,0.08962216474841465,0.0,0.03062850032845218,0.0
Li Y.H.; Schafer W.D.,Increasing the homogeneity of CAT's item-exposure rates by minimizing or maximizing varied target functions while assembling shadow tests,2005,42,"A computerized adaptive testing (CAT) algorithm that has the potential to increase the homogeneity of CAT's item-exposure rates without significantly sacrificing the precision of ability estimates was proposed and assessed in the shadow-test (van der Linden & Reese, 1998) CAT context. This CAT algorithm was formed by a combination of maximizing or minimizing varied target functions while assembling shadow tests. There were four target functions to be separately used in the first, second, third, and fourth quarter test of CAT. The elements to be used in the four functions were associated with (a) a random number assigned to each item, (b) the absolute difference between an examinee's current ability estimate and an item difficulty, (c) the absolute difference between an examinee's current ability estimate and an optimum item difficulty, and (d) item information. The results indicated that this combined CAT fully utilized all the items in the pool, reduced the maximum exposure rates, and achieved more homogeneous exposure rates. Moreover, its precision in recovering ability estimates was similar to that of the maximum item-information method. The combined CAT method resulted in the best overall results compared with the other individual CAT item-selection methods. The findings from the combined CAT are encouraging. Future uses are discussed.",Increasing the homogeneity of CAT's item-exposure rates by minimizing or maximizing varied target functions while assembling shadow tests,"A computerized adaptive testing (CAT) algorithm that has the potential to increase the homogeneity of CAT's item-exposure rates without significantly sacrificing the precision of ability estimates was proposed and assessed in the shadow-test (van der Linden & Reese, 1998) CAT context. This CAT algorithm was formed by a combination of maximizing or minimizing varied target functions while assembling shadow tests. There were four target functions to be separately used in the first, second, third, and fourth quarter test of CAT. The elements to be used in the four functions were associated with (a) a random number assigned to each item, (b) the absolute difference between an examinee's current ability estimate and an item difficulty, (c) the absolute difference between an examinee's current ability estimate and an optimum item difficulty, and (d) item information. The results indicated that this combined CAT fully utilized all the items in the pool, reduced the maximum exposure rates, and achieved more homogeneous exposure rates. Moreover, its precision in recovering ability estimates was similar to that of the maximum item-information method. The combined CAT method resulted in the best overall results compared with the other individual CAT item-selection methods. The findings from the combined CAT are encouraging. Future uses are discussed.","['computerized', 'adaptive', 'testing', 'CAT', 'algorithm', 'potential', 'increase', 'homogeneity', 'cats', 'itemexposure', 'rate', 'significantly', 'sacrifice', 'precision', 'ability', 'estimate', 'propose', 'assess', 'shadowtest', 'van', 'der', 'Linden', 'Reese', '1998', 'CAT', 'context', 'CAT', 'algorithm', 'form', 'combination', 'maximize', 'minimize', 'varied', 'target', 'function', 'assemble', 'shadow', 'test', 'target', 'function', 'separately', 'second', 'fourth', 'quarter', 'test', 'CAT', 'element', 'function', 'associate', 'random', 'number', 'assign', 'item', 'b', 'absolute', 'difference', 'examinees', 'current', 'ability', 'estimate', 'item', 'difficulty', 'c', 'absolute', 'difference', 'examinees', 'current', 'ability', 'estimate', 'optimum', 'item', 'difficulty', 'd', 'item', 'information', 'result', 'indicate', 'combine', 'CAT', 'fully', 'utilize', 'item', 'pool', 'reduce', 'maximum', 'exposure', 'rate', 'achieve', 'homogeneous', 'exposure', 'rate', 'precision', 'recover', 'ability', 'estimate', 'similar', 'maximum', 'iteminformation', 'method', 'combined', 'CAT', 'method', 'result', 'good', 'overall', 'result', 'compare', 'individual', 'CAT', 'itemselection', 'method', 'finding', 'combined', 'CAT', 'encourage', 'Future', 'discuss']","['increase', 'homogeneity', 'cats', 'itemexposure', 'rate', 'minimize', 'maximize', 'varied', 'target', 'function', 'assemble', 'shadow', 'test']",computerized adaptive testing CAT algorithm potential increase homogeneity cats itemexposure rate significantly sacrifice precision ability estimate propose assess shadowtest van der Linden Reese 1998 CAT context CAT algorithm form combination maximize minimize varied target function assemble shadow test target function separately second fourth quarter test CAT element function associate random number assign item b absolute difference examinees current ability estimate item difficulty c absolute difference examinees current ability estimate optimum item difficulty d item information result indicate combine CAT fully utilize item pool reduce maximum exposure rate achieve homogeneous exposure rate precision recover ability estimate similar maximum iteminformation method combined CAT method result good overall result compare individual CAT itemselection method finding combined CAT encourage Future discuss,increase homogeneity cats itemexposure rate minimize maximize varied target function assemble shadow test,0.024028418388355722,0.024028574205637667,0.02409458663891288,0.9037326795103002,0.024115741256793513,0.0,0.0,0.0,0.12550916206541518,0.0
Penfield R.D.; Algina J.,A generalized DIF effect variance estimator for measuring unsigned differential test functioning in mixed format tests,2006,43,"One approach to measuring unsigned differential test functioning is to estimate the variance of the differential item functioning (DIF) effect across the items of the test. This article proposes two estimators of the DIF effect variance for tests containing dichotomous and polytomous items. The proposed estimators are direct extensions of the noniterative estimators developed by Camilli and Penfield (1997) for tests composed of dichotomous items. A small simulation study is reported in which the statistical properties of the generalized variance estimators are assessed, and guidelines are proposed for interpreting values of DIF effect variance estimators.",A generalized DIF effect variance estimator for measuring unsigned differential test functioning in mixed format tests,"One approach to measuring unsigned differential test functioning is to estimate the variance of the differential item functioning (DIF) effect across the items of the test. This article proposes two estimators of the DIF effect variance for tests containing dichotomous and polytomous items. The proposed estimators are direct extensions of the noniterative estimators developed by Camilli and Penfield (1997) for tests composed of dichotomous items. A small simulation study is reported in which the statistical properties of the generalized variance estimators are assessed, and guidelines are proposed for interpreting values of DIF effect variance estimators.","['approach', 'measure', 'unsigned', 'differential', 'test', 'functioning', 'estimate', 'variance', 'differential', 'item', 'function', 'DIF', 'effect', 'item', 'test', 'article', 'propose', 'estimator', 'DIF', 'effect', 'variance', 'test', 'contain', 'dichotomous', 'polytomous', 'item', 'propose', 'estimator', 'direct', 'extension', 'noniterative', 'estimator', 'develop', 'Camilli', 'Penfield', '1997', 'test', 'compose', 'dichotomous', 'item', 'small', 'simulation', 'study', 'report', 'statistical', 'property', 'generalized', 'variance', 'estimator', 'assess', 'guideline', 'propose', 'interpret', 'value', 'DIF', 'effect', 'variance', 'estimator']","['generalized', 'DIF', 'effect', 'variance', 'estimator', 'measure', 'unsigned', 'differential', 'test', 'function', 'mixed', 'format', 'test']",approach measure unsigned differential test functioning estimate variance differential item function DIF effect item test article propose estimator DIF effect variance test contain dichotomous polytomous item propose estimator direct extension noniterative estimator develop Camilli Penfield 1997 test compose dichotomous item small simulation study report statistical property generalized variance estimator assess guideline propose interpret value DIF effect variance estimator,generalized DIF effect variance estimator measure unsigned differential test function mixed format test,0.033683227594869476,0.03368444525813969,0.03368143189248548,0.4157882922104192,0.4831626030440861,0.13157788500203946,0.0,0.0,0.0,0.0
Clauser B.E.; Mee J.; Baldwin S.G.; Margolis M.J.; Dillon G.F.,Judges' Use of Examinee Performance Data in an Angoff Standard-Setting Exercise for a Medical Licensing Examination: An Experimental Study,2009,46,"Although the Angoff procedure is among the most widely used standard setting procedures for tests comprising multiple-choice items, research has shown that subject matter experts have considerable difficulty accurately making the required judgments in the absence of examinee performance data. Some authors have viewed the need to provide performance data as a fatal flaw for the procedure; others have considered it appropriate for experts to integrate performance data into their judgments but have been concerned that experts may rely too heavily on the data. There have, however, been relatively few studies examining how experts use the data. This article reports on two studies that examine how experts modify their judgments after reviewing data. In both studies, data for some items were accurate and data for other items had been manipulated. Judges in both studies substantially modified their judgments whether the data were accurate or not. © 2009 by the National Council on Measurement in Education.",Judges' Use of Examinee Performance Data in an Angoff Standard-Setting Exercise for a Medical Licensing Examination: An Experimental Study,"Although the Angoff procedure is among the most widely used standard setting procedures for tests comprising multiple-choice items, research has shown that subject matter experts have considerable difficulty accurately making the required judgments in the absence of examinee performance data. Some authors have viewed the need to provide performance data as a fatal flaw for the procedure; others have considered it appropriate for experts to integrate performance data into their judgments but have been concerned that experts may rely too heavily on the data. There have, however, been relatively few studies examining how experts use the data. This article reports on two studies that examine how experts modify their judgments after reviewing data. In both studies, data for some items were accurate and data for other items had been manipulated. Judges in both studies substantially modified their judgments whether the data were accurate or not. © 2009 by the National Council on Measurement in Education.","['Angoff', 'procedure', 'widely', 'standard', 'set', 'procedure', 'test', 'comprise', 'multiplechoice', 'item', 'research', 'subject', 'matter', 'expert', 'considerable', 'difficulty', 'accurately', 'require', 'judgment', 'absence', 'examinee', 'performance', 'datum', 'author', 'view', 'need', 'provide', 'performance', 'datum', 'fatal', 'flaw', 'procedure', 'consider', 'appropriate', 'expert', 'integrate', 'performance', 'datum', 'judgment', 'concern', 'expert', 'rely', 'heavily', 'datum', 'relatively', 'study', 'examine', 'expert', 'datum', 'article', 'report', 'study', 'examine', 'expert', 'modify', 'judgment', 'review', 'datum', 'study', 'datum', 'item', 'accurate', 'datum', 'item', 'manipulate', 'judge', 'study', 'substantially', 'modify', 'judgment', 'datum', 'accurate', '©', '2009', 'National', 'Council']","['judge', 'Use', 'Examinee', 'Performance', 'Data', 'Angoff', 'StandardSetting', 'Exercise', 'Medical', 'Licensing', 'Examination', 'Experimental', 'Study']",Angoff procedure widely standard set procedure test comprise multiplechoice item research subject matter expert considerable difficulty accurately require judgment absence examinee performance datum author view need provide performance datum fatal flaw procedure consider appropriate expert integrate performance datum judgment concern expert rely heavily datum relatively study examine expert datum article report study examine expert modify judgment review datum study datum item accurate datum item manipulate judge study substantially modify judgment datum accurate © 2009 National Council,judge Use Examinee Performance Data Angoff StandardSetting Exercise Medical Licensing Examination Experimental Study,0.02998789697622005,0.029987952648099697,0.06983941871132907,0.8399733669794667,0.03021136468488458,0.0,0.12338808818928344,0.0,0.016610026201192247,0.003374004203596449
Zimmerman D.W.,The reliability of difference scores in populations and samples,2009,46,"This study was an investigation of the relation between the reliability of difference scores, considered as a parameter characterizing a population of examinees, and the reliability estimates obtained from random samples from the population. The parameters in familiar equations for the reliability of difference scores were redefined in such a way that determinants of reliability in both populations and samples become more transparent. Computer simulation was used to find sample values and to plot frequency distributions of various correlations and variance ratios relevant to the reliability of differences. The shape of frequency distributions resulting from the simulations and the means and standard deviations of these distributions reveal the extent to which reliability estimates based on sample data can be expected to meaningfully represent population reliability. © 2009 by the National Council on Measurement in Education.",The reliability of difference scores in populations and samples,"This study was an investigation of the relation between the reliability of difference scores, considered as a parameter characterizing a population of examinees, and the reliability estimates obtained from random samples from the population. The parameters in familiar equations for the reliability of difference scores were redefined in such a way that determinants of reliability in both populations and samples become more transparent. Computer simulation was used to find sample values and to plot frequency distributions of various correlations and variance ratios relevant to the reliability of differences. The shape of frequency distributions resulting from the simulations and the means and standard deviations of these distributions reveal the extent to which reliability estimates based on sample data can be expected to meaningfully represent population reliability. © 2009 by the National Council on Measurement in Education.","['study', 'investigation', 'relation', 'reliability', 'difference', 'score', 'consider', 'parameter', 'characterize', 'population', 'examinee', 'reliability', 'estimate', 'obtain', 'random', 'sample', 'population', 'parameter', 'familiar', 'equation', 'reliability', 'difference', 'score', 'redefine', 'way', 'determinant', 'reliability', 'population', 'sample', 'transparent', 'Computer', 'simulation', 'find', 'sample', 'value', 'plot', 'frequency', 'distribution', 'correlation', 'variance', 'ratio', 'relevant', 'reliability', 'difference', 'shape', 'frequency', 'distribution', 'result', 'simulation', 'mean', 'standard', 'deviation', 'distribution', 'reveal', 'extent', 'reliability', 'estimate', 'base', 'sample', 'datum', 'expect', 'meaningfully', 'represent', 'population', 'reliability', '©', '2009', 'National', 'Council']","['reliability', 'difference', 'score', 'population', 'sample']",study investigation relation reliability difference score consider parameter characterize population examinee reliability estimate obtain random sample population parameter familiar equation reliability difference score redefine way determinant reliability population sample transparent Computer simulation find sample value plot frequency distribution correlation variance ratio relevant reliability difference shape frequency distribution result simulation mean standard deviation distribution reveal extent reliability estimate base sample datum expect meaningfully represent population reliability © 2009 National Council,reliability difference score population sample,0.03044176667400851,0.030442744844992466,0.0304491996403111,0.8779859517561506,0.030680337084537487,0.001838602804572562,0.12056476266862237,0.025410113671847298,0.0,0.00016733089061535643
Roussos L.A.; Templin J.L.; Henson R.A.,Skills diagnosis using IRT-based latent class models,2007,44,"This article describes a latent trait approach to skills diagnosis based on a particular variety of latent class models that employ item response functions (IRFs) as in typical item response theory (IRT) models. To enable and encourage comparisons with other approaches, this description is provided in terms of the main components of any psychometric approach: the ability model and the IRF structure; review of research on estimation, model checking, reliability, validity, equating, and scoring; and a brief review of real data applications. In this manner the article demonstrates that this approach to skills diagnosis has built a strong initial foundation of research and resources available to potential users. The outlook for future research and applications is discussed with special emphasis on a call for pilot studies and concomitant increased validity research. © 2007 by the National Council on Measurement in Education.",Skills diagnosis using IRT-based latent class models,"This article describes a latent trait approach to skills diagnosis based on a particular variety of latent class models that employ item response functions (IRFs) as in typical item response theory (IRT) models. To enable and encourage comparisons with other approaches, this description is provided in terms of the main components of any psychometric approach: the ability model and the IRF structure; review of research on estimation, model checking, reliability, validity, equating, and scoring; and a brief review of real data applications. In this manner the article demonstrates that this approach to skills diagnosis has built a strong initial foundation of research and resources available to potential users. The outlook for future research and applications is discussed with special emphasis on a call for pilot studies and concomitant increased validity research. © 2007 by the National Council on Measurement in Education.","['article', 'describe', 'latent', 'trait', 'approach', 'skill', 'diagnosis', 'base', 'particular', 'variety', 'latent', 'class', 'employ', 'item', 'response', 'function', 'irf', 'typical', 'item', 'response', 'theory', 'IRT', 'enable', 'encourage', 'comparison', 'approach', 'description', 'provide', 'term', 'main', 'component', 'psychometric', 'approach', 'ability', 'IRF', 'structure', 'review', 'research', 'estimation', 'check', 'reliability', 'validity', 'equate', 'scoring', 'brief', 'review', 'real', 'datum', 'application', 'manner', 'article', 'demonstrate', 'approach', 'skill', 'diagnosis', 'build', 'strong', 'initial', 'foundation', 'research', 'resource', 'available', 'potential', 'user', 'outlook', 'future', 'research', 'application', 'discuss', 'special', 'emphasis', 'pilot', 'study', 'concomitant', 'increase', 'validity', 'research', '©', '2007', 'National', 'Council']","['skill', 'diagnosis', 'irtbased', 'latent', 'class']",article describe latent trait approach skill diagnosis base particular variety latent class employ item response function irf typical item response theory IRT enable encourage comparison approach description provide term main component psychometric approach ability IRF structure review research estimation check reliability validity equate scoring brief review real datum application manner article demonstrate approach skill diagnosis build strong initial foundation research resource available potential user outlook future research application discuss special emphasis pilot study concomitant increase validity research © 2007 National Council,skill diagnosis irtbased latent class,0.023995509065242043,0.02399045798413154,0.023991370883097368,0.9039421357851798,0.024080526282349392,0.0,0.13026901740166724,0.002269045350164684,0.01611060140850294,0.0
Jang E.E.; Roussos L.,An investigation into the dimensionality of TOEFL using conditional covariance-based nonparametric approach,2007,44,This article reports two studies to illustrate methodologies for conducting a conditional covariance-based nonparametric dimensionality assessment using data from two forms of the Test of English as a Foreign Language (TOEFL). Study 1 illustrates how to assess overall dimensionality of the TOEFL including all three subtests. Study 2 is aimed at illustrating how to conduct dimensionality analyses for a testlet-based test by focusing on the Reading Comprehension (RC) section in combination with item content analyses and hypothesis testing. The results of Study 1 indicated that both TOEFL forms involve two dominant dimensions corresponding to the Listening Comprehension section and the combination of the Reading Comprehension section and Structure and Written Expression section. The extensive RC analyses from Study 2 revealed strong evidence that a significant amount of the RC multidimensionality came from testlet effects. Confirmatory analyses coupled with exploratory cluster analyses and substantive item content analyses further identified dimensionality structure having to do with reading subskills. © 2007 by the National Council on Measurement in Education.,An investigation into the dimensionality of TOEFL using conditional covariance-based nonparametric approach,This article reports two studies to illustrate methodologies for conducting a conditional covariance-based nonparametric dimensionality assessment using data from two forms of the Test of English as a Foreign Language (TOEFL). Study 1 illustrates how to assess overall dimensionality of the TOEFL including all three subtests. Study 2 is aimed at illustrating how to conduct dimensionality analyses for a testlet-based test by focusing on the Reading Comprehension (RC) section in combination with item content analyses and hypothesis testing. The results of Study 1 indicated that both TOEFL forms involve two dominant dimensions corresponding to the Listening Comprehension section and the combination of the Reading Comprehension section and Structure and Written Expression section. The extensive RC analyses from Study 2 revealed strong evidence that a significant amount of the RC multidimensionality came from testlet effects. Confirmatory analyses coupled with exploratory cluster analyses and substantive item content analyses further identified dimensionality structure having to do with reading subskills. © 2007 by the National Council on Measurement in Education.,"['article', 'report', 'study', 'illustrate', 'methodology', 'conduct', 'conditional', 'covariancebase', 'nonparametric', 'dimensionality', 'assessment', 'datum', 'form', 'Test', 'English', 'Foreign', 'Language', 'TOEFL', 'Study', '1', 'illustrate', 'assess', 'overall', 'dimensionality', 'toefl', 'include', 'subtest', 'Study', '2', 'aim', 'illustrate', 'conduct', 'dimensionality', 'analysis', 'testletbased', 'test', 'focus', 'Reading', 'Comprehension', 'RC', 'section', 'combination', 'item', 'content', 'analysis', 'hypothesis', 'test', 'result', 'Study', '1', 'indicate', 'toefl', 'form', 'involve', 'dominant', 'dimension', 'correspond', 'Listening', 'Comprehension', 'section', 'combination', 'Reading', 'Comprehension', 'section', 'Structure', 'Written', 'Expression', 'section', 'extensive', 'RC', 'analysis', 'Study', '2', 'reveal', 'strong', 'evidence', 'significant', 'RC', 'multidimensionality', 'come', 'testlet', 'effect', 'Confirmatory', 'analysis', 'couple', 'exploratory', 'cluster', 'analysis', 'substantive', 'item', 'content', 'analyse', 'far', 'identify', 'dimensionality', 'structure', 'reading', 'subskill', '©', '2007', 'National', 'Council']","['investigation', 'dimensionality', 'toefl', 'conditional', 'covariancebase', 'nonparametric', 'approach']",article report study illustrate methodology conduct conditional covariancebase nonparametric dimensionality assessment datum form Test English Foreign Language TOEFL Study 1 illustrate assess overall dimensionality toefl include subtest Study 2 aim illustrate conduct dimensionality analysis testletbased test focus Reading Comprehension RC section combination item content analysis hypothesis test result Study 1 indicate toefl form involve dominant dimension correspond Listening Comprehension section combination Reading Comprehension section Structure Written Expression section extensive RC analysis Study 2 reveal strong evidence significant RC multidimensionality come testlet effect Confirmatory analysis couple exploratory cluster analysis substantive item content analyse far identify dimensionality structure reading subskill © 2007 National Council,investigation dimensionality toefl conditional covariancebase nonparametric approach,0.025536350710247988,0.025538091230794584,0.2701677819318577,0.6529855542582985,0.0257722218688012,0.028820876860882885,0.02634979829213482,0.0018913587717201382,0.014524956361982963,0.021752156684277973
Liu J.; Low A.C.,A comparison of the kernel equating method with traditional equating methods using SAT® data,2008,45,"This study applied kernel equating (KE) in two scenarios: equating to a very similar population and equating to a very different population, referred to as a distant population, using SAT® data. The KE results were compared to the results obtained from analogous traditional equating methods in both scenarios. The results indicate that KE results are comparable to the results of other methods. Further, the results show that when the two populations taking the two tests are similar on the anchor score distributions, different equating methods yield the same or very similar results, even though they have different assumptions. © 2008 by the National Council on Measurement in Education.",A comparison of the kernel equating method with traditional equating methods using SAT® data,"This study applied kernel equating (KE) in two scenarios: equating to a very similar population and equating to a very different population, referred to as a distant population, using SAT® data. The KE results were compared to the results obtained from analogous traditional equating methods in both scenarios. The results indicate that KE results are comparable to the results of other methods. Further, the results show that when the two populations taking the two tests are similar on the anchor score distributions, different equating methods yield the same or very similar results, even though they have different assumptions. © 2008 by the National Council on Measurement in Education.","['study', 'apply', 'kernel', 'equate', 'KE', 'scenario', 'equate', 'similar', 'population', 'equate', 'different', 'population', 'refer', 'distant', 'population', 'SAT', '®', 'datum', 'KE', 'result', 'compare', 'result', 'obtain', 'analogous', 'traditional', 'equate', 'method', 'scenario', 'result', 'indicate', 'KE', 'result', 'comparable', 'result', 'method', 'far', 'result', 'population', 'test', 'similar', 'anchor', 'score', 'distribution', 'different', 'equate', 'method', 'yield', 'similar', 'result', 'different', 'assumption', '©', '2008', 'National', 'Council']","['comparison', 'kernel', 'equate', 'method', 'traditional', 'equate', 'method', 'SAT', '®', 'datum']",study apply kernel equate KE scenario equate similar population equate different population refer distant population SAT ® datum KE result compare result obtain analogous traditional equate method scenario result indicate KE result comparable result method far result population test similar anchor score distribution different equate method yield similar result different assumption © 2008 National Council,comparison kernel equate method traditional equate method SAT ® datum,0.03629913773031761,0.036294309308511234,0.03631823590710528,0.555509165910676,0.33557915114338993,0.0,0.0,0.15429289102149712,0.0,0.006383349449670269
Yang W.-L.,Sensitivity of linkings between AP multiple-choice scores and composite scores to geographical region: An illustration of checking for population invariance,2004,41,"Educational Testing Service This application study investigates whether the multiple-choice to composite linking functions that determine Advanced Placement Program exam grades remain invariant over subgroups defined by region. Three years of test data from an AP exam are used to study invariance across regions. The study focuses on two questions: (a) How invariant are grade thresholds across regions? and (b) Do the small sample sizes for some regional groups present particular problems for assessing thresholds invariance? The equatability index proposed by Dorans and Holland (2000) is employed to evaluate the invariance of the linking functions, and cross-classification is used to evaluate the invariance of the composite cut scores. Overall, the linkings across regions seem to hold up reasonably well Nevertheless, more exams need to be examined.",Sensitivity of linkings between AP multiple-choice scores and composite scores to geographical region: An illustration of checking for population invariance,"Educational Testing Service This application study investigates whether the multiple-choice to composite linking functions that determine Advanced Placement Program exam grades remain invariant over subgroups defined by region. Three years of test data from an AP exam are used to study invariance across regions. The study focuses on two questions: (a) How invariant are grade thresholds across regions? and (b) Do the small sample sizes for some regional groups present particular problems for assessing thresholds invariance? The equatability index proposed by Dorans and Holland (2000) is employed to evaluate the invariance of the linking functions, and cross-classification is used to evaluate the invariance of the composite cut scores. Overall, the linkings across regions seem to hold up reasonably well Nevertheless, more exams need to be examined.","['Educational', 'Testing', 'Service', 'application', 'study', 'investigate', 'multiplechoice', 'composite', 'linking', 'function', 'determine', 'Advanced', 'Placement', 'Program', 'exam', 'grade', 'remain', 'invariant', 'subgroup', 'define', 'region', 'year', 'test', 'datum', 'AP', 'exam', 'study', 'invariance', 'region', 'study', 'focus', 'question', 'invariant', 'grade', 'threshold', 'region', 'b', 'small', 'sample', 'size', 'regional', 'group', 'present', 'particular', 'problem', 'assess', 'threshold', 'invariance', 'equatability', 'index', 'propose', 'Dorans', 'Holland', '2000', 'employ', 'evaluate', 'invariance', 'link', 'function', 'crossclassification', 'evaluate', 'invariance', 'composite', 'cut', 'score', 'overall', 'linking', 'region', 'hold', 'reasonably', 'exam', 'need', 'examine']","['sensitivity', 'linking', 'AP', 'multiplechoice', 'score', 'composite', 'score', 'geographical', 'region', 'illustration', 'check', 'population', 'invariance']",Educational Testing Service application study investigate multiplechoice composite linking function determine Advanced Placement Program exam grade remain invariant subgroup define region year test datum AP exam study invariance region study focus question invariant grade threshold region b small sample size regional group present particular problem assess threshold invariance equatability index propose Dorans Holland 2000 employ evaluate invariance link function crossclassification evaluate invariance composite cut score overall linking region hold reasonably exam need examine,sensitivity linking AP multiplechoice score composite score geographical region illustration check population invariance,0.028029145066988507,0.02802356348212099,0.02818229671792114,0.37144596975306743,0.5443190249799019,0.024288826040218023,0.007310296628420449,0.023956516837933824,0.0,0.05511499850803567
Korobko O.B.; Glas C.A.W.; Bosker R.J.; Luyten J.W.,Comparing the difficulty of examination subjects with item response theory,2008,45,"Methods are presented for comparing grades obtained in a situation where students can choose between different subjects. It must be expected that the comparison between the grades is complicated by the interaction between the students' pattern and level of proficiency on one hand, and the choice of the subjects on the other hand. Three methods based on item response theory (IRT) for the estimation of proficiency measures that are comparable over students and subjects are discussed: a method based on a model with a unidimensional representation of proficiency, a method based on a model with a multidimensional representation of proficiency, and a method based on a multidimensional representation of proficiency where the stochastic nature of the choice of examination subjects is explicitly modeled. The methods are compared using the data from the Central Examinations in Secondary Education in the Netherlands. The results show that the unidimensional IRT model produces unrealistic results, which do not appear when using the two multidimensional IRT models. Further, it is shown that both the multidimensional models produce acceptable model fit. However, the model that explicitly takes the choice process into account produces the best model fit. © 2008 by the National Council on Measurement in Education.",Comparing the difficulty of examination subjects with item response theory,"Methods are presented for comparing grades obtained in a situation where students can choose between different subjects. It must be expected that the comparison between the grades is complicated by the interaction between the students' pattern and level of proficiency on one hand, and the choice of the subjects on the other hand. Three methods based on item response theory (IRT) for the estimation of proficiency measures that are comparable over students and subjects are discussed: a method based on a model with a unidimensional representation of proficiency, a method based on a model with a multidimensional representation of proficiency, and a method based on a multidimensional representation of proficiency where the stochastic nature of the choice of examination subjects is explicitly modeled. The methods are compared using the data from the Central Examinations in Secondary Education in the Netherlands. The results show that the unidimensional IRT model produces unrealistic results, which do not appear when using the two multidimensional IRT models. Further, it is shown that both the multidimensional models produce acceptable model fit. However, the model that explicitly takes the choice process into account produces the best model fit. © 2008 by the National Council on Measurement in Education.","['method', 'present', 'compare', 'grade', 'obtain', 'situation', 'student', 'choose', 'different', 'subject', 'expect', 'comparison', 'grade', 'complicate', 'interaction', 'student', 'pattern', 'level', 'proficiency', 'hand', 'choice', 'subject', 'hand', 'method', 'base', 'item', 'response', 'theory', 'IRT', 'estimation', 'proficiency', 'measure', 'comparable', 'student', 'subject', 'discuss', 'method', 'base', 'unidimensional', 'representation', 'proficiency', 'method', 'base', 'multidimensional', 'representation', 'proficiency', 'method', 'base', 'multidimensional', 'representation', 'proficiency', 'stochastic', 'nature', 'choice', 'examination', 'subject', 'explicitly', 'method', 'compare', 'datum', 'Central', 'Examinations', 'Secondary', 'Netherlands', 'result', 'unidimensional', 'IRT', 'produce', 'unrealistic', 'result', 'appear', 'multidimensional', 'IRT', 'far', 'multidimensional', 'produce', 'acceptable', 'fit', 'explicitly', 'choice', 'process', 'account', 'produce', 'good', 'fit', '©', '2008', 'National', 'Council']","['compare', 'difficulty', 'examination', 'subject', 'item', 'response', 'theory']",method present compare grade obtain situation student choose different subject expect comparison grade complicate interaction student pattern level proficiency hand choice subject hand method base item response theory IRT estimation proficiency measure comparable student subject discuss method base unidimensional representation proficiency method base multidimensional representation proficiency method base multidimensional representation proficiency stochastic nature choice examination subject explicitly method compare datum Central Examinations Secondary Netherlands result unidimensional IRT produce unrealistic result appear multidimensional IRT far multidimensional produce acceptable fit explicitly choice process account produce good fit © 2008 National Council,compare difficulty examination subject item response theory,0.028496988520489066,0.028490136741808732,0.028551138185583618,0.885651488051006,0.02881024850111251,0.0,0.03069244586623246,0.005671391536117243,0.05433307835271686,0.057516106446053705
Kim J.-S.,Using the distractor categories of multiple-choice items to improve IRT linking,2006,43,"Simulation and real data studies are used to investigate the value of modeling multiple-choice distractors on item response theory linking. Using the characteristic curve linking procedure for Bock's (1972) nominal response model presented by Kim and Hanson (2002), all-category linking (i.e., a linking based on all category characteristic curves of the linking items) is compared against correct-only (CO) linking (i.e., linking based on the correct category characteristic curves only) using a common-item nonequivalent groups design. The CO linking is shown to represent an approximation to what occurs when using a traditional correct/incorrect item response model for linking. Results suggest that the number of linking items needed to achieve an equivalent level of linking precision declines substantially when incorporating the distractor categories.",Using the distractor categories of multiple-choice items to improve IRT linking,"Simulation and real data studies are used to investigate the value of modeling multiple-choice distractors on item response theory linking. Using the characteristic curve linking procedure for Bock's (1972) nominal response model presented by Kim and Hanson (2002), all-category linking (i.e., a linking based on all category characteristic curves of the linking items) is compared against correct-only (CO) linking (i.e., linking based on the correct category characteristic curves only) using a common-item nonequivalent groups design. The CO linking is shown to represent an approximation to what occurs when using a traditional correct/incorrect item response model for linking. Results suggest that the number of linking items needed to achieve an equivalent level of linking precision declines substantially when incorporating the distractor categories.","['simulation', 'real', 'datum', 'study', 'investigate', 'value', 'multiplechoice', 'distractor', 'item', 'response', 'theory', 'link', 'characteristic', 'curve', 'link', 'procedure', 'Bocks', '1972', 'nominal', 'response', 'present', 'Kim', 'Hanson', '2002', 'allcategory', 'link', 'ie', 'linking', 'base', 'category', 'characteristic', 'curve', 'link', 'item', 'compare', 'correctonly', 'CO', 'link', 'ie', 'link', 'base', 'correct', 'category', 'characteristic', 'curve', 'commonitem', 'nonequivalent', 'group', 'design', 'CO', 'linking', 'represent', 'approximation', 'occur', 'traditional', 'correctincorrect', 'item', 'response', 'link', 'result', 'suggest', 'number', 'link', 'item', 'need', 'achieve', 'equivalent', 'level', 'link', 'precision', 'decline', 'substantially', 'incorporate', 'distractor', 'category']","['distractor', 'category', 'multiplechoice', 'item', 'improve', 'IRT', 'link']",simulation real datum study investigate value multiplechoice distractor item response theory link characteristic curve link procedure Bocks 1972 nominal response present Kim Hanson 2002 allcategory link ie linking base category characteristic curve link item compare correctonly CO link ie link base correct category characteristic curve commonitem nonequivalent group design CO linking represent approximation occur traditional correctincorrect item response link result suggest number link item need achieve equivalent level link precision decline substantially incorporate distractor category,distractor category multiplechoice item improve IRT link,0.033013027583890714,0.033014760096363296,0.03319391553820043,0.8657787843782153,0.03499951240333034,0.0,0.0060863909855284355,0.0316190544177401,0.051753152409629855,0.0
Kim S.; Lee W.-C.,An extension of four IRT linking methods for mixed-format tests,2006,43,"Under item response theory (IRT), linking proficiency scales from separate calibrations of multiple forms of a test to achieve a common scale is required in many applications. Four IRT linking methods including the mean/mean, mean/sigma, Haebara, and Stocking-Lord methods have been presented for use with single-format tests. This study extends the four linking methods to a mixture of unidimensional IRT models for mixed-format tests. Each linking method extended is intended to handle mixed-format tests using any mixture of the following five IRT models: the three-parameter logistic, graded response, generalized partial credit, nominal response (NR), and multiple-choice (MC) models. A simulation study is conducted to investigate the performance of the four linking methods extended to mixed-format tests. Overall, the Haebara and Stocking-Lord methods yield more accurate linking results than the mean/mean and mean/sigma methods. When the NR model or the MC model is used to analyze data from mixed-format tests, limitations of the mean/mean, mean/sigma, and Stocking-Lord methods are described. © Copyright 2006 by the National Council on Measurement in Education.",An extension of four IRT linking methods for mixed-format tests,"Under item response theory (IRT), linking proficiency scales from separate calibrations of multiple forms of a test to achieve a common scale is required in many applications. Four IRT linking methods including the mean/mean, mean/sigma, Haebara, and Stocking-Lord methods have been presented for use with single-format tests. This study extends the four linking methods to a mixture of unidimensional IRT models for mixed-format tests. Each linking method extended is intended to handle mixed-format tests using any mixture of the following five IRT models: the three-parameter logistic, graded response, generalized partial credit, nominal response (NR), and multiple-choice (MC) models. A simulation study is conducted to investigate the performance of the four linking methods extended to mixed-format tests. Overall, the Haebara and Stocking-Lord methods yield more accurate linking results than the mean/mean and mean/sigma methods. When the NR model or the MC model is used to analyze data from mixed-format tests, limitations of the mean/mean, mean/sigma, and Stocking-Lord methods are described. © Copyright 2006 by the National Council on Measurement in Education.","['item', 'response', 'theory', 'IRT', 'link', 'proficiency', 'scale', 'separate', 'calibration', 'multiple', 'form', 'test', 'achieve', 'common', 'scale', 'require', 'application', 'IRT', 'link', 'method', 'include', 'meanmean', 'meansigma', 'Haebara', 'StockingLord', 'method', 'present', 'singleformat', 'test', 'study', 'extend', 'link', 'method', 'mixture', 'unidimensional', 'IRT', 'mixedformat', 'test', 'link', 'method', 'extend', 'intend', 'handle', 'mixedformat', 'test', 'mixture', 'follow', 'IRT', 'threeparameter', 'logistic', 'grade', 'response', 'generalize', 'partial', 'credit', 'nominal', 'response', 'NR', 'multiplechoice', 'MC', 'simulation', 'study', 'conduct', 'investigate', 'performance', 'link', 'method', 'extend', 'mixedformat', 'test', 'overall', 'Haebara', 'StockingLord', 'method', 'yield', 'accurate', 'linking', 'result', 'meanmean', 'meansigma', 'method', 'NR', 'MC', 'analyze', 'datum', 'mixedformat', 'test', 'limitation', 'meanmean', 'meansigma', 'StockingLord', 'method', 'describe', '©', 'copyright', '2006', 'National', 'Council']","['extension', 'IRT', 'link', 'method', 'mixedformat', 'test']",item response theory IRT link proficiency scale separate calibration multiple form test achieve common scale require application IRT link method include meanmean meansigma Haebara StockingLord method present singleformat test study extend link method mixture unidimensional IRT mixedformat test link method extend intend handle mixedformat test mixture follow IRT threeparameter logistic grade response generalize partial credit nominal response NR multiplechoice MC simulation study conduct investigate performance link method extend mixedformat test overall Haebara StockingLord method yield accurate linking result meanmean meansigma method NR MC analyze datum mixedformat test limitation meanmean meansigma StockingLord method describe © copyright 2006 National Council,extension IRT link method mixedformat test,0.028900600471260626,0.028898081464359588,0.028915745222651935,0.884212343335,0.02907322950672797,0.0,0.013012609210003252,0.030793520582918385,0.06834390474742005,0.0031412457050473334
Kim S.; Von Davier A.A.; Haberman S.,Small-sample equating using a synthetic linking function,2008,45,"This study addressed the sampling error and linking bias that occur with small samples in a nonequivalent groups anchor test design. We proposed a linking method called the synthetic function, which is a weighted average of the identity function and a traditional equating function (in this case, the chained linear equating function). Specifically, we compared the synthetic, identity, and chained linear functions for various-sized samples from two types of national assessments. One design used a highly reliable test and an external anchor, and the other used a relatively low-reliability test and an internal anchor. The results from each of these methods were compared to the criterion equating function derived from the total samples with respect to linking bias and error. The study indicated that the synthetic functions might be a better choice than the chained linear equating method when samples are not large and, as a result, unrepresentative. © 2008 by the National Council on Measurement in Education.",Small-sample equating using a synthetic linking function,"This study addressed the sampling error and linking bias that occur with small samples in a nonequivalent groups anchor test design. We proposed a linking method called the synthetic function, which is a weighted average of the identity function and a traditional equating function (in this case, the chained linear equating function). Specifically, we compared the synthetic, identity, and chained linear functions for various-sized samples from two types of national assessments. One design used a highly reliable test and an external anchor, and the other used a relatively low-reliability test and an internal anchor. The results from each of these methods were compared to the criterion equating function derived from the total samples with respect to linking bias and error. The study indicated that the synthetic functions might be a better choice than the chained linear equating method when samples are not large and, as a result, unrepresentative. © 2008 by the National Council on Measurement in Education.","['study', 'address', 'sample', 'error', 'link', 'bias', 'occur', 'small', 'sample', 'nonequivalent', 'group', 'anchor', 'test', 'design', 'propose', 'link', 'method', 'synthetic', 'function', 'weighted', 'average', 'identity', 'function', 'traditional', 'equating', 'function', 'case', 'chain', 'linear', 'equate', 'function', 'specifically', 'compare', 'synthetic', 'identity', 'chain', 'linear', 'function', 'varioussized', 'sample', 'type', 'national', 'assessment', 'design', 'highly', 'reliable', 'test', 'external', 'anchor', 'relatively', 'lowreliability', 'test', 'internal', 'anchor', 'result', 'method', 'compare', 'criterion', 'equate', 'function', 'derive', 'total', 'sample', 'respect', 'link', 'bias', 'error', 'study', 'indicate', 'synthetic', 'function', 'choice', 'chain', 'linear', 'equate', 'method', 'sample', 'large', 'result', 'unrepresentative', '©', '2008', 'National', 'Council']","['smallsample', 'equating', 'synthetic', 'linking', 'function']",study address sample error link bias occur small sample nonequivalent group anchor test design propose link method synthetic function weighted average identity function traditional equating function case chain linear equate function specifically compare synthetic identity chain linear function varioussized sample type national assessment design highly reliable test external anchor relatively lowreliability test internal anchor result method compare criterion equate function derive total sample respect link bias error study indicate synthetic function choice chain linear equate method sample large result unrepresentative © 2008 National Council,smallsample equating synthetic linking function,0.029335002018063237,0.02933327259090287,0.02974695552958918,0.5791245546293886,0.3324602152320561,0.018305409691133038,0.0,0.15030864507756353,0.0,0.0
Penfield R.D.,An odds ratio approach for assessing differential distractor functioning effects under the nominal response model,2008,45,"Investigations of differential distractor functioning (DDF) can provide valuable information concerning the location and possible causes of measurement invariance within a multiple-choice item. In this article, I propose an odds ratio estimator of the DDF effect as modeled under the nominal response model. In addition, I propose a simultaneous distractor-level (SDL) test of invariance based on the results of the distractor-level tests of DDF. The results of a simulation study indicated that the DDF effect estimator maintained good statistical properties under a variety of conditions, and the SDL test displayed substantially higher power than the traditional Mantel-Haenszel test of no DIF when the DDF effect varied in magnitude and/or size across the distractors. © 2008 by the National Council on Measurement in Education.",An odds ratio approach for assessing differential distractor functioning effects under the nominal response model,"Investigations of differential distractor functioning (DDF) can provide valuable information concerning the location and possible causes of measurement invariance within a multiple-choice item. In this article, I propose an odds ratio estimator of the DDF effect as modeled under the nominal response model. In addition, I propose a simultaneous distractor-level (SDL) test of invariance based on the results of the distractor-level tests of DDF. The results of a simulation study indicated that the DDF effect estimator maintained good statistical properties under a variety of conditions, and the SDL test displayed substantially higher power than the traditional Mantel-Haenszel test of no DIF when the DDF effect varied in magnitude and/or size across the distractors. © 2008 by the National Council on Measurement in Education.","['investigation', 'differential', 'distractor', 'function', 'DDF', 'provide', 'valuable', 'information', 'concern', 'location', 'possible', 'cause', 'invariance', 'multiplechoice', 'item', 'article', 'I', 'propose', 'odd', 'ratio', 'estimator', 'DDF', 'effect', 'nominal', 'response', 'addition', 'I', 'propose', 'simultaneous', 'distractorlevel', 'SDL', 'test', 'invariance', 'base', 'result', 'distractorlevel', 'test', 'DDF', 'result', 'simulation', 'study', 'indicate', 'DDF', 'effect', 'estimator', 'maintain', 'good', 'statistical', 'property', 'variety', 'condition', 'SDL', 'test', 'display', 'substantially', 'high', 'power', 'traditional', 'MantelHaenszel', 'test', 'dif', 'DDF', 'effect', 'vary', 'magnitude', 'andor', 'size', 'distractor', '©', '2008', 'National', 'Council']","['odd', 'ratio', 'approach', 'assess', 'differential', 'distractor', 'function', 'effect', 'nominal', 'response']",investigation differential distractor function DDF provide valuable information concern location possible cause invariance multiplechoice item article I propose odd ratio estimator DDF effect nominal response addition I propose simultaneous distractorlevel SDL test invariance base result distractorlevel test DDF result simulation study indicate DDF effect estimator maintain good statistical property variety condition SDL test display substantially high power traditional MantelHaenszel test dif DDF effect vary magnitude andor size distractor © 2008 National Council,odd ratio approach assess differential distractor function effect nominal response,0.0297467870493072,0.02974189091075062,0.029903344486310417,0.28658017078884995,0.6240278067647819,0.08818674754583354,0.0,0.00649434764911422,0.0,0.0026072306058282217
Finkelman M.; Nering M.L.; Roussos L.A.,A conditional exposure control method for multidimensional adaptive testing,2009,46,"In computerized adaptive testing (CAT), ensuring the security of test items is a crucial practical consideration. A common approach to reducing item theft is to define maximum item exposure rates, i.e., to limit the proportion of examinees to whom a given item can be administered. Numerous methods for controlling exposure rates have been proposed for tests employing the unidimensional 3-PL model. The present article explores the issues associated with controlling exposure rates when a multidimensional item response theory (MIRT) model is utilized and exposure rates must be controlled conditional upon ability. This situation is complicated by the exponentially increasing number of possible ability values in multiple dimensions. The article introduces a new procedure, called the generalized Stocking-Lewis method, that controls the exposure rate for students of comparable ability as well as with respect to the overall population. A realistic simulation set compares the new method with three other approaches: Kullback-Leibler information with no exposure control, Kullback-Leibler information with unconditional Sympson-Hetter exposure control, and random item selection. © 2009 by the National Council on Measurement in Education.",A conditional exposure control method for multidimensional adaptive testing,"In computerized adaptive testing (CAT), ensuring the security of test items is a crucial practical consideration. A common approach to reducing item theft is to define maximum item exposure rates, i.e., to limit the proportion of examinees to whom a given item can be administered. Numerous methods for controlling exposure rates have been proposed for tests employing the unidimensional 3-PL model. The present article explores the issues associated with controlling exposure rates when a multidimensional item response theory (MIRT) model is utilized and exposure rates must be controlled conditional upon ability. This situation is complicated by the exponentially increasing number of possible ability values in multiple dimensions. The article introduces a new procedure, called the generalized Stocking-Lewis method, that controls the exposure rate for students of comparable ability as well as with respect to the overall population. A realistic simulation set compares the new method with three other approaches: Kullback-Leibler information with no exposure control, Kullback-Leibler information with unconditional Sympson-Hetter exposure control, and random item selection. © 2009 by the National Council on Measurement in Education.","['computerized', 'adaptive', 'testing', 'CAT', 'ensure', 'security', 'test', 'item', 'crucial', 'practical', 'consideration', 'common', 'approach', 'reduce', 'item', 'theft', 'define', 'maximum', 'item', 'exposure', 'rate', 'ie', 'limit', 'proportion', 'examinee', 'item', 'administer', 'numerous', 'method', 'control', 'exposure', 'rate', 'propose', 'test', 'employ', 'unidimensional', '3pl', 'present', 'article', 'explore', 'issue', 'associate', 'control', 'exposure', 'rate', 'multidimensional', 'item', 'response', 'theory', 'MIRT', 'utilize', 'exposure', 'rate', 'control', 'conditional', 'ability', 'situation', 'complicate', 'exponentially', 'increase', 'number', 'possible', 'ability', 'value', 'multiple', 'dimension', 'article', 'introduce', 'new', 'procedure', 'generalized', 'StockingLewis', 'method', 'control', 'exposure', 'rate', 'student', 'comparable', 'ability', 'respect', 'overall', 'population', 'realistic', 'simulation', 'set', 'compare', 'new', 'method', 'approach', 'KullbackLeibler', 'information', 'exposure', 'control', 'KullbackLeibler', 'information', 'unconditional', 'SympsonHetter', 'exposure', 'control', 'random', 'item', 'selection', '©', '2009', 'National', 'Council']","['conditional', 'exposure', 'control', 'method', 'multidimensional', 'adaptive', 'testing']",computerized adaptive testing CAT ensure security test item crucial practical consideration common approach reduce item theft define maximum item exposure rate ie limit proportion examinee item administer numerous method control exposure rate propose test employ unidimensional 3pl present article explore issue associate control exposure rate multidimensional item response theory MIRT utilize exposure rate control conditional ability situation complicate exponentially increase number possible ability value multiple dimension article introduce new procedure generalized StockingLewis method control exposure rate student comparable ability respect overall population realistic simulation set compare new method approach KullbackLeibler information exposure control KullbackLeibler information unconditional SympsonHetter exposure control random item selection © 2009 National Council,conditional exposure control method multidimensional adaptive testing,0.02594634240833109,0.025947080578532743,0.025944847125482867,0.8961325761984217,0.026029153689231574,0.0,0.0,0.0,0.13072602974851363,0.0
Gierl M.J.; Bisanz J.; Bisanz G.L.; Boughton K.A.,Identifying Content and Cognitive Skills that Produce Gender Differences in Mathematics: A Demonstration of the Multidimensionality-Based DIF Analysis Paradigm,2003,40,"Progress has been made in developing statistical methods for identifying DIF items, but procedures to aid with the substantive interpretations of these items have lagged behind. To overcome this problem, Roussos and Stout (1996) proposed a multidimensionality-based DIF analysis paradigm. We illustrate and evaluate an application of this framework as it applied to the study of gender differences in mathematics. Four characteristics distinguish this study from previous research: the substantive analysis was guided by past research on the content and cognitive-related sources of gender differences in mathematics achievement, as presented in the taxonomy by Gallagher, De Lisi, Holst, McGillicuddy-De Lisi, Morely, and Cahalan (2000); the substantive analysis was conducted by reviewers who were highly knowledgeable about the cognitive strategies students use to solve math problems; three statistical methods were used to test hypotheses about gender differences, including SIBTEST, DIMTEST, and multiple linear regression; and the data were from a curriculum-based achievement test developed with the goal of minimizing obvious, content-related gender differences. We show that the framework can lead to clearly interpretable results and we highlight both the strengths and weaknesses of applying the Roussos and Stout framework to the study of group differences.",Identifying Content and Cognitive Skills that Produce Gender Differences in Mathematics: A Demonstration of the Multidimensionality-Based DIF Analysis Paradigm,"Progress has been made in developing statistical methods for identifying DIF items, but procedures to aid with the substantive interpretations of these items have lagged behind. To overcome this problem, Roussos and Stout (1996) proposed a multidimensionality-based DIF analysis paradigm. We illustrate and evaluate an application of this framework as it applied to the study of gender differences in mathematics. Four characteristics distinguish this study from previous research: the substantive analysis was guided by past research on the content and cognitive-related sources of gender differences in mathematics achievement, as presented in the taxonomy by Gallagher, De Lisi, Holst, McGillicuddy-De Lisi, Morely, and Cahalan (2000); the substantive analysis was conducted by reviewers who were highly knowledgeable about the cognitive strategies students use to solve math problems; three statistical methods were used to test hypotheses about gender differences, including SIBTEST, DIMTEST, and multiple linear regression; and the data were from a curriculum-based achievement test developed with the goal of minimizing obvious, content-related gender differences. We show that the framework can lead to clearly interpretable results and we highlight both the strengths and weaknesses of applying the Roussos and Stout framework to the study of group differences.","['progress', 'develop', 'statistical', 'method', 'identify', 'dif', 'item', 'procedure', 'aid', 'substantive', 'interpretation', 'item', 'lag', 'overcome', 'problem', 'Roussos', 'Stout', '1996', 'propose', 'multidimensionalitybase', 'DIF', 'analysis', 'paradigm', 'illustrate', 'evaluate', 'application', 'framework', 'apply', 'study', 'gender', 'difference', 'mathematics', 'characteristic', 'distinguish', 'study', 'previous', 'research', 'substantive', 'analysis', 'guide', 'past', 'research', 'content', 'cognitiverelate', 'source', 'gender', 'difference', 'mathematics', 'achievement', 'present', 'taxonomy', 'Gallagher', 'De', 'Lisi', 'Holst', 'mcgillicuddyde', 'Lisi', 'morely', 'Cahalan', '2000', 'substantive', 'analysis', 'conduct', 'reviewer', 'highly', 'knowledgeable', 'cognitive', 'strategy', 'student', 'solve', 'math', 'problem', 'statistical', 'method', 'test', 'hypothesis', 'gender', 'difference', 'include', 'SIBTEST', 'DIMTEST', 'multiple', 'linear', 'regression', 'datum', 'curriculumbase', 'achievement', 'test', 'develop', 'goal', 'minimize', 'obvious', 'contentrelated', 'gender', 'difference', 'framework', 'lead', 'clearly', 'interpretable', 'result', 'highlight', 'strength', 'weakness', 'apply', 'Roussos', 'Stout', 'framework', 'study', 'group', 'difference']","['identify', 'Content', 'Cognitive', 'Skills', 'produce', 'Gender', 'Differences', 'Mathematics', 'A', 'Demonstration', 'MultidimensionalityBased', 'DIF', 'Analysis', 'Paradigm']",progress develop statistical method identify dif item procedure aid substantive interpretation item lag overcome problem Roussos Stout 1996 propose multidimensionalitybase DIF analysis paradigm illustrate evaluate application framework apply study gender difference mathematics characteristic distinguish study previous research substantive analysis guide past research content cognitiverelate source gender difference mathematics achievement present taxonomy Gallagher De Lisi Holst mcgillicuddyde Lisi morely Cahalan 2000 substantive analysis conduct reviewer highly knowledgeable cognitive strategy student solve math problem statistical method test hypothesis gender difference include SIBTEST DIMTEST multiple linear regression datum curriculumbase achievement test develop goal minimize obvious contentrelated gender difference framework lead clearly interpretable result highlight strength weakness apply Roussos Stout framework study group difference,identify Content Cognitive Skills produce Gender Differences Mathematics A Demonstration MultidimensionalityBased DIF Analysis Paradigm,0.02207976000862333,0.022081122670102656,0.022102339624417378,0.9113293661845641,0.02240741151229235,0.056240604064740754,0.022134470772119923,0.0,0.0005711090220793684,0.06154827281055026
Pommerich M.; Segall D.O.,Local dependence in an operational CAT: Diagnosis and implications,2008,45,"The accuracy of CAT scores can be negatively affected by local dependence if the CAT utilizes parameters that are misspecified due to the presence of local dependence and/or fails to control for local dependence in responses during the administration stage. This article evaluates the existence and effect of local dependence in a test of Mathematics Knowledge. Diagnostic tools were first used to evaluate the existence of local dependence in items that were calibrated under a 3PL model. A simulation study was then used to evaluate the effect of local dependence on the precision of examinee CAT scores when the 3PL model was used for selection and scoring. The diagnostic evaluation showed strong evidence for local dependence. The simulation suggested that local dependence in parameters had a minimal effect on CAT score precision, while local dependence in responses had a substantial effect on score precision, depending on the degree of local dependence present. © 2008 by the National Council on Measurement in Education.",Local dependence in an operational CAT: Diagnosis and implications,"The accuracy of CAT scores can be negatively affected by local dependence if the CAT utilizes parameters that are misspecified due to the presence of local dependence and/or fails to control for local dependence in responses during the administration stage. This article evaluates the existence and effect of local dependence in a test of Mathematics Knowledge. Diagnostic tools were first used to evaluate the existence of local dependence in items that were calibrated under a 3PL model. A simulation study was then used to evaluate the effect of local dependence on the precision of examinee CAT scores when the 3PL model was used for selection and scoring. The diagnostic evaluation showed strong evidence for local dependence. The simulation suggested that local dependence in parameters had a minimal effect on CAT score precision, while local dependence in responses had a substantial effect on score precision, depending on the degree of local dependence present. © 2008 by the National Council on Measurement in Education.","['accuracy', 'CAT', 'score', 'negatively', 'affect', 'local', 'dependence', 'CAT', 'utilize', 'parameter', 'misspecifie', 'presence', 'local', 'dependence', 'andor', 'fail', 'control', 'local', 'dependence', 'response', 'administration', 'stage', 'article', 'evaluate', 'existence', 'effect', 'local', 'dependence', 'test', 'Mathematics', 'Knowledge', 'Diagnostic', 'tool', 'evaluate', 'existence', 'local', 'dependence', 'item', 'calibrate', '3pl', 'simulation', 'study', 'evaluate', 'effect', 'local', 'dependence', 'precision', 'examinee', 'CAT', 'score', '3pl', 'selection', 'scoring', 'diagnostic', 'evaluation', 'strong', 'evidence', 'local', 'dependence', 'simulation', 'suggest', 'local', 'dependence', 'parameter', 'minimal', 'effect', 'CAT', 'score', 'precision', 'local', 'dependence', 'response', 'substantial', 'effect', 'score', 'precision', 'depend', 'degree', 'local', 'dependence', 'present', '©', '2008', 'National', 'Council']","['local', 'dependence', 'operational', 'CAT', 'Diagnosis', 'implication']",accuracy CAT score negatively affect local dependence CAT utilize parameter misspecifie presence local dependence andor fail control local dependence response administration stage article evaluate existence effect local dependence test Mathematics Knowledge Diagnostic tool evaluate existence local dependence item calibrate 3pl simulation study evaluate effect local dependence precision examinee CAT score 3pl selection scoring diagnostic evaluation strong evidence local dependence simulation suggest local dependence parameter minimal effect CAT score precision local dependence response substantial effect score precision depend degree local dependence present © 2008 National Council,local dependence operational CAT Diagnosis implication,0.03867368160471256,0.03867427578831368,0.0386696371866303,0.8452106983867815,0.03877170703356198,0.0,0.024101007290463237,0.0032769027334224143,0.03562253004748419,1.6613555301697876e-05
Penfield R.D.,Assessing differential step functioning in polytomous items using a common odds ratio estimator,2007,44,"Many statistics used in the assessment of differential item functioning (DIF) in polytomous items yield a single item-level index of measurement invariance that collapses information across all response options of the polytomous item. Utilizing a single item-level index of DIF can, however, be misleading if the magnitude or direction of the DIF changes across the steps underlying the polytomous response process. A more comprehensive approach to examining measurement invariance in polytomous item formats is to examine invariance at the level of each step of the polytomous item, a framework described in this article as differential step functioning (DSF). This article proposes a nonparametric DSF estimator that is based on the Mantel-Haenszel common odds ratio estimator (Mantel & Haenszel, 1959), which is frequently implemented in the detection of DIF in dichotomous items. A simulation study demonstrated that when the level of DSF varied in magnitude or sign across the steps underlying the polytomous response options, the DSF-based approach typically provided a more powerful and accurate test of measurement invariance than did corresponding item-level DIF estimators. © 2007 by the National Council on Measurement in Education.",Assessing differential step functioning in polytomous items using a common odds ratio estimator,"Many statistics used in the assessment of differential item functioning (DIF) in polytomous items yield a single item-level index of measurement invariance that collapses information across all response options of the polytomous item. Utilizing a single item-level index of DIF can, however, be misleading if the magnitude or direction of the DIF changes across the steps underlying the polytomous response process. A more comprehensive approach to examining measurement invariance in polytomous item formats is to examine invariance at the level of each step of the polytomous item, a framework described in this article as differential step functioning (DSF). This article proposes a nonparametric DSF estimator that is based on the Mantel-Haenszel common odds ratio estimator (Mantel & Haenszel, 1959), which is frequently implemented in the detection of DIF in dichotomous items. A simulation study demonstrated that when the level of DSF varied in magnitude or sign across the steps underlying the polytomous response options, the DSF-based approach typically provided a more powerful and accurate test of measurement invariance than did corresponding item-level DIF estimators. © 2007 by the National Council on Measurement in Education.","['statistic', 'assessment', 'differential', 'item', 'function', 'DIF', 'polytomous', 'item', 'yield', 'single', 'itemlevel', 'index', 'invariance', 'collapse', 'information', 'response', 'option', 'polytomous', 'item', 'utilize', 'single', 'itemlevel', 'index', 'DIF', 'misleading', 'magnitude', 'direction', 'DIF', 'change', 'step', 'underlie', 'polytomous', 'response', 'process', 'comprehensive', 'approach', 'examine', 'invariance', 'polytomous', 'item', 'format', 'examine', 'invariance', 'level', 'step', 'polytomous', 'item', 'framework', 'describe', 'article', 'differential', 'step', 'function', 'DSF', 'article', 'propose', 'nonparametric', 'DSF', 'estimator', 'base', 'MantelHaenszel', 'common', 'odd', 'ratio', 'estimator', 'Mantel', 'Haenszel', '1959', 'frequently', 'implement', 'detection', 'dif', 'dichotomous', 'item', 'simulation', 'study', 'demonstrate', 'level', 'DSF', 'varied', 'magnitude', 'sign', 'step', 'underlie', 'polytomous', 'response', 'option', 'dsfbase', 'approach', 'typically', 'provide', 'powerful', 'accurate', 'test', 'invariance', 'correspond', 'itemlevel', 'dif', 'estimator', '©', '2007', 'National', 'Council']","['assess', 'differential', 'step', 'function', 'polytomous', 'item', 'common', 'odd', 'ratio', 'estimator']",statistic assessment differential item function DIF polytomous item yield single itemlevel index invariance collapse information response option polytomous item utilize single itemlevel index DIF misleading magnitude direction DIF change step underlie polytomous response process comprehensive approach examine invariance polytomous item format examine invariance level step polytomous item framework describe article differential step function DSF article propose nonparametric DSF estimator base MantelHaenszel common odd ratio estimator Mantel Haenszel 1959 frequently implement detection dif dichotomous item simulation study demonstrate level DSF varied magnitude sign step underlie polytomous response option dsfbase approach typically provide powerful accurate test invariance correspond itemlevel dif estimator © 2007 National Council,assess differential step function polytomous item common odd ratio estimator,0.0273233384623441,0.02732426862004108,0.027364116205819092,0.26725825482689075,0.650730021884905,0.1287989451157624,0.0,0.0,0.0,0.0
Finch H.; Habing B.,Comparison of NOHARM and DETECT in item cluster recovery: Counting dimensions and allocating items,2005,42,"This study examines the performance of a new method for assessing and characterizing dimensionality in test data using the NOHARM model, and comparing it with DETECT. Dimensionality assessment is carried out using two goodness-of-fit statistics that are compared to reference x 2 distributions. A Monte Carlo study is used with item parameters based on a statewide basic skills assessment and the SAT. Other factors that are varied include the correlation among the latent traits, the number of items, the number of subjects, skewness of the latent traits, and the presence or absence of guessing. The performance of the two procedures is judged by the accuracy in determining the number of underlying dimensions, and the degree to which items are correctly clustered together. Results indicate that the new, NOHARM-based method appears to perform comparably to DETECT in terms of simultaneously finding the correct number of dimensions and clustering items correctly. NOHARM is generally better able to determine the number of underlying dimensions, but less able to group items together, than DETECT. When errors in item cluster assignment are made, DETECT is more likely to incorrectly separate items while NOHARM more often incorrectly groups them together.",Comparison of NOHARM and DETECT in item cluster recovery: Counting dimensions and allocating items,"This study examines the performance of a new method for assessing and characterizing dimensionality in test data using the NOHARM model, and comparing it with DETECT. Dimensionality assessment is carried out using two goodness-of-fit statistics that are compared to reference x 2 distributions. A Monte Carlo study is used with item parameters based on a statewide basic skills assessment and the SAT. Other factors that are varied include the correlation among the latent traits, the number of items, the number of subjects, skewness of the latent traits, and the presence or absence of guessing. The performance of the two procedures is judged by the accuracy in determining the number of underlying dimensions, and the degree to which items are correctly clustered together. Results indicate that the new, NOHARM-based method appears to perform comparably to DETECT in terms of simultaneously finding the correct number of dimensions and clustering items correctly. NOHARM is generally better able to determine the number of underlying dimensions, but less able to group items together, than DETECT. When errors in item cluster assignment are made, DETECT is more likely to incorrectly separate items while NOHARM more often incorrectly groups them together.","['study', 'examine', 'performance', 'new', 'method', 'assess', 'characterize', 'dimensionality', 'test', 'datum', 'NOHARM', 'compare', 'DETECT', 'Dimensionality', 'assessment', 'carry', 'goodnessoffit', 'statistic', 'compare', 'reference', 'x', '2', 'distribution', 'A', 'Monte', 'Carlo', 'study', 'item', 'parameter', 'base', 'statewide', 'basic', 'skill', 'assessment', 'SAT', 'factor', 'varied', 'include', 'correlation', 'latent', 'trait', 'number', 'item', 'number', 'subject', 'skewness', 'latent', 'trait', 'presence', 'absence', 'guess', 'performance', 'procedure', 'judge', 'accuracy', 'determine', 'number', 'underlie', 'dimension', 'degree', 'item', 'correctly', 'cluster', 'result', 'indicate', 'new', 'NOHARMbased', 'method', 'appear', 'perform', 'comparably', 'detect', 'term', 'simultaneously', 'find', 'correct', 'number', 'dimension', 'cluster', 'item', 'correctly', 'NOHARM', 'generally', 'able', 'determine', 'number', 'underlie', 'dimension', 'able', 'group', 'item', 'detect', 'error', 'item', 'cluster', 'assignment', 'DETECT', 'likely', 'incorrectly', 'separate', 'item', 'NOHARM', 'incorrectly', 'group']","['Comparison', 'NOHARM', 'detect', 'item', 'cluster', 'recovery', 'counting', 'dimension', 'allocate', 'item']",study examine performance new method assess characterize dimensionality test datum NOHARM compare DETECT Dimensionality assessment carry goodnessoffit statistic compare reference x 2 distribution A Monte Carlo study item parameter base statewide basic skill assessment SAT factor varied include correlation latent trait number item number subject skewness latent trait presence absence guess performance procedure judge accuracy determine number underlie dimension degree item correctly cluster result indicate new NOHARMbased method appear perform comparably detect term simultaneously find correct number dimension cluster item correctly NOHARM generally able determine number underlie dimension able group item detect error item cluster assignment DETECT likely incorrectly separate item NOHARM incorrectly group,Comparison NOHARM detect item cluster recovery counting dimension allocate item,0.024733050795419004,0.024733633861610226,0.02477139681930245,0.9009198458703506,0.02484207265331773,0.01694119226495422,0.01989367358566541,0.0,0.07535959141106317,0.010351217704730584
Andries Van Der Ark L.; Emons W.H.M.; Sijtsma K.,Detecting answer copying using alternate test forms and seat locations in small-scale examinations,2008,45,"Two types of answer-copying statistics for detecting copiers in small-scale examinations are proposed. One statistic identifies the ""copier- source"" pair, and the other in addition suggests who is copier and who is source. Both types of statistics can be used when the examination has alternate test forms. A simulation study shows that the statistics do not depend on the total-test score. Another simulation study compares the statistics with two known statistics, and shows that they have substantial power. The new statistics are applied to data from a small-scale examination (N = 230) with two alternate test forms. Auxiliary information on the seat location of the examinees and the test scores of the examinees was used to determine whether or not examinees could be suspected. © 2008 by the National Council on Measurement in Education.",Detecting answer copying using alternate test forms and seat locations in small-scale examinations,"Two types of answer-copying statistics for detecting copiers in small-scale examinations are proposed. One statistic identifies the ""copier- source"" pair, and the other in addition suggests who is copier and who is source. Both types of statistics can be used when the examination has alternate test forms. A simulation study shows that the statistics do not depend on the total-test score. Another simulation study compares the statistics with two known statistics, and shows that they have substantial power. The new statistics are applied to data from a small-scale examination (N = 230) with two alternate test forms. Auxiliary information on the seat location of the examinees and the test scores of the examinees was used to determine whether or not examinees could be suspected. © 2008 by the National Council on Measurement in Education.","['type', 'answercopye', 'statistic', 'detect', 'copier', 'smallscale', 'examination', 'propose', 'statistic', 'identify', 'copi', 'source', 'pair', 'addition', 'suggest', 'copi', 'source', 'type', 'statistic', 'examination', 'alternate', 'test', 'form', 'simulation', 'study', 'statistic', 'depend', 'totalt', 'score', 'simulation', 'study', 'compare', 'statistic', 'know', 'statistic', 'substantial', 'power', 'new', 'statistic', 'apply', 'datum', 'smallscale', 'examination', 'N', '230', 'alternate', 'test', 'form', 'auxiliary', 'information', 'seat', 'location', 'examinee', 'test', 'score', 'examinee', 'determine', 'examinee', 'suspect', '©', '2008', 'National', 'Council']","['detect', 'answer', 'copying', 'alternate', 'test', 'form', 'seat', 'location', 'smallscale', 'examination']",type answercopye statistic detect copier smallscale examination propose statistic identify copi source pair addition suggest copi source type statistic examination alternate test form simulation study statistic depend totalt score simulation study compare statistic know statistic substantial power new statistic apply datum smallscale examination N 230 alternate test form auxiliary information seat location examinee test score examinee determine examinee suspect © 2008 National Council,detect answer copying alternate test form seat location smallscale examination,0.03246442154302448,0.032465840873483885,0.032458135018920455,0.4482775761240909,0.45433402644048015,0.03354304243709477,0.01971500543648841,0.009771980892526455,0.021641694328612432,0.0034117362702209427
Holland P.W.; Sinharay S.; Von Davier A.A.; Han N.,An approach to evaluating the missing data assumptions of the chain and post-stratification equating methods for the NEAT design,2008,45,"Two important types of observed score equating (OSE) methods for the non-equivalent groups with Anchor Test (NEAT) design are chain equating (CE) and post-stratification equating (PSE). CE and PSE reflect two distinctly different ways of using the information provided by the anchor test for computing OSE functions. Both types of methods include linear and nonlinear equating functions. In practical situations, it is known that the PSE and CE methods will give different results when the two groups of examinees differ on the anchor test. However, given that both types of methods are justified as OSE methods by making different assumptions about the missing data in the NEAT design, it is difficult to conclude which, if either, of the two is more correct in a particular situation. This study compares the predictions of the PSE and CE assumptions for the missing data using a special data set for which the usually missing data are available. Our results indicate that in an equating setting where the linking function is decidedly non-linear and CE and PSE ought to be different, both sets of predictions are quite similar but those for CE are slightly more accurate. © 2007 by the National Council on Measurement in Education.",An approach to evaluating the missing data assumptions of the chain and post-stratification equating methods for the NEAT design,"Two important types of observed score equating (OSE) methods for the non-equivalent groups with Anchor Test (NEAT) design are chain equating (CE) and post-stratification equating (PSE). CE and PSE reflect two distinctly different ways of using the information provided by the anchor test for computing OSE functions. Both types of methods include linear and nonlinear equating functions. In practical situations, it is known that the PSE and CE methods will give different results when the two groups of examinees differ on the anchor test. However, given that both types of methods are justified as OSE methods by making different assumptions about the missing data in the NEAT design, it is difficult to conclude which, if either, of the two is more correct in a particular situation. This study compares the predictions of the PSE and CE assumptions for the missing data using a special data set for which the usually missing data are available. Our results indicate that in an equating setting where the linking function is decidedly non-linear and CE and PSE ought to be different, both sets of predictions are quite similar but those for CE are slightly more accurate. © 2007 by the National Council on Measurement in Education.","['important', 'type', 'observed', 'score', 'equate', 'OSE', 'method', 'nonequivalent', 'group', 'Anchor', 'Test', 'NEAT', 'design', 'chain', 'equate', 'CE', 'poststratification', 'equate', 'PSE', 'CE', 'PSE', 'reflect', 'distinctly', 'different', 'way', 'information', 'provide', 'anchor', 'test', 'compute', 'OSE', 'function', 'type', 'method', 'include', 'linear', 'nonlinear', 'equate', 'function', 'practical', 'situation', 'know', 'PSE', 'CE', 'method', 'different', 'result', 'group', 'examinee', 'differ', 'anchor', 'test', 'type', 'method', 'justify', 'OSE', 'method', 'different', 'assumption', 'miss', 'datum', 'NEAT', 'design', 'difficult', 'conclude', 'correct', 'particular', 'situation', 'study', 'compare', 'prediction', 'PSE', 'CE', 'assumption', 'miss', 'datum', 'special', 'datum', 'set', 'usually', 'missing', 'datum', 'available', 'result', 'indicate', 'equating', 'set', 'linking', 'function', 'decidedly', 'nonlinear', 'CE', 'PSE', 'ought', 'different', 'set', 'prediction', 'similar', 'CE', 'slightly', 'accurate', '©', '2007', 'National', 'Council']","['approach', 'evaluate', 'miss', 'data', 'assumption', 'chain', 'poststratification', 'equate', 'method', 'NEAT', 'design']",important type observed score equate OSE method nonequivalent group Anchor Test NEAT design chain equate CE poststratification equate PSE CE PSE reflect distinctly different way information provide anchor test compute OSE function type method include linear nonlinear equate function practical situation know PSE CE method different result group examinee differ anchor test type method justify OSE method different assumption miss datum NEAT design difficult conclude correct particular situation study compare prediction PSE CE assumption miss datum special datum set usually missing datum available result indicate equating set linking function decidedly nonlinear CE PSE ought different set prediction similar CE slightly accurate © 2007 National Council,approach evaluate miss data assumption chain poststratification equate method NEAT design,0.029639331649913066,0.02964061490633951,0.029672016552837688,0.48466153715493304,0.4263864997359767,0.0,0.008962717484893853,0.1183840329929066,0.004391323312004402,0.005909970715314443
Zwick R.; Greif Green J.,"New perspectives on the correlation of SAT scores, high school grades, and socioeconomic factors",2007,44,"In studies of the SAT, correlations of SAT scores, high school grades, and socioeconomic factors (SES) are usually obtained using a university as the unit of analysis. This approach obscures an important structural aspect of the data: The high school grades received by a given institution come from a large number of high schools, all of which have potentially different grading standards. SAT scores, on the other hand, can be assumed to have the same meaning across high schools. Our analyses of a large national sample show that, when pooled within-high-school analyses are applied, high school grades and class rank have larger correlations with family income and education than is evident in the results of typical analyses, and SAT scores have smaller associations with socioeconomic factors. SAT scores and high school grades, therefore, have more similar associations with SES than they do when only the usual across-high-school correlations are considered. © 2007 by the National Council on Measurement in Education.","New perspectives on the correlation of SAT scores, high school grades, and socioeconomic factors","In studies of the SAT, correlations of SAT scores, high school grades, and socioeconomic factors (SES) are usually obtained using a university as the unit of analysis. This approach obscures an important structural aspect of the data: The high school grades received by a given institution come from a large number of high schools, all of which have potentially different grading standards. SAT scores, on the other hand, can be assumed to have the same meaning across high schools. Our analyses of a large national sample show that, when pooled within-high-school analyses are applied, high school grades and class rank have larger correlations with family income and education than is evident in the results of typical analyses, and SAT scores have smaller associations with socioeconomic factors. SAT scores and high school grades, therefore, have more similar associations with SES than they do when only the usual across-high-school correlations are considered. © 2007 by the National Council on Measurement in Education.","['study', 'SAT', 'correlation', 'SAT', 'score', 'high', 'school', 'grade', 'socioeconomic', 'factor', 'SES', 'usually', 'obtain', 'university', 'unit', 'analysis', 'approach', 'obscure', 'important', 'structural', 'aspect', 'datum', 'high', 'school', 'grade', 'receive', 'institution', 'come', 'large', 'number', 'high', 'school', 'potentially', 'different', 'grade', 'standard', 'SAT', 'score', 'hand', 'assume', 'meaning', 'high', 'school', 'analysis', 'large', 'national', 'sample', 'pool', 'withinhighschool', 'analysis', 'apply', 'high', 'school', 'grade', 'class', 'rank', 'large', 'correlation', 'family', 'income', 'evident', 'result', 'typical', 'analysis', 'SAT', 'score', 'small', 'association', 'socioeconomic', 'factor', 'SAT', 'score', 'high', 'school', 'grade', 'similar', 'association', 'SES', 'usual', 'acrosshighschool', 'correlation', 'consider', '©', '2007', 'National', 'Council']","['new', 'perspective', 'correlation', 'SAT', 'score', 'high', 'school', 'grade', 'socioeconomic', 'factor']",study SAT correlation SAT score high school grade socioeconomic factor SES usually obtain university unit analysis approach obscure important structural aspect datum high school grade receive institution come large number high school potentially different grade standard SAT score hand assume meaning high school analysis large national sample pool withinhighschool analysis apply high school grade class rank large correlation family income evident result typical analysis SAT score small association socioeconomic factor SAT score high school grade similar association SES usual acrosshighschool correlation consider © 2007 National Council,new perspective correlation SAT score high school grade socioeconomic factor,0.02906403978347112,0.029060424621145563,0.029151839464741938,0.3751516961976986,0.5375719999329427,0.0,0.0,0.0022062152708170308,0.0,0.18962037027066053
Bridgeman B.; Cline F.,Effects of differentially time-consuming tests on computer-adaptive test scores,2004,41,"Time limits on some computer-adaptive tests (CATs) are such that many examinees have difficulty finishing, and some examinees may be administered tests with more time-consuming items than others. Results from over 100,000 examinees suggested that about half of the examinees must guess on the final six questions of the analytical section of the Graduate Record Examination if they were to finish before time expires. At the higher-ability levels, even more guessing was required because the questions administered to higher-ability examinees were typically more time consuming. Because the scoring model is not designed to cope with extended strings of guesses, substantial errors in ability estimates can be introduced when CATs have strict time limits. Furthermore, examinees who are administered tests with a disproportionate number of time-consuming items appear to get lower scores than examinees of comparable ability who are administered tests containing items that can be answered more quickly, though the issue is very complex because of the relationship of time and difficulty, and the multidimensionality of the test.",Effects of differentially time-consuming tests on computer-adaptive test scores,"Time limits on some computer-adaptive tests (CATs) are such that many examinees have difficulty finishing, and some examinees may be administered tests with more time-consuming items than others. Results from over 100,000 examinees suggested that about half of the examinees must guess on the final six questions of the analytical section of the Graduate Record Examination if they were to finish before time expires. At the higher-ability levels, even more guessing was required because the questions administered to higher-ability examinees were typically more time consuming. Because the scoring model is not designed to cope with extended strings of guesses, substantial errors in ability estimates can be introduced when CATs have strict time limits. Furthermore, examinees who are administered tests with a disproportionate number of time-consuming items appear to get lower scores than examinees of comparable ability who are administered tests containing items that can be answered more quickly, though the issue is very complex because of the relationship of time and difficulty, and the multidimensionality of the test.","['time', 'limit', 'computeradaptive', 'test', 'cat', 'examinee', 'difficulty', 'finish', 'examinee', 'administer', 'test', 'timeconsuming', 'item', 'result', '100000', 'examinee', 'suggest', 'half', 'examinee', 'guess', 'final', 'question', 'analytical', 'section', 'Graduate', 'Record', 'Examination', 'finish', 'time', 'expire', 'higherability', 'level', 'guess', 'require', 'question', 'administer', 'higherability', 'examinee', 'typically', 'time', 'consume', 'scoring', 'design', 'cope', 'extended', 'string', 'guess', 'substantial', 'error', 'ability', 'estimate', 'introduce', 'cat', 'strict', 'time', 'limit', 'furthermore', 'examinee', 'administer', 'test', 'disproportionate', 'number', 'timeconsuming', 'item', 'appear', 'low', 'score', 'examinee', 'comparable', 'ability', 'administer', 'test', 'contain', 'item', 'answer', 'quickly', 'issue', 'complex', 'relationship', 'time', 'difficulty', 'multidimensionality', 'test']","['effect', 'differentially', 'timeconsuming', 'test', 'computeradaptive', 'test', 'score']",time limit computeradaptive test cat examinee difficulty finish examinee administer test timeconsuming item result 100000 examinee suggest half examinee guess final question analytical section Graduate Record Examination finish time expire higherability level guess require question administer higherability examinee typically time consume scoring design cope extended string guess substantial error ability estimate introduce cat strict time limit furthermore examinee administer test disproportionate number timeconsuming item appear low score examinee comparable ability administer test contain item answer quickly issue complex relationship time difficulty multidimensionality test,effect differentially timeconsuming test computeradaptive test score,0.027610351310482922,0.027608856380207904,0.028053392514946002,0.8890095366809773,0.027717863113385973,0.0,0.0,0.0,0.0855515107016412,0.003714695235545181
Oshima T.C.; Raju N.S.; Nanda A.O.,A new method for assessing the statistical significance in the Differential Functioning of Items and Tests (DFIT) framework,2006,43,"A new item parameter replication method is proposed for assessing the statistical significance of the noncompensatory differential item functioning (NCDIF) index associated with the differential functioning of items and tests framework. In this new method, a cutoff score for each item is determined by obtaining a (1 -α) percentile rank score from a frequency distribution of NCDIF values under the no-DIF condition by generating a large number of item parameters based on the item parameter estimates and their variance-covariance structures from a computer program such as BIILOG-MG3. This cutoff for each item can be used as the basis for determining whether a given NCDIF index is significantly different from zero. This new method has definite advantages over the current method and yields cutoff values that are tailored to a particular data set and a particular item. A Monte Carlo assessment of this new method is presented and discussed. © Copyright 2006 by the National Council on Measurement in Education.",A new method for assessing the statistical significance in the Differential Functioning of Items and Tests (DFIT) framework,"A new item parameter replication method is proposed for assessing the statistical significance of the noncompensatory differential item functioning (NCDIF) index associated with the differential functioning of items and tests framework. In this new method, a cutoff score for each item is determined by obtaining a (1 -α) percentile rank score from a frequency distribution of NCDIF values under the no-DIF condition by generating a large number of item parameters based on the item parameter estimates and their variance-covariance structures from a computer program such as BIILOG-MG3. This cutoff for each item can be used as the basis for determining whether a given NCDIF index is significantly different from zero. This new method has definite advantages over the current method and yields cutoff values that are tailored to a particular data set and a particular item. A Monte Carlo assessment of this new method is presented and discussed. © Copyright 2006 by the National Council on Measurement in Education.","['new', 'item', 'parameter', 'replication', 'method', 'propose', 'assess', 'statistical', 'significance', 'noncompensatory', 'differential', 'item', 'function', 'NCDIF', 'index', 'associate', 'differential', 'functioning', 'item', 'test', 'framework', 'new', 'method', 'cutoff', 'score', 'item', 'determine', 'obtain', '1', 'α', 'percentile', 'rank', 'score', 'frequency', 'distribution', 'ncdif', 'value', 'nodif', 'condition', 'generate', 'large', 'number', 'item', 'parameter', 'base', 'item', 'parameter', 'estimate', 'variancecovariance', 'structure', 'computer', 'program', 'BIILOGMG3', 'cutoff', 'item', 'basis', 'determine', 'NCDIF', 'index', 'significantly', 'different', 'zero', 'new', 'method', 'definite', 'advantage', 'current', 'method', 'yield', 'cutoff', 'value', 'tailor', 'particular', 'datum', 'set', 'particular', 'item', 'A', 'Monte', 'Carlo', 'assessment', 'new', 'method', 'present', 'discuss', '©', 'copyright', '2006', 'National', 'Council']","['new', 'method', 'assess', 'statistical', 'significance', 'Differential', 'Functioning', 'Items', 'Tests', 'dfit', 'framework']",new item parameter replication method propose assess statistical significance noncompensatory differential item function NCDIF index associate differential functioning item test framework new method cutoff score item determine obtain 1 α percentile rank score frequency distribution ncdif value nodif condition generate large number item parameter base item parameter estimate variancecovariance structure computer program BIILOGMG3 cutoff item basis determine NCDIF index significantly different zero new method definite advantage current method yield cutoff value tailor particular datum set particular item A Monte Carlo assessment new method present discuss © copyright 2006 National Council,new method assess statistical significance Differential Functioning Items Tests dfit framework,0.025964259760635638,0.025964966295217377,0.025975395095375154,0.8960018603614748,0.02609351848729714,0.01612782528688514,0.011584813304960594,0.012254222664095022,0.09266941505517223,0.0017509861007255045
Wollack J.A.; Cohen A.S.; Wells C.S.,A Method for Maintaining Scale Stability in the Presence of Test Speededness,2003,40,"Administering tests under time constraints may result in poorly estimated item parameters, particularly for items at the end of the test (Douglas, Kim, Habing, & Gao, 1998; Oshima, 1994). Bolt, Cohen, and Wollack (2002) developed an item response theory mixture model to identify a latent group of examinees for whom a test is overly speeded, and found that item parameter estimates for end-of-test items in the nonspeeded group were similar to estimates for those same items when administered earlier in the test. In this study, we used the Bolt et al. (2002) method to study the effect of removing speeded examinees on the stability of a score scale over an 11-year period. Results indicated that using only the nonspeeded examinees for equating and estimating item parameters provided a more unidimensional scale, smaller effects of item parameter drift (including fewer drifting items), and less scale drift (i.e., bias) and variability (i.e., root mean squared errors) when compared to the total group of examinees.",A Method for Maintaining Scale Stability in the Presence of Test Speededness,"Administering tests under time constraints may result in poorly estimated item parameters, particularly for items at the end of the test (Douglas, Kim, Habing, & Gao, 1998; Oshima, 1994). Bolt, Cohen, and Wollack (2002) developed an item response theory mixture model to identify a latent group of examinees for whom a test is overly speeded, and found that item parameter estimates for end-of-test items in the nonspeeded group were similar to estimates for those same items when administered earlier in the test. In this study, we used the Bolt et al. (2002) method to study the effect of removing speeded examinees on the stability of a score scale over an 11-year period. Results indicated that using only the nonspeeded examinees for equating and estimating item parameters provided a more unidimensional scale, smaller effects of item parameter drift (including fewer drifting items), and less scale drift (i.e., bias) and variability (i.e., root mean squared errors) when compared to the total group of examinees.","['administer', 'test', 'time', 'constraint', 'result', 'poorly', 'estimate', 'item', 'parameter', 'particularly', 'item', 'end', 'test', 'Douglas', 'Kim', 'Habing', 'Gao', '1998', 'Oshima', '1994', 'Bolt', 'Cohen', 'Wollack', '2002', 'develop', 'item', 'response', 'theory', 'mixture', 'identify', 'latent', 'group', 'examinee', 'test', 'overly', 'speed', 'find', 'item', 'parameter', 'estimate', 'endoftest', 'item', 'nonspeede', 'group', 'similar', 'estimate', 'item', 'administer', 'early', 'test', 'study', 'Bolt', 'et', 'al', '2002', 'method', 'study', 'effect', 'remove', 'speed', 'examinee', 'stability', 'score', 'scale', '11year', 'period', 'result', 'indicate', 'nonspeede', 'examinee', 'equate', 'estimate', 'item', 'parameter', 'provide', 'unidimensional', 'scale', 'small', 'effect', 'item', 'parameter', 'drift', 'include', 'drift', 'item', 'scale', 'drift', 'ie', 'bias', 'variability', 'ie', 'root', 'mean', 'square', 'error', 'compare', 'total', 'group', 'examinee']","['Method', 'maintain', 'Scale', 'Stability', 'Presence', 'Test', 'Speededness']",administer test time constraint result poorly estimate item parameter particularly item end test Douglas Kim Habing Gao 1998 Oshima 1994 Bolt Cohen Wollack 2002 develop item response theory mixture identify latent group examinee test overly speed find item parameter estimate endoftest item nonspeede group similar estimate item administer early test study Bolt et al 2002 method study effect remove speed examinee stability score scale 11year period result indicate nonspeede examinee equate estimate item parameter provide unidimensional scale small effect item parameter drift include drift item scale drift ie bias variability ie root mean square error compare total group examinee,Method maintain Scale Stability Presence Test Speededness,0.024765929756053025,0.024766587446152167,0.024762091260628057,0.9008371520492138,0.02486823948795292,0.005179135264918576,0.0029677738312369065,0.018484260246453468,0.10453903635155087,0.007829753438456672
Lei P.-W.; Chen S.-Y.; Yu L.,Comparing methods of assessing differential item functioning in a computerized adaptive testing environment,2006,43,"Mantel-Haenszel and SIBTEST, which have known difficulty in detecting non-unidirectional differential item functioning (DIF), have been adapted with some success for computerized adaptive testing (CAT). This study adapts logistic regression (LR) and the item-response-theory-likelihood-ratio test (IRT-LRT), capable of detecting both unidirectional and non-unidirectional DIP, to the CAT environment in which pretest items are assumed to be seeded in CATs but not used for trait estimation. The proposed adaptation methods were evaluated with simulated data under different sample size ratios and impact conditions in terms of Type I error, power, and specificity in identifying the form of DIF. The adapted LR and IRT-LRT procedures are more powerful than the CAT version of SIBTEST for non-unidirectional DIF detection. The good Type I error control provided by IRT-LRT under extremely unequal sample sizes and large impact is encouraging. Implications of these and other findings are discussed.",Comparing methods of assessing differential item functioning in a computerized adaptive testing environment,"Mantel-Haenszel and SIBTEST, which have known difficulty in detecting non-unidirectional differential item functioning (DIF), have been adapted with some success for computerized adaptive testing (CAT). This study adapts logistic regression (LR) and the item-response-theory-likelihood-ratio test (IRT-LRT), capable of detecting both unidirectional and non-unidirectional DIP, to the CAT environment in which pretest items are assumed to be seeded in CATs but not used for trait estimation. The proposed adaptation methods were evaluated with simulated data under different sample size ratios and impact conditions in terms of Type I error, power, and specificity in identifying the form of DIF. The adapted LR and IRT-LRT procedures are more powerful than the CAT version of SIBTEST for non-unidirectional DIF detection. The good Type I error control provided by IRT-LRT under extremely unequal sample sizes and large impact is encouraging. Implications of these and other findings are discussed.","['MantelHaenszel', 'SIBTEST', 'know', 'difficulty', 'detect', 'nonunidirectional', 'differential', 'item', 'function', 'DIF', 'adapt', 'success', 'computerized', 'adaptive', 'testing', 'CAT', 'study', 'adapt', 'logistic', 'regression', 'LR', 'itemresponsetheorylikelihoodratio', 'test', 'IRTLRT', 'capable', 'detect', 'unidirectional', 'nonunidirectional', 'DIP', 'CAT', 'environment', 'pretest', 'item', 'assume', 'seed', 'cats', 'trait', 'estimation', 'propose', 'adaptation', 'method', 'evaluate', 'simulated', 'datum', 'different', 'sample', 'size', 'ratio', 'impact', 'condition', 'term', 'Type', 'I', 'error', 'power', 'specificity', 'identify', 'form', 'DIF', 'adapt', 'LR', 'IRTLRT', 'procedure', 'powerful', 'CAT', 'version', 'SIBTEST', 'nonunidirectional', 'dif', 'detection', 'good', 'Type', 'I', 'error', 'control', 'provide', 'IRTLRT', 'extremely', 'unequal', 'sample', 'size', 'large', 'impact', 'encourage', 'implication', 'finding', 'discuss']","['compare', 'method', 'assess', 'differential', 'item', 'function', 'computerized', 'adaptive', 'testing', 'environment']",MantelHaenszel SIBTEST know difficulty detect nonunidirectional differential item function DIF adapt success computerized adaptive testing CAT study adapt logistic regression LR itemresponsetheorylikelihoodratio test IRTLRT capable detect unidirectional nonunidirectional DIP CAT environment pretest item assume seed cats trait estimation propose adaptation method evaluate simulated datum different sample size ratio impact condition term Type I error power specificity identify form DIF adapt LR IRTLRT procedure powerful CAT version SIBTEST nonunidirectional dif detection good Type I error control provide IRTLRT extremely unequal sample size large impact encourage implication finding discuss,compare method assess differential item function computerized adaptive testing environment,0.02536328620270924,0.025364530357679133,0.02538100974782461,0.5243809355262347,0.39951023816555237,0.10542445653301441,0.0,0.0,0.015966599769961235,0.0
Briggs D.C.; Wilson M.,Generalizability in item response modeling,2007,44,"An approach called generalizability in item response modeling (GIRM) is introduced in this article. The GIRM approach essentially incorporates the sampling model of generalizability theory (GT) into the scaling model of item response theory (IRT) by making distributional assumptions about the relevant measurement facets. By specifying a random effects measurement model, and taking advantage of the flexibility of Markov Chain Monte Carlo (MCMC) estimation methods, it becomes possible to estimate GT variance components simultaneously with traditional IRT parameters. It is shown how GT and IRT can be linked together, in the context of a single-facet measurement design with binary items. Using both simulated and empirical data with the software WinBUGS, the GIRM approach is shown to produce results comparable to those from a standard GT analysis, while also producing results from a random effects IRT model. © 2007 by the National Council on Measurement in Education.",,"An approach called generalizability in item response modeling (GIRM) is introduced in this article. The GIRM approach essentially incorporates the sampling model of generalizability theory (GT) into the scaling model of item response theory (IRT) by making distributional assumptions about the relevant measurement facets. By specifying a random effects measurement model, and taking advantage of the flexibility of Markov Chain Monte Carlo (MCMC) estimation methods, it becomes possible to estimate GT variance components simultaneously with traditional IRT parameters. It is shown how GT and IRT can be linked together, in the context of a single-facet measurement design with binary items. Using both simulated and empirical data with the software WinBUGS, the GIRM approach is shown to produce results comparable to those from a standard GT analysis, while also producing results from a random effects IRT model. © 2007 by the National Council on Measurement in Education.","['approach', 'generalizability', 'item', 'response', 'GIRM', 'introduce', 'article', 'GIRM', 'approach', 'essentially', 'incorporate', 'sample', 'generalizability', 'theory', 'GT', 'scaling', 'item', 'response', 'theory', 'IRT', 'distributional', 'assumption', 'relevant', 'facet', 'specify', 'random', 'effect', 'advantage', 'flexibility', 'Markov', 'Chain', 'Monte', 'Carlo', 'MCMC', 'estimation', 'method', 'possible', 'estimate', 'GT', 'variance', 'component', 'simultaneously', 'traditional', 'IRT', 'parameter', 'GT', 'IRT', 'link', 'context', 'singlefacet', 'design', 'binary', 'item', 'simulated', 'empirical', 'datum', 'software', 'WinBUGS', 'GIRM', 'approach', 'produce', 'result', 'comparable', 'standard', 'GT', 'analysis', 'produce', 'result', 'random', 'effect', 'IRT', '©', '2007', 'National', 'Council']",,approach generalizability item response GIRM introduce article GIRM approach essentially incorporate sample generalizability theory GT scaling item response theory IRT distributional assumption relevant facet specify random effect advantage flexibility Markov Chain Monte Carlo MCMC estimation method possible estimate GT variance component simultaneously traditional IRT parameter GT IRT link context singlefacet design binary item simulated empirical datum software WinBUGS GIRM approach produce result comparable standard GT analysis produce result random effect IRT © 2007 National Council,,0.028220883368548753,0.0282198137697405,0.028214125923863062,0.8868475495045822,0.028497627433265463,0.0,0.08422048895729894,0.0,0.03979221227757361,0.0
Skaggs G.,Accuracy of random groups equating with very small samples,2005,42,"This study investigated the effectiveness of equating with very small samples using the random groups design. Of particular interest was equating accuracy at specific scores where performance standards might be set. Two sets of simulations were carried out, one in which the two forms were identical and one in which they differed by a tenth of a standard deviation in overall difficulty. These forms were equated using mean equating, linear equating, unsmoothed equipercentile equating, and equipercentile equating using two through six moments of log-linear presmoothing with samples of 25, 50, 75, 100, 150, and 200. The results indicated that identity equating was preferable to any equating method when samples were as small as 25. For samples of 50 and above, the choice of an equating method over identity equating depended on the location of the passing score relative to examinee performance. If passing scores were located below the mean, where data were sparser, mean equating produced the smallest percentage of misclassified examinees. For passing scores near the mean, all methods produced similar results with linear equating being the most accurate. For passing scores above the mean, equipercentile equating with 2-and 3-moment presmoothing were the best equating methods. Higher levels of presmoothing did not improve the results.",Accuracy of random groups equating with very small samples,"This study investigated the effectiveness of equating with very small samples using the random groups design. Of particular interest was equating accuracy at specific scores where performance standards might be set. Two sets of simulations were carried out, one in which the two forms were identical and one in which they differed by a tenth of a standard deviation in overall difficulty. These forms were equated using mean equating, linear equating, unsmoothed equipercentile equating, and equipercentile equating using two through six moments of log-linear presmoothing with samples of 25, 50, 75, 100, 150, and 200. The results indicated that identity equating was preferable to any equating method when samples were as small as 25. For samples of 50 and above, the choice of an equating method over identity equating depended on the location of the passing score relative to examinee performance. If passing scores were located below the mean, where data were sparser, mean equating produced the smallest percentage of misclassified examinees. For passing scores near the mean, all methods produced similar results with linear equating being the most accurate. For passing scores above the mean, equipercentile equating with 2-and 3-moment presmoothing were the best equating methods. Higher levels of presmoothing did not improve the results.","['study', 'investigate', 'effectiveness', 'equate', 'small', 'sample', 'random', 'group', 'design', 'particular', 'interest', 'equate', 'accuracy', 'specific', 'score', 'performance', 'standard', 'set', 'set', 'simulation', 'carry', 'form', 'identical', 'differ', 'tenth', 'standard', 'deviation', 'overall', 'difficulty', 'form', 'equate', 'mean', 'equate', 'linear', 'equate', 'unsmoothed', 'equipercentile', 'equating', 'equipercentile', 'equating', 'moment', 'loglinear', 'presmoothe', 'sample', '25', '50', '75', '100', '150', '200', 'result', 'indicate', 'identity', 'equating', 'preferable', 'equate', 'method', 'sample', 'small', '25', 'sample', '50', 'choice', 'equate', 'method', 'identity', 'equating', 'depend', 'location', 'pass', 'score', 'relative', 'examinee', 'performance', 'pass', 'score', 'locate', 'mean', 'datum', 'sparse', 'mean', 'equating', 'produce', 'small', 'percentage', 'misclassifie', 'examinee', 'pass', 'score', 'near', 'mean', 'method', 'produce', 'similar', 'result', 'linear', 'equate', 'accurate', 'pass', 'score', 'mean', 'equipercentile', 'equate', '2and', '3moment', 'presmoothing', 'good', 'equate', 'method', 'high', 'level', 'presmoothing', 'improve', 'result']","['accuracy', 'random', 'group', 'equate', 'small', 'sample']",study investigate effectiveness equate small sample random group design particular interest equate accuracy specific score performance standard set set simulation carry form identical differ tenth standard deviation overall difficulty form equate mean equate linear equate unsmoothed equipercentile equating equipercentile equating moment loglinear presmoothe sample 25 50 75 100 150 200 result indicate identity equating preferable equate method sample small 25 sample 50 choice equate method identity equating depend location pass score relative examinee performance pass score locate mean datum sparse mean equating produce small percentage misclassifie examinee pass score near mean method produce similar result linear equate accurate pass score mean equipercentile equate 2and 3moment presmoothing good equate method high level presmoothing improve result,accuracy random group equate small sample,0.02858622382148226,0.02857723183275541,0.028592082520572167,0.8855724520152448,0.02867200980994543,0.0,0.0,0.19224997098441957,0.0,0.0
Schulz E.M.; Betebenner D.; Ahn M.,Hierarchical logistic regression in course placement,2004,41,"Whether hierarchical logistic regression can reduce the sample size requirement for estimating optimal cutoff scores in a course placement service where predictive validity is measured by a threshold utility function is explored. Data from courses with varying class size were randomly partitioned into two halves per course. Non-hierarchical and hierarchical analyses were performed on each half. Compared to their nonhierarchical counterparts, hierarchically estimated cutoff scores from different halves were more stable and predicted course outcomes in the other half more accurately. These differences were mostpronounced with small samples. Sample size requirements for developing cutoff scores for course placement can be substantially reduced if hierarchical logistic regression is used.",Hierarchical logistic regression in course placement,"Whether hierarchical logistic regression can reduce the sample size requirement for estimating optimal cutoff scores in a course placement service where predictive validity is measured by a threshold utility function is explored. Data from courses with varying class size were randomly partitioned into two halves per course. Non-hierarchical and hierarchical analyses were performed on each half. Compared to their nonhierarchical counterparts, hierarchically estimated cutoff scores from different halves were more stable and predicted course outcomes in the other half more accurately. These differences were mostpronounced with small samples. Sample size requirements for developing cutoff scores for course placement can be substantially reduced if hierarchical logistic regression is used.","['hierarchical', 'logistic', 'regression', 'reduce', 'sample', 'size', 'requirement', 'estimate', 'optimal', 'cutoff', 'score', 'course', 'placement', 'service', 'predictive', 'validity', 'measure', 'threshold', 'utility', 'function', 'explore', 'datum', 'course', 'vary', 'class', 'size', 'randomly', 'partition', 'half', 'course', 'Nonhierarchical', 'hierarchical', 'analysis', 'perform', 'half', 'compare', 'nonhierarchical', 'counterpart', 'hierarchically', 'estimate', 'cutoff', 'score', 'different', 'half', 'stable', 'predict', 'course', 'outcome', 'half', 'accurately', 'difference', 'mostpronounce', 'small', 'sample', 'Sample', 'size', 'requirement', 'develop', 'cutoff', 'score', 'course', 'placement', 'substantially', 'reduce', 'hierarchical', 'logistic', 'regression']","['hierarchical', 'logistic', 'regression', 'course', 'placement']",hierarchical logistic regression reduce sample size requirement estimate optimal cutoff score course placement service predictive validity measure threshold utility function explore datum course vary class size randomly partition half course Nonhierarchical hierarchical analysis perform half compare nonhierarchical counterpart hierarchically estimate cutoff score different half stable predict course outcome half accurately difference mostpronounce small sample Sample size requirement develop cutoff score course placement substantially reduce hierarchical logistic regression,hierarchical logistic regression course placement,0.03224325382738313,0.03224486854168751,0.36247297506012843,0.5403428842504926,0.03269601832030837,0.010756385159637004,0.02952649497425786,0.017063074048022153,0.0,0.022448805659897396
Puhan G.; Moses T.P.; Grant M.C.; McHale F.,Small-sample equating using a single-group nearly equivalent test (SiGNET) design,2009,46,"A single-group (SG) equating with nearly equivalent test forms (SiGNET) design was developed by Grant to equate small-volume tests. Under this design, the scored items for the operational form are divided into testlets or mini tests. An additional testlet is created but not scored for the first form. If the scored testlets are testlets 1-6 and the unscored testlet is testlet 7, then the first form is composed of testlets 1-6 and the second form is composed of testlets 2-7. The seven testlets are administered as a single administered form, and when a sufficient number of examinees have taken the administered form, the second form (testlets 2-7) is equated to the first form (testlets 1-6) using an SG equating design. As evident, this design facilitates the use of an SG equating and allows for the accumulation of data, both of which may reduce equating error. This study compared equatings under the SiGNET and common-item equating designs and found lower equating error for the SiGNET design in very small sample size conditions (e.g., N = 10). © 2009 by the National Council on Measurement in Education.",Small-sample equating using a single-group nearly equivalent test (SiGNET) design,"A single-group (SG) equating with nearly equivalent test forms (SiGNET) design was developed by Grant to equate small-volume tests. Under this design, the scored items for the operational form are divided into testlets or mini tests. An additional testlet is created but not scored for the first form. If the scored testlets are testlets 1-6 and the unscored testlet is testlet 7, then the first form is composed of testlets 1-6 and the second form is composed of testlets 2-7. The seven testlets are administered as a single administered form, and when a sufficient number of examinees have taken the administered form, the second form (testlets 2-7) is equated to the first form (testlets 1-6) using an SG equating design. As evident, this design facilitates the use of an SG equating and allows for the accumulation of data, both of which may reduce equating error. This study compared equatings under the SiGNET and common-item equating designs and found lower equating error for the SiGNET design in very small sample size conditions (e.g., N = 10). © 2009 by the National Council on Measurement in Education.","['singlegroup', 'SG', 'equate', 'nearly', 'equivalent', 'test', 'form', 'SiGNET', 'design', 'develop', 'Grant', 'equate', 'smallvolume', 'test', 'design', 'score', 'item', 'operational', 'form', 'divide', 'testlet', 'mini', 'test', 'additional', 'testlet', 'create', 'score', 'form', 'score', 'testlet', 'testlet', '16', 'unscored', 'testlet', 'testlet', '7', 'form', 'compose', 'testlet', '16', 'second', 'form', 'compose', 'testlet', '27', 'seven', 'testlet', 'administer', 'single', 'administer', 'form', 'sufficient', 'number', 'examinee', 'administer', 'form', 'second', 'form', 'testlet', '27', 'equate', 'form', 'testlet', '16', 'SG', 'equate', 'design', 'evident', 'design', 'facilitate', 'SG', 'equating', 'allow', 'accumulation', 'datum', 'reduce', 'equate', 'error', 'study', 'compare', 'equating', 'SiGNET', 'commonitem', 'equate', 'design', 'find', 'low', 'equate', 'error', 'SiGNET', 'design', 'small', 'sample', 'size', 'condition', 'eg', 'N', '10', '©', '2009', 'National', 'Council']","['smallsample', 'equating', 'singlegroup', 'nearly', 'equivalent', 'test', 'SiGNET', 'design']",singlegroup SG equate nearly equivalent test form SiGNET design develop Grant equate smallvolume test design score item operational form divide testlet mini test additional testlet create score form score testlet testlet 16 unscored testlet testlet 7 form compose testlet 16 second form compose testlet 27 seven testlet administer single administer form sufficient number examinee administer form second form testlet 27 equate form testlet 16 SG equate design evident design facilitate SG equating allow accumulation datum reduce equate error study compare equating SiGNET commonitem equate design find low equate error SiGNET design small sample size condition eg N 10 © 2009 National Council,smallsample equating singlegroup nearly equivalent test SiGNET design,0.03405952298936088,0.03406164726906435,0.03405800378424086,0.8633016037891434,0.03451922216819043,0.0,0.0,0.11978478771026829,0.0,0.0
Kupermintz H.,On the reliability of categorically scored examinations,2004,41,A decision-theoretic approach to the question of reliability in categorically scored examinations is explored. The concepts of true scores and errors are discussed as they deviate from conventional psychometric definitions and measurement error in categorical scores is cast in terms of misclassifications. A reliability measure based on proportional reduction in loss (PRL) is then presented and exemplified with data from a large-scale assessment. The link between the PRL approach and the classical conception of reliability is discussed. Some design considerations for reliability studies are also discussed.,On the reliability of categorically scored examinations,A decision-theoretic approach to the question of reliability in categorically scored examinations is explored. The concepts of true scores and errors are discussed as they deviate from conventional psychometric definitions and measurement error in categorical scores is cast in terms of misclassifications. A reliability measure based on proportional reduction in loss (PRL) is then presented and exemplified with data from a large-scale assessment. The link between the PRL approach and the classical conception of reliability is discussed. Some design considerations for reliability studies are also discussed.,"['decisiontheoretic', 'approach', 'question', 'reliability', 'categorically', 'score', 'examination', 'explore', 'concept', 'true', 'score', 'error', 'discuss', 'deviate', 'conventional', 'psychometric', 'definition', 'error', 'categorical', 'score', 'cast', 'term', 'misclassification', 'reliability', 'measure', 'base', 'proportional', 'reduction', 'loss', 'PRL', 'present', 'exemplify', 'datum', 'largescale', 'assessment', 'link', 'prl', 'approach', 'classical', 'conception', 'reliability', 'discuss', 'design', 'consideration', 'reliability', 'study', 'discuss']","['reliability', 'categorically', 'score', 'examination']",decisiontheoretic approach question reliability categorically score examination explore concept true score error discuss deviate conventional psychometric definition error categorical score cast term misclassification reliability measure base proportional reduction loss PRL present exemplify datum largescale assessment link prl approach classical conception reliability discuss design consideration reliability study discuss,reliability categorically score examination,0.03080284965020159,0.03080397048615492,0.030796147288325883,0.876730552348638,0.030866480226679587,0.0,0.12669748460156974,0.002331495679968317,0.0,0.0
Bolt D.M.; Gierl M.J.,Testing features of graphical DIF: Application of a regression correction to three nonparametric statistical tests,2006,43,"Inspection of differential item functioning (DIF) in translated test items can be informed by graphical comparisons of item response functions (IRFs) across translated forms. Due to the many forms of DIF that can emerge in such analyses, it is important to develop statistical tests that can confirm various characteristics of DIF when present. Traditional nonparametric tests of DIF (Mantel-Haenszel, SIBTEST) are not designed to test for the presence of nonuniform or local DIF, while common probability difference (P-DIF) tests (e.g., SIBTEST) do not optimize power in testing for uniform DIF, and thus may be less useful in the context of graphical DIF analyses. In this article, modifications of three alternative nonparametric statistical tests for DIF, Fisher's χ 2 test, Cochran's Z test, and Goodman's U test (Marascuilo & Slaughter, 1981), are investigated for these purposes. A simulation study demonstrates the effectiveness of a regression correction procedure in improving the statistical performance of the tests when using an internal test score as the matching criterion. Simulation power and real data analyses demonstrate the unique information provided by these alternative methods compared to SIBTEST and Mantel-Haenszel in confirming various forms of DIF in translated tests.",Testing features of graphical DIF: Application of a regression correction to three nonparametric statistical tests,"Inspection of differential item functioning (DIF) in translated test items can be informed by graphical comparisons of item response functions (IRFs) across translated forms. Due to the many forms of DIF that can emerge in such analyses, it is important to develop statistical tests that can confirm various characteristics of DIF when present. Traditional nonparametric tests of DIF (Mantel-Haenszel, SIBTEST) are not designed to test for the presence of nonuniform or local DIF, while common probability difference (P-DIF) tests (e.g., SIBTEST) do not optimize power in testing for uniform DIF, and thus may be less useful in the context of graphical DIF analyses. In this article, modifications of three alternative nonparametric statistical tests for DIF, Fisher's χ 2 test, Cochran's Z test, and Goodman's U test (Marascuilo & Slaughter, 1981), are investigated for these purposes. A simulation study demonstrates the effectiveness of a regression correction procedure in improving the statistical performance of the tests when using an internal test score as the matching criterion. Simulation power and real data analyses demonstrate the unique information provided by these alternative methods compared to SIBTEST and Mantel-Haenszel in confirming various forms of DIF in translated tests.","['inspection', 'differential', 'item', 'function', 'DIF', 'translate', 'test', 'item', 'inform', 'graphical', 'comparison', 'item', 'response', 'function', 'irf', 'translate', 'form', 'form', 'DIF', 'emerge', 'analysis', 'important', 'develop', 'statistical', 'test', 'confirm', 'characteristic', 'DIF', 'present', 'traditional', 'nonparametric', 'test', 'DIF', 'MantelHaenszel', 'SIBTEST', 'design', 'test', 'presence', 'nonuniform', 'local', 'DIF', 'common', 'probability', 'difference', 'PDIF', 'test', 'eg', 'SIBTEST', 'optimize', 'power', 'testing', 'uniform', 'DIF', 'useful', 'context', 'graphical', 'DIF', 'analyse', 'article', 'modification', 'alternative', 'nonparametric', 'statistical', 'test', 'DIF', 'fisher', 'χ', '2', 'test', 'cochran', 'z', 'test', 'goodman', 'u', 'test', 'marascuilo', 'Slaughter', '1981', 'investigate', 'purpose', 'simulation', 'study', 'demonstrate', 'effectiveness', 'regression', 'correction', 'procedure', 'improve', 'statistical', 'performance', 'test', 'internal', 'test', 'score', 'matching', 'criterion', 'Simulation', 'power', 'real', 'datum', 'analysis', 'demonstrate', 'unique', 'information', 'provide', 'alternative', 'method', 'compare', 'SIBTEST', 'MantelHaenszel', 'confirm', 'form', 'dif', 'translate', 'test']","['testing', 'feature', 'graphical', 'DIF', 'application', 'regression', 'correction', 'nonparametric', 'statistical', 'test']",inspection differential item function DIF translate test item inform graphical comparison item response function irf translate form form DIF emerge analysis important develop statistical test confirm characteristic DIF present traditional nonparametric test DIF MantelHaenszel SIBTEST design test presence nonuniform local DIF common probability difference PDIF test eg SIBTEST optimize power testing uniform DIF useful context graphical DIF analyse article modification alternative nonparametric statistical test DIF fisher χ 2 test cochran z test goodman u test marascuilo Slaughter 1981 investigate purpose simulation study demonstrate effectiveness regression correction procedure improve statistical performance test internal test score matching criterion Simulation power real datum analysis demonstrate unique information provide alternative method compare SIBTEST MantelHaenszel confirm form dif translate test,testing feature graphical DIF application regression correction nonparametric statistical test,0.025165003963697554,0.025169435384843904,0.025270250503605977,0.8989004858533305,0.025494824294522128,0.16317649405330437,0.0,0.00367541750508865,0.0,0.0
Moses T.; Yang W.-L.; Wilson C.,Using kernel equating to assess item order effects on test scores,2007,44,"This study explored the use of kernel equating for integrating and extending two procedures proposed for assessing item order effects in test forms that have been administered to randomly equivalent groups. When these procedures are used together, they can provide complementary information about the extent to which item order effects impact test scores, in overall score distributions and also at specific test scores. In addition to detecting item order effects, the integrated procedures also suggest the equating function that most adequately adjusts the scores to mitigate the effects. To demonstrate, the statistical equivalences of alternate versions of two large-volume advanced placement exams were assessed. © 2007 by the National Council on Measurement in Education.",Using kernel equating to assess item order effects on test scores,"This study explored the use of kernel equating for integrating and extending two procedures proposed for assessing item order effects in test forms that have been administered to randomly equivalent groups. When these procedures are used together, they can provide complementary information about the extent to which item order effects impact test scores, in overall score distributions and also at specific test scores. In addition to detecting item order effects, the integrated procedures also suggest the equating function that most adequately adjusts the scores to mitigate the effects. To demonstrate, the statistical equivalences of alternate versions of two large-volume advanced placement exams were assessed. © 2007 by the National Council on Measurement in Education.","['study', 'explore', 'kernel', 'equate', 'integrate', 'extend', 'procedure', 'propose', 'assess', 'item', 'order', 'effect', 'test', 'form', 'administer', 'randomly', 'equivalent', 'group', 'procedure', 'provide', 'complementary', 'information', 'extent', 'item', 'order', 'effect', 'impact', 'test', 'score', 'overall', 'score', 'distribution', 'specific', 'test', 'score', 'addition', 'detect', 'item', 'order', 'effect', 'integrate', 'procedure', 'suggest', 'equate', 'function', 'adequately', 'adjust', 'score', 'mitigate', 'effect', 'demonstrate', 'statistical', 'equivalence', 'alternate', 'version', 'largevolume', 'advanced', 'placement', 'exam', 'assess', '©', '2007', 'National', 'Council']","['kernel', 'equating', 'assess', 'item', 'order', 'effect', 'test', 'score']",study explore kernel equate integrate extend procedure propose assess item order effect test form administer randomly equivalent group procedure provide complementary information extent item order effect impact test score overall score distribution specific test score addition detect item order effect integrate procedure suggest equate function adequately adjust score mitigate effect demonstrate statistical equivalence alternate version largevolume advanced placement exam assess © 2007 National Council,kernel equating assess item order effect test score,0.027867050617705256,0.02786785276051505,0.027899943669558707,0.8879608557963684,0.028404297155852472,0.03526980798553993,0.06121786389614855,0.06978433207756372,0.0007790328964767458,0.00415679880441901
Dodeen H.,The relationship between item parameters and item fit,2004,41,"The effect of item parameters (discrimination, difficulty, and level of guessing) on the item fit statistic was investigated using simulated dichotomous data. Nine tests were simulated using 1, 000 persons, 50 items, three levels of item discrimination, three levels of item difficulty, and three levels of guessing. The item fit was estimated using two fit statistics: the likelihood ratio statistic (ξB2), and the standardized residuals (SRs). All the item parameters were simulated to be normally distributed. Results showed that the levels of item discrimination and guessing affected the item-fit values. As the level of item discrimination or guessing increased, item-fit values increased and more items misfit the model. The level of item difficulty did not affect the item-fit statistic.",The relationship between item parameters and item fit,"The effect of item parameters (discrimination, difficulty, and level of guessing) on the item fit statistic was investigated using simulated dichotomous data. Nine tests were simulated using 1, 000 persons, 50 items, three levels of item discrimination, three levels of item difficulty, and three levels of guessing. The item fit was estimated using two fit statistics: the likelihood ratio statistic (ξB2), and the standardized residuals (SRs). All the item parameters were simulated to be normally distributed. Results showed that the levels of item discrimination and guessing affected the item-fit values. As the level of item discrimination or guessing increased, item-fit values increased and more items misfit the model. The level of item difficulty did not affect the item-fit statistic.","['effect', 'item', 'parameter', 'discrimination', 'difficulty', 'level', 'guess', 'item', 'fit', 'statistic', 'investigate', 'simulate', 'dichotomous', 'datum', 'test', 'simulate', '1', '000', 'person', '50', 'item', 'level', 'item', 'discrimination', 'level', 'item', 'difficulty', 'level', 'guess', 'item', 'fit', 'estimate', 'fit', 'statistic', 'likelihood', 'ratio', 'statistic', 'ξB2', 'standardized', 'residual', 'sr', 'item', 'parameter', 'simulate', 'normally', 'distribute', 'result', 'level', 'item', 'discrimination', 'guess', 'affect', 'itemfit', 'value', 'level', 'item', 'discrimination', 'guess', 'increase', 'itemfit', 'value', 'increase', 'item', 'misfit', 'level', 'item', 'difficulty', 'affect', 'itemfit', 'statistic']","['relationship', 'item', 'parameter', 'item', 'fit']",effect item parameter discrimination difficulty level guess item fit statistic investigate simulate dichotomous datum test simulate 1 000 person 50 item level item discrimination level item difficulty level guess item fit estimate fit statistic likelihood ratio statistic ξB2 standardized residual sr item parameter simulate normally distribute result level item discrimination guess affect itemfit value level item discrimination guess increase itemfit value increase item misfit level item difficulty affect itemfit statistic,relationship item parameter item fit,0.03500082304480817,0.03500174164516344,0.035013433274525795,0.859911191612095,0.03507281042340773,0.006022503370540527,0.0,0.0,0.10782183804889113,0.0
Wang W.-C.; Wilson M.; Shih C.-L.,Modeling randomness in judging rating scales with a random-effects rating scale model,2006,43,"This study presents the random-effects rating scale model (RE-RSM) which takes into account randomness in the thresholds over persons by treating them as random-effects and adding a random variable for each threshold in the rating scale model (RSM) (Andrich, 1978). The RE-RSM turns out to be a special case of the multidimensional random coefficients multinomial logit model (MRCMLM) (Adams, Wilson, & Wang, 1997) so that the estimation procedures for the MRCMLM can be directly applied. The results of the simulation indicated that when the data were generated from the RSM, using the RSM and the RE-RSM to fit the data made little difference: both resulting in accurate parameter recovery. When the data were generated from the RE-RSM, using the RE-RSM to fit the data resulted in unbiased estimates, whereas using the RSM resulted in biased estimates, large fit statistics for the thresholds, and inflated test reliability. An empirical example of 10 items with four-point rating scales was illustrated in which four models were compared: the RSM, the RE-RSM, the partial credit model (Masters, 1982), and the constrained random-effects partial credit model. In this real data set, the need for a random-effects formulation becomes clear.",Modeling randomness in judging rating scales with a random-effects rating scale model,"This study presents the random-effects rating scale model (RE-RSM) which takes into account randomness in the thresholds over persons by treating them as random-effects and adding a random variable for each threshold in the rating scale model (RSM) (Andrich, 1978). The RE-RSM turns out to be a special case of the multidimensional random coefficients multinomial logit model (MRCMLM) (Adams, Wilson, & Wang, 1997) so that the estimation procedures for the MRCMLM can be directly applied. The results of the simulation indicated that when the data were generated from the RSM, using the RSM and the RE-RSM to fit the data made little difference: both resulting in accurate parameter recovery. When the data were generated from the RE-RSM, using the RE-RSM to fit the data resulted in unbiased estimates, whereas using the RSM resulted in biased estimates, large fit statistics for the thresholds, and inflated test reliability. An empirical example of 10 items with four-point rating scales was illustrated in which four models were compared: the RSM, the RE-RSM, the partial credit model (Masters, 1982), and the constrained random-effects partial credit model. In this real data set, the need for a random-effects formulation becomes clear.","['study', 'present', 'randomeffect', 'rating', 'scale', 'RERSM', 'account', 'randomness', 'threshold', 'person', 'treat', 'randomeffect', 'add', 'random', 'variable', 'threshold', 'rating', 'scale', 'RSM', 'Andrich', '1978', 'RERSM', 'turn', 'special', 'case', 'multidimensional', 'random', 'coefficient', 'multinomial', 'logit', 'MRCMLM', 'Adams', 'Wilson', 'Wang', '1997', 'estimation', 'procedure', 'MRCMLM', 'directly', 'apply', 'result', 'simulation', 'indicate', 'datum', 'generate', 'RSM', 'RSM', 'RERSM', 'fit', 'datum', 'little', 'difference', 'result', 'accurate', 'parameter', 'recovery', 'datum', 'generate', 'RERSM', 'RERSM', 'fit', 'datum', 'result', 'unbiased', 'estimate', 'RSM', 'result', 'bias', 'estimate', 'large', 'fit', 'statistic', 'threshold', 'inflated', 'test', 'reliability', 'empirical', 'example', '10', 'item', 'fourpoint', 'rating', 'scale', 'illustrate', 'compare', 'RSM', 'RERSM', 'partial', 'credit', 'Masters', '1982', 'constrain', 'randomeffect', 'partial', 'credit', 'real', 'datum', 'set', 'need', 'randomeffect', 'formulation', 'clear']","['randomness', 'judge', 'rating', 'scale', 'randomeffect', 'rating', 'scale']",study present randomeffect rating scale RERSM account randomness threshold person treat randomeffect add random variable threshold rating scale RSM Andrich 1978 RERSM turn special case multidimensional random coefficient multinomial logit MRCMLM Adams Wilson Wang 1997 estimation procedure MRCMLM directly apply result simulation indicate datum generate RSM RSM RERSM fit datum little difference result accurate parameter recovery datum generate RERSM RERSM fit datum result unbiased estimate RSM result bias estimate large fit statistic threshold inflated test reliability empirical example 10 item fourpoint rating scale illustrate compare RSM RERSM partial credit Masters 1982 constrain randomeffect partial credit real datum set need randomeffect formulation clear,randomness judge rating scale randomeffect rating scale,0.027947395075885888,0.027949135328864744,0.028005834164942046,0.887924192434809,0.02817344299549822,0.0,0.0392924520474609,0.0,0.029338517716032363,0.010314269092987414
Kang T.; Chen T.T.,Performance of the generalized S-X2 item fit index for polytomous IRT models,2008,45,"Orlando and Thissen's S-X2 item fit index has performed better than traditional item fit statistics such as Yen's Q1 and McKinley and Mill's G2 for dichotomous item response theory (IRT) models. This study extends the utility of S-X2 to polytomous IRT models, including the generalized partial credit model, partial credit model, and rating scale model. The performance of the generalized S-X2 in assessing item model fit was studied in terms of empirical Type I error rates and power and compared to G2. The results suggest that the generalized S-X 2 is promising for polytomous items in educational and psychological testing programs. © 2008 by the National Council on Measurement in Education.",Performance of the generalized S-X2 item fit index for polytomous IRT models,"Orlando and Thissen's S-X2 item fit index has performed better than traditional item fit statistics such as Yen's Q1 and McKinley and Mill's G2 for dichotomous item response theory (IRT) models. This study extends the utility of S-X2 to polytomous IRT models, including the generalized partial credit model, partial credit model, and rating scale model. The performance of the generalized S-X2 in assessing item model fit was studied in terms of empirical Type I error rates and power and compared to G2. The results suggest that the generalized S-X 2 is promising for polytomous items in educational and psychological testing programs. © 2008 by the National Council on Measurement in Education.","['Orlando', 'Thissens', 'SX2', 'item', 'fit', 'index', 'perform', 'traditional', 'item', 'fit', 'statistic', 'Yens', 'Q1', 'McKinley', 'Mills', 'G2', 'dichotomous', 'item', 'response', 'theory', 'IRT', 'study', 'extend', 'utility', 'SX2', 'polytomous', 'IRT', 'include', 'generalized', 'partial', 'credit', 'partial', 'credit', 'rating', 'scale', 'performance', 'generalized', 'SX2', 'assess', 'item', 'fit', 'study', 'term', 'empirical', 'type', 'I', 'error', 'rate', 'power', 'compare', 'G2', 'result', 'suggest', 'generalized', 'sx', '2', 'promise', 'polytomous', 'item', 'educational', 'psychological', 'testing', 'program', '©', '2008', 'National', 'Council']","['performance', 'generalized', 'SX2', 'item', 'fit', 'index', 'polytomous', 'IRT']",Orlando Thissens SX2 item fit index perform traditional item fit statistic Yens Q1 McKinley Mills G2 dichotomous item response theory IRT study extend utility SX2 polytomous IRT include generalized partial credit partial credit rating scale performance generalized SX2 assess item fit study term empirical type I error rate power compare G2 result suggest generalized sx 2 promise polytomous item educational psychological testing program © 2008 National Council,performance generalized SX2 item fit index polytomous IRT,0.028353603412078276,0.02835205338219347,0.028358573467116486,0.8863891935657194,0.028546576172892277,0.029025686886926312,0.013706395404347826,0.0,0.05865330225670455,0.0
Sinharay S.; Holland P.W.,Is it necessary to make anchor tests mini-versions of the tests being equated or can some restrictions be relaxed?,2007,44,"It is a widely held belief that anchor tests should be miniature versions (i.e., minitests), with respect to content and statistical characteristics, of the tests being equated. This article examines the foundations for this belief regarding statistical characteristics. It examines the requirement of statistical representativeness of anchor tests that are content representative. The equating performance of several types of anchor tests, including those having statistical characteristics that differ from those of the tests being equated, is examined through several simulation studies and a real data example. Anchor tests with a spread of item difficulties less than that of a total test seem to perform as well as a minitest with respect to equating bias and equating standard error. Hence, the results demonstrate that requiring an anchor test to mimic the statistical characteristics of the total test may be too restrictive and need not be optimal. As a side benefit, this article also provides a comparison of the equating performance of post-stratification equating and chain equipercentile equating. © 2007 by the National Council on Measurement in Education.",Is it necessary to make anchor tests mini-versions of the tests being equated or can some restrictions be relaxed?,"It is a widely held belief that anchor tests should be miniature versions (i.e., minitests), with respect to content and statistical characteristics, of the tests being equated. This article examines the foundations for this belief regarding statistical characteristics. It examines the requirement of statistical representativeness of anchor tests that are content representative. The equating performance of several types of anchor tests, including those having statistical characteristics that differ from those of the tests being equated, is examined through several simulation studies and a real data example. Anchor tests with a spread of item difficulties less than that of a total test seem to perform as well as a minitest with respect to equating bias and equating standard error. Hence, the results demonstrate that requiring an anchor test to mimic the statistical characteristics of the total test may be too restrictive and need not be optimal. As a side benefit, this article also provides a comparison of the equating performance of post-stratification equating and chain equipercentile equating. © 2007 by the National Council on Measurement in Education.","['widely', 'hold', 'belief', 'anchor', 'test', 'miniature', 'version', 'ie', 'minitest', 'respect', 'content', 'statistical', 'characteristic', 'test', 'equate', 'article', 'examine', 'foundation', 'belief', 'regard', 'statistical', 'characteristic', 'examine', 'requirement', 'statistical', 'representativeness', 'anchor', 'test', 'content', 'representative', 'equate', 'performance', 'type', 'anchor', 'test', 'include', 'statistical', 'characteristic', 'differ', 'test', 'equate', 'examine', 'simulation', 'study', 'real', 'data', 'example', 'Anchor', 'test', 'spread', 'item', 'difficulty', 'total', 'test', 'perform', 'minitest', 'respect', 'equate', 'bias', 'equate', 'standard', 'error', 'result', 'demonstrate', 'require', 'anchor', 'test', 'mimic', 'statistical', 'characteristic', 'total', 'test', 'restrictive', 'need', 'optimal', 'benefit', 'article', 'provide', 'comparison', 'equate', 'performance', 'poststratification', 'equating', 'chain', 'equipercentile', 'equate', '©', '2007', 'National', 'Council']","['necessary', 'anchor', 'test', 'miniversion', 'test', 'equate', 'restriction', 'relax']",widely hold belief anchor test miniature version ie minitest respect content statistical characteristic test equate article examine foundation belief regard statistical characteristic examine requirement statistical representativeness anchor test content representative equate performance type anchor test include statistical characteristic differ test equate examine simulation study real data example Anchor test spread item difficulty total test perform minitest respect equate bias equate standard error result demonstrate require anchor test mimic statistical characteristic total test restrictive need optimal benefit article provide comparison equate performance poststratification equating chain equipercentile equate © 2007 National Council,necessary anchor test miniversion test equate restriction relax,0.030187449020507264,0.03018821982856501,0.033357015638410856,0.8089863786272962,0.09728093688522063,0.009354533548819339,0.0,0.1605201545302168,0.0,0.0
Cui Z.; Kolen M.J.,Evaluation of two new Smoothing methods in equating: The cubic B-spline presmoothing method and the direct presmoothing method,2009,46,"This article considers two new smoothing methods in equipercentile equating, the cubic B-spline presmoothing method and the direct presmoothing method. Using a simulation study, these two methods are compared with established methods, the beta-4 method, the polynomial loglinear method, and the cubic spline postsmoothing method, under three sample sizes (300, 1,000, and 3,000) and for three test content areas (ITBS Maps and Diagrams, ITBS Reference and Materials, and ITBS Capitalization). Ten thousand random samples were simulated from population distributions, and the standard error, bias, and RMSE statistics were calculated. The cubic B-spline presmoothing method performed well in reducing total error of equating, whereas the direct presmoothing method appeared to need some modification for it to be as accurate as other smoothing methods. © 2009 by the National Council on Measurement in Education.",Evaluation of two new Smoothing methods in equating: The cubic B-spline presmoothing method and the direct presmoothing method,"This article considers two new smoothing methods in equipercentile equating, the cubic B-spline presmoothing method and the direct presmoothing method. Using a simulation study, these two methods are compared with established methods, the beta-4 method, the polynomial loglinear method, and the cubic spline postsmoothing method, under three sample sizes (300, 1,000, and 3,000) and for three test content areas (ITBS Maps and Diagrams, ITBS Reference and Materials, and ITBS Capitalization). Ten thousand random samples were simulated from population distributions, and the standard error, bias, and RMSE statistics were calculated. The cubic B-spline presmoothing method performed well in reducing total error of equating, whereas the direct presmoothing method appeared to need some modification for it to be as accurate as other smoothing methods. © 2009 by the National Council on Measurement in Education.","['article', 'consider', 'new', 'smoothing', 'method', 'equipercentile', 'equate', 'cubic', 'Bspline', 'presmoothe', 'method', 'direct', 'presmoothing', 'method', 'simulation', 'study', 'method', 'compare', 'establish', 'method', 'beta4', 'method', 'polynomial', 'loglinear', 'method', 'cubic', 'spline', 'postsmoothing', 'method', 'sample', 'size', '300', '1000', '3000', 'test', 'content', 'area', 'ITBS', 'Maps', 'Diagrams', 'ITBS', 'Reference', 'Materials', 'ITBS', 'Capitalization', 'Ten', 'thousand', 'random', 'sample', 'simulate', 'population', 'distribution', 'standard', 'error', 'bias', 'RMSE', 'statistic', 'calculate', 'cubic', 'Bspline', 'presmoothe', 'method', 'perform', 'reduce', 'total', 'error', 'equate', 'direct', 'presmoothing', 'method', 'appear', 'need', 'modification', 'accurate', 'smoothing', 'method', '©', '2009', 'National', 'Council']","['evaluation', 'new', 'smoothing', 'method', 'equate', 'cubic', 'Bspline', 'presmoothe', 'method', 'direct', 'presmoothing', 'method']",article consider new smoothing method equipercentile equate cubic Bspline presmoothe method direct presmoothing method simulation study method compare establish method beta4 method polynomial loglinear method cubic spline postsmoothing method sample size 300 1000 3000 test content area ITBS Maps Diagrams ITBS Reference Materials ITBS Capitalization Ten thousand random sample simulate population distribution standard error bias RMSE statistic calculate cubic Bspline presmoothe method perform reduce total error equate direct presmoothing method appear need modification accurate smoothing method © 2009 National Council,evaluation new smoothing method equate cubic Bspline presmoothe method direct presmoothing method,0.028430276070123384,0.3392723367157522,0.028500675283377415,0.5752824508876222,0.028514261043124815,0.0,0.0,0.08618231847445756,0.03478028517262078,0.0
Bolt D.,The present and future of IRT-based cognitive diagnostic models (ICDMs) and related methods,2007,44,"As the goals of educational assessment evolve from the strictly evaluative to the diagnostically useful, so also evolve the statistical methods used to build, validate, and interpret educational tests. The methods discussed in this special issue all approach diagnosis in an item response theory (IRT) related way, with models that are parameterized at the item level and that extract information from individual item responses. Clearly, their most distinguishing feature is their more complex, multidimensional representation of examinee proficiency. This representation can be built directly into an item response model (as seen in most clearly in Almond, DiBello, Moulder, & Zapata-Rivera, 2007; Henson, Templin, & Douglas, 2007; Roussos, Templin, & Henson, 2007; Stout, 2007) or else it can provide a framework for interpreting (residual) pattems in item responses (as is seen in Gierl, 2007). The complexity of the proficiency space introduces corresponding complexities into the statistical modeling and score reporting aspects of diagnosis. A high level of expert judgment is needed in formulating appropriate models. One of the primary challenges in implementing IRT-based cognitively diagnostic model (ICDMs) requires determining which aspects of the modeling process should be constrained through expert judgment and which can and should be informed by observed item response data. The vast array of psychometric models now available for diagnosis and the different ways they handle these complexities (e.g., how many levels for each skill, how do skills interact, how does skill mastery translate to item performance, etc.) make model selection a central issue. At the same time, it can be challenging to compare models according to goodness of fit due to the many other aspects within each model that must be informed by experts (e.g., entries of the item-by-skill Q-
matrix, structure of the proficiency space, etc). Data-driven model re-specification is often messy. Collectively, the papers presented in this Special Issue provide a comprehensive overview of the state of the art in IRT-based diagnosis. While all emphasize a common end-goal of examinee diagnosis, the process by which this is achieved and the balance of data-driven and expert-driven decision making used along the way also introduce important differences.",The present and future of IRT-based cognitive diagnostic models (ICDMs) and related methods,"As the goals of educational assessment evolve from the strictly evaluative to the diagnostically useful, so also evolve the statistical methods used to build, validate, and interpret educational tests. The methods discussed in this special issue all approach diagnosis in an item response theory (IRT) related way, with models that are parameterized at the item level and that extract information from individual item responses. Clearly, their most distinguishing feature is their more complex, multidimensional representation of examinee proficiency. This representation can be built directly into an item response model (as seen in most clearly in Almond, DiBello, Moulder, & Zapata-Rivera, 2007; Henson, Templin, & Douglas, 2007; Roussos, Templin, & Henson, 2007; Stout, 2007) or else it can provide a framework for interpreting (residual) pattems in item responses (as is seen in Gierl, 2007). The complexity of the proficiency space introduces corresponding complexities into the statistical modeling and score reporting aspects of diagnosis. A high level of expert judgment is needed in formulating appropriate models. One of the primary challenges in implementing IRT-based cognitively diagnostic model (ICDMs) requires determining which aspects of the modeling process should be constrained through expert judgment and which can and should be informed by observed item response data. The vast array of psychometric models now available for diagnosis and the different ways they handle these complexities (e.g., how many levels for each skill, how do skills interact, how does skill mastery translate to item performance, etc.) make model selection a central issue. At the same time, it can be challenging to compare models according to goodness of fit due to the many other aspects within each model that must be informed by experts (e.g., entries of the item-by-skill Q-
matrix, structure of the proficiency space, etc). Data-driven model re-specification is often messy. Collectively, the papers presented in this Special Issue provide a comprehensive overview of the state of the art in IRT-based diagnosis. While all emphasize a common end-goal of examinee diagnosis, the process by which this is achieved and the balance of data-driven and expert-driven decision making used along the way also introduce important differences.","['goal', 'educational', 'assessment', 'evolve', 'strictly', 'evaluative', 'diagnostically', 'useful', 'evolve', 'statistical', 'method', 'build', 'validate', 'interpret', 'educational', 'test', 'method', 'discuss', 'special', 'issue', 'approach', 'diagnosis', 'item', 'response', 'theory', 'IRT', 'related', 'way', 'parameterize', 'item', 'level', 'extract', 'information', 'individual', 'item', 'response', 'clearly', 'distinguishing', 'feature', 'complex', 'multidimensional', 'representation', 'examinee', 'proficiency', 'representation', 'build', 'directly', 'item', 'response', 'clearly', 'Almond', 'DiBello', 'Moulder', 'ZapataRivera', '2007', 'Henson', 'Templin', 'Douglas', '2007', 'Roussos', 'Templin', 'Henson', '2007', 'Stout', '2007', 'provide', 'framework', 'interpret', 'residual', 'pattem', 'item', 'response', 'Gierl', '2007', 'complexity', 'proficiency', 'space', 'introduce', 'correspond', 'complexity', 'statistical', 'modeling', 'score', 'report', 'aspect', 'diagnosis', 'high', 'level', 'expert', 'judgment', 'need', 'formulate', 'appropriate', 'primary', 'challenge', 'implement', 'irtbased', 'cognitively', 'diagnostic', 'ICDMs', 'require', 'determine', 'aspect', 'modeling', 'process', 'constrain', 'expert', 'judgment', 'inform', 'observe', 'item', 'response', 'datum', 'vast', 'array', 'psychometric', 'available', 'diagnosis', 'different', 'way', 'handle', 'complexity', 'eg', 'level', 'skill', 'skill', 'interact', 'skill', 'mastery', 'translate', 'item', 'performance', 'etc', 'selection', 'central', 'issue', 'time', 'challenge', 'compare', 'accord', 'goodness', 'fit', 'aspect', 'inform', 'expert', 'eg', 'entry', 'itembyskill', 'q', 'matrix', 'structure', 'proficiency', 'space', 'etc', 'datadriven', 'respecification', 'messy', 'collectively', 'paper', 'present', 'Special', 'Issue', 'provide', 'comprehensive', 'overview', 'state', 'art', 'irtbased', 'diagnosis', 'emphasize', 'common', 'endgoal', 'examinee', 'diagnosis', 'process', 'achieve', 'balance', 'datadriven', 'expertdriven', 'decision', 'way', 'introduce', 'important', 'difference']","['present', 'future', 'irtbased', 'cognitive', 'diagnostic', 'icdm', 'relate', 'method']",goal educational assessment evolve strictly evaluative diagnostically useful evolve statistical method build validate interpret educational test method discuss special issue approach diagnosis item response theory IRT related way parameterize item level extract information individual item response clearly distinguishing feature complex multidimensional representation examinee proficiency representation build directly item response clearly Almond DiBello Moulder ZapataRivera 2007 Henson Templin Douglas 2007 Roussos Templin Henson 2007 Stout 2007 provide framework interpret residual pattem item response Gierl 2007 complexity proficiency space introduce correspond complexity statistical modeling score report aspect diagnosis high level expert judgment need formulate appropriate primary challenge implement irtbased cognitively diagnostic ICDMs require determine aspect modeling process constrain expert judgment inform observe item response datum vast array psychometric available diagnosis different way handle complexity eg level skill skill interact skill mastery translate item performance etc selection central issue time challenge compare accord goodness fit aspect inform expert eg entry itembyskill q matrix structure proficiency space etc datadriven respecification messy collectively paper present Special Issue provide comprehensive overview state art irtbased diagnosis emphasize common endgoal examinee diagnosis process achieve balance datadriven expertdriven decision way introduce important difference,present future irtbased cognitive diagnostic icdm relate method,0.01845558187950238,0.01845703153110453,0.01851149195267882,0.926026703332997,0.01854919130371741,0.002713460407437817,0.08864126090200719,0.0,0.03833181758813924,0.00120017284988971
Cui Y.; Leighton J.P.,The hierarchy consistency index: Evaluating person fit for cognitive diagnostic assessment,2009,46,"In this article, we introduce a person-fit statistic called the hierarchy consistency index (HCI) to help detect misfitting item response vectors for tests developed and analyzed based on a cognitive model. The HCI ranges from -1.0 to 1.0, with values close to -1.0 indicating that students respond unexpectedly or differently from the responses expected under a given cognitive model. A simulation study was conducted to evaluate the power of the HCI in detecting different types of misfitting item response vectors. Simulation results revealed that the detection rate of the HCI was a function of type of misfit, item discriminating power, and test length. The best detection rates were achieved when the HCI was applied to tests that consisted of a large number of highly discriminating items. In addition, whether a misfitting item response vector can be correctly identified depends, to a large degree, on the number of misfits of the item response vector relative to the cognitive model. When misfitting response behavior only affects a small number of item responses, the resulting item response vector will not be substantially different from the expectations under the cognitive model and consequently may not be statistically identified as misfitting. As an item response vector deviates further from the model expectations, misfits are more easily identified and consequently higher detection rates of the HCI are expected. © 2009 by the National Council on Measurement in Education.",The hierarchy consistency index: Evaluating person fit for cognitive diagnostic assessment,"In this article, we introduce a person-fit statistic called the hierarchy consistency index (HCI) to help detect misfitting item response vectors for tests developed and analyzed based on a cognitive model. The HCI ranges from -1.0 to 1.0, with values close to -1.0 indicating that students respond unexpectedly or differently from the responses expected under a given cognitive model. A simulation study was conducted to evaluate the power of the HCI in detecting different types of misfitting item response vectors. Simulation results revealed that the detection rate of the HCI was a function of type of misfit, item discriminating power, and test length. The best detection rates were achieved when the HCI was applied to tests that consisted of a large number of highly discriminating items. In addition, whether a misfitting item response vector can be correctly identified depends, to a large degree, on the number of misfits of the item response vector relative to the cognitive model. When misfitting response behavior only affects a small number of item responses, the resulting item response vector will not be substantially different from the expectations under the cognitive model and consequently may not be statistically identified as misfitting. As an item response vector deviates further from the model expectations, misfits are more easily identified and consequently higher detection rates of the HCI are expected. © 2009 by the National Council on Measurement in Education.","['article', 'introduce', 'personfit', 'statistic', 'hierarchy', 'consistency', 'index', 'HCI', 'help', 'detect', 'misfitting', 'item', 'response', 'vector', 'test', 'develop', 'analyze', 'base', 'cognitive', 'HCI', 'range', '10', '10', 'value', 'close', '10', 'indicate', 'student', 'respond', 'unexpectedly', 'differently', 'response', 'expect', 'cognitive', 'simulation', 'study', 'conduct', 'evaluate', 'power', 'HCI', 'detect', 'different', 'type', 'misfit', 'item', 'response', 'vector', 'Simulation', 'result', 'reveal', 'detection', 'rate', 'HCI', 'function', 'type', 'misfit', 'item', 'discriminate', 'power', 'test', 'length', 'good', 'detection', 'rate', 'achieve', 'HCI', 'apply', 'test', 'consist', 'large', 'number', 'highly', 'discriminate', 'item', 'addition', 'misfitting', 'item', 'response', 'vector', 'correctly', 'identify', 'depend', 'large', 'degree', 'number', 'misfit', 'item', 'response', 'vector', 'relative', 'cognitive', 'misfit', 'response', 'behavior', 'affect', 'small', 'number', 'item', 'response', 'result', 'item', 'response', 'vector', 'substantially', 'different', 'expectation', 'cognitive', 'consequently', 'statistically', 'identify', 'misfitting', 'item', 'response', 'vector', 'deviate', 'far', 'expectation', 'misfit', 'easily', 'identify', 'consequently', 'high', 'detection', 'rate', 'HCI', 'expect', '©', '2009', 'National', 'Council']","['hierarchy', 'consistency', 'index', 'evaluate', 'person', 'fit', 'cognitive', 'diagnostic', 'assessment']",article introduce personfit statistic hierarchy consistency index HCI help detect misfitting item response vector test develop analyze base cognitive HCI range 10 10 value close 10 indicate student respond unexpectedly differently response expect cognitive simulation study conduct evaluate power HCI detect different type misfit item response vector Simulation result reveal detection rate HCI function type misfit item discriminate power test length good detection rate achieve HCI apply test consist large number highly discriminate item addition misfitting item response vector correctly identify depend large degree number misfit item response vector relative cognitive misfit response behavior affect small number item response result item response vector substantially different expectation cognitive consequently statistically identify misfitting item response vector deviate far expectation misfit easily identify consequently high detection rate HCI expect © 2009 National Council,hierarchy consistency index evaluate person fit cognitive diagnostic assessment,0.027879814661423308,0.027880490800176922,0.02791435704090437,0.888223693752632,0.028101643744863472,0.03308916588156369,0.015455359863833895,0.0,0.059527433805914146,0.0
Davis S.L.; Buckendahl C.W.; Plake B.S.,When adaptation is not an option: An application of multilingual standard setting,2008,45,"As an alternative to adaptation, tests may also be developed simultaneously in multiple languages. Although the items on such tests could vary substantially, scores from these tests may be used to make the same types of decisions about different groups of examinees. The ability to make such decisions is contingent upon setting performance standards for each exam that allow for comparable interpretations of test results. This article describes a standard setting process used for a multilingual high school literacy assessment constructed under these conditions. This methodology was designed to address the specific challenges presented by this testing program including maintaining equivalent expectations for performance across different student populations. The validity evidence collected to support the methodology and results is discussed along with recommendations for future practice. © 2008 by the National Council on Measurement in Education.",When adaptation is not an option: An application of multilingual standard setting,"As an alternative to adaptation, tests may also be developed simultaneously in multiple languages. Although the items on such tests could vary substantially, scores from these tests may be used to make the same types of decisions about different groups of examinees. The ability to make such decisions is contingent upon setting performance standards for each exam that allow for comparable interpretations of test results. This article describes a standard setting process used for a multilingual high school literacy assessment constructed under these conditions. This methodology was designed to address the specific challenges presented by this testing program including maintaining equivalent expectations for performance across different student populations. The validity evidence collected to support the methodology and results is discussed along with recommendations for future practice. © 2008 by the National Council on Measurement in Education.","['alternative', 'adaptation', 'test', 'develop', 'simultaneously', 'multiple', 'language', 'item', 'test', 'vary', 'substantially', 'score', 'test', 'type', 'decision', 'different', 'group', 'examinee', 'ability', 'decision', 'contingent', 'set', 'performance', 'standard', 'exam', 'allow', 'comparable', 'interpretation', 'test', 'result', 'article', 'describe', 'standard', 'set', 'process', 'multilingual', 'high', 'school', 'literacy', 'assessment', 'construct', 'condition', 'methodology', 'design', 'address', 'specific', 'challenge', 'present', 'testing', 'program', 'include', 'maintain', 'equivalent', 'expectation', 'performance', 'different', 'student', 'population', 'validity', 'evidence', 'collect', 'support', 'methodology', 'result', 'discuss', 'recommendation', 'future', 'practice', '©', '2008', 'National', 'Council']","['adaptation', 'option', 'application', 'multilingual', 'standard', 'setting']",alternative adaptation test develop simultaneously multiple language item test vary substantially score test type decision different group examinee ability decision contingent set performance standard exam allow comparable interpretation test result article describe standard set process multilingual high school literacy assessment construct condition methodology design address specific challenge present testing program include maintain equivalent expectation performance different student population validity evidence collect support methodology result discuss recommendation future practice © 2008 National Council,adaptation option application multilingual standard setting,0.023797262011422717,0.02379155034837106,0.02380684684545643,0.9043265055880013,0.024277835206748446,0.01745547167875093,0.06546731895886064,0.010345775279097254,0.01868917194353269,0.06728853769544753
Moses T.; Holland P.W.,Selection strategies for univariate loglinear smoothing models and their effect on equating function accuracy,2009,46,"In this study, we compared 12 statistical strategies proposed for selecting loglinear models for smoothing univariate test score distributions and for enhancing the stability of equipercentile equating functions. The major focus was on evaluating the effects of the selection strategies on equating function accuracy. Selection strategies' influence on the estimation of cumulative test score distributions was also assessed. The results of this simulation study differentiate the selection strategies and define the situations where their use has the most important implications for equating function accuracy. The recommended strategy for estimating test score distributions and for equating is AIC minimization. © 2009 by the National Council on Measurement in Education.",Selection strategies for univariate loglinear smoothing models and their effect on equating function accuracy,"In this study, we compared 12 statistical strategies proposed for selecting loglinear models for smoothing univariate test score distributions and for enhancing the stability of equipercentile equating functions. The major focus was on evaluating the effects of the selection strategies on equating function accuracy. Selection strategies' influence on the estimation of cumulative test score distributions was also assessed. The results of this simulation study differentiate the selection strategies and define the situations where their use has the most important implications for equating function accuracy. The recommended strategy for estimating test score distributions and for equating is AIC minimization. © 2009 by the National Council on Measurement in Education.","['study', 'compare', '12', 'statistical', 'strategy', 'propose', 'select', 'loglinear', 'smooth', 'univariate', 'test', 'score', 'distribution', 'enhance', 'stability', 'equipercentile', 'equating', 'function', 'major', 'focus', 'evaluate', 'effect', 'selection', 'strategy', 'equate', 'function', 'accuracy', 'Selection', 'strategy', 'influence', 'estimation', 'cumulative', 'test', 'score', 'distribution', 'assess', 'result', 'simulation', 'study', 'differentiate', 'selection', 'strategy', 'define', 'situation', 'important', 'implication', 'equate', 'function', 'accuracy', 'recommend', 'strategy', 'estimate', 'test', 'score', 'distribution', 'equating', 'AIC', 'minimization', '©', '2009', 'National', 'Council']","['selection', 'strategy', 'univariate', 'loglinear', 'smoothing', 'effect', 'equate', 'function', 'accuracy']",study compare 12 statistical strategy propose select loglinear smooth univariate test score distribution enhance stability equipercentile equating function major focus evaluate effect selection strategy equate function accuracy Selection strategy influence estimation cumulative test score distribution assess result simulation study differentiate selection strategy define situation important implication equate function accuracy recommend strategy estimate test score distribution equating AIC minimization © 2009 National Council,selection strategy univariate loglinear smoothing effect equate function accuracy,0.031224037342764258,0.03122483246726327,0.031239342418353053,0.8747344063670263,0.031577381404593145,0.014845583021514407,0.0,0.12608227567284597,0.0,0.013781766393412002
Kim D.-I.; Brennan R.; Kolen M.,A comparison of IRT equating and beta 4 equating,2005,42,"Four equating methods (3PL true score equating, 3PL observed score equating, beta 4 true score equating, and beta 4 observed score equating) were compared using four equating criteria: first-order equity (FOE), second-order equity (SOE), conditional-mean-squared-error (CMSE) difference, and the equipercentile equating property. True score equating more closely achieved estimated FOE than observed score equating when the true score distribution was estimated using the psychometric model that was used in the equating. Observed score equating more closely achieved estimated SOE, estimated CMSE difference, and the equipercentile equating property than true score equating. Among the four equating methods, 3PL observed score equating most closely achieved estimated SOE and had the smallest estimated CMSE difference, and beta 4 observed score equating was the method that most closely met the equipercentile equating property.",,"Four equating methods (3PL true score equating, 3PL observed score equating, beta 4 true score equating, and beta 4 observed score equating) were compared using four equating criteria: first-order equity (FOE), second-order equity (SOE), conditional-mean-squared-error (CMSE) difference, and the equipercentile equating property. True score equating more closely achieved estimated FOE than observed score equating when the true score distribution was estimated using the psychometric model that was used in the equating. Observed score equating more closely achieved estimated SOE, estimated CMSE difference, and the equipercentile equating property than true score equating. Among the four equating methods, 3PL observed score equating most closely achieved estimated SOE and had the smallest estimated CMSE difference, and beta 4 observed score equating was the method that most closely met the equipercentile equating property.","['equating', 'method', '3pl', 'true', 'score', 'equate', '3pl', 'observe', 'score', 'equate', 'beta', '4', 'true', 'score', 'equating', 'beta', '4', 'observe', 'score', 'equating', 'compare', 'equate', 'criterion', 'firstorder', 'equity', 'FOE', 'secondorder', 'equity', 'SOE', 'conditionalmeansquarederror', 'CMSE', 'difference', 'equipercentile', 'equate', 'property', 'true', 'score', 'equate', 'closely', 'achieve', 'estimated', 'FOE', 'observed', 'score', 'equate', 'true', 'score', 'distribution', 'estimate', 'psychometric', 'equate', 'observe', 'score', 'equate', 'closely', 'achieve', 'estimate', 'SOE', 'estimate', 'CMSE', 'difference', 'equipercentile', 'equate', 'property', 'true', 'score', 'equate', 'equate', 'method', '3PL', 'observe', 'score', 'equate', 'closely', 'achieve', 'estimated', 'SOE', 'small', 'estimate', 'CMSE', 'difference', 'beta', '4', 'observe', 'score', 'equating', 'method', 'closely', 'meet', 'equipercentile', 'equate', 'property']",,equating method 3pl true score equate 3pl observe score equate beta 4 true score equating beta 4 observe score equating compare equate criterion firstorder equity FOE secondorder equity SOE conditionalmeansquarederror CMSE difference equipercentile equate property true score equate closely achieve estimated FOE observed score equate true score distribution estimate psychometric equate observe score equate closely achieve estimate SOE estimate CMSE difference equipercentile equate property true score equate equate method 3PL observe score equate closely achieve estimated SOE small estimate CMSE difference beta 4 observe score equating method closely meet equipercentile equate property,,0.0404325321122955,0.0404344632383688,0.04042914073734533,0.8378232883543211,0.040880575557669475,0.0,0.0,0.17786806644263065,0.0,0.0
Sinharay S.; Lu Y.,A further look at the correlation between item parameters and item fit statistics,2008,45,"Dodeen (2004) studied the correlation between the item parameters of the three-parameter logistic model and two item fit statistics, and found some linear relationships (e.g., a positive correlation between item discrimination parameters and item fit statistics) that have the potential for influencing the work of practitioners who employ item response theory. This article examines the same type of linear relationships as studied by Dodeen. However, this article adds to the literature by employing item fit statistics not considered by Dodeen, which have been recently suggested and whose Type I error rates have been demonstrated to be generally close to the nominal level. Detailed simulations show that if one uses certain of the recently suggested item fit statistics, there is no need to worry about any linear relationships between the item parameters and item fit statistics. © 2007 by the National Council on Measurement in Education.",A further look at the correlation between item parameters and item fit statistics,"Dodeen (2004) studied the correlation between the item parameters of the three-parameter logistic model and two item fit statistics, and found some linear relationships (e.g., a positive correlation between item discrimination parameters and item fit statistics) that have the potential for influencing the work of practitioners who employ item response theory. This article examines the same type of linear relationships as studied by Dodeen. However, this article adds to the literature by employing item fit statistics not considered by Dodeen, which have been recently suggested and whose Type I error rates have been demonstrated to be generally close to the nominal level. Detailed simulations show that if one uses certain of the recently suggested item fit statistics, there is no need to worry about any linear relationships between the item parameters and item fit statistics. © 2007 by the National Council on Measurement in Education.","['dodeen', '2004', 'study', 'correlation', 'item', 'parameter', 'threeparameter', 'logistic', 'item', 'fit', 'statistic', 'find', 'linear', 'relationship', 'eg', 'positive', 'correlation', 'item', 'discrimination', 'parameter', 'item', 'fit', 'statistic', 'potential', 'influence', 'work', 'practitioner', 'employ', 'item', 'response', 'theory', 'article', 'examine', 'type', 'linear', 'relationship', 'study', 'Dodeen', 'article', 'add', 'literature', 'employ', 'item', 'fit', 'statistic', 'consider', 'dodeen', 'recently', 'suggest', 'type', 'I', 'error', 'rate', 'demonstrate', 'generally', 'close', 'nominal', 'level', 'detailed', 'simulation', 'certain', 'recently', 'suggest', 'item', 'fit', 'statistic', 'need', 'worry', 'linear', 'relationship', 'item', 'parameter', 'item', 'fit', 'statistic', '©', '2007', 'National', 'Council']","['look', 'correlation', 'item', 'parameter', 'item', 'fit', 'statistic']",dodeen 2004 study correlation item parameter threeparameter logistic item fit statistic find linear relationship eg positive correlation item discrimination parameter item fit statistic potential influence work practitioner employ item response theory article examine type linear relationship study Dodeen article add literature employ item fit statistic consider dodeen recently suggest type I error rate demonstrate generally close nominal level detailed simulation certain recently suggest item fit statistic need worry linear relationship item parameter item fit statistic © 2007 National Council,look correlation item parameter item fit statistic,0.030010790512598792,0.03001131015041712,0.03003318664770657,0.8798360585393401,0.030108654149937512,0.012945751444295843,0.0,0.0,0.09494549379717757,0.0
Culpepper S.A.; Davenport E.C.,Assessing differential prediction of college grades by race/ethnicity with a multilevel model,2009,46,"Previous research notes the importance of understanding racial/ethnic differential prediction of college grades across multiple institutions. Institutional variation in selection indices is especially important given some states' laws governing public institutions' admissions decisions. This paper employed multilevel moderated multiple regression to study the variation of selection indices across 30 institutions and the accuracy of selection indices in predicting college grades for students of different racial/ethnic backgrounds. Several benefits of multilevel models for cross-institutional differential prediction studies were described and include: controlling for institutional differences in range restriction, providing reliability estimates of least squares estimates, and adjusting criterion scores for differences in coursework difficulty. The findings from this study provide evidence of institutional variation in selection indices, which challenges current laws aimed at standardizing them. Specifically, there was evidence that the predictor slope coefficients varied across institutions, in addition to the estimates that measured intercept differences for African and Asian American students. Across universities, the results mirrored previous findings: high school grade point average (GPA) differentially predicted grades for African Americans, SAT verbal scores differentially predict grades for Asian Americans, and SAT math scores were better predictors of Asian Americans' grades. © 2009 by the National Council on Measurement in Education.",Assessing differential prediction of college grades by race/ethnicity with a multilevel model,"Previous research notes the importance of understanding racial/ethnic differential prediction of college grades across multiple institutions. Institutional variation in selection indices is especially important given some states' laws governing public institutions' admissions decisions. This paper employed multilevel moderated multiple regression to study the variation of selection indices across 30 institutions and the accuracy of selection indices in predicting college grades for students of different racial/ethnic backgrounds. Several benefits of multilevel models for cross-institutional differential prediction studies were described and include: controlling for institutional differences in range restriction, providing reliability estimates of least squares estimates, and adjusting criterion scores for differences in coursework difficulty. The findings from this study provide evidence of institutional variation in selection indices, which challenges current laws aimed at standardizing them. Specifically, there was evidence that the predictor slope coefficients varied across institutions, in addition to the estimates that measured intercept differences for African and Asian American students. Across universities, the results mirrored previous findings: high school grade point average (GPA) differentially predicted grades for African Americans, SAT verbal scores differentially predict grades for Asian Americans, and SAT math scores were better predictors of Asian Americans' grades. © 2009 by the National Council on Measurement in Education.","['previous', 'research', 'note', 'importance', 'understand', 'racialethnic', 'differential', 'prediction', 'college', 'grade', 'multiple', 'institution', 'institutional', 'variation', 'selection', 'index', 'especially', 'important', 'state', 'law', 'govern', 'public', 'institution', 'admission', 'decision', 'paper', 'employ', 'multilevel', 'moderated', 'multiple', 'regression', 'study', 'variation', 'selection', 'indice', '30', 'institution', 'accuracy', 'selection', 'index', 'predict', 'college', 'grade', 'student', 'different', 'racialethnic', 'background', 'benefit', 'multilevel', 'crossinstitutional', 'differential', 'prediction', 'study', 'describe', 'include', 'control', 'institutional', 'difference', 'range', 'restriction', 'provide', 'reliability', 'estimate', 'square', 'estimate', 'adjust', 'criterion', 'score', 'difference', 'coursework', 'difficulty', 'finding', 'study', 'provide', 'evidence', 'institutional', 'variation', 'selection', 'index', 'challenge', 'current', 'law', 'aim', 'standardize', 'specifically', 'evidence', 'predictor', 'slope', 'coefficient', 'varied', 'institution', 'addition', 'estimate', 'measure', 'intercept', 'difference', 'african', 'asian', 'american', 'student', 'university', 'result', 'mirror', 'previous', 'finding', 'high', 'school', 'grade', 'point', 'average', 'GPA', 'differentially', 'predict', 'grade', 'African', 'Americans', 'sat', 'verbal', 'score', 'differentially', 'predict', 'grade', 'Asian', 'Americans', 'SAT', 'math', 'score', 'predictor', 'Asian', 'Americans', 'grade', '©', '2009', 'National', 'Council']","['assess', 'differential', 'prediction', 'college', 'grade', 'raceethnicity', 'multilevel']",previous research note importance understand racialethnic differential prediction college grade multiple institution institutional variation selection index especially important state law govern public institution admission decision paper employ multilevel moderated multiple regression study variation selection indice 30 institution accuracy selection index predict college grade student different racialethnic background benefit multilevel crossinstitutional differential prediction study describe include control institutional difference range restriction provide reliability estimate square estimate adjust criterion score difference coursework difficulty finding study provide evidence institutional variation selection index challenge current law aim standardize specifically evidence predictor slope coefficient varied institution addition estimate measure intercept difference african asian american student university result mirror previous finding high school grade point average GPA differentially predict grade African Americans sat verbal score differentially predict grade Asian Americans SAT math score predictor Asian Americans grade © 2009 National Council,assess differential prediction college grade raceethnicity multilevel,0.022487049402348303,0.022485586655418402,0.02255999747312233,0.1515832093223037,0.7808841571468073,0.0,0.0002864854450619746,0.0014055517088091799,0.0,0.1669423661387497
DeCarlo L.T.,A model of rater behavior in essay grading based on signal detection theory,2005,42,"An approach to essay grading based on signal detection theory (SDT) is presented. SDT offers a basis for understanding rater behavior with respect to the scoring of construct responses, in that it provides a theory of psychological processes underlying the raters' behavior. The approach also provides measures of the precision of the raters and the accuracy of classifications. An application of latent class SDT to essay grading is detailed, and similarities to and differences from item response theory (IRT) are noted. The validity and utility of classifications obtained from the SDT model and scores obtained from IRT models are compared. Validity coefficients were found to be about equal in magnitude across SDT and IRT models. Results from a simulation study of a 5-class SDT model with eight raters are also presented.",A model of rater behavior in essay grading based on signal detection theory,"An approach to essay grading based on signal detection theory (SDT) is presented. SDT offers a basis for understanding rater behavior with respect to the scoring of construct responses, in that it provides a theory of psychological processes underlying the raters' behavior. The approach also provides measures of the precision of the raters and the accuracy of classifications. An application of latent class SDT to essay grading is detailed, and similarities to and differences from item response theory (IRT) are noted. The validity and utility of classifications obtained from the SDT model and scores obtained from IRT models are compared. Validity coefficients were found to be about equal in magnitude across SDT and IRT models. Results from a simulation study of a 5-class SDT model with eight raters are also presented.","['approach', 'essay', 'grade', 'base', 'signal', 'detection', 'theory', 'SDT', 'present', 'SDT', 'offer', 'basis', 'understand', 'rater', 'behavior', 'respect', 'scoring', 'construct', 'response', 'provide', 'theory', 'psychological', 'process', 'underlie', 'rater', 'behavior', 'approach', 'provide', 'measure', 'precision', 'rater', 'accuracy', 'classification', 'application', 'latent', 'class', 'SDT', 'essay', 'grade', 'detailed', 'similaritie', 'difference', 'item', 'response', 'theory', 'IRT', 'note', 'validity', 'utility', 'classification', 'obtain', 'SDT', 'score', 'obtain', 'IRT', 'compare', 'Validity', 'coefficient', 'find', 'equal', 'magnitude', 'SDT', 'IRT', 'result', 'simulation', 'study', '5class', 'SDT', 'rater', 'present']","['rater', 'behavior', 'essay', 'grade', 'base', 'signal', 'detection', 'theory']",approach essay grade base signal detection theory SDT present SDT offer basis understand rater behavior respect scoring construct response provide theory psychological process underlie rater behavior approach provide measure precision rater accuracy classification application latent class SDT essay grade detailed similaritie difference item response theory IRT note validity utility classification obtain SDT score obtain IRT compare Validity coefficient find equal magnitude SDT IRT result simulation study 5class SDT rater present,rater behavior essay grade base signal detection theory,0.032723450038739894,0.03271490409773163,0.03280602200270107,0.553038106330309,0.3487175175305183,0.0002395173940138166,0.04053889076351482,0.0,0.009716741433283835,0.06778383933706249
Chen S.-Y.; Ankenmann R.D.,Effects of practical constraints on item selection rules at the early stages of computerized adaptive testing,2004,41,"The purpose of this study was to compare the effects of four item selection rules-(1) Fisher information (F), (2) Fisher information with a posterior distribution (FP), (3) Kullback-Leibler information with a posterior distribution (KP), and (4) completely randomized item selection (RN)-with respect to the precision of trait estimation and the extent of item usage at the early stages of computerized adaptive testing. The comparison of the four item selection rules was carried out under three conditions: (1) using only the item information function as the item selection criterion; (2) using both the item information function and content balancing; and (3) using the item information function, content balancing, and item exposure control. When test length was less than 10 items, FP and KP tended to outperform F at extreme trait levels in Condition 1. However, in more realistic settings, it could not be concluded that FP and KP outperformed F, especially when item exposure control was imposed. When test length was greater than 10 items, the three nonrandom item selection procedures performed similarly no matter what the condition was, while F had slightly higher item usage.",Effects of practical constraints on item selection rules at the early stages of computerized adaptive testing,"The purpose of this study was to compare the effects of four item selection rules-(1) Fisher information (F), (2) Fisher information with a posterior distribution (FP), (3) Kullback-Leibler information with a posterior distribution (KP), and (4) completely randomized item selection (RN)-with respect to the precision of trait estimation and the extent of item usage at the early stages of computerized adaptive testing. The comparison of the four item selection rules was carried out under three conditions: (1) using only the item information function as the item selection criterion; (2) using both the item information function and content balancing; and (3) using the item information function, content balancing, and item exposure control. When test length was less than 10 items, FP and KP tended to outperform F at extreme trait levels in Condition 1. However, in more realistic settings, it could not be concluded that FP and KP outperformed F, especially when item exposure control was imposed. When test length was greater than 10 items, the three nonrandom item selection procedures performed similarly no matter what the condition was, while F had slightly higher item usage.","['purpose', 'study', 'compare', 'effect', 'item', 'selection', 'rules1', 'Fisher', 'information', 'F', '2', 'Fisher', 'information', 'posterior', 'distribution', 'FP', '3', 'KullbackLeibler', 'information', 'posterior', 'distribution', 'KP', '4', 'completely', 'randomize', 'item', 'selection', 'RNwith', 'respect', 'precision', 'trait', 'estimation', 'extent', 'item', 'usage', 'early', 'stage', 'computerized', 'adaptive', 'testing', 'comparison', 'item', 'selection', 'rule', 'carry', 'condition', '1', 'item', 'information', 'function', 'item', 'selection', 'criterion', '2', 'item', 'information', 'function', 'content', 'balancing', '3', 'item', 'information', 'function', 'content', 'balance', 'item', 'exposure', 'control', 'test', 'length', '10', 'item', 'FP', 'KP', 'tend', 'outperform', 'F', 'extreme', 'trait', 'level', 'Condition', '1', 'realistic', 'setting', 'conclude', 'FP', 'KP', 'outperform', 'F', 'especially', 'item', 'exposure', 'control', 'impose', 'test', 'length', 'great', '10', 'item', 'nonrandom', 'item', 'selection', 'procedure', 'perform', 'similarly', 'matter', 'condition', 'F', 'slightly', 'high', 'item', 'usage']","['effect', 'practical', 'constraint', 'item', 'selection', 'rule', 'early', 'stage', 'computerized', 'adaptive', 'testing']",purpose study compare effect item selection rules1 Fisher information F 2 Fisher information posterior distribution FP 3 KullbackLeibler information posterior distribution KP 4 completely randomize item selection RNwith respect precision trait estimation extent item usage early stage computerized adaptive testing comparison item selection rule carry condition 1 item information function item selection criterion 2 item information function content balancing 3 item information function content balance item exposure control test length 10 item FP KP tend outperform F extreme trait level Condition 1 realistic setting conclude FP KP outperform F especially item exposure control impose test length great 10 item nonrandom item selection procedure perform similarly matter condition F slightly high item usage,effect practical constraint item selection rule early stage computerized adaptive testing,0.02658686670766805,0.02658839810082381,0.15250385237094,0.7675709580100255,0.026749924810542634,0.008895971373279016,0.0,0.0,0.0974411022816449,0.0
Harik P.; Clauser B.E.; Grabovsky I.; Nungester R.J.; Swanson D.; Nandakumar R.,An examination of rater drift within a generalizability theory framework,2009,46,"The present study examined the long-term usefulness of estimated parameters used to adjust the scores from a performance assessment to account for differences in rater stringency. Ratings from four components of the USMLE® Step 2 Clinical Skills Examination data were analyzed. A generalizability-theory framework was used to examine the extent to which rater-related sources of error could be eliminated through statistical adjustment. Particular attention was given to the stability of these estimated parameters over time. The results suggest that rater stringency estimates obtained at a point in time and then used to adjust ratings over a period of months may substantially decrease in usefulness. In some cases, over several months, the use of these adjustments may become counterproductive. Additionally, it is hypothesized that the rate of deterioration in the usefulness of estimated parameters may be a function of the characteristics of the scale. © 2009 by the National Council on Measurement in Education.",An examination of rater drift within a generalizability theory framework,"The present study examined the long-term usefulness of estimated parameters used to adjust the scores from a performance assessment to account for differences in rater stringency. Ratings from four components of the USMLE® Step 2 Clinical Skills Examination data were analyzed. A generalizability-theory framework was used to examine the extent to which rater-related sources of error could be eliminated through statistical adjustment. Particular attention was given to the stability of these estimated parameters over time. The results suggest that rater stringency estimates obtained at a point in time and then used to adjust ratings over a period of months may substantially decrease in usefulness. In some cases, over several months, the use of these adjustments may become counterproductive. Additionally, it is hypothesized that the rate of deterioration in the usefulness of estimated parameters may be a function of the characteristics of the scale. © 2009 by the National Council on Measurement in Education.","['present', 'study', 'examine', 'longterm', 'usefulness', 'estimate', 'parameter', 'adjust', 'score', 'performance', 'assessment', 'account', 'difference', 'rater', 'stringency', 'rating', 'component', 'USMLE', '®', 'step', '2', 'Clinical', 'Skills', 'Examination', 'datum', 'analyze', 'generalizabilitytheory', 'framework', 'examine', 'extent', 'raterrelate', 'source', 'error', 'eliminate', 'statistical', 'adjustment', 'Particular', 'attention', 'stability', 'estimate', 'parameter', 'time', 'result', 'suggest', 'rater', 'stringency', 'estimate', 'obtain', 'point', 'time', 'adjust', 'rating', 'period', 'month', 'substantially', 'decrease', 'usefulness', 'case', 'month', 'adjustment', 'counterproductive', 'additionally', 'hypothesize', 'rate', 'deterioration', 'usefulness', 'estimate', 'parameter', 'function', 'characteristic', 'scale', '©', '2009', 'National', 'Council']","['examination', 'rater', 'drift', 'generalizability', 'theory', 'framework']",present study examine longterm usefulness estimate parameter adjust score performance assessment account difference rater stringency rating component USMLE ® step 2 Clinical Skills Examination datum analyze generalizabilitytheory framework examine extent raterrelate source error eliminate statistical adjustment Particular attention stability estimate parameter time result suggest rater stringency estimate obtain point time adjust rating period month substantially decrease usefulness case month adjustment counterproductive additionally hypothesize rate deterioration usefulness estimate parameter function characteristic scale © 2009 National Council,examination rater drift generalizability theory framework,0.025729195989387506,0.02572865987457835,0.025731152317081132,0.244675610606495,0.678135381212458,0.0015252878799953222,0.025617098250286168,0.005405814640018903,0.024384547478451087,0.07198870017934789
Gierl M.J.,Making diagnostic inferences about cognitive attributes using the rule-space model and attribute hierarchy method,2007,44,"The purpose of this paper is to describe the logic and identify key assumptions associated with making cognitive inferences using two attribute-based psychometric methods. The first method is Kikumi Tatsuoka's rule-space model. This model provides a strong point of reference for studying the nature of diagnostic inferences because it is important in the evolution of skills diagnostic testing and it is well documented. The second method is a new procedure called the attribute hierarchy method that was developed from the rule-space approach. Although the attribute hierarchy method shares many commonalities with rule space, it represents an extension by including an attribute hierarchy that serves as an explicit cognitive model of task performance designed to link psychometric practices with contemporary cognitive theories. In this paper, we describe and compare these two attribute-based psychometric methods and identify new directions for research and practice in skills diagnostic testing. © 2007 by the National Council on Measurement in Education.",Making diagnostic inferences about cognitive attributes using the rule-space model and attribute hierarchy method,"The purpose of this paper is to describe the logic and identify key assumptions associated with making cognitive inferences using two attribute-based psychometric methods. The first method is Kikumi Tatsuoka's rule-space model. This model provides a strong point of reference for studying the nature of diagnostic inferences because it is important in the evolution of skills diagnostic testing and it is well documented. The second method is a new procedure called the attribute hierarchy method that was developed from the rule-space approach. Although the attribute hierarchy method shares many commonalities with rule space, it represents an extension by including an attribute hierarchy that serves as an explicit cognitive model of task performance designed to link psychometric practices with contemporary cognitive theories. In this paper, we describe and compare these two attribute-based psychometric methods and identify new directions for research and practice in skills diagnostic testing. © 2007 by the National Council on Measurement in Education.","['purpose', 'paper', 'describe', 'logic', 'identify', 'key', 'assumption', 'associate', 'cognitive', 'inference', 'attributebase', 'psychometric', 'method', 'method', 'Kikumi', 'Tatsuokas', 'rulespace', 'provide', 'strong', 'point', 'reference', 'study', 'nature', 'diagnostic', 'inference', 'important', 'evolution', 'skill', 'diagnostic', 'testing', 'document', 'second', 'method', 'new', 'procedure', 'attribute', 'hierarchy', 'method', 'develop', 'rulespace', 'approach', 'attribute', 'hierarchy', 'method', 'share', 'commonality', 'rule', 'space', 'represent', 'extension', 'include', 'attribute', 'hierarchy', 'serve', 'explicit', 'cognitive', 'task', 'performance', 'design', 'link', 'psychometric', 'practice', 'contemporary', 'cognitive', 'theory', 'paper', 'describe', 'compare', 'attributebase', 'psychometric', 'method', 'identify', 'new', 'direction', 'research', 'practice', 'skill', 'diagnostic', 'testing', '©', '2007', 'National', 'Council']","['diagnostic', 'inference', 'cognitive', 'attribute', 'rulespace', 'attribute', 'hierarchy', 'method']",purpose paper describe logic identify key assumption associate cognitive inference attributebase psychometric method method Kikumi Tatsuokas rulespace provide strong point reference study nature diagnostic inference important evolution skill diagnostic testing document second method new procedure attribute hierarchy method develop rulespace approach attribute hierarchy method share commonality rule space represent extension include attribute hierarchy serve explicit cognitive task performance design link psychometric practice contemporary cognitive theory paper describe compare attributebase psychometric method identify new direction research practice skill diagnostic testing © 2007 National Council,diagnostic inference cognitive attribute rulespace attribute hierarchy method,0.026121234175153666,0.026110547983100293,0.026113675848243884,0.8954788933478837,0.026175648645618575,0.0,0.17585514513408054,0.0,0.0,0.0
Roussos L.A.; Ozbek O.Y.,Formulation of the DETECT population parameter and evaluation of DETECT estimator bias,2006,43,"The development of the DETECT procedure marked an important advancement in nonparametric dimensionality analysis. DETECT is the first nonparametric technique to estimate the number of dimensions in a data set, estimate an effect size for multidimensionality, and identify which dimension is predominantly measured by each item. The efficacy of DETECT critically depends on accurate, minimally biased estimation of the expected conditional covariances of all the item pairs. However, the amount of bias in the DETECT estimator has been studied only in a few simulated unidimensional data sets. This is because the value of the DETECT population parameter is known to be zero for this case and has been unknown for cases when multidimensionality is present. In this article, integral formulas for the DETECT population parameter are derived for the most commonly used parametric multidimensional item response theory model, the Reckase and McKinley model. These formulas are then used to evaluate the bias in DETECT by positing a multidimensional model, simulating data from the model using a very large sample size (to eliminate random error), calculating the large-sample DETECT statistic, and finally calculating the DETECT population parameter to compare with the large-sample statistic. A wide variety of two- and three-dimensional models, including both simple structure and approximate simple structure, were investigated. The results indicated that DETECT does exhibit statistical bias in the large-sample estimation of the item-pair conditional covariances; but, for the simulated tests that had 20 or more items, the bias was small enough to result in the large-sample DETECT almost always correctly partitioning the items and the DETECT effect size estimator exhibiting negligible bias.",Formulation of the DETECT population parameter and evaluation of DETECT estimator bias,"The development of the DETECT procedure marked an important advancement in nonparametric dimensionality analysis. DETECT is the first nonparametric technique to estimate the number of dimensions in a data set, estimate an effect size for multidimensionality, and identify which dimension is predominantly measured by each item. The efficacy of DETECT critically depends on accurate, minimally biased estimation of the expected conditional covariances of all the item pairs. However, the amount of bias in the DETECT estimator has been studied only in a few simulated unidimensional data sets. This is because the value of the DETECT population parameter is known to be zero for this case and has been unknown for cases when multidimensionality is present. In this article, integral formulas for the DETECT population parameter are derived for the most commonly used parametric multidimensional item response theory model, the Reckase and McKinley model. These formulas are then used to evaluate the bias in DETECT by positing a multidimensional model, simulating data from the model using a very large sample size (to eliminate random error), calculating the large-sample DETECT statistic, and finally calculating the DETECT population parameter to compare with the large-sample statistic. A wide variety of two- and three-dimensional models, including both simple structure and approximate simple structure, were investigated. The results indicated that DETECT does exhibit statistical bias in the large-sample estimation of the item-pair conditional covariances; but, for the simulated tests that had 20 or more items, the bias was small enough to result in the large-sample DETECT almost always correctly partitioning the items and the DETECT effect size estimator exhibiting negligible bias.","['development', 'detect', 'procedure', 'mark', 'important', 'advancement', 'nonparametric', 'dimensionality', 'analysis', 'detect', 'nonparametric', 'technique', 'estimate', 'number', 'dimension', 'data', 'set', 'estimate', 'effect', 'size', 'multidimensionality', 'identify', 'dimension', 'predominantly', 'measure', 'item', 'efficacy', 'DETECT', 'critically', 'depend', 'accurate', 'minimally', 'bias', 'estimation', 'expect', 'conditional', 'covariance', 'item', 'pair', 'bias', 'DETECT', 'estimator', 'study', 'simulated', 'unidimensional', 'datum', 'set', 'value', 'DETECT', 'population', 'parameter', 'know', 'zero', 'case', 'unknown', 'case', 'multidimensionality', 'present', 'article', 'integral', 'formula', 'DETECT', 'population', 'parameter', 'derive', 'commonly', 'parametric', 'multidimensional', 'item', 'response', 'theory', 'Reckase', 'McKinley', 'formula', 'evaluate', 'bias', 'detect', 'posit', 'multidimensional', 'simulate', 'datum', 'large', 'sample', 'size', 'eliminate', 'random', 'error', 'calculate', 'largesample', 'detect', 'statistic', 'finally', 'calculate', 'detect', 'population', 'parameter', 'compare', 'largesample', 'statistic', 'wide', 'variety', 'threedimensional', 'include', 'simple', 'structure', 'approximate', 'simple', 'structure', 'investigate', 'result', 'indicate', 'detect', 'exhibit', 'statistical', 'bias', 'largesample', 'estimation', 'itempair', 'conditional', 'covariance', 'simulate', 'test', '20', 'item', 'bias', 'small', 'result', 'largesample', 'detect', 'correctly', 'partition', 'item', 'detect', 'effect', 'size', 'estimator', 'exhibit', 'negligible', 'bias']","['formulation', 'detect', 'population', 'parameter', 'evaluation', 'DETECT', 'estimator', 'bias']",development detect procedure mark important advancement nonparametric dimensionality analysis detect nonparametric technique estimate number dimension data set estimate effect size multidimensionality identify dimension predominantly measure item efficacy DETECT critically depend accurate minimally bias estimation expect conditional covariance item pair bias DETECT estimator study simulated unidimensional datum set value DETECT population parameter know zero case unknown case multidimensionality present article integral formula DETECT population parameter derive commonly parametric multidimensional item response theory Reckase McKinley formula evaluate bias detect posit multidimensional simulate datum large sample size eliminate random error calculate largesample detect statistic finally calculate detect population parameter compare largesample statistic wide variety threedimensional include simple structure approximate simple structure investigate result indicate detect exhibit statistical bias largesample estimation itempair conditional covariance simulate test 20 item bias small result largesample detect correctly partition item detect effect size estimator exhibit negligible bias,formulation detect population parameter evaluation DETECT estimator bias,0.024516108746896605,0.024517439633818548,0.02455856940469937,0.9013590077129443,0.025048874501641127,0.030193880548209564,0.02195893595468886,0.0031275331071065585,0.05338600176439373,0.0
Meijer R.R.,Using patterns of summed scores in paper-and-pencil tests and computer-adaptive tests to detect misfitting item score patterns,2004,41,"Two new methods have been proposed to determine unexpected sum scores on subtests (testlets) both for paper-and-pencil tests and computer adaptive tests. A method based on a conservative bound using the hypergeometric distribution, denoted p, was compared with a method where the probability for each score combination was calculated using a highest density region (HDR). Furthermore, these methods were compared with the standardized log-likelihood statistic with and without a correction for the estimated latent trait value (denoted as l z* and lz, respectively). Data were simulated on the basis of the one-parameter logistic model, and both parametric and nonparametric logistic regression was used to obtain estimates of the latent trait. Results showed that it is important to take the trait level into account when comparing subtest scores. In a nonparametric item response theory (IRT) context, on adapted version of the HDR method was a powerful alterative to ρ. In a parametric IRT context, results showed that lz* had the highest power when the data were simulated conditionally on the estimated latent trait level.",Using patterns of summed scores in paper-and-pencil tests and computer-adaptive tests to detect misfitting item score patterns,"Two new methods have been proposed to determine unexpected sum scores on subtests (testlets) both for paper-and-pencil tests and computer adaptive tests. A method based on a conservative bound using the hypergeometric distribution, denoted p, was compared with a method where the probability for each score combination was calculated using a highest density region (HDR). Furthermore, these methods were compared with the standardized log-likelihood statistic with and without a correction for the estimated latent trait value (denoted as l z* and lz, respectively). Data were simulated on the basis of the one-parameter logistic model, and both parametric and nonparametric logistic regression was used to obtain estimates of the latent trait. Results showed that it is important to take the trait level into account when comparing subtest scores. In a nonparametric item response theory (IRT) context, on adapted version of the HDR method was a powerful alterative to ρ. In a parametric IRT context, results showed that lz* had the highest power when the data were simulated conditionally on the estimated latent trait level.","['new', 'method', 'propose', 'determine', 'unexpected', 'sum', 'score', 'subtest', 'testlet', 'paperandpencil', 'test', 'computer', 'adaptive', 'test', 'method', 'base', 'conservative', 'bind', 'hypergeometric', 'distribution', 'denote', 'p', 'compare', 'method', 'probability', 'score', 'combination', 'calculate', 'high', 'density', 'region', 'HDR', 'furthermore', 'method', 'compare', 'standardized', 'loglikelihood', 'statistic', 'correction', 'estimate', 'latent', 'trait', 'value', 'denote', 'l', 'z', 'lz', 'respectively', 'Data', 'simulate', 'basis', 'oneparameter', 'logistic', 'parametric', 'nonparametric', 'logistic', 'regression', 'obtain', 'estimate', 'latent', 'trait', 'result', 'important', 'trait', 'level', 'account', 'compare', 'subtest', 'score', 'nonparametric', 'item', 'response', 'theory', 'IRT', 'context', 'adapt', 'version', 'HDR', 'method', 'powerful', 'alterative', 'ρ', 'parametric', 'IRT', 'context', 'result', 'lz', 'high', 'power', 'datum', 'simulate', 'conditionally', 'estimate', 'latent', 'trait', 'level']","['pattern', 'sum', 'score', 'paperandpencil', 'test', 'computeradaptive', 'test', 'detect', 'misfitting', 'item', 'score', 'pattern']",new method propose determine unexpected sum score subtest testlet paperandpencil test computer adaptive test method base conservative bind hypergeometric distribution denote p compare method probability score combination calculate high density region HDR furthermore method compare standardized loglikelihood statistic correction estimate latent trait value denote l z lz respectively Data simulate basis oneparameter logistic parametric nonparametric logistic regression obtain estimate latent trait result important trait level account compare subtest score nonparametric item response theory IRT context adapt version HDR method powerful alterative ρ parametric IRT context result lz high power datum simulate conditionally estimate latent trait level,pattern sum score paperandpencil test computeradaptive test detect misfitting item score pattern,0.024011292460951218,0.024012318460078837,0.02400988414925833,0.9036737937759953,0.024292711153716284,0.01072758650235989,0.035265726898955234,0.014486614009592329,0.058250617853680416,0.0021541309695244913
Camilli G.; Prowker A.; Dossey J.A.; Lindquist M.M.; Chiu T.-W.; Vargas S.; De La Torre J.,Summarizing item difficulty variation with parcel scores,2008,45,"A new method for analyzing differential item functioning is proposed to investigate the relative strengths and weaknesses of multiple groups of examinees. Accordingly, the notion of a conditional measure of difference between two groups (Reference and Focal) is generalized to a conditional variance. The objective of this article is to present and illustrate a strategy for aggregating results across sets of similar items that exhibit item difficulty variation. Logically, this aggregation strategy is related to the idea of DIF amplification, but estimation is ultimately carried out in the framework of a confirmatory multidimensional Rasch model. Grade 4 data from the 2000 National Assessment of Educational Progress are used to illustrate the technique. © 2008 by the National Council on Measurement in Education.",Summarizing item difficulty variation with parcel scores,"A new method for analyzing differential item functioning is proposed to investigate the relative strengths and weaknesses of multiple groups of examinees. Accordingly, the notion of a conditional measure of difference between two groups (Reference and Focal) is generalized to a conditional variance. The objective of this article is to present and illustrate a strategy for aggregating results across sets of similar items that exhibit item difficulty variation. Logically, this aggregation strategy is related to the idea of DIF amplification, but estimation is ultimately carried out in the framework of a confirmatory multidimensional Rasch model. Grade 4 data from the 2000 National Assessment of Educational Progress are used to illustrate the technique. © 2008 by the National Council on Measurement in Education.","['new', 'method', 'analyze', 'differential', 'item', 'functioning', 'propose', 'investigate', 'relative', 'strength', 'weakness', 'multiple', 'group', 'examine', 'accordingly', 'notion', 'conditional', 'measure', 'difference', 'group', 'Reference', 'Focal', 'generalize', 'conditional', 'variance', 'objective', 'article', 'present', 'illustrate', 'strategy', 'aggregate', 'result', 'set', 'similar', 'item', 'exhibit', 'item', 'difficulty', 'variation', 'logically', 'aggregation', 'strategy', 'relate', 'idea', 'DIF', 'amplification', 'estimation', 'ultimately', 'carry', 'framework', 'confirmatory', 'multidimensional', 'Rasch', 'Grade', '4', 'datum', '2000', 'National', 'Assessment', 'Educational', 'Progress', 'illustrate', 'technique', '©', '2008', 'National', 'Council']","['summarize', 'item', 'difficulty', 'variation', 'parcel', 'score']",new method analyze differential item functioning propose investigate relative strength weakness multiple group examine accordingly notion conditional measure difference group Reference Focal generalize conditional variance objective article present illustrate strategy aggregate result set similar item exhibit item difficulty variation logically aggregation strategy relate idea DIF amplification estimation ultimately carry framework confirmatory multidimensional Rasch Grade 4 datum 2000 National Assessment Educational Progress illustrate technique © 2008 National Council,summarize item difficulty variation parcel score,0.024062091970771526,0.024058556152543802,0.024179383662802298,0.9033156135765587,0.024384354637323802,0.045326024157574446,0.00811656167426095,0.004115501945282448,0.030845613053234928,0.07640480691522832
Schulz E.M.; Lee W.-C.; Mullen K.,A domain-level approach to describing growth in achievement,2005,42,"Descriptions of growth in educational achievement often rely on the notion that higher-level students can do whatever lower-level students can do, plus at least one more thing. This article presents a method of supporting such descriptions using the data of a subject-area achievement test. Multiple content domains with an expected order of difficulty were defined within the Grade 8 National Assessment of Educational Progress (NAEP) in mathematics. Teachers were able to reliably classify items into the domains by content. Using expected percentage correct scores on the domains, it was possible to describe each achievement level boundary (Basic, Proficient, and Advanced) on the NAEP scale by patterns of skill that include both mastery and non-mastery, and to show that higher achievement levels are associated with mastery of more skills. We conclude that general achievement tests like NAEP can be used to provide criterion-referenced descriptions of growth in achievement as a sequential mastery of skills.",A domain-level approach to describing growth in achievement,"Descriptions of growth in educational achievement often rely on the notion that higher-level students can do whatever lower-level students can do, plus at least one more thing. This article presents a method of supporting such descriptions using the data of a subject-area achievement test. Multiple content domains with an expected order of difficulty were defined within the Grade 8 National Assessment of Educational Progress (NAEP) in mathematics. Teachers were able to reliably classify items into the domains by content. Using expected percentage correct scores on the domains, it was possible to describe each achievement level boundary (Basic, Proficient, and Advanced) on the NAEP scale by patterns of skill that include both mastery and non-mastery, and to show that higher achievement levels are associated with mastery of more skills. We conclude that general achievement tests like NAEP can be used to provide criterion-referenced descriptions of growth in achievement as a sequential mastery of skills.","['description', 'growth', 'educational', 'achievement', 'rely', 'notion', 'higherlevel', 'student', 'lowerlevel', 'student', 'plus', 'thing', 'article', 'present', 'method', 'support', 'description', 'datum', 'subjectarea', 'achievement', 'test', 'multiple', 'content', 'domain', 'expected', 'order', 'difficulty', 'define', 'Grade', '8', 'National', 'Assessment', 'Educational', 'Progress', 'NAEP', 'mathematic', 'Teachers', 'able', 'reliably', 'classify', 'item', 'domain', 'content', 'expected', 'percentage', 'correct', 'score', 'domain', 'possible', 'describe', 'achievement', 'level', 'boundary', 'Basic', 'Proficient', 'Advanced', 'naep', 'scale', 'pattern', 'skill', 'include', 'mastery', 'nonmastery', 'high', 'achievement', 'level', 'associate', 'mastery', 'skill', 'conclude', 'general', 'achievement', 'test', 'like', 'naep', 'provide', 'criterionreference', 'description', 'growth', 'achievement', 'sequential', 'mastery', 'skill']","['domainlevel', 'approach', 'describe', 'growth', 'achievement']",description growth educational achievement rely notion higherlevel student lowerlevel student plus thing article present method support description datum subjectarea achievement test multiple content domain expected order difficulty define Grade 8 National Assessment Educational Progress NAEP mathematic Teachers able reliably classify item domain content expected percentage correct score domain possible describe achievement level boundary Basic Proficient Advanced naep scale pattern skill include mastery nonmastery high achievement level associate mastery skill conclude general achievement test like naep provide criterionreference description growth achievement sequential mastery skill,domainlevel approach describe growth achievement,0.027122644199433686,0.02712302533657993,0.027185499715046173,0.45863025985191413,0.45993857089702617,0.0,0.027049420379186256,0.0,0.004430189284817533,0.10080889442934318
Livingston S.A.; Kim S.,The circle-arc method for equating in small samples,2009,46,"This article suggests a method for estimating a test-score equating relationship from small samples of test takers. The method does not require the estimated equating transformation to be linear. Instead, it constrains the estimated equating curve to pass through two pre-specified end points and a middle point determined from the data. In a resampling study with two test forms that differed substantially in difficulty, the proposed method compared favorably with other equating methods, especially for equating scores below the 10th percentile and above the 90th percentile. © 2009 by the National Council on Measurement in Education.",The circle-arc method for equating in small samples,"This article suggests a method for estimating a test-score equating relationship from small samples of test takers. The method does not require the estimated equating transformation to be linear. Instead, it constrains the estimated equating curve to pass through two pre-specified end points and a middle point determined from the data. In a resampling study with two test forms that differed substantially in difficulty, the proposed method compared favorably with other equating methods, especially for equating scores below the 10th percentile and above the 90th percentile. © 2009 by the National Council on Measurement in Education.","['article', 'suggest', 'method', 'estimate', 'testscore', 'equating', 'relationship', 'small', 'sample', 'test', 'taker', 'method', 'require', 'estimate', 'equate', 'transformation', 'linear', 'instead', 'constrain', 'estimate', 'equating', 'curve', 'pass', 'prespecified', 'end', 'point', 'middle', 'point', 'determine', 'datum', 'resampling', 'study', 'test', 'form', 'differ', 'substantially', 'difficulty', 'propose', 'method', 'compare', 'favorably', 'equate', 'method', 'especially', 'equate', 'score', '10th', 'percentile', '90th', 'percentile', '©', '2009', 'National', 'Council']","['circlearc', 'method', 'equate', 'small', 'sample']",article suggest method estimate testscore equating relationship small sample test taker method require estimate equate transformation linear instead constrain estimate equating curve pass prespecified end point middle point determine datum resampling study test form differ substantially difficulty propose method compare favorably equate method especially equate score 10th percentile 90th percentile © 2009 National Council,circlearc method equate small sample,0.030542638777439568,0.030543262325309038,0.030541445480070067,0.8777396083632144,0.030633045053966915,0.0,0.0,0.16762257652214907,0.007403134728740315,0.0
Van Der Linden W.J.,Conceptual issues in response-time modeling,2009,46,"Two different traditions of response-time (RT) modeling are reviewed: the tradition of distinct models for RTs and responses, and the tradition of model integration in which RTs are incorporated in response models or the other way around. Several conceptual issues underlying both traditions are made explicit and analyzed for their consequences. We then propose a hierarchical modeling framework consistent with the first tradition but with the integration of their parameter structures as a second level of modeling. Two examples of the framework are presented. Also, a fundamental equation is derived which relates the RTs on test items to the speed of the test taker and the time intensity of the items. The equation serves as the core of the RT model in the framework. Finally, empirical applications of the framework demonstrating its practical value are reviewed. © 2009 by the National Council on Measurement in Education.",,"Two different traditions of response-time (RT) modeling are reviewed: the tradition of distinct models for RTs and responses, and the tradition of model integration in which RTs are incorporated in response models or the other way around. Several conceptual issues underlying both traditions are made explicit and analyzed for their consequences. We then propose a hierarchical modeling framework consistent with the first tradition but with the integration of their parameter structures as a second level of modeling. Two examples of the framework are presented. Also, a fundamental equation is derived which relates the RTs on test items to the speed of the test taker and the time intensity of the items. The equation serves as the core of the RT model in the framework. Finally, empirical applications of the framework demonstrating its practical value are reviewed. © 2009 by the National Council on Measurement in Education.","['different', 'tradition', 'responsetime', 'RT', 'modeling', 'review', 'tradition', 'distinct', 'rt', 'response', 'tradition', 'integration', 'rt', 'incorporate', 'response', 'way', 'conceptual', 'issue', 'underlie', 'tradition', 'explicit', 'analyze', 'consequence', 'propose', 'hierarchical', 'modeling', 'framework', 'consistent', 'tradition', 'integration', 'parameter', 'structure', 'second', 'level', 'example', 'framework', 'present', 'fundamental', 'equation', 'derive', 'relate', 'rt', 'test', 'item', 'speed', 'test', 'taker', 'time', 'intensity', 'item', 'equation', 'serve', 'core', 'RT', 'framework', 'finally', 'empirical', 'application', 'framework', 'demonstrate', 'practical', 'value', 'review', '©', '2009', 'National', 'Council']",,different tradition responsetime RT modeling review tradition distinct rt response tradition integration rt incorporate response way conceptual issue underlie tradition explicit analyze consequence propose hierarchical modeling framework consistent tradition integration parameter structure second level example framework present fundamental equation derive relate rt test item speed test taker time intensity item equation serve core RT framework finally empirical application framework demonstrate practical value review © 2009 National Council,,0.033112165051890054,0.03310587425122682,0.03323077909436497,0.5358308512314829,0.36472033037103524,0.005011986861951664,0.01192415864380054,0.0,0.03177336898068574,0.006224216712273752
Huitzing H.A.,An interactive method to solve infeasibility in linear programming test assembling models,2004,41,"In optimal assembly of tests from item banks, linear programming (LP) models have proved to be very useful. Assembly by hand has become nearly impossible, but these LP techniques are able to find the best solutions, given the demands and needs of the test to be assembled and the specifics of the item bank from which it is assembled. However, sometimes even LP techniques do not offer an acceptable solution to the test assembler. Infeasibility occurs when the demands are contradictory. These contradictions may be rather complex, especially when stated in terms of LP models. Techniques are described that can solve these infeasibility problems in different manners. The objectives are twofold. First, the assembler is given a helping hand to identify the bottlenecks in the specifications of the LP model. Second, a solution is forced, such that the test assembler is always presented a test as close as possible to the original specifications. These objectives should be realizable both automatically and interactively with the test assembler.",An interactive method to solve infeasibility in linear programming test assembling models,"In optimal assembly of tests from item banks, linear programming (LP) models have proved to be very useful. Assembly by hand has become nearly impossible, but these LP techniques are able to find the best solutions, given the demands and needs of the test to be assembled and the specifics of the item bank from which it is assembled. However, sometimes even LP techniques do not offer an acceptable solution to the test assembler. Infeasibility occurs when the demands are contradictory. These contradictions may be rather complex, especially when stated in terms of LP models. Techniques are described that can solve these infeasibility problems in different manners. The objectives are twofold. First, the assembler is given a helping hand to identify the bottlenecks in the specifications of the LP model. Second, a solution is forced, such that the test assembler is always presented a test as close as possible to the original specifications. These objectives should be realizable both automatically and interactively with the test assembler.","['optimal', 'assembly', 'test', 'item', 'bank', 'linear', 'program', 'lp', 'prove', 'useful', 'Assembly', 'hand', 'nearly', 'impossible', 'lp', 'technique', 'able', 'find', 'good', 'solution', 'demand', 'need', 'test', 'assemble', 'specific', 'item', 'bank', 'assemble', 'lp', 'technique', 'offer', 'acceptable', 'solution', 'test', 'assembler', 'Infeasibility', 'occur', 'demand', 'contradictory', 'contradiction', 'complex', 'especially', 'state', 'term', 'lp', 'technique', 'describe', 'solve', 'infeasibility', 'problem', 'different', 'manner', 'objective', 'twofold', 'assembler', 'help', 'hand', 'identify', 'bottleneck', 'specification', 'LP', 'Second', 'solution', 'force', 'test', 'assembler', 'present', 'test', 'close', 'possible', 'original', 'specification', 'objective', 'realizable', 'automatically', 'interactively', 'test', 'assembler']","['interactive', 'method', 'solve', 'infeasibility', 'linear', 'programming', 'test', 'assemble']",optimal assembly test item bank linear program lp prove useful Assembly hand nearly impossible lp technique able find good solution demand need test assemble specific item bank assemble lp technique offer acceptable solution test assembler Infeasibility occur demand contradictory contradiction complex especially state term lp technique describe solve infeasibility problem different manner objective twofold assembler help hand identify bottleneck specification LP Second solution force test assembler present test close possible original specification objective realizable automatically interactively test assembler,interactive method solve infeasibility linear programming test assemble,0.029019798658111495,0.029000042135050925,0.6434481845633714,0.26929960909331974,0.029232365550146442,0.004501440229221396,0.0,0.002094572190602774,0.030779321542929436,0.013028315234275332
Sinharay S.,Assessing fit of unidimensional item response theory models using a bayesian approach,2005,42,"Even though Bayesian estimation has recently become quite popular in item response theory (IRT), there is a lack of works on model checking from a Bayesian perspective. This paper applies the posterior predictive model checking (PPMC) method (Guttman, 1967; Rubin, 1984), a popular Bayesian model checking tool, to a number of real applications of unidimensional IRT models. The applications demonstrate how to exploit the flexibility of the posterior predictive checks to meet the need of the researcher. This paper also examines practical consequences of misfit, an area often ignored in educational measurement literature while assessing model fit.",Assessing fit of unidimensional item response theory models using a bayesian approach,"Even though Bayesian estimation has recently become quite popular in item response theory (IRT), there is a lack of works on model checking from a Bayesian perspective. This paper applies the posterior predictive model checking (PPMC) method (Guttman, 1967; Rubin, 1984), a popular Bayesian model checking tool, to a number of real applications of unidimensional IRT models. The applications demonstrate how to exploit the flexibility of the posterior predictive checks to meet the need of the researcher. This paper also examines practical consequences of misfit, an area often ignored in educational measurement literature while assessing model fit.","['bayesian', 'estimation', 'recently', 'popular', 'item', 'response', 'theory', 'IRT', 'lack', 'work', 'check', 'bayesian', 'perspective', 'paper', 'apply', 'posterior', 'predictive', 'check', 'PPMC', 'method', 'Guttman', '1967', 'Rubin', '1984', 'popular', 'bayesian', 'check', 'tool', 'number', 'real', 'application', 'unidimensional', 'IRT', 'application', 'demonstrate', 'exploit', 'flexibility', 'posterior', 'predictive', 'check', 'meet', 'need', 'researcher', 'paper', 'examine', 'practical', 'consequence', 'misfit', 'area', 'ignore', 'educational', 'literature', 'assess', 'fit']","['assess', 'fit', 'unidimensional', 'item', 'response', 'theory', 'bayesian', 'approach']",bayesian estimation recently popular item response theory IRT lack work check bayesian perspective paper apply posterior predictive check PPMC method Guttman 1967 Rubin 1984 popular bayesian check tool number real application unidimensional IRT application demonstrate exploit flexibility posterior predictive check meet need researcher paper examine practical consequence misfit area ignore educational literature assess fit,assess fit unidimensional item response theory bayesian approach,0.03080320796044363,0.030804959024586912,0.030806060458756897,0.8766305958484103,0.03095517670780217,0.0,0.05803215784227143,0.0,0.019400115864664388,0.0
Liu J.; Cahn M.F.; Dorans N.J.,An application of score equity assessment: Invariance of linkage of new SAT® to old SAT across gender groups,2006,43,"The College Board's SAT® data are used to illustrate how the score equity assessment (SEA) can help inform the program about equatability. SEA is used to examine whether the content change(s) to the revised new SAT result in differential linking functions across gender groups. Results of population sensitivity analyses are reported on the linkage of the new SAT critical reading (CR) prototype to an old SAT verbal (OV). Based on the criteria used in this study, population invariance was achieved with respect to gender groups.",An application of score equity assessment: Invariance of linkage of new SAT® to old SAT across gender groups,"The College Board's SAT® data are used to illustrate how the score equity assessment (SEA) can help inform the program about equatability. SEA is used to examine whether the content change(s) to the revised new SAT result in differential linking functions across gender groups. Results of population sensitivity analyses are reported on the linkage of the new SAT critical reading (CR) prototype to an old SAT verbal (OV). Based on the criteria used in this study, population invariance was achieved with respect to gender groups.","['College', 'Boards', 'SAT', '®', 'datum', 'illustrate', 'score', 'equity', 'assessment', 'SEA', 'help', 'inform', 'program', 'equatability', 'sea', 'examine', 'content', 'change', 'revise', 'new', 'SAT', 'result', 'differential', 'linking', 'function', 'gender', 'group', 'result', 'population', 'sensitivity', 'analysis', 'report', 'linkage', 'new', 'SAT', 'critical', 'reading', 'CR', 'prototype', 'old', 'SAT', 'verbal', 'OV', 'base', 'criterion', 'study', 'population', 'invariance', 'achieve', 'respect', 'gender', 'group']","['application', 'score', 'equity', 'assessment', 'Invariance', 'linkage', 'new', 'SAT', '®', 'old', 'SAT', 'gender', 'group']",College Boards SAT ® datum illustrate score equity assessment SEA help inform program equatability sea examine content change revise new SAT result differential linking function gender group result population sensitivity analysis report linkage new SAT critical reading CR prototype old SAT verbal OV base criterion study population invariance achieve respect gender group,application score equity assessment Invariance linkage new SAT ® old SAT gender group,0.028170563943593584,0.028171210784000127,0.0281862970404268,0.0294062844179038,0.8860656438140757,0.01581287371787198,0.019064598599836606,0.022656245606747397,0.0,0.05247874987127169
Klockars A.J.; Lee Y.,Simulated tests of differential item functioning using SIBTEST with and without impact,2008,45,"Monte Carlo simulations with 20,000 replications are reported to estimate the probability of rejecting the null hypothesis regarding DIF using SIBTEST when there is DIF present and/or when impact is present due to differences on the primary dimension to be measured. Sample sizes are varied from 250 to 2000 and test lengths from 10 to 40 items. Results generally support previous findings for Type I error rates and power. Impact is inversely related to test length. The combination of DIF and impact, with the focal group having lower ability on both the primary and secondary dimensions, results in impact partially masking DIF so that items biased toward the reference group are less likely to be detected. © 2008 by the National Council on Measurement in Education.",Simulated tests of differential item functioning using SIBTEST with and without impact,"Monte Carlo simulations with 20,000 replications are reported to estimate the probability of rejecting the null hypothesis regarding DIF using SIBTEST when there is DIF present and/or when impact is present due to differences on the primary dimension to be measured. Sample sizes are varied from 250 to 2000 and test lengths from 10 to 40 items. Results generally support previous findings for Type I error rates and power. Impact is inversely related to test length. The combination of DIF and impact, with the focal group having lower ability on both the primary and secondary dimensions, results in impact partially masking DIF so that items biased toward the reference group are less likely to be detected. © 2008 by the National Council on Measurement in Education.","['Monte', 'Carlo', 'simulation', '20000', 'replication', 'report', 'estimate', 'probability', 'reject', 'null', 'hypothesis', 'regard', 'DIF', 'sibtest', 'DIF', 'present', 'andor', 'impact', 'present', 'difference', 'primary', 'dimension', 'measure', 'Sample', 'size', 'varied', '250', '2000', 'test', 'length', '10', '40', 'item', 'result', 'generally', 'support', 'previous', 'finding', 'Type', 'I', 'error', 'rate', 'power', 'Impact', 'inversely', 'relate', 'test', 'length', 'combination', 'dif', 'impact', 'focal', 'group', 'low', 'ability', 'primary', 'secondary', 'dimension', 'result', 'impact', 'partially', 'mask', 'DIF', 'item', 'bias', 'reference', 'group', 'likely', 'detect', '©', '2008', 'National', 'Council']","['simulate', 'test', 'differential', 'item', 'function', 'sibtest', 'impact']",Monte Carlo simulation 20000 replication report estimate probability reject null hypothesis regard DIF sibtest DIF present andor impact present difference primary dimension measure Sample size varied 250 2000 test length 10 40 item result generally support previous finding Type I error rate power Impact inversely relate test length combination dif impact focal group low ability primary secondary dimension result impact partially mask DIF item bias reference group likely detect © 2008 National Council,simulate test differential item function sibtest impact,0.02564796686602367,0.02564852979628904,0.025655016626037468,0.8972573349557801,0.025791151755869778,0.13192464420276379,0.0,0.0,0.0,0.0
Petridou A.; Williams J.,Accounting for aberrant test response patterns using multilevel models,2007,44,"Hypotheses about aberrant test-response behavior and hence invalid person-measurement have hitherto included factors like ability, gender, language, test-anxiety, and motivation, but these have not previously been collectively investigated with real data, or with multilevel models. This study analyzes the effect of these factors on person aberrance using a real mathematics assessment data set under the framework of a two-level (person and classroom) hierarchical model. The results suggest that higher-scoring pupils, and, to a lesser extent, second-language learners are significantly more often aberrant. But more importantly, we find that the classroom makes a significant contribution to person aberrance and conclude that studies that investigate the sources of person aberrance with real data should model the classroom as well as individual levels. © 2007 by the National Council on Measurement in Education.",Accounting for aberrant test response patterns using multilevel models,"Hypotheses about aberrant test-response behavior and hence invalid person-measurement have hitherto included factors like ability, gender, language, test-anxiety, and motivation, but these have not previously been collectively investigated with real data, or with multilevel models. This study analyzes the effect of these factors on person aberrance using a real mathematics assessment data set under the framework of a two-level (person and classroom) hierarchical model. The results suggest that higher-scoring pupils, and, to a lesser extent, second-language learners are significantly more often aberrant. But more importantly, we find that the classroom makes a significant contribution to person aberrance and conclude that studies that investigate the sources of person aberrance with real data should model the classroom as well as individual levels. © 2007 by the National Council on Measurement in Education.","['hypothesis', 'aberrant', 'testresponse', 'behavior', 'invalid', 'personmeasurement', 'hitherto', 'include', 'factor', 'like', 'ability', 'gender', 'language', 'testanxiety', 'motivation', 'previously', 'collectively', 'investigate', 'real', 'datum', 'multilevel', 'study', 'analyze', 'effect', 'factor', 'person', 'aberrance', 'real', 'mathematic', 'assessment', 'datum', 'set', 'framework', 'twolevel', 'person', 'classroom', 'hierarchical', 'result', 'suggest', 'higherscore', 'pupil', 'extent', 'secondlanguage', 'learner', 'significantly', 'aberrant', 'importantly', 'find', 'classroom', 'significant', 'contribution', 'person', 'aberrance', 'conclude', 'study', 'investigate', 'source', 'person', 'aberrance', 'real', 'datum', 'classroom', 'individual', 'level', '©', '2007', 'National', 'Council']","['account', 'aberrant', 'test', 'response', 'pattern', 'multilevel']",hypothesis aberrant testresponse behavior invalid personmeasurement hitherto include factor like ability gender language testanxiety motivation previously collectively investigate real datum multilevel study analyze effect factor person aberrance real mathematic assessment datum set framework twolevel person classroom hierarchical result suggest higherscore pupil extent secondlanguage learner significantly aberrant importantly find classroom significant contribution person aberrance conclude study investigate source person aberrance real datum classroom individual level © 2007 National Council,account aberrant test response pattern multilevel,0.027888272030778004,0.02788945444137913,0.027933168505801704,0.8877201657653189,0.028568939256722388,0.01508215430631342,0.012680549410241772,0.0,0.009718697822777435,0.04109672629304946
Van Nijlen D.; Janssen R.,Modeling judgments in the Angoff and contrasting-groups method of standard setting,2008,45,"Essential for the validity of the judgments in a standard-setting study is that they follow the implicit task assumptions. In the Angoff method, judgments are assumed to be inversely related to the difficulty of the items; contrasting-groups judgments are assumed to be positively related to the ability of the students. In the present study, judgments from both procedures were modeled with a random-effects probit regression model. The Angoff judgments showed a weaker link with the position of the items on the latent scale than the contrasting-groups judgments with the position of the students. Hence, in the specific context of the study, the contrasting-groups judgments were more aligned with the underlying assumptions of the method than the Angoff judgments. © 2007 by the National Council on Measurement in Education.",Modeling judgments in the Angoff and contrasting-groups method of standard setting,"Essential for the validity of the judgments in a standard-setting study is that they follow the implicit task assumptions. In the Angoff method, judgments are assumed to be inversely related to the difficulty of the items; contrasting-groups judgments are assumed to be positively related to the ability of the students. In the present study, judgments from both procedures were modeled with a random-effects probit regression model. The Angoff judgments showed a weaker link with the position of the items on the latent scale than the contrasting-groups judgments with the position of the students. Hence, in the specific context of the study, the contrasting-groups judgments were more aligned with the underlying assumptions of the method than the Angoff judgments. © 2007 by the National Council on Measurement in Education.","['essential', 'validity', 'judgment', 'standardsette', 'study', 'follow', 'implicit', 'task', 'assumption', 'Angoff', 'method', 'judgment', 'assume', 'inversely', 'relate', 'difficulty', 'item', 'contrastinggroup', 'judgment', 'assume', 'positively', 'relate', 'ability', 'student', 'present', 'study', 'judgment', 'procedure', 'randomeffect', 'probit', 'regression', 'The', 'Angoff', 'judgment', 'weak', 'link', 'position', 'item', 'latent', 'scale', 'contrastinggroup', 'judgment', 'position', 'student', 'specific', 'context', 'study', 'contrastinggroup', 'judgment', 'aligned', 'underlie', 'assumption', 'method', 'Angoff', 'judgment', '©', '2007', 'National', 'Council']","['judgment', 'Angoff', 'contrastinggroup', 'method', 'standard', 'setting']",essential validity judgment standardsette study follow implicit task assumption Angoff method judgment assume inversely relate difficulty item contrastinggroup judgment assume positively relate ability student present study judgment procedure randomeffect probit regression The Angoff judgment weak link position item latent scale contrastinggroup judgment position student specific context study contrastinggroup judgment aligned underlie assumption method Angoff judgment © 2007 National Council,judgment Angoff contrastinggroup method standard setting,0.03699440082813654,0.036996043347690216,0.3684926654871645,0.5203457966416511,0.03717109369535757,0.0,0.07067425328812198,0.0,0.01621989205154997,0.0026136304164376344
Kim S.-H.; Cohen A.S.; Alagoz C.; Kim S.,DIF detection and effect size measures for polytomously scored items,2007,44,"Data from a large-scale performance assessment (N = 105,731) were analyzed with five differential item functioning (DIF) detection methods for polytomous items to examine the congruence among the DIF detection methods. Two different versions of the item response theory (IRT) model-based likelihood ratio test, the logistic regression likelihood ratio test, the Mantel test, and the generalized Mantel-Haenszel test were compared. Results indicated some agreement among the five DIF detection methods. Because statistical power is a function of the sample size, the DIF detection results from extremely large data sets are not practically useful. As alternatives to the DIF detection methods, four IRT model-based indices of standardized impact and four observed-score indices of standardized impact for polytomous items were obtained and compared with the R2 measures of logistic regression. © 2007 by the National Council on Measurement in Education.",DIF detection and effect size measures for polytomously scored items,"Data from a large-scale performance assessment (N = 105,731) were analyzed with five differential item functioning (DIF) detection methods for polytomous items to examine the congruence among the DIF detection methods. Two different versions of the item response theory (IRT) model-based likelihood ratio test, the logistic regression likelihood ratio test, the Mantel test, and the generalized Mantel-Haenszel test were compared. Results indicated some agreement among the five DIF detection methods. Because statistical power is a function of the sample size, the DIF detection results from extremely large data sets are not practically useful. As alternatives to the DIF detection methods, four IRT model-based indices of standardized impact and four observed-score indices of standardized impact for polytomous items were obtained and compared with the R2 measures of logistic regression. © 2007 by the National Council on Measurement in Education.","['datum', 'largescale', 'performance', 'assessment', 'N', '105731', 'analyze', 'differential', 'item', 'function', 'dif', 'detection', 'method', 'polytomous', 'item', 'examine', 'congruence', 'DIF', 'detection', 'method', 'different', 'version', 'item', 'response', 'theory', 'IRT', 'modelbase', 'likelihood', 'ratio', 'test', 'logistic', 'regression', 'likelihood', 'ratio', 'test', 'Mantel', 'test', 'generalized', 'MantelHaenszel', 'test', 'compare', 'result', 'indicate', 'agreement', 'dif', 'detection', 'method', 'statistical', 'power', 'function', 'sample', 'size', 'DIF', 'detection', 'result', 'extremely', 'large', 'data', 'set', 'practically', 'useful', 'alternative', 'DIF', 'detection', 'method', 'IRT', 'modelbase', 'index', 'standardized', 'impact', 'observedscore', 'index', 'standardized', 'impact', 'polytomous', 'item', 'obtain', 'compare', 'R2', 'measure', 'logistic', 'regression', '©', '2007', 'National', 'Council']","['DIF', 'detection', 'effect', 'size', 'measure', 'polytomously', 'score', 'item']",datum largescale performance assessment N 105731 analyze differential item function dif detection method polytomous item examine congruence DIF detection method different version item response theory IRT modelbase likelihood ratio test logistic regression likelihood ratio test Mantel test generalized MantelHaenszel test compare result indicate agreement dif detection method statistical power function sample size DIF detection result extremely large data set practically useful alternative DIF detection method IRT modelbase index standardized impact observedscore index standardized impact polytomous item obtain compare R2 measure logistic regression © 2007 National Council,DIF detection effect size measure polytomously score item,0.027250390807184863,0.027252047565569794,0.027299023674802303,0.7300584062617006,0.18814013169074248,0.14737359308499562,0.005485661646238736,0.005952927340729424,0.008843263844529851,0.0
Huitzing H.A.; Veldkamp B.P.; Verschoor A.J.,Infeasibility in automated test assembly models: A comparison study of different methods,2005,42,"Several techniques exist to automatically put together a test meeting a number of specifications. In an item bank, the items are stored with their characteristics. A test is constructed by selecting a set of items that fulfills the specifications set by the test assembler. Test assembly problems are often formulated in terms of a model consisting of restrictions and an objective to be maximized or minimized. A problem arises when it is impossible to construct a test from the item pool that meets all specifications, that is, when the model is not feasible. Several methods exist to handle these infeasibility problems. In this article, test assembly models resulting from two practical testing programs were reconstructed to be infeasible. These models were analyzed using methods that forced a solution (Goal Programming, Multiple-Goal Programming, Greedy Heuristic), that analyzed the causes (Relaxed and Ordered Deletion Algorithm (RODA), Integer Randomized Deletion Algorithm (IRDA), Set Covering (SC), and Item Sampling), or that analyzed the causes and used this information to force a solution (Irreducible Infeasible Set-Solver). Specialized methods such as the IRDA and the Irreducible Infeasible Set-Solver performed best. Recommendations about the use of different methods are given.",Infeasibility in automated test assembly models: A comparison study of different methods,"Several techniques exist to automatically put together a test meeting a number of specifications. In an item bank, the items are stored with their characteristics. A test is constructed by selecting a set of items that fulfills the specifications set by the test assembler. Test assembly problems are often formulated in terms of a model consisting of restrictions and an objective to be maximized or minimized. A problem arises when it is impossible to construct a test from the item pool that meets all specifications, that is, when the model is not feasible. Several methods exist to handle these infeasibility problems. In this article, test assembly models resulting from two practical testing programs were reconstructed to be infeasible. These models were analyzed using methods that forced a solution (Goal Programming, Multiple-Goal Programming, Greedy Heuristic), that analyzed the causes (Relaxed and Ordered Deletion Algorithm (RODA), Integer Randomized Deletion Algorithm (IRDA), Set Covering (SC), and Item Sampling), or that analyzed the causes and used this information to force a solution (Irreducible Infeasible Set-Solver). Specialized methods such as the IRDA and the Irreducible Infeasible Set-Solver performed best. Recommendations about the use of different methods are given.","['technique', 'exist', 'automatically', 'test', 'meeting', 'number', 'specification', 'item', 'bank', 'item', 'store', 'characteristic', 'test', 'construct', 'select', 'set', 'item', 'fulfill', 'specification', 'set', 'test', 'assembler', 'Test', 'assembly', 'problem', 'formulate', 'term', 'consist', 'restriction', 'objective', 'maximize', 'minimize', 'problem', 'arise', 'impossible', 'construct', 'test', 'item', 'pool', 'meet', 'specification', 'feasible', 'method', 'exist', 'handle', 'infeasibility', 'problem', 'article', 'test', 'assembly', 'result', 'practical', 'testing', 'program', 'reconstruct', 'infeasible', 'analyze', 'method', 'force', 'solution', 'Goal', 'Programming', 'MultipleGoal', 'Programming', 'Greedy', 'Heuristic', 'analyze', 'cause', 'relaxed', 'Ordered', 'Deletion', 'Algorithm', 'RODA', 'Integer', 'Randomized', 'Deletion', 'Algorithm', 'IRDA', 'set', 'cover', 'SC', 'Item', 'Sampling', 'analyze', 'cause', 'information', 'force', 'solution', 'Irreducible', 'Infeasible', 'SetSolver', 'Specialized', 'method', 'irda', 'Irreducible', 'Infeasible', 'SetSolver', 'perform', 'good', 'recommendation', 'different', 'method']","['infeasibility', 'automate', 'test', 'assembly', 'comparison', 'study', 'different', 'method']",technique exist automatically test meeting number specification item bank item store characteristic test construct select set item fulfill specification set test assembler Test assembly problem formulate term consist restriction objective maximize minimize problem arise impossible construct test item pool meet specification feasible method exist handle infeasibility problem article test assembly result practical testing program reconstruct infeasible analyze method force solution Goal Programming MultipleGoal Programming Greedy Heuristic analyze cause relaxed Ordered Deletion Algorithm RODA Integer Randomized Deletion Algorithm IRDA set cover SC Item Sampling analyze cause information force solution Irreducible Infeasible SetSolver Specialized method irda Irreducible Infeasible SetSolver perform good recommendation different method,infeasibility automate test assembly comparison study different method,0.02379729488174761,0.023798596516540267,0.6568523852526861,0.271644706922609,0.023907016426417134,0.0030157799385103936,0.0,0.006026959038842225,0.06895861759625554,4.7053403234646653e-05
Puhan G.; Moses T.P.; Yu L.; Dorans N.J.,Using log-linear smoothing to improve small-sample DIF estimation,2009,46,"This study examined the extent to which log-linear smoothing could improve the accuracy of differential item functioning (DIF) estimates in small samples of examinees. Examinee responses from a certification test were analyzed using White examinees in the reference group and African American examinees in the focal group. Using a simulation approach, separate DIF estimates for seven small-sample-size conditions were obtained using unsmoothed (U) and smoothed (S) score distributions. These small sample U and S DIF estimates were compared to a criterion (i.e., DIF estimates obtained using the unsmoothed total data) to assess their degree of variability (random error) and accuracy (bias). Results indicate that for most studied items smoothing the raw score distributions reduced random error and bias of the DIF estimates, especially in the small-sample-size conditions. Implications of these results for operational testing programs are discussed. © 2009 by the National Council on Measurement in Education.",Using log-linear smoothing to improve small-sample DIF estimation,"This study examined the extent to which log-linear smoothing could improve the accuracy of differential item functioning (DIF) estimates in small samples of examinees. Examinee responses from a certification test were analyzed using White examinees in the reference group and African American examinees in the focal group. Using a simulation approach, separate DIF estimates for seven small-sample-size conditions were obtained using unsmoothed (U) and smoothed (S) score distributions. These small sample U and S DIF estimates were compared to a criterion (i.e., DIF estimates obtained using the unsmoothed total data) to assess their degree of variability (random error) and accuracy (bias). Results indicate that for most studied items smoothing the raw score distributions reduced random error and bias of the DIF estimates, especially in the small-sample-size conditions. Implications of these results for operational testing programs are discussed. © 2009 by the National Council on Measurement in Education.","['study', 'examine', 'extent', 'loglinear', 'smoothing', 'improve', 'accuracy', 'differential', 'item', 'function', 'DIF', 'estimate', 'small', 'sample', 'examine', 'Examinee', 'response', 'certification', 'test', 'analyze', 'White', 'examinee', 'reference', 'group', 'african', 'American', 'examinee', 'focal', 'group', 'simulation', 'approach', 'separate', 'DIF', 'estimate', 'seven', 'smallsamplesize', 'condition', 'obtain', 'unsmoothed', 'U', 'smooth', 'S', 'score', 'distribution', 'small', 'sample', 'u', 'S', 'DIF', 'estimate', 'compare', 'criterion', 'ie', 'DIF', 'estimate', 'obtain', 'unsmoothed', 'total', 'datum', 'assess', 'degree', 'variability', 'random', 'error', 'accuracy', 'bias', 'result', 'indicate', 'studied', 'item', 'smooth', 'raw', 'score', 'distribution', 'reduce', 'random', 'error', 'bias', 'DIF', 'estimate', 'especially', 'smallsamplesize', 'condition', 'implication', 'result', 'operational', 'testing', 'program', 'discuss', '©', '2009', 'National', 'Council']","['loglinear', 'smooth', 'improve', 'smallsample', 'DIF', 'estimation']",study examine extent loglinear smoothing improve accuracy differential item function DIF estimate small sample examine Examinee response certification test analyze White examinee reference group african American examinee focal group simulation approach separate DIF estimate seven smallsamplesize condition obtain unsmoothed U smooth S score distribution small sample u S DIF estimate compare criterion ie DIF estimate obtain unsmoothed total datum assess degree variability random error accuracy bias result indicate studied item smooth raw score distribution reduce random error bias DIF estimate especially smallsamplesize condition implication result operational testing program discuss © 2009 National Council,loglinear smooth improve smallsample DIF estimation,0.02574468822459965,0.025746494986382637,0.025738430699107213,0.8959514571124177,0.026818928977492752,0.1069810267300085,0.029859460243259695,0.02096466532949294,0.003375181161837001,0.0
Finch H.,Estimation of item response theory parameters in the presence of missing data,2008,45,"Missing data are a common problem in a variety of measurement settings, including responses to items on both cognitive and affective assessments. Researchers have shown that such missing data may create problems in the estimation of item difficulty parameters in the Item Response Theory (IRT) context, particularly if they are ignored. At the same time, a number of data imputation methods have been developed outside of the IRT framework and been shown to be effective tools for dealing with missing data. The current study takes several of these methods that have been found to be useful in other contexts and investigates their performance with IRT data that contain missing values. Through a simulation study, it is shown that these methods exhibit varying degrees of effectiveness in terms of imputing data that in turn produce accurate sample estimates of item difficulty and discrimination parameters. © 2008 by the National Council on Measurement in Education.",Estimation of item response theory parameters in the presence of missing data,"Missing data are a common problem in a variety of measurement settings, including responses to items on both cognitive and affective assessments. Researchers have shown that such missing data may create problems in the estimation of item difficulty parameters in the Item Response Theory (IRT) context, particularly if they are ignored. At the same time, a number of data imputation methods have been developed outside of the IRT framework and been shown to be effective tools for dealing with missing data. The current study takes several of these methods that have been found to be useful in other contexts and investigates their performance with IRT data that contain missing values. Through a simulation study, it is shown that these methods exhibit varying degrees of effectiveness in terms of imputing data that in turn produce accurate sample estimates of item difficulty and discrimination parameters. © 2008 by the National Council on Measurement in Education.","['miss', 'datum', 'common', 'problem', 'variety', 'setting', 'include', 'response', 'item', 'cognitive', 'affective', 'assessment', 'Researchers', 'missing', 'datum', 'create', 'problem', 'estimation', 'item', 'difficulty', 'parameter', 'Item', 'Response', 'Theory', 'IRT', 'context', 'particularly', 'ignore', 'time', 'number', 'datum', 'imputation', 'method', 'develop', 'outside', 'IRT', 'framework', 'effective', 'tool', 'deal', 'missing', 'datum', 'current', 'study', 'method', 'find', 'useful', 'context', 'investigate', 'performance', 'IRT', 'datum', 'contain', 'miss', 'value', 'simulation', 'study', 'method', 'exhibit', 'vary', 'degree', 'effectiveness', 'term', 'impute', 'datum', 'turn', 'produce', 'accurate', 'sample', 'estimate', 'item', 'difficulty', 'discrimination', 'parameter', '©', '2008', 'National', 'Council']","['estimation', 'item', 'response', 'theory', 'parameter', 'presence', 'miss', 'datum']",miss datum common problem variety setting include response item cognitive affective assessment Researchers missing datum create problem estimation item difficulty parameter Item Response Theory IRT context particularly ignore time number datum imputation method develop outside IRT framework effective tool deal missing datum current study method find useful context investigate performance IRT datum contain miss value simulation study method exhibit vary degree effectiveness term impute datum turn produce accurate sample estimate item difficulty discrimination parameter © 2008 National Council,estimation item response theory parameter presence miss datum,0.026308016104123547,0.02630853205519813,0.02632730874267877,0.8946623297431426,0.026393813354856903,0.0,0.0444044910983296,1.884127868045379e-05,0.08567102780739781,0.019615905451666946
Kim S.; Feldt L.S.,A comparison of tests for equality of two or more independent alpha coefficients,2008,45,"This article extends the Bonett (2003a) approach to testing the equality of alpha coefficients from two independent samples to the case of m ≥ 2 independent samples. The extended Fisher-Bonett test and its competitor, the Hakstian-Whalen (1976) test, are illustrated with numerical examples of both hypothesis testing and power calculation. Computer simulations are used to compare the performance of the two tests and the Feldt (1969) test (for m = 2) in terms of power and Type I error control. It is shown that the Fisher-Bonett test is just as effective as its competitors in controlling Type I error, is comparable to them in power, and is equally robust against heterogeneity of error variance. © 2008 by the National Council on Measurement in Education.",A comparison of tests for equality of two or more independent alpha coefficients,"This article extends the Bonett (2003a) approach to testing the equality of alpha coefficients from two independent samples to the case of m ≥ 2 independent samples. The extended Fisher-Bonett test and its competitor, the Hakstian-Whalen (1976) test, are illustrated with numerical examples of both hypothesis testing and power calculation. Computer simulations are used to compare the performance of the two tests and the Feldt (1969) test (for m = 2) in terms of power and Type I error control. It is shown that the Fisher-Bonett test is just as effective as its competitors in controlling Type I error, is comparable to them in power, and is equally robust against heterogeneity of error variance. © 2008 by the National Council on Measurement in Education.","['article', 'extend', 'Bonett', '2003a', 'approach', 'test', 'equality', 'alpha', 'coefficient', 'independent', 'sample', 'case', 'm', '≥', '2', 'independent', 'sample', 'extended', 'FisherBonett', 'test', 'competitor', 'HakstianWhalen', '1976', 'test', 'illustrate', 'numerical', 'example', 'hypothesis', 'testing', 'power', 'calculation', 'Computer', 'simulation', 'compare', 'performance', 'test', 'Feldt', '1969', 'test', 'm', '2', 'term', 'power', 'Type', 'I', 'error', 'control', 'FisherBonett', 'test', 'effective', 'competitor', 'control', 'Type', 'I', 'error', 'comparable', 'power', 'equally', 'robust', 'heterogeneity', 'error', 'variance', '©', '2008', 'National', 'Council']","['comparison', 'test', 'equality', 'independent', 'alpha', 'coefficient']",article extend Bonett 2003a approach test equality alpha coefficient independent sample case m ≥ 2 independent sample extended FisherBonett test competitor HakstianWhalen 1976 test illustrate numerical example hypothesis testing power calculation Computer simulation compare performance test Feldt 1969 test m 2 term power Type I error control FisherBonett test effective competitor control Type I error comparable power equally robust heterogeneity error variance © 2008 National Council,comparison test equality independent alpha coefficient,0.027790121902250605,0.02779123846879616,0.0278284087787149,0.24424943161865903,0.6723407992315794,0.049200115080924665,0.0287237154767489,0.00840307280042403,0.005429577550104098,0.0
Clauser B.E.; Harik P.; Margolis M.J.,A multivariate generalizability analysis of data from a performance assessment of physicians' clinical skills,2006,43,"Although multivariate generalizability theory was developed more than 30 years ago, little published research utilizing this framework exists and most of what does exist examines tests built from tables of specifications. In this context, it is assumed that the universe scores from levels of the fixed multivariate facet will be correlated, but the error terms will be uncorrelated because subscores result from mutually exclusive sets of test items. This paper reports on an application in which multiple subscores are derived from each task completed by the examinee. In this context, both universe scores and errors may be correlated across levels of the fixed multi-variate facet. The data described come from the United States Medical Licensing Examination® Step 2 Clinical Skills Examination. In this test, each examinee interacts with a series of standardized patients and each interaction results in four component scores. The paper focuses on the application of multivariate generalizability theory in this context and on the practical interpretation of the resulting estimated variance and covariance components.",A multivariate generalizability analysis of data from a performance assessment of physicians' clinical skills,"Although multivariate generalizability theory was developed more than 30 years ago, little published research utilizing this framework exists and most of what does exist examines tests built from tables of specifications. In this context, it is assumed that the universe scores from levels of the fixed multivariate facet will be correlated, but the error terms will be uncorrelated because subscores result from mutually exclusive sets of test items. This paper reports on an application in which multiple subscores are derived from each task completed by the examinee. In this context, both universe scores and errors may be correlated across levels of the fixed multi-variate facet. The data described come from the United States Medical Licensing Examination® Step 2 Clinical Skills Examination. In this test, each examinee interacts with a series of standardized patients and each interaction results in four component scores. The paper focuses on the application of multivariate generalizability theory in this context and on the practical interpretation of the resulting estimated variance and covariance components.","['multivariate', 'generalizability', 'theory', 'develop', '30', 'year', 'ago', 'little', 'publish', 'research', 'utilize', 'framework', 'exist', 'exist', 'examine', 'test', 'build', 'table', 'specification', 'context', 'assume', 'universe', 'score', 'level', 'fix', 'multivariate', 'facet', 'correlate', 'error', 'term', 'uncorrelate', 'subscore', 'result', 'mutually', 'exclusive', 'set', 'test', 'item', 'paper', 'report', 'application', 'multiple', 'subscore', 'derive', 'task', 'complete', 'examinee', 'context', 'universe', 'score', 'error', 'correlate', 'level', 'fix', 'multivariate', 'facet', 'datum', 'describe', 'come', 'United', 'States', 'Medical', 'Licensing', 'Examination', '®', 'step', '2', 'Clinical', 'Skills', 'Examination', 'test', 'examinee', 'interact', 'series', 'standardized', 'patient', 'interaction', 'result', 'component', 'score', 'paper', 'focus', 'application', 'multivariate', 'generalizability', 'theory', 'context', 'practical', 'interpretation', 'result', 'estimate', 'variance', 'covariance', 'component']","['multivariate', 'generalizability', 'analysis', 'datum', 'performance', 'assessment', 'physician', 'clinical', 'skill']",multivariate generalizability theory develop 30 year ago little publish research utilize framework exist exist examine test build table specification context assume universe score level fix multivariate facet correlate error term uncorrelate subscore result mutually exclusive set test item paper report application multiple subscore derive task complete examinee context universe score error correlate level fix multivariate facet datum describe come United States Medical Licensing Examination ® step 2 Clinical Skills Examination test examinee interact series standardized patient interaction result component score paper focus application multivariate generalizability theory context practical interpretation result estimate variance covariance component,multivariate generalizability analysis datum performance assessment physician clinical skill,0.02380491802446004,0.023787023724534433,0.02390393439110743,0.45337637461020147,0.47512774924969664,0.0037983907359664033,0.10312077679509489,0.0,0.012266386804489647,0.004985825883540181
Wise S.L.; Demars C.E.,An application of item response time: The effort-moderated IRT model,2006,43,"The validity of inferences based on achievement test scores is dependent on the amount of effort that examinees put forth while taking the test. With low-stakes tests, for which this problem is particularly prevalent, there is a consequent need for psychometric models that can take into account differing levels of examinee effort. This article introduces the effort-moderated IRT model, which incorporates item response time into proficiency estimation and item parameter estimation. In two studies of the effort-moderated model when rapid guessing (i.e., reflecting low examinee effort) was present, one based on real data and the other on simulated data, the effort-moderated model performed better than the standard 3PL model. Specifically, it was found that the effort-moderated model (a) showed better model fit, (b) yielded more accurate item parameter estimates, (c) more accurately estimated test information, and (d) yielded proficiency estimates with higher convergent validity. © Copyright 2006 by the National Council on Measurement in Education.",An application of item response time: The effort-moderated IRT model,"The validity of inferences based on achievement test scores is dependent on the amount of effort that examinees put forth while taking the test. With low-stakes tests, for which this problem is particularly prevalent, there is a consequent need for psychometric models that can take into account differing levels of examinee effort. This article introduces the effort-moderated IRT model, which incorporates item response time into proficiency estimation and item parameter estimation. In two studies of the effort-moderated model when rapid guessing (i.e., reflecting low examinee effort) was present, one based on real data and the other on simulated data, the effort-moderated model performed better than the standard 3PL model. Specifically, it was found that the effort-moderated model (a) showed better model fit, (b) yielded more accurate item parameter estimates, (c) more accurately estimated test information, and (d) yielded proficiency estimates with higher convergent validity. © Copyright 2006 by the National Council on Measurement in Education.","['validity', 'inference', 'base', 'achievement', 'test', 'score', 'dependent', 'effort', 'examine', 'forth', 'test', 'lowstake', 'test', 'problem', 'particularly', 'prevalent', 'consequent', 'need', 'psychometric', 'account', 'differ', 'level', 'examinee', 'effort', 'article', 'introduce', 'effortmoderated', 'IRT', 'incorporate', 'item', 'response', 'time', 'proficiency', 'estimation', 'item', 'parameter', 'estimation', 'study', 'effortmoderated', 'rapid', 'guess', 'ie', 'reflect', 'low', 'examinee', 'effort', 'present', 'base', 'real', 'datum', 'simulate', 'datum', 'effortmoderated', 'perform', 'standard', '3pl', 'specifically', 'find', 'effortmoderated', 'fit', 'b', 'yield', 'accurate', 'item', 'parameter', 'estimate', 'c', 'accurately', 'estimate', 'test', 'information', 'd', 'yield', 'proficiency', 'estimate', 'high', 'convergent', 'validity', '©', 'copyright', '2006', 'National', 'Council']","['application', 'item', 'response', 'time', 'effortmoderated', 'IRT']",validity inference base achievement test score dependent effort examine forth test lowstake test problem particularly prevalent consequent need psychometric account differ level examinee effort article introduce effortmoderated IRT incorporate item response time proficiency estimation item parameter estimation study effortmoderated rapid guess ie reflect low examinee effort present base real datum simulate datum effortmoderated perform standard 3pl specifically find effortmoderated fit b yield accurate item parameter estimate c accurately estimate test information d yield proficiency estimate high convergent validity © copyright 2006 National Council,application item response time effortmoderated IRT,0.02625934136340259,0.02625795849401901,0.026261425256710526,0.894854258262675,0.02636701662319303,0.0,0.04384773135481454,0.0,0.07590656293589602,0.013458156721240296
Stone C.A.; Zhang B.,Assessing Goodness of Fit of Item Response Theory Models: A Comparison of Traditional and Alternative Procedures,2003,40,"Testing the goodness of fit of item response theory (IRT) models is relevant to validating IRT models, and new procedures have been proposed. These alternatives compare observed and expected response frequencies conditional on observed total scores, and use posterior probabilities for responses across θ levels rather than cross-classifying examinees using point estimates of θ and score responses. This research compared these alternatives with regard to their methods, properties (Type I error rates and empirical power), available research, and practical issues (computational demands, treatment of missing data, effects of sample size and sparse data, and available computer programs). Different advantages and disadvantages related to these characteristics are discussed, A simulation study provided additional information about empirical power and Type I error rates.",Assessing Goodness of Fit of Item Response Theory Models: A Comparison of Traditional and Alternative Procedures,"Testing the goodness of fit of item response theory (IRT) models is relevant to validating IRT models, and new procedures have been proposed. These alternatives compare observed and expected response frequencies conditional on observed total scores, and use posterior probabilities for responses across θ levels rather than cross-classifying examinees using point estimates of θ and score responses. This research compared these alternatives with regard to their methods, properties (Type I error rates and empirical power), available research, and practical issues (computational demands, treatment of missing data, effects of sample size and sparse data, and available computer programs). Different advantages and disadvantages related to these characteristics are discussed, A simulation study provided additional information about empirical power and Type I error rates.","['test', 'goodness', 'fit', 'item', 'response', 'theory', 'IRT', 'relevant', 'validate', 'IRT', 'new', 'procedure', 'propose', 'alternative', 'compare', 'observed', 'expect', 'response', 'frequency', 'conditional', 'observed', 'total', 'score', 'posterior', 'probability', 'response', 'θ', 'level', 'crossclassifying', 'examine', 'point', 'estimate', 'θ', 'score', 'response', 'research', 'compare', 'alternative', 'regard', 'method', 'property', 'Type', 'I', 'error', 'rate', 'empirical', 'power', 'available', 'research', 'practical', 'issue', 'computational', 'demand', 'treatment', 'miss', 'datum', 'effect', 'sample', 'size', 'sparse', 'datum', 'available', 'computer', 'program', 'different', 'advantage', 'disadvantage', 'relate', 'characteristic', 'discuss', 'simulation', 'study', 'provide', 'additional', 'information', 'empirical', 'power', 'Type', 'I', 'error', 'rate']","['assess', 'Goodness', 'Fit', 'Item', 'Response', 'Theory', 'Models', 'Comparison', 'traditional', 'Alternative', 'procedure']",test goodness fit item response theory IRT relevant validate IRT new procedure propose alternative compare observed expect response frequency conditional observed total score posterior probability response θ level crossclassifying examine point estimate θ score response research compare alternative regard method property Type I error rate empirical power available research practical issue computational demand treatment miss datum effect sample size sparse datum available computer program different advantage disadvantage relate characteristic discuss simulation study provide additional information empirical power Type I error rate,assess Goodness Fit Item Response Theory Models Comparison traditional Alternative procedure,0.024171873554143363,0.02417230961118238,0.024204600749409392,0.9031954918614143,0.024255724223850573,0.029708560377763288,0.09341989627162728,0.007143026445369266,0.0342454979918701,0.0
Allen N.L.; Holland P.W.; Thayer D.T.,Measuring the benefits of examinee-selected questions,2005,42,"Allowing students to choose the question(s) that they will answer from among several possible alternatives is often viewed as a mechanism for increasing fairness in certain types of assessments. The fairness of optional topic choice is not a universally accepted fact, however, and various studies have been done to assess this question. We examine an important class of experiments that we call CI-A, ""choose one, answer all,"" designs, and point out an important problem that they face. We suggest two analytical methods that can be used to circumvent this problem. We illustrate our ideas using the data from Bridgeman et al. (1997). Our reanalysis of these data show: (a) that differential topic difficulty exists in real choice data, (b) that it affects naïve analyses of such data and masks the effects, positive or negative, of examinee choice, (c) that in this study there is a measurable and positive effect of examinee choice that follows predicted patterns in most but not all cases, (d) that the beneficial strength of examinee choice varies from case to case, and (e) that while the benefits of choice in terms of average points scored on the essays are usually positive, there is a substantial amount of variation around these averages and it is not uncommon for ""incorrect"" choices to be associated with higher test performance.",Measuring the benefits of examinee-selected questions,"Allowing students to choose the question(s) that they will answer from among several possible alternatives is often viewed as a mechanism for increasing fairness in certain types of assessments. The fairness of optional topic choice is not a universally accepted fact, however, and various studies have been done to assess this question. We examine an important class of experiments that we call CI-A, ""choose one, answer all,"" designs, and point out an important problem that they face. We suggest two analytical methods that can be used to circumvent this problem. We illustrate our ideas using the data from Bridgeman et al. (1997). Our reanalysis of these data show: (a) that differential topic difficulty exists in real choice data, (b) that it affects naïve analyses of such data and masks the effects, positive or negative, of examinee choice, (c) that in this study there is a measurable and positive effect of examinee choice that follows predicted patterns in most but not all cases, (d) that the beneficial strength of examinee choice varies from case to case, and (e) that while the benefits of choice in terms of average points scored on the essays are usually positive, there is a substantial amount of variation around these averages and it is not uncommon for ""incorrect"" choices to be associated with higher test performance.","['allow', 'student', 'choose', 'question', 'answer', 'possible', 'alternative', 'view', 'mechanism', 'increase', 'fairness', 'certain', 'type', 'assessment', 'fairness', 'optional', 'topic', 'choice', 'universally', 'accept', 'fact', 'study', 'assess', 'question', 'examine', 'important', 'class', 'experiment', 'CIA', 'choose', 'answer', 'design', 'point', 'important', 'problem', 'face', 'suggest', 'analytical', 'method', 'circumvent', 'problem', 'illustrate', 'idea', 'datum', 'Bridgeman', 'et', 'al', '1997', 'reanalysis', 'datum', 'differential', 'topic', 'difficulty', 'exist', 'real', 'choice', 'datum', 'b', 'affect', 'naïve', 'analysis', 'datum', 'mask', 'effect', 'positive', 'negative', 'examinee', 'choice', 'c', 'study', 'measurable', 'positive', 'effect', 'examinee', 'choice', 'follow', 'predict', 'pattern', 'case', 'd', 'beneficial', 'strength', 'examinee', 'choice', 'vary', 'case', 'case', 'e', 'benefit', 'choice', 'term', 'average', 'point', 'score', 'essay', 'usually', 'positive', 'substantial', 'variation', 'average', 'uncommon', 'incorrect', 'choice', 'associate', 'high', 'test', 'performance']","['measure', 'benefit', 'examineeselecte', 'question']",allow student choose question answer possible alternative view mechanism increase fairness certain type assessment fairness optional topic choice universally accept fact study assess question examine important class experiment CIA choose answer design point important problem face suggest analytical method circumvent problem illustrate idea datum Bridgeman et al 1997 reanalysis datum differential topic difficulty exist real choice datum b affect naïve analysis datum mask effect positive negative examinee choice c study measurable positive effect examinee choice follow predict pattern case d beneficial strength examinee choice vary case case e benefit choice term average point score essay usually positive substantial variation average uncommon incorrect choice associate high test performance,measure benefit examineeselecte question,0.023361839179343945,0.023136825940294908,0.023172951931056302,0.9069361427550999,0.02339224019420473,0.0024461343528700006,0.012373769526115842,0.00777335603250034,0.030054877972474435,0.05871292585306148
De La Torre J.; Deng W.,Improving person-fit assessment by correcting the ability estimate and its reference distribution,2008,45,"The standardized log-likelihood of a response vector (lz) is a popular IRT-based person-fit test statistic for identifying model-misfitting response patterns. Traditional use of lz is overly conservative in detecting aberrance due to its incorrect assumption regarding its theoretical null distribution. This study proposes a method for improving the accuracy of person-fit analysis using lz which takes into account test unreliability when estimating the ability and constructs the distribution for each lz through resampling methods. The Type I error and power (or detection rate) of the proposed method were examined at different test lengths, ability levels, and nominal α levels along with other methods, and power to detect three types of aberrance - cheating, lack of motivation, and speeding - was considered. Results indicate that the proposed method is a viable and promising approach. It has Type I error rates close to the nominal value for most ability levels and reasonably good power. © 2008 by the National Council on Measurement in Education.",Improving person-fit assessment by correcting the ability estimate and its reference distribution,"The standardized log-likelihood of a response vector (lz) is a popular IRT-based person-fit test statistic for identifying model-misfitting response patterns. Traditional use of lz is overly conservative in detecting aberrance due to its incorrect assumption regarding its theoretical null distribution. This study proposes a method for improving the accuracy of person-fit analysis using lz which takes into account test unreliability when estimating the ability and constructs the distribution for each lz through resampling methods. The Type I error and power (or detection rate) of the proposed method were examined at different test lengths, ability levels, and nominal α levels along with other methods, and power to detect three types of aberrance - cheating, lack of motivation, and speeding - was considered. Results indicate that the proposed method is a viable and promising approach. It has Type I error rates close to the nominal value for most ability levels and reasonably good power. © 2008 by the National Council on Measurement in Education.","['standardized', 'loglikelihood', 'response', 'vector', 'lz', 'popular', 'irtbased', 'personfit', 'test', 'statistic', 'identify', 'modelmisfitte', 'response', 'pattern', 'traditional', 'lz', 'overly', 'conservative', 'detect', 'aberrance', 'incorrect', 'assumption', 'regard', 'theoretical', 'null', 'distribution', 'study', 'propose', 'method', 'improve', 'accuracy', 'personfit', 'analysis', 'lz', 'account', 'test', 'unreliability', 'estimate', 'ability', 'construct', 'distribution', 'lz', 'resample', 'method', 'type', 'I', 'error', 'power', 'detection', 'rate', 'propose', 'method', 'examine', 'different', 'test', 'length', 'ability', 'level', 'nominal', 'α', 'level', 'method', 'power', 'detect', 'type', 'aberrance', 'cheat', 'lack', 'motivation', 'speeding', 'consider', 'result', 'indicate', 'propose', 'method', 'viable', 'promising', 'approach', 'Type', 'I', 'error', 'rate', 'close', 'nominal', 'value', 'ability', 'level', 'reasonably', 'good', 'power', '©', '2008', 'National', 'Council']","['improve', 'personfit', 'assessment', 'correct', 'ability', 'estimate', 'reference', 'distribution']",standardized loglikelihood response vector lz popular irtbased personfit test statistic identify modelmisfitte response pattern traditional lz overly conservative detect aberrance incorrect assumption regard theoretical null distribution study propose method improve accuracy personfit analysis lz account test unreliability estimate ability construct distribution lz resample method type I error power detection rate propose method examine different test length ability level nominal α level method power detect type aberrance cheat lack motivation speeding consider result indicate propose method viable promising approach Type I error rate close nominal value ability level reasonably good power © 2008 National Council,improve personfit assessment correct ability estimate reference distribution,0.024850795026185375,0.024850081143728102,0.024848907403338456,0.9004664523725622,0.02498376405418579,0.04640887324518609,0.012310527132559925,0.012629745995963629,0.05005481761614878,0.0
Finkelman M.; Kim W.; Roussos L.A.,Automated test assembly for cognitive diagnosis models using a genetic algorithm,2009,46,"Much recent psychometric literature has focused on cognitive diagnosis models (CDMs), a promising class of instruments used to measure the strengths and weaknesses of examinees. This article introduces a genetic algorithm to perform automated test assembly alongside CDMs. The algorithm is flexible in that it can be applied whether the goal is to minimize the average number of classification errors, minimize the maximum error rate across all attributes being measured, hit a target set of error rates, or optimize any other prescribed objective function. Under multiple simulation conditions, the algorithm compared favorably with a standard method of automated test assembly, successfully finding solutions that were appropriate for each stated goal. © 2009 by the National Council on Measurement in Education.",Automated test assembly for cognitive diagnosis models using a genetic algorithm,"Much recent psychometric literature has focused on cognitive diagnosis models (CDMs), a promising class of instruments used to measure the strengths and weaknesses of examinees. This article introduces a genetic algorithm to perform automated test assembly alongside CDMs. The algorithm is flexible in that it can be applied whether the goal is to minimize the average number of classification errors, minimize the maximum error rate across all attributes being measured, hit a target set of error rates, or optimize any other prescribed objective function. Under multiple simulation conditions, the algorithm compared favorably with a standard method of automated test assembly, successfully finding solutions that were appropriate for each stated goal. © 2009 by the National Council on Measurement in Education.","['recent', 'psychometric', 'literature', 'focus', 'cognitive', 'diagnosis', 'cdm', 'promising', 'class', 'instrument', 'measure', 'strength', 'weakness', 'examinee', 'article', 'introduce', 'genetic', 'algorithm', 'perform', 'automate', 'test', 'assembly', 'alongside', 'CDMs', 'algorithm', 'flexible', 'apply', 'goal', 'minimize', 'average', 'number', 'classification', 'error', 'minimize', 'maximum', 'error', 'rate', 'attribute', 'measure', 'hit', 'target', 'set', 'error', 'rate', 'optimize', 'prescribe', 'objective', 'function', 'multiple', 'simulation', 'condition', 'algorithm', 'compare', 'favorably', 'standard', 'method', 'automate', 'test', 'assembly', 'successfully', 'find', 'solution', 'appropriate', 'state', 'goal', '©', '2009', 'National', 'Council']","['automate', 'test', 'assembly', 'cognitive', 'diagnosis', 'genetic', 'algorithm']",recent psychometric literature focus cognitive diagnosis cdm promising class instrument measure strength weakness examinee article introduce genetic algorithm perform automate test assembly alongside CDMs algorithm flexible apply goal minimize average number classification error minimize maximum error rate attribute measure hit target set error rate optimize prescribe objective function multiple simulation condition algorithm compare favorably standard method automate test assembly successfully find solution appropriate state goal © 2009 National Council,automate test assembly cognitive diagnosis genetic algorithm,0.025321391512110995,0.025322420602432533,0.2600651672809184,0.663883493077917,0.02540752752662111,0.004195127138955335,0.05959371213858839,0.003989821031502314,0.03261049877485757,0.007727135781990732
Myford C.M.; Wolfe E.W.,Monitoring rater performance over time: A framework for detecting differential accuracy and differential scale category use,2009,46,"In this study, we describe a framework for monitoring rater performance over time. We present several statistical indices to identify raters whose standards drift and explain how to use those indices operationally. To illustrate the use of the framework, we analyzed rating data from the 2002 Advanced Placement English Literature and Composition examination, employing a multifaceted Rasch approach to determine whether raters exhibited evidence of two types of differential rater functioning over time (i.e., changes in levels of accuracy or scale category use). Some raters showed statistically significant changes in their levels of accuracy as the scoring progressed, while other raters displayed evidence of differential scale category use over time. © 2009 by the National Council on Measurement in Education.",Monitoring rater performance over time: A framework for detecting differential accuracy and differential scale category use,"In this study, we describe a framework for monitoring rater performance over time. We present several statistical indices to identify raters whose standards drift and explain how to use those indices operationally. To illustrate the use of the framework, we analyzed rating data from the 2002 Advanced Placement English Literature and Composition examination, employing a multifaceted Rasch approach to determine whether raters exhibited evidence of two types of differential rater functioning over time (i.e., changes in levels of accuracy or scale category use). Some raters showed statistically significant changes in their levels of accuracy as the scoring progressed, while other raters displayed evidence of differential scale category use over time. © 2009 by the National Council on Measurement in Education.","['study', 'describe', 'framework', 'monitor', 'rater', 'performance', 'time', 'present', 'statistical', 'index', 'identify', 'rater', 'standard', 'drift', 'explain', 'index', 'operationally', 'illustrate', 'framework', 'analyze', 'rating', 'datum', '2002', 'Advanced', 'Placement', 'English', 'Literature', 'Composition', 'examination', 'employ', 'multifacete', 'Rasch', 'approach', 'determine', 'rater', 'exhibit', 'evidence', 'type', 'differential', 'rater', 'function', 'time', 'ie', 'change', 'level', 'accuracy', 'scale', 'category', 'rater', 'statistically', 'significant', 'change', 'level', 'accuracy', 'scoring', 'progress', 'rater', 'display', 'evidence', 'differential', 'scale', 'category', 'time', '©', '2009', 'National', 'Council']","['monitor', 'rater', 'performance', 'time', 'framework', 'detect', 'differential', 'accuracy', 'differential', 'scale', 'category']",study describe framework monitor rater performance time present statistical index identify rater standard drift explain index operationally illustrate framework analyze rating datum 2002 Advanced Placement English Literature Composition examination employ multifacete Rasch approach determine rater exhibit evidence type differential rater function time ie change level accuracy scale category rater statistically significant change level accuracy scoring progress rater display evidence differential scale category time © 2009 National Council,monitor rater performance time framework detect differential accuracy differential scale category,0.02938820322088111,0.029382150101485153,0.029511752647278965,0.5228403307491833,0.3888775632811715,0.013550074285119144,0.006980135153494143,0.0,0.013501653367141896,0.08764086879241227
Van Der Linden W.J.; Breithaupt K.; Chuah S.C.; Zhang Y.,Detecting differential speededness in multistage testing,2007,44,"A potential undesirable effect of multistage testing is differential speededness, which happens if some of the test takers run out of time because they receive subtests with items that are more time intensive than others. This article shows how a probabilistic response-time model can be used for estimating differences in time intensities and speed between subtests and test takers and detecting differential speededness. An empirical data set for a multistage test in the computerized CPA Exam was used to demonstrate the procedures. Although the more difficult subtests appeared to have items that were more time intensive than the easier subtests, an analysis of the residual response times did not reveal any significant differential speededness because the time limit appeared to be appropriate. In a separate analysis, within each of the subtests, we found minor but consistent patterns of residual times that are believed to be due to a warm-up effect, that is, use of more time on the initial items than they actually need. © 2007 by the National Council on Measurement in Education.",Detecting differential speededness in multistage testing,"A potential undesirable effect of multistage testing is differential speededness, which happens if some of the test takers run out of time because they receive subtests with items that are more time intensive than others. This article shows how a probabilistic response-time model can be used for estimating differences in time intensities and speed between subtests and test takers and detecting differential speededness. An empirical data set for a multistage test in the computerized CPA Exam was used to demonstrate the procedures. Although the more difficult subtests appeared to have items that were more time intensive than the easier subtests, an analysis of the residual response times did not reveal any significant differential speededness because the time limit appeared to be appropriate. In a separate analysis, within each of the subtests, we found minor but consistent patterns of residual times that are believed to be due to a warm-up effect, that is, use of more time on the initial items than they actually need. © 2007 by the National Council on Measurement in Education.","['potential', 'undesirable', 'effect', 'multistage', 'testing', 'differential', 'speededness', 'happen', 'test', 'taker', 'run', 'time', 'receive', 'subtest', 'item', 'time', 'intensive', 'article', 'probabilistic', 'responsetime', 'estimate', 'difference', 'time', 'intensity', 'speed', 'subtest', 'test', 'taker', 'detect', 'differential', 'speededness', 'empirical', 'datum', 'set', 'multistage', 'test', 'computerized', 'cpa', 'Exam', 'demonstrate', 'procedure', 'difficult', 'subtest', 'appear', 'item', 'time', 'intensive', 'easy', 'subtest', 'analysis', 'residual', 'response', 'time', 'reveal', 'significant', 'differential', 'speededness', 'time', 'limit', 'appear', 'appropriate', 'separate', 'analysis', 'subtest', 'find', 'minor', 'consistent', 'pattern', 'residual', 'time', 'believe', 'warmup', 'effect', 'time', 'initial', 'item', 'actually', 'need', '©', '2007', 'National', 'Council']","['detect', 'differential', 'speededness', 'multistage', 'testing']",potential undesirable effect multistage testing differential speededness happen test taker run time receive subtest item time intensive article probabilistic responsetime estimate difference time intensity speed subtest test taker detect differential speededness empirical datum set multistage test computerized cpa Exam demonstrate procedure difficult subtest appear item time intensive easy subtest analysis residual response time reveal significant differential speededness time limit appear appropriate separate analysis subtest find minor consistent pattern residual time believe warmup effect time initial item actually need © 2007 National Council,detect differential speededness multistage testing,0.029747786654359613,0.029749048589028073,0.0298015104051406,0.8808395593656728,0.02986209498579883,0.011623336362913584,0.001511888003417848,0.0,0.04399379596029091,0.024911525924786643
Stout W.,Skills diagnosis using IRT-based continuous latent trait models,2007,44,"This article summarizes the continuous latent trait IRT approach to skills diagnosis as particularized by a representative variety of continuous latent trait models using item response functions (IRFs). First, several basic IRT-based continuous latent trait approaches are presented in some detail. Then a brief summary of estimation, model checking, and assessment scoring aspects are discussed. Finally, the University of California at Berkeley multidimensional Rasch-model-grounded SEPUP middle school science-focused embedded assessment project is briefly described as one significant illustrative application. © 2007 by the National Council on Measurement in Education.",Skills diagnosis using IRT-based continuous latent trait models,"This article summarizes the continuous latent trait IRT approach to skills diagnosis as particularized by a representative variety of continuous latent trait models using item response functions (IRFs). First, several basic IRT-based continuous latent trait approaches are presented in some detail. Then a brief summary of estimation, model checking, and assessment scoring aspects are discussed. Finally, the University of California at Berkeley multidimensional Rasch-model-grounded SEPUP middle school science-focused embedded assessment project is briefly described as one significant illustrative application. © 2007 by the National Council on Measurement in Education.","['article', 'summarize', 'continuous', 'latent', 'trait', 'IRT', 'approach', 'skill', 'diagnosis', 'particularize', 'representative', 'variety', 'continuous', 'latent', 'trait', 'item', 'response', 'function', 'irf', 'basic', 'irtbased', 'continuous', 'latent', 'trait', 'approach', 'present', 'detail', 'brief', 'summary', 'estimation', 'checking', 'assessment', 'scoring', 'aspect', 'discuss', 'finally', 'University', 'California', 'Berkeley', 'multidimensional', 'Raschmodelgrounded', 'sepup', 'middle', 'school', 'sciencefocuse', 'embed', 'assessment', 'project', 'briefly', 'describe', 'significant', 'illustrative', 'application', '©', '2007', 'National', 'Council']","['skill', 'diagnosis', 'irtbased', 'continuous', 'latent', 'trait']",article summarize continuous latent trait IRT approach skill diagnosis particularize representative variety continuous latent trait item response function irf basic irtbased continuous latent trait approach present detail brief summary estimation checking assessment scoring aspect discuss finally University California Berkeley multidimensional Raschmodelgrounded sepup middle school sciencefocuse embed assessment project briefly describe significant illustrative application © 2007 National Council,skill diagnosis irtbased continuous latent trait,0.1556594637363264,0.02809364157258526,0.028075251808335228,0.7596583533783499,0.028513289504403315,0.0013026724223471312,0.06280233878536584,0.0,0.007023078851166712,0.02277597434967164
Gierl M.J.; Zheng Y.; Cui Y.,Using the attribute hierarchy method to identify and interpret cognitive skills that produce group differences,2008,45,"The purpose of this study is to describe how the attribute hierarchy method (AHM) can be used to evaluate differential group performance at the cognitive attribute level. The AHM is a psychometric method for classifying examinees' test item responses into a set of attribute-mastery patterns associated with different components in a cognitive model of task performance. Attribute probabilities, computed using a neural network, can be estimated on each attribute for each examinee thereby providing specific information about the examinee's attribute-mastery level. These probabilities can also be compared across groups. We describe a four-step procedure for estimating and interpreting group differences using the AHM. We also provide an example using student response data from a sample of algebra items on the SAT to illustrate our pattern recognition approach for studying group differences. © 2007 by the National Council on Measurement in Education.",Using the attribute hierarchy method to identify and interpret cognitive skills that produce group differences,"The purpose of this study is to describe how the attribute hierarchy method (AHM) can be used to evaluate differential group performance at the cognitive attribute level. The AHM is a psychometric method for classifying examinees' test item responses into a set of attribute-mastery patterns associated with different components in a cognitive model of task performance. Attribute probabilities, computed using a neural network, can be estimated on each attribute for each examinee thereby providing specific information about the examinee's attribute-mastery level. These probabilities can also be compared across groups. We describe a four-step procedure for estimating and interpreting group differences using the AHM. We also provide an example using student response data from a sample of algebra items on the SAT to illustrate our pattern recognition approach for studying group differences. © 2007 by the National Council on Measurement in Education.","['purpose', 'study', 'describe', 'attribute', 'hierarchy', 'method', 'AHM', 'evaluate', 'differential', 'group', 'performance', 'cognitive', 'attribute', 'level', 'AHM', 'psychometric', 'method', 'classify', 'examine', 'test', 'item', 'response', 'set', 'attributemastery', 'pattern', 'associate', 'different', 'component', 'cognitive', 'task', 'performance', 'Attribute', 'probability', 'compute', 'neural', 'network', 'estimate', 'attribute', 'examinee', 'provide', 'specific', 'information', 'examinee', 'attributemastery', 'level', 'probability', 'compare', 'group', 'describe', 'fourstep', 'procedure', 'estimate', 'interpret', 'group', 'difference', 'AHM', 'provide', 'example', 'student', 'response', 'datum', 'sample', 'algebra', 'item', 'SAT', 'illustrate', 'pattern', 'recognition', 'approach', 'study', 'group', 'difference', '©', '2007', 'National', 'Council']","['attribute', 'hierarchy', 'method', 'identify', 'interpret', 'cognitive', 'skill', 'produce', 'group', 'difference']",purpose study describe attribute hierarchy method AHM evaluate differential group performance cognitive attribute level AHM psychometric method classify examine test item response set attributemastery pattern associate different component cognitive task performance Attribute probability compute neural network estimate attribute examinee provide specific information examinee attributemastery level probability compare group describe fourstep procedure estimate interpret group difference AHM provide example student response datum sample algebra item SAT illustrate pattern recognition approach study group difference © 2007 National Council,attribute hierarchy method identify interpret cognitive skill produce group difference,0.028004007840483524,0.028004495753171474,0.028002562137284862,0.8877848498130523,0.028204084456007954,0.0068499424167888885,0.2107199792735984,0.0,0.0,0.00283555099114897
Leighton J.P.; Gierl M.J.; Hunka S.M.,The attribute hierarchy method for cognitive assessment: A variation on Tatsuoka's rule-space approach,2004,41,"A cognitive item response theory model called the attribute hierarchy method (AHM) is introduced and illustrated. This method represents a variation of Tatsuoka's rule-space approach. The AHM is designed explicitly to link cognitive theory and psychometric practice to facilitate the development and analyses of educational and psychological tests. The following are described: cognitive properties of the AHM; psychometric properties of the AHM, as well as a demonstration of how the AHM differs from Tatsuoka's rule-space approach; and application of the AHM to the domain of syllogistic reasoning to illustrate how this approach can be used to evaluate the cognitive competencies required in a higher-level thinking task. Future directions for research are also outlined.",The attribute hierarchy method for cognitive assessment: A variation on Tatsuoka's rule-space approach,"A cognitive item response theory model called the attribute hierarchy method (AHM) is introduced and illustrated. This method represents a variation of Tatsuoka's rule-space approach. The AHM is designed explicitly to link cognitive theory and psychometric practice to facilitate the development and analyses of educational and psychological tests. The following are described: cognitive properties of the AHM; psychometric properties of the AHM, as well as a demonstration of how the AHM differs from Tatsuoka's rule-space approach; and application of the AHM to the domain of syllogistic reasoning to illustrate how this approach can be used to evaluate the cognitive competencies required in a higher-level thinking task. Future directions for research are also outlined.","['cognitive', 'item', 'response', 'theory', 'attribute', 'hierarchy', 'method', 'AHM', 'introduce', 'illustrate', 'method', 'represent', 'variation', 'Tatsuokas', 'rulespace', 'approach', 'AHM', 'design', 'explicitly', 'link', 'cognitive', 'theory', 'psychometric', 'practice', 'facilitate', 'development', 'analysis', 'educational', 'psychological', 'test', 'follow', 'describe', 'cognitive', 'property', 'AHM', 'psychometric', 'property', 'AHM', 'demonstration', 'AHM', 'differ', 'Tatsuokas', 'rulespace', 'approach', 'application', 'AHM', 'domain', 'syllogistic', 'reasoning', 'illustrate', 'approach', 'evaluate', 'cognitive', 'competency', 'require', 'higherlevel', 'thinking', 'task', 'future', 'direction', 'research', 'outline']","['attribute', 'hierarchy', 'method', 'cognitive', 'assessment', 'variation', 'Tatsuokas', 'rulespace', 'approach']",cognitive item response theory attribute hierarchy method AHM introduce illustrate method represent variation Tatsuokas rulespace approach AHM design explicitly link cognitive theory psychometric practice facilitate development analysis educational psychological test follow describe cognitive property AHM psychometric property AHM demonstration AHM differ Tatsuokas rulespace approach application AHM domain syllogistic reasoning illustrate approach evaluate cognitive competency require higherlevel thinking task future direction research outline,attribute hierarchy method cognitive assessment variation Tatsuokas rulespace approach,0.032158309086491144,0.032150246614451136,0.032143508662601655,0.8712526376665263,0.03229529796992974,0.0,0.1596909419250556,0.0,0.0,0.0
Walker C.M.; Beretvas S.N.,Comparing Multidimensional and Unidimensional Proficiency Classifications: Multidimensional IRT as a Diagnostic Aid,2003,40,"This research examined the effect of scoring items thought to be multidimensional using a unidimensional model and demonstrated the use of multidimensional item response theory (MIRT) as a diagnostic tool. Using real data from a large-scale mathematics test, previously shown to function differentially in favor of proficient writers, the difference in proficiency classifications was explored when a two-versus one-dimensional confirmatory model was fit. The estimate of ability obtained when using the unidimensional model was considered to represent general mathematical ability. Under the two-dimensional model, one of the two dimensions was also considered to represent general mathematical ability. The second dimension was considered to represent the ability to communicate in mathematics. The resulting pattern of mismatched proficiency classifications suggested that examinees found to have less mathematics communication ability were more likely to be placed in a lower general mathematics proficiency classification under the unidimensional than multidimensional model. Results and implications are discussed.",Comparing Multidimensional and Unidimensional Proficiency Classifications: Multidimensional IRT as a Diagnostic Aid,"This research examined the effect of scoring items thought to be multidimensional using a unidimensional model and demonstrated the use of multidimensional item response theory (MIRT) as a diagnostic tool. Using real data from a large-scale mathematics test, previously shown to function differentially in favor of proficient writers, the difference in proficiency classifications was explored when a two-versus one-dimensional confirmatory model was fit. The estimate of ability obtained when using the unidimensional model was considered to represent general mathematical ability. Under the two-dimensional model, one of the two dimensions was also considered to represent general mathematical ability. The second dimension was considered to represent the ability to communicate in mathematics. The resulting pattern of mismatched proficiency classifications suggested that examinees found to have less mathematics communication ability were more likely to be placed in a lower general mathematics proficiency classification under the unidimensional than multidimensional model. Results and implications are discussed.","['research', 'examine', 'effect', 'scoring', 'item', 'think', 'multidimensional', 'unidimensional', 'demonstrate', 'multidimensional', 'item', 'response', 'theory', 'MIRT', 'diagnostic', 'tool', 'real', 'datum', 'largescale', 'mathematic', 'test', 'previously', 'function', 'differentially', 'favor', 'proficient', 'writer', 'difference', 'proficiency', 'classification', 'explore', 'twoversus', 'onedimensional', 'confirmatory', 'fit', 'estimate', 'ability', 'obtain', 'unidimensional', 'consider', 'represent', 'general', 'mathematical', 'ability', 'twodimensional', 'dimension', 'consider', 'represent', 'general', 'mathematical', 'ability', 'second', 'dimension', 'consider', 'represent', 'ability', 'communicate', 'mathematic', 'result', 'pattern', 'mismatch', 'proficiency', 'classification', 'suggest', 'examinee', 'find', 'mathematics', 'communication', 'ability', 'likely', 'place', 'low', 'general', 'mathematic', 'proficiency', 'classification', 'unidimensional', 'multidimensional', 'result', 'implication', 'discuss']","['compare', 'Multidimensional', 'Unidimensional', 'Proficiency', 'Classifications', 'Multidimensional', 'IRT', 'diagnostic', 'aid']",research examine effect scoring item think multidimensional unidimensional demonstrate multidimensional item response theory MIRT diagnostic tool real datum largescale mathematic test previously function differentially favor proficient writer difference proficiency classification explore twoversus onedimensional confirmatory fit estimate ability obtain unidimensional consider represent general mathematical ability twodimensional dimension consider represent general mathematical ability second dimension consider represent ability communicate mathematic result pattern mismatch proficiency classification suggest examinee find mathematics communication ability likely place low general mathematic proficiency classification unidimensional multidimensional result implication discuss,compare Multidimensional Unidimensional Proficiency Classifications Multidimensional IRT diagnostic aid,0.027234054217147603,0.02723174826182821,0.027228880816891694,0.8909045252938519,0.027400791410280664,0.005371041295494793,0.050520022818781124,0.0,0.04435254952317061,0.012138472849569464
Lee G.,A comparison of methods of estimating conditional standard errors of measurement for testlet-based test scores using simulation techniques,2000,37,"The primary purpose of this study was to investigate the appropriateness and implication of incorporating a testlet definition into the estimation of procedures of the conditional standard error of measurement (SEM) for tests composed of testlets. Another purpose was to investigate the bias in estimates of the conditional SEM when using item-based methods instead of testlet-based methods. Several item-based and testlet-based estimation methods were proposed and compared. In general, item-based estimation methods underestimated the conditional SEM for tests composed for testlets, and the magnitude of this negative bias increased as the degree of conditional dependence among items within testlets increased. However, an item-based method using a generalizability theory model provided good estimates of the conditional SEM under mild violation of the assumptions for measurement modeling. Under moderate or somewhat severe violation, testlet-based methods with item response models provided good estimates.",A comparison of methods of estimating conditional standard errors of measurement for testlet-based test scores using simulation techniques,"The primary purpose of this study was to investigate the appropriateness and implication of incorporating a testlet definition into the estimation of procedures of the conditional standard error of measurement (SEM) for tests composed of testlets. Another purpose was to investigate the bias in estimates of the conditional SEM when using item-based methods instead of testlet-based methods. Several item-based and testlet-based estimation methods were proposed and compared. In general, item-based estimation methods underestimated the conditional SEM for tests composed for testlets, and the magnitude of this negative bias increased as the degree of conditional dependence among items within testlets increased. However, an item-based method using a generalizability theory model provided good estimates of the conditional SEM under mild violation of the assumptions for measurement modeling. Under moderate or somewhat severe violation, testlet-based methods with item response models provided good estimates.","['primary', 'purpose', 'study', 'investigate', 'appropriateness', 'implication', 'incorporate', 'testlet', 'definition', 'estimation', 'procedure', 'conditional', 'standard', 'error', 'SEM', 'test', 'compose', 'testlet', 'purpose', 'investigate', 'bias', 'estimate', 'conditional', 'SEM', 'itembase', 'method', 'instead', 'testletbase', 'method', 'itembased', 'testletbase', 'estimation', 'method', 'propose', 'compare', 'general', 'itembased', 'estimation', 'method', 'underestimate', 'conditional', 'SEM', 'test', 'compose', 'testlet', 'magnitude', 'negative', 'bias', 'increase', 'degree', 'conditional', 'dependence', 'item', 'testlet', 'increase', 'itembased', 'method', 'generalizability', 'theory', 'provide', 'good', 'estimate', 'conditional', 'SEM', 'mild', 'violation', 'assumption', 'modeling', 'moderate', 'somewhat', 'severe', 'violation', 'testletbase', 'method', 'item', 'response', 'provide', 'good', 'estimate']","['comparison', 'method', 'estimate', 'conditional', 'standard', 'error', 'testletbase', 'test', 'score', 'simulation', 'technique']",primary purpose study investigate appropriateness implication incorporate testlet definition estimation procedure conditional standard error SEM test compose testlet purpose investigate bias estimate conditional SEM itembase method instead testletbase method itembased testletbase estimation method propose compare general itembased estimation method underestimate conditional SEM test compose testlet magnitude negative bias increase degree conditional dependence item testlet increase itembased method generalizability theory provide good estimate conditional SEM mild violation assumption modeling moderate somewhat severe violation testletbase method item response provide good estimate,comparison method estimate conditional standard error testletbase test score simulation technique,0.032056187417866845,0.03205707600819229,0.03206123841462697,0.8715857863693902,0.03223971178992363,0.0,0.03772296928799592,0.015910252683752346,0.05174005339670813,0.0
Lee W.-C.; Brennan R.L.; Kolen M.J.,Estimators of conditional scale-score standard errors of measurement: A simulation study,2000,37,"This paper describes four procedures previously developed for estimating conditional standard errors of measurement for scale scores: the IRT procedure (Kolen, Zeng, & Hanson, 1996), the binomial procedure (Brennan & Lee, 1999), the compound binomial procedure (Brennan & Lee, 1999), and the Feldt-Qualls procedure (1998). These four procedures are based on different underlying assumptions. The IRT procedure is based on the unidimensional IRT model assumptions. The binomial and compound binomial procedures employ, as the distribution of errors, the binomial model and compound binomial model, respectively. By contrast, the Feldt-Qualls procedure does not depend on a particular psychometric model, and it simply translates any estimated conditional raw-score SEM to a conditional scale-score SEM. These procedures are compared in a simulation study, which involves two-dimensional data sets. The presence of two category dimensions reflects a violation of the IRT unidimensionality assumption. The relative accuracy of these procedures for estimating conditional scale-score standard errors of measurement is evaluated under various circumstances. The effects of three different types of transformations of raw scores are investigated including developmental standard scores, grade equivalents, and percentile ranks. All the procedures discussed appear viable. A general recommendation is made that test users select a procedure based on various factors such as the type of scale score of concern, characteristics of the test, assumptions involved in the estimation procedure, and feasibility and practicability of the estimation procedure.",Estimators of conditional scale-score standard errors of measurement: A simulation study,"This paper describes four procedures previously developed for estimating conditional standard errors of measurement for scale scores: the IRT procedure (Kolen, Zeng, & Hanson, 1996), the binomial procedure (Brennan & Lee, 1999), the compound binomial procedure (Brennan & Lee, 1999), and the Feldt-Qualls procedure (1998). These four procedures are based on different underlying assumptions. The IRT procedure is based on the unidimensional IRT model assumptions. The binomial and compound binomial procedures employ, as the distribution of errors, the binomial model and compound binomial model, respectively. By contrast, the Feldt-Qualls procedure does not depend on a particular psychometric model, and it simply translates any estimated conditional raw-score SEM to a conditional scale-score SEM. These procedures are compared in a simulation study, which involves two-dimensional data sets. The presence of two category dimensions reflects a violation of the IRT unidimensionality assumption. The relative accuracy of these procedures for estimating conditional scale-score standard errors of measurement is evaluated under various circumstances. The effects of three different types of transformations of raw scores are investigated including developmental standard scores, grade equivalents, and percentile ranks. All the procedures discussed appear viable. A general recommendation is made that test users select a procedure based on various factors such as the type of scale score of concern, characteristics of the test, assumptions involved in the estimation procedure, and feasibility and practicability of the estimation procedure.","['paper', 'describe', 'procedure', 'previously', 'develop', 'estimate', 'conditional', 'standard', 'error', 'scale', 'score', 'IRT', 'procedure', 'Kolen', 'Zeng', 'Hanson', '1996', 'binomial', 'procedure', 'Brennan', 'Lee', '1999', 'compound', 'binomial', 'procedure', 'Brennan', 'Lee', '1999', 'FeldtQualls', 'procedure', '1998', 'procedure', 'base', 'different', 'underlying', 'assumption', 'IRT', 'procedure', 'base', 'unidimensional', 'IRT', 'assumption', 'binomial', 'compound', 'binomial', 'procedure', 'employ', 'distribution', 'error', 'binomial', 'compound', 'binomial', 'respectively', 'contrast', 'FeldtQualls', 'procedure', 'depend', 'particular', 'psychometric', 'simply', 'translate', 'estimate', 'conditional', 'rawscore', 'SEM', 'conditional', 'scalescore', 'SEM', 'procedure', 'compare', 'simulation', 'study', 'involve', 'twodimensional', 'datum', 'set', 'presence', 'category', 'dimension', 'reflect', 'violation', 'IRT', 'unidimensionality', 'assumption', 'relative', 'accuracy', 'procedure', 'estimate', 'conditional', 'scalescore', 'standard', 'error', 'evaluate', 'circumstance', 'effect', 'different', 'type', 'transformation', 'raw', 'score', 'investigate', 'include', 'developmental', 'standard', 'score', 'grade', 'equivalent', 'percentile', 'rank', 'procedure', 'discuss', 'appear', 'viable', 'general', 'recommendation', 'test', 'user', 'select', 'procedure', 'base', 'factor', 'type', 'scale', 'score', 'concern', 'characteristic', 'test', 'assumption', 'involve', 'estimation', 'procedure', 'feasibility', 'practicability', 'estimation', 'procedure']","['estimator', 'conditional', 'scalescore', 'standard', 'error', 'simulation', 'study']",paper describe procedure previously develop estimate conditional standard error scale score IRT procedure Kolen Zeng Hanson 1996 binomial procedure Brennan Lee 1999 compound binomial procedure Brennan Lee 1999 FeldtQualls procedure 1998 procedure base different underlying assumption IRT procedure base unidimensional IRT assumption binomial compound binomial procedure employ distribution error binomial compound binomial respectively contrast FeldtQualls procedure depend particular psychometric simply translate estimate conditional rawscore SEM conditional scalescore SEM procedure compare simulation study involve twodimensional datum set presence category dimension reflect violation IRT unidimensionality assumption relative accuracy procedure estimate conditional scalescore standard error evaluate circumstance effect different type transformation raw score investigate include developmental standard score grade equivalent percentile rank procedure discuss appear viable general recommendation test user select procedure base factor type scale score concern characteristic test assumption involve estimation procedure feasibility practicability estimation procedure,estimator conditional scalescore standard error simulation study,0.025513582315194055,0.025512735932882048,0.025513410518821483,0.8978545860859733,0.025605685147129093,0.0,0.1649748455518818,0.0,0.0,0.0
Wollack J.A.,Comparison of Answer Copying Indices with Real Data,2003,40,"This study investigated the Type I error rate and power of four copying indices, K-index (Holland, 1996), Scrutiny! (Assessment Systems Corporation, 1993), g2 (Frary, Tideman, & Watts, 1977), and ω (Wollack, 1997) using real test data from 20,000 examinees over a 2-year period. The data were divided into three different test lengths (20, 40, and 80 items) and nine different sample sizes (ranging from 50 to 20,000). Four different amounts of answer copying were simulated (10%, 20%, 30%, and 40% of the items) within each condition. The ω index demonstrated the best Type I error control and power in all conditions and at all a. levels. Scrutiny! and the K-index were uniformly conservative, and both had poor power to detect true copiers at the small α levels typically used in answer copying detection, whereas g 2 was generally too liberal, particularly at small a levels. Some comments on the proper uses of copying indices are provided.",Comparison of Answer Copying Indices with Real Data,"This study investigated the Type I error rate and power of four copying indices, K-index (Holland, 1996), Scrutiny! (Assessment Systems Corporation, 1993), g2 (Frary, Tideman, & Watts, 1977), and ω (Wollack, 1997) using real test data from 20,000 examinees over a 2-year period. The data were divided into three different test lengths (20, 40, and 80 items) and nine different sample sizes (ranging from 50 to 20,000). Four different amounts of answer copying were simulated (10%, 20%, 30%, and 40% of the items) within each condition. The ω index demonstrated the best Type I error control and power in all conditions and at all a. levels. Scrutiny! and the K-index were uniformly conservative, and both had poor power to detect true copiers at the small α levels typically used in answer copying detection, whereas g 2 was generally too liberal, particularly at small a levels. Some comments on the proper uses of copying indices are provided.","['study', 'investigate', 'type', 'I', 'error', 'rate', 'power', 'copying', 'indice', 'Kindex', 'Holland', '1996', 'Scrutiny', 'Assessment', 'Systems', 'Corporation', '1993', 'g2', 'Frary', 'Tideman', 'Watts', '1977', 'ω', 'Wollack', '1997', 'real', 'test', 'datum', '20000', 'examine', '2year', 'period', 'datum', 'divide', 'different', 'test', 'length', '20', '40', '80', 'item', 'different', 'sample', 'size', 'range', '50', '20000', 'different', 'answer', 'copying', 'simulate', '10', '20', '30', '40', 'item', 'condition', 'ω', 'index', 'demonstrate', 'good', 'type', 'I', 'error', 'control', 'power', 'condition', 'level', 'Scrutiny', 'Kindex', 'uniformly', 'conservative', 'poor', 'power', 'detect', 'true', 'copier', 'small', 'α', 'level', 'typically', 'answer', 'copying', 'detection', 'g', '2', 'generally', 'liberal', 'particularly', 'small', 'level', 'comment', 'proper', 'copy', 'index', 'provide']","['Comparison', 'Answer', 'Copying', 'Indices', 'Real', 'Data']",study investigate type I error rate power copying indice Kindex Holland 1996 Scrutiny Assessment Systems Corporation 1993 g2 Frary Tideman Watts 1977 ω Wollack 1997 real test datum 20000 examine 2year period datum divide different test length 20 40 80 item different sample size range 50 20000 different answer copying simulate 10 20 30 40 item condition ω index demonstrate good type I error control power condition level Scrutiny Kindex uniformly conservative poor power detect true copier small α level typically answer copying detection g 2 generally liberal particularly small level comment proper copy index provide,Comparison Answer Copying Indices Real Data,0.022983135052992546,0.022984061769349234,0.023043019689351643,0.9078486364413334,0.023141147046973183,0.06324287788826038,0.005366034707545331,0.0013132517431114269,0.01702993149797426,0.0
Brennan R.L.; Yin P.; Kane M.T.,Methodology for Examining the Reliability of Group Mean Difference Scores,2003,40,"This article treats various procedures for examining the reliability of group mean difference scores, with particular emphasis on procedures from univariate and multivariate generalizability theory. Attention is given to both traditional norm-referenced perspectives on reliability as well as criterion-referenced perspectives that focus on error-tolerance ratios and functions of them. The procedures discussed are illustrated using three cohorts of data for third- and fourth-grade students in Iowa who took the Iowa Tests of Basic Skills in recent years. For these data, estimates of reliability for norm-referenced decisions tend to be relatively low. By contrast, for criterion-referenced decisions, estimates of reliability-like coefficients based on error-tolerance ratios tend to be noticeably larger.",Methodology for Examining the Reliability of Group Mean Difference Scores,"This article treats various procedures for examining the reliability of group mean difference scores, with particular emphasis on procedures from univariate and multivariate generalizability theory. Attention is given to both traditional norm-referenced perspectives on reliability as well as criterion-referenced perspectives that focus on error-tolerance ratios and functions of them. The procedures discussed are illustrated using three cohorts of data for third- and fourth-grade students in Iowa who took the Iowa Tests of Basic Skills in recent years. For these data, estimates of reliability for norm-referenced decisions tend to be relatively low. By contrast, for criterion-referenced decisions, estimates of reliability-like coefficients based on error-tolerance ratios tend to be noticeably larger.","['article', 'treat', 'procedure', 'examine', 'reliability', 'group', 'mean', 'difference', 'score', 'particular', 'emphasis', 'procedure', 'univariate', 'multivariate', 'generalizability', 'theory', 'Attention', 'traditional', 'normreferenced', 'perspective', 'reliability', 'criterionreference', 'perspective', 'focus', 'errortolerance', 'ratio', 'function', 'procedure', 'discuss', 'illustrate', 'cohort', 'datum', 'fourthgrade', 'student', 'Iowa', 'Iowa', 'Tests', 'Basic', 'Skills', 'recent', 'year', 'datum', 'estimate', 'reliability', 'normreferenced', 'decision', 'tend', 'relatively', 'low', 'contrast', 'criterionreference', 'decision', 'estimate', 'reliabilitylike', 'coefficient', 'base', 'errortolerance', 'ratio', 'tend', 'noticeably', 'large']","['methodology', 'examine', 'Reliability', 'Group', 'Mean', 'Difference', 'Scores']",article treat procedure examine reliability group mean difference score particular emphasis procedure univariate multivariate generalizability theory Attention traditional normreferenced perspective reliability criterionreference perspective focus errortolerance ratio function procedure discuss illustrate cohort datum fourthgrade student Iowa Iowa Tests Basic Skills recent year datum estimate reliability normreferenced decision tend relatively low contrast criterionreference decision estimate reliabilitylike coefficient base errortolerance ratio tend noticeably large,methodology examine Reliability Group Mean Difference Scores,0.028541536829158862,0.028536957583008265,0.028531653447953963,0.8856298457174238,0.028760006422455086,0.0,0.13233819297964827,0.0,0.0,0.007098821107593178
Katz I.R.; Bennett R.E.; Berger A.E.,Effects of response format on difficulty of SAT-mathematics items: It's not the strategy,2000,37,"Problem-solving strategy is frequently cited as mediating the effects of response format (multiple-choice, constructed response) on item difficulty, yet there are few direct investigations of examinee solution procedures. Fifty-five high school students solved parallel constructed response and multiple-choice items that differed only in the presence of response options. Student performance was videotaped to assess solution strategies. Strategies were categorized as ""traditional"" - those associated with constructed response problem solving (e.g., writing and solving algebraic equations) - or ""nontraditional"" - those associated with multiple-choice problem solving (e.g., estimating a potential solution). Surprisingly, participants sometimes adopted nontraditional strategies to solve constructed response items. Furthermore, differences in difficulty between response formats did not correspond to differences in strategy choice: some items showed a format effect on strategy but no effect on difficulty; other items showed the reverse. We interpret these results in light of the relative comprehension challenges posed by the two groups of items.",Effects of response format on difficulty of SAT-mathematics items: It's not the strategy,"Problem-solving strategy is frequently cited as mediating the effects of response format (multiple-choice, constructed response) on item difficulty, yet there are few direct investigations of examinee solution procedures. Fifty-five high school students solved parallel constructed response and multiple-choice items that differed only in the presence of response options. Student performance was videotaped to assess solution strategies. Strategies were categorized as ""traditional"" - those associated with constructed response problem solving (e.g., writing and solving algebraic equations) - or ""nontraditional"" - those associated with multiple-choice problem solving (e.g., estimating a potential solution). Surprisingly, participants sometimes adopted nontraditional strategies to solve constructed response items. Furthermore, differences in difficulty between response formats did not correspond to differences in strategy choice: some items showed a format effect on strategy but no effect on difficulty; other items showed the reverse. We interpret these results in light of the relative comprehension challenges posed by the two groups of items.","['problemsolving', 'strategy', 'frequently', 'cite', 'mediate', 'effect', 'response', 'format', 'multiplechoice', 'construct', 'response', 'item', 'difficulty', 'direct', 'investigation', 'examinee', 'solution', 'procedure', 'fiftyfive', 'high', 'school', 'student', 'solve', 'parallel', 'construct', 'response', 'multiplechoice', 'item', 'differ', 'presence', 'response', 'option', 'student', 'performance', 'videotape', 'assess', 'solution', 'strategy', 'strategy', 'categorize', 'traditional', 'associate', 'construct', 'response', 'problem', 'solve', 'eg', 'writing', 'solve', 'algebraic', 'equation', 'nontraditional', 'associate', 'multiplechoice', 'problem', 'solve', 'eg', 'estimate', 'potential', 'solution', 'surprisingly', 'participant', 'adopt', 'nontraditional', 'strategy', 'solve', 'construct', 'response', 'item', 'furthermore', 'difference', 'difficulty', 'response', 'format', 'correspond', 'difference', 'strategy', 'choice', 'item', 'format', 'effect', 'strategy', 'effect', 'difficulty', 'item', 'reverse', 'interpret', 'result', 'light', 'relative', 'comprehension', 'challenge', 'pose', 'group', 'item']","['effect', 'response', 'format', 'difficulty', 'satmathematic', 'item', 'strategy']",problemsolving strategy frequently cite mediate effect response format multiplechoice construct response item difficulty direct investigation examinee solution procedure fiftyfive high school student solve parallel construct response multiplechoice item differ presence response option student performance videotape assess solution strategy strategy categorize traditional associate construct response problem solve eg writing solve algebraic equation nontraditional associate multiplechoice problem solve eg estimate potential solution surprisingly participant adopt nontraditional strategy solve construct response item furthermore difference difficulty response format correspond difference strategy choice item format effect strategy effect difficulty item reverse interpret result light relative comprehension challenge pose group item,effect response format difficulty satmathematic item strategy,0.02811984120780908,0.028119123119967393,0.028574224391369706,0.4936401017042259,0.42154670957662793,0.006612796711779143,0.0,0.0,0.06453129810026609,0.03766541447421425
Rodriguez M.C.,Construct equivalence of multiple-choice and constructed-response items: A random effects synthesis of correlations,2003,40,"A thorough search of the literature was conducted to locate empirical studies investigating the trait or construct equivalence of multiple-choice (MC) and conslructed-response (CR) items. Of the 67 studies identified, 29 studies included 56 correlations between items in both formats. These 56 correlations were corrected for attenuation and synthesized to establish evidence for a common estimate of correlation (true-score correlations). The 56 disattenuated correlations were highly heterogeneous. A search for moderators to explain this variation uncovered the role of the design characteristics of test items used in the studies. When items are constructed in both formats using the same stem (stem equivalent), the mean correlation between the two formats approaches unity and is significantly higher than when using non-stem-equivalent items (particularly when using essay-type items). Construct equivalence, in part, appears to be a function of the item design method or the item writer's intent.",Construct equivalence of multiple-choice and constructed-response items: A random effects synthesis of correlations,"A thorough search of the literature was conducted to locate empirical studies investigating the trait or construct equivalence of multiple-choice (MC) and conslructed-response (CR) items. Of the 67 studies identified, 29 studies included 56 correlations between items in both formats. These 56 correlations were corrected for attenuation and synthesized to establish evidence for a common estimate of correlation (true-score correlations). The 56 disattenuated correlations were highly heterogeneous. A search for moderators to explain this variation uncovered the role of the design characteristics of test items used in the studies. When items are constructed in both formats using the same stem (stem equivalent), the mean correlation between the two formats approaches unity and is significantly higher than when using non-stem-equivalent items (particularly when using essay-type items). Construct equivalence, in part, appears to be a function of the item design method or the item writer's intent.","['thorough', 'search', 'literature', 'conduct', 'locate', 'empirical', 'study', 'investigate', 'trait', 'construct', 'equivalence', 'multiplechoice', 'MC', 'conslructedresponse', 'CR', 'item', '67', 'study', 'identify', '29', 'study', 'include', '56', 'correlation', 'item', 'format', '56', 'correlation', 'correct', 'attenuation', 'synthesize', 'establish', 'evidence', 'common', 'estimate', 'correlation', 'truescore', 'correlation', '56', 'disattenuate', 'correlation', 'highly', 'heterogeneous', 'search', 'moderator', 'explain', 'variation', 'uncover', 'role', 'design', 'characteristic', 'test', 'item', 'study', 'item', 'construct', 'format', 'stem', 'stem', 'equivalent', 'mean', 'correlation', 'format', 'approach', 'unity', 'significantly', 'high', 'nonstemequivalent', 'item', 'particularly', 'essaytype', 'item', 'construct', 'equivalence', 'appear', 'function', 'item', 'design', 'method', 'item', 'writer', 'intent']","['construct', 'equivalence', 'multiplechoice', 'constructedresponse', 'item', 'random', 'effect', 'synthesis', 'correlation']",thorough search literature conduct locate empirical study investigate trait construct equivalence multiplechoice MC conslructedresponse CR item 67 study identify 29 study include 56 correlation item format 56 correlation correct attenuation synthesize establish evidence common estimate correlation truescore correlation 56 disattenuate correlation highly heterogeneous search moderator explain variation uncover role design characteristic test item study item construct format stem stem equivalent mean correlation format approach unity significantly high nonstemequivalent item particularly essaytype item construct equivalence appear function item design method item writer intent,construct equivalence multiplechoice constructedresponse item random effect synthesis correlation,0.02739512313809779,0.02739619817376876,0.420404810050142,0.49706330103865837,0.027740567599332966,0.00394709064926718,0.0,0.0,0.06976578340527996,0.018933294730698994
Chen S.-Y.; Ankenmann R.D.; Spray J.A.,The relationship between item exposure and test overlap in computerized adaptive testing,2003,40,"The purpose of this article is to present an analytical derivation for the mathematical form of an average between-test overlap index as a function of the item exposure index, for fixed-length computerized adaptive tests (CATs). This algebraic relationship is used to investigate the simultaneous control of item exposure at both the item and test levels. The results indicate that, in fixed-length CATs, control of the average between-test overlap is achieved via the mean and variance of the item exposure rates of the items that constitute the CAT item pool. The mean of the item exposure rates is easily manipulated. Control over the variance of the item exposure rates can be achieved via the maximum item exposure rate (rmax. Therefore, item exposure control methods which implement a specification of rmax (e.g., Sympson & Hetter, 1985) provide the most direct control at both the item and test levels.",The relationship between item exposure and test overlap in computerized adaptive testing,"The purpose of this article is to present an analytical derivation for the mathematical form of an average between-test overlap index as a function of the item exposure index, for fixed-length computerized adaptive tests (CATs). This algebraic relationship is used to investigate the simultaneous control of item exposure at both the item and test levels. The results indicate that, in fixed-length CATs, control of the average between-test overlap is achieved via the mean and variance of the item exposure rates of the items that constitute the CAT item pool. The mean of the item exposure rates is easily manipulated. Control over the variance of the item exposure rates can be achieved via the maximum item exposure rate (rmax. Therefore, item exposure control methods which implement a specification of rmax (e.g., Sympson & Hetter, 1985) provide the most direct control at both the item and test levels.","['purpose', 'article', 'present', 'analytical', 'derivation', 'mathematical', 'form', 'average', 'betweentest', 'overlap', 'index', 'function', 'item', 'exposure', 'index', 'fixedlength', 'computerize', 'adaptive', 'test', 'CATs', 'algebraic', 'relationship', 'investigate', 'simultaneous', 'control', 'item', 'exposure', 'item', 'test', 'level', 'result', 'indicate', 'fixedlength', 'cats', 'control', 'average', 'betweentest', 'overlap', 'achieve', 'mean', 'variance', 'item', 'exposure', 'rate', 'item', 'constitute', 'CAT', 'item', 'pool', 'mean', 'item', 'exposure', 'rate', 'easily', 'manipulate', 'Control', 'variance', 'item', 'exposure', 'rate', 'achieve', 'maximum', 'item', 'exposure', 'rate', 'rmax', 'Therefore', 'item', 'exposure', 'control', 'method', 'implement', 'specification', 'rmax', 'eg', 'Sympson', 'Hetter', '1985', 'provide', 'direct', 'control', 'item', 'test', 'level']","['relationship', 'item', 'exposure', 'test', 'overlap', 'computerized', 'adaptive', 'testing']",purpose article present analytical derivation mathematical form average betweentest overlap index function item exposure index fixedlength computerize adaptive test CATs algebraic relationship investigate simultaneous control item exposure item test level result indicate fixedlength cats control average betweentest overlap achieve mean variance item exposure rate item constitute CAT item pool mean item exposure rate easily manipulate Control variance item exposure rate achieve maximum item exposure rate rmax Therefore item exposure control method implement specification rmax eg Sympson Hetter 1985 provide direct control item test level,relationship item exposure test overlap computerized adaptive testing,0.03168848206401209,0.031689655106754845,0.03196492961337284,0.8727300669461199,0.03192686626974034,0.0015166562160604446,0.0,0.0,0.12534922629598952,0.0
Vispoel W.P.; Clough S.J.; Bleiler T.; Hendrickson A.B.; Ihrig D.,Can examinees use judgments of item difficulty to improve proficiency estimates on computerized adaptive vocabulary tests?,2002,39,"Recent simulation studies indicate that there are occasions when examinees can use judgments of relative item difficulty to obtain positively biased proficiency estimates on computerized adaptive tests (CATs) that permit item review and answer change, Our purpose in the study reported here was to evaluate examinees' success in using these strategies while taking CATs in a live testing setting. We taught examinees two item difficulty judgment strategies designed to increase proficiency estimates. Examinees who were taught each strategy and examinees who were taught neither strategy were assigned at random to complete vocabulary CATs under conditions in which review was allowed after completing all items and when review was allowed only within successive blocks of items. We found that proficiency estimate changes following review were significantly higher in the regular review conditions than in the strategy conditions. Failure to obtain systematically higher scores in the strategy conditions was due in large part to errors examinees made in judging the relative difficulty of CAT items.",Can examinees use judgments of item difficulty to improve proficiency estimates on computerized adaptive vocabulary tests?,"Recent simulation studies indicate that there are occasions when examinees can use judgments of relative item difficulty to obtain positively biased proficiency estimates on computerized adaptive tests (CATs) that permit item review and answer change, Our purpose in the study reported here was to evaluate examinees' success in using these strategies while taking CATs in a live testing setting. We taught examinees two item difficulty judgment strategies designed to increase proficiency estimates. Examinees who were taught each strategy and examinees who were taught neither strategy were assigned at random to complete vocabulary CATs under conditions in which review was allowed after completing all items and when review was allowed only within successive blocks of items. We found that proficiency estimate changes following review were significantly higher in the regular review conditions than in the strategy conditions. Failure to obtain systematically higher scores in the strategy conditions was due in large part to errors examinees made in judging the relative difficulty of CAT items.","['recent', 'simulation', 'study', 'indicate', 'occasion', 'examinee', 'judgment', 'relative', 'item', 'difficulty', 'obtain', 'positively', 'bias', 'proficiency', 'estimate', 'computerized', 'adaptive', 'test', 'cat', 'permit', 'item', 'review', 'answer', 'change', 'purpose', 'study', 'report', 'evaluate', 'examinee', 'success', 'strategy', 'CATs', 'live', 'testing', 'set', 'teach', 'examine', 'item', 'difficulty', 'judgment', 'strategy', 'design', 'increase', 'proficiency', 'estimate', 'examine', 'teach', 'strategy', 'examine', 'teach', 'strategy', 'assign', 'random', 'complete', 'vocabulary', 'cat', 'condition', 'review', 'allow', 'complete', 'item', 'review', 'allow', 'successive', 'block', 'item', 'find', 'proficiency', 'estimate', 'change', 'follow', 'review', 'significantly', 'high', 'regular', 'review', 'condition', 'strategy', 'condition', 'failure', 'obtain', 'systematically', 'high', 'score', 'strategy', 'condition', 'large', 'error', 'examinee', 'judge', 'relative', 'difficulty', 'CAT', 'item']","['examinees', 'judgment', 'item', 'difficulty', 'improve', 'proficiency', 'estimate', 'computerized', 'adaptive', 'vocabulary', 'test']",recent simulation study indicate occasion examinee judgment relative item difficulty obtain positively bias proficiency estimate computerized adaptive test cat permit item review answer change purpose study report evaluate examinee success strategy CATs live testing set teach examine item difficulty judgment strategy design increase proficiency estimate examine teach strategy examine teach strategy assign random complete vocabulary cat condition review allow complete item review allow successive block item find proficiency estimate change follow review significantly high regular review condition strategy condition failure obtain systematically high score strategy condition large error examinee judge relative difficulty CAT item,examinees judgment item difficulty improve proficiency estimate computerized adaptive vocabulary test,0.02868583820158787,0.028679273474590747,0.028876687758068828,0.8849061301948878,0.028852070370864687,0.0,0.0,0.0,0.09468438888524223,0.02277860486595343
DeMars C.E.,Detecting multidimensionality due to curricular differences,2003,40,"Data were generated to simulate multidimensionality resulting from including two or four subtopics on a test. Each item was dependent on an ability trait due to instruction and learning, which was the same across all items, as well as an ability trait unique to the subtopic of the test (such as biology on a general science test). The eigenvalues of the item correlation matrix and Yen's Q3 were not greatly influenced by multidimensionality under conditions where the responses of a large proportion of students shared the influence of common instruction across subtopics. In contrast, Stout's T procedure was effective at detecting this type of multidimensionality, unless the subtopic abilities were correlated.",Detecting multidimensionality due to curricular differences,"Data were generated to simulate multidimensionality resulting from including two or four subtopics on a test. Each item was dependent on an ability trait due to instruction and learning, which was the same across all items, as well as an ability trait unique to the subtopic of the test (such as biology on a general science test). The eigenvalues of the item correlation matrix and Yen's Q3 were not greatly influenced by multidimensionality under conditions where the responses of a large proportion of students shared the influence of common instruction across subtopics. In contrast, Stout's T procedure was effective at detecting this type of multidimensionality, unless the subtopic abilities were correlated.","['datum', 'generate', 'simulate', 'multidimensionality', 'result', 'include', 'subtopic', 'test', 'item', 'dependent', 'ability', 'trait', 'instruction', 'learning', 'item', 'ability', 'trait', 'unique', 'subtopic', 'test', 'biology', 'general', 'science', 'test', 'eigenvalue', 'item', 'correlation', 'matrix', 'Yens', 'Q3', 'greatly', 'influence', 'multidimensionality', 'condition', 'response', 'large', 'proportion', 'student', 'share', 'influence', 'common', 'instruction', 'subtopic', 'contrast', 'Stouts', 'T', 'procedure', 'effective', 'detect', 'type', 'multidimensionality', 'subtopic', 'ability', 'correlate']","['detect', 'multidimensionality', 'curricular', 'difference']",datum generate simulate multidimensionality result include subtopic test item dependent ability trait instruction learning item ability trait unique subtopic test biology general science test eigenvalue item correlation matrix Yens Q3 greatly influence multidimensionality condition response large proportion student share influence common instruction subtopic contrast Stouts T procedure effective detect type multidimensionality subtopic ability correlate,detect multidimensionality curricular difference,0.03213505955409595,0.03213668481850492,0.39577243567401516,0.5073512284876894,0.03260459146569451,0.008671264134170319,0.010128249523327968,0.0,0.05034228974212033,0.004326608913213943
Chang S.-W.; Ansley T.N.,A comparative study of item exposure control methods in computerized adaptive testing,2003,40,"This study compared the properties of five methods of item exposure control within the purview of estimating examinees' abilities in a computerized adaptive testing (CAT) context. Each exposure control algorithm was incorporated into the item selection procedure and the adaptive testing progressed based on the CAT design established for this study. The merits and shortcomings of these strategies were considered under different item pool sizes and different desired maximum exposure rates and were evaluated in light of the observed maximum exposure rates, the test overlap rates, and the conditional standard errors of measurement. Each method had its advantages and disadvantages, but no one possessed all of the desired characteristics. There was a clear and logical trade-off between item exposure control and measurement precision. The Stocking and Lewis conditional multinomial procedure and, to a slightly lesser extent, the Davey and Parshall method seemed to be the most promising considering all of the factors that this study addressed.",A comparative study of item exposure control methods in computerized adaptive testing,"This study compared the properties of five methods of item exposure control within the purview of estimating examinees' abilities in a computerized adaptive testing (CAT) context. Each exposure control algorithm was incorporated into the item selection procedure and the adaptive testing progressed based on the CAT design established for this study. The merits and shortcomings of these strategies were considered under different item pool sizes and different desired maximum exposure rates and were evaluated in light of the observed maximum exposure rates, the test overlap rates, and the conditional standard errors of measurement. Each method had its advantages and disadvantages, but no one possessed all of the desired characteristics. There was a clear and logical trade-off between item exposure control and measurement precision. The Stocking and Lewis conditional multinomial procedure and, to a slightly lesser extent, the Davey and Parshall method seemed to be the most promising considering all of the factors that this study addressed.","['study', 'compare', 'property', 'method', 'item', 'exposure', 'control', 'purview', 'estimating', 'examine', 'ability', 'computerized', 'adaptive', 'testing', 'CAT', 'context', 'exposure', 'control', 'algorithm', 'incorporate', 'item', 'selection', 'procedure', 'adaptive', 'testing', 'progress', 'base', 'CAT', 'design', 'establish', 'study', 'merit', 'shortcoming', 'strategy', 'consider', 'different', 'item', 'pool', 'size', 'different', 'desire', 'maximum', 'exposure', 'rate', 'evaluate', 'light', 'observe', 'maximum', 'exposure', 'rate', 'test', 'overlap', 'rate', 'conditional', 'standard', 'error', 'method', 'advantage', 'disadvantage', 'possess', 'desire', 'characteristic', 'clear', 'logical', 'tradeoff', 'item', 'exposure', 'control', 'precision', 'Stocking', 'Lewis', 'conditional', 'multinomial', 'procedure', 'slightly', 'extent', 'Davey', 'Parshall', 'method', 'promising', 'consider', 'factor', 'study', 'address']","['comparative', 'study', 'item', 'exposure', 'control', 'method', 'computerized', 'adaptive', 'testing']",study compare property method item exposure control purview estimating examine ability computerized adaptive testing CAT context exposure control algorithm incorporate item selection procedure adaptive testing progress base CAT design establish study merit shortcoming strategy consider different item pool size different desire maximum exposure rate evaluate light observe maximum exposure rate test overlap rate conditional standard error method advantage disadvantage possess desire characteristic clear logical tradeoff item exposure control precision Stocking Lewis conditional multinomial procedure slightly extent Davey Parshall method promising consider factor study address,comparative study item exposure control method computerized adaptive testing,0.025859189990596866,0.025860027943732492,0.02591763156292935,0.896467019650607,0.025896130852134252,0.0,0.0007136164723107678,0.0,0.12466269531923967,0.0
Bolt D.M.; Cohen A.S.; Wollack J.A.,Item parameter estimation under conditions of test speededness: Application of a mixture Rasch model with ordinal constraints,2002,39,"When tests are administered under fixed time constraints, test performances can be affected by speededness. Among other consequences, speededness can result in inaccurate parameter estimates in item response theory (IRT) models, especially for items located near the end of tests (Oshima, 1994). This article presents an IRT strategy for reducing contamination in item difficulty estimates due to speededness. Ordinal constraints are applied to a mixture Rasch model (Rost, 1990) so as to distinguish two latent classes of examinees: (a) a ""speeded"" class, comprised of examinees that had insufficient time to adequately answer end-of-test items, and (b) a ""nonspeeded"" class, comprised of examinees that had sufficient time to answer all items. The parameter estimates obtained for end-of-test items in the nonspeeded class are shown to more accurately approximate their difficulties when the items are administered at earlier locations on a different form of the test. A mixture model can also be used to estimate the class memberships of individual examinees. In this way, it can be determined whether membership in the speeded class is associated with other student characteristics. Results are reported for gender and ethnicity.",Item parameter estimation under conditions of test speededness: Application of a mixture Rasch model with ordinal constraints,"When tests are administered under fixed time constraints, test performances can be affected by speededness. Among other consequences, speededness can result in inaccurate parameter estimates in item response theory (IRT) models, especially for items located near the end of tests (Oshima, 1994). This article presents an IRT strategy for reducing contamination in item difficulty estimates due to speededness. Ordinal constraints are applied to a mixture Rasch model (Rost, 1990) so as to distinguish two latent classes of examinees: (a) a ""speeded"" class, comprised of examinees that had insufficient time to adequately answer end-of-test items, and (b) a ""nonspeeded"" class, comprised of examinees that had sufficient time to answer all items. The parameter estimates obtained for end-of-test items in the nonspeeded class are shown to more accurately approximate their difficulties when the items are administered at earlier locations on a different form of the test. A mixture model can also be used to estimate the class memberships of individual examinees. In this way, it can be determined whether membership in the speeded class is associated with other student characteristics. Results are reported for gender and ethnicity.","['test', 'administer', 'fix', 'time', 'constraint', 'test', 'performance', 'affect', 'speededness', 'consequence', 'speededness', 'result', 'inaccurate', 'parameter', 'estimate', 'item', 'response', 'theory', 'IRT', 'especially', 'item', 'locate', 'near', 'end', 'test', 'Oshima', '1994', 'article', 'present', 'IRT', 'strategy', 'reduce', 'contamination', 'item', 'difficulty', 'estimate', 'speededness', 'ordinal', 'constraint', 'apply', 'mixture', 'Rasch', 'Rost', '1990', 'distinguish', 'latent', 'class', 'examine', 'speeded', 'class', 'comprise', 'examinee', 'insufficient', 'time', 'adequately', 'answer', 'endoftest', 'item', 'b', 'nonspeede', 'class', 'comprise', 'examinee', 'sufficient', 'time', 'answer', 'item', 'parameter', 'estimate', 'obtain', 'endoftest', 'item', 'nonspeede', 'class', 'accurately', 'approximate', 'difficulty', 'item', 'administer', 'early', 'location', 'different', 'form', 'test', 'mixture', 'estimate', 'class', 'membership', 'individual', 'examine', 'way', 'determine', 'membership', 'speeded', 'class', 'associate', 'student', 'characteristic', 'result', 'report', 'gender', 'ethnicity']","['item', 'parameter', 'estimation', 'condition', 'test', 'speededness', 'application', 'mixture', 'Rasch', 'ordinal', 'constraint']",test administer fix time constraint test performance affect speededness consequence speededness result inaccurate parameter estimate item response theory IRT especially item locate near end test Oshima 1994 article present IRT strategy reduce contamination item difficulty estimate speededness ordinal constraint apply mixture Rasch Rost 1990 distinguish latent class examine speeded class comprise examinee insufficient time adequately answer endoftest item b nonspeede class comprise examinee sufficient time answer item parameter estimate obtain endoftest item nonspeede class accurately approximate difficulty item administer early location different form test mixture estimate class membership individual examine way determine membership speeded class associate student characteristic result report gender ethnicity,item parameter estimation condition test speededness application mixture Rasch ordinal constraint,0.025251279627936454,0.02525041618729269,0.02525129005733314,0.8988605899409666,0.02538642418647094,0.0,0.0,0.0,0.09383469259780922,0.018551420310429646
Meijer R.R.,Outlier detection in high-stakes certification testing,2002,39,"Recent developments of person-fit analysis in computerized adaptive testing (CAT) are discussed. Methods from statistical process control are presented that have been proposed to classify an item score pattern as fitting or misfitting the underlying item response theory model in CAT. Most person-fit research in CAT is restricted to simulated data. In this study, empirical data from a certification test were used. Alternatives are discussed to generate norms so that bounds can be determined to classify an item score pattern as fitting or misfitting. Using bounds determined from a sample of a high-stakes certification test, the empirical analysis showed that different types of misfit can be distinguished. Further applications using statistical process control methods to detect misfitting item score patterns are discussed.",Outlier detection in high-stakes certification testing,"Recent developments of person-fit analysis in computerized adaptive testing (CAT) are discussed. Methods from statistical process control are presented that have been proposed to classify an item score pattern as fitting or misfitting the underlying item response theory model in CAT. Most person-fit research in CAT is restricted to simulated data. In this study, empirical data from a certification test were used. Alternatives are discussed to generate norms so that bounds can be determined to classify an item score pattern as fitting or misfitting. Using bounds determined from a sample of a high-stakes certification test, the empirical analysis showed that different types of misfit can be distinguished. Further applications using statistical process control methods to detect misfitting item score patterns are discussed.","['recent', 'development', 'personfit', 'analysis', 'computerized', 'adaptive', 'testing', 'CAT', 'discuss', 'method', 'statistical', 'process', 'control', 'present', 'propose', 'classify', 'item', 'score', 'pattern', 'fitting', 'misfit', 'underlie', 'item', 'response', 'theory', 'CAT', 'Most', 'personfit', 'research', 'CAT', 'restrict', 'simulated', 'datum', 'study', 'empirical', 'datum', 'certification', 'test', 'alternative', 'discuss', 'generate', 'norm', 'bound', 'determined', 'classify', 'item', 'score', 'pattern', 'fitting', 'misfitting', 'bound', 'determine', 'sample', 'highstake', 'certification', 'test', 'empirical', 'analysis', 'different', 'type', 'misfit', 'distinguish', 'application', 'statistical', 'process', 'control', 'method', 'detect', 'misfitting', 'item', 'score', 'pattern', 'discuss']","['outlier', 'detection', 'highstake', 'certification', 'testing']",recent development personfit analysis computerized adaptive testing CAT discuss method statistical process control present propose classify item score pattern fitting misfit underlie item response theory CAT Most personfit research CAT restrict simulated datum study empirical datum certification test alternative discuss generate norm bound determined classify item score pattern fitting misfitting bound determine sample highstake certification test empirical analysis different type misfit distinguish application statistical process control method detect misfitting item score pattern discuss,outlier detection highstake certification testing,0.02831997490964459,0.028320300417228002,0.028319993865819612,0.886650134531135,0.028389596276172834,0.004743836768849132,0.05010885304836154,0.0,0.07679396654477533,0.0
Zenisky A.L.; Hambleton R.K.; Sireci S.G.,Identification and evaluation of local item dependencies in the medical college admissions test,2002,39,"Measurement specialists routinely assume examinee responses to test items are independent of one another. However, previous research has shown that many contemporary tests contain item dependencies and not accounting for these dependencies leads to misleading estimates of item, test, and ability parameters. The goals of the study were (a) to review methods for detecting local item dependence (LID), (b) to discuss the use of testlets to account for LID in context-dependent item sets, (c) to apply LID detection methods and testlet-based item calibrations to data from a large-scale, high-stakes admissions test, and (d) to evaluate the results with respect to test score reliability and examinee proficiency estimation. Item dependencies were found in the test and these were due to test speededness or context dependence (related to passage structure). Also, the results highlight that steps taken to correct for the presence of LID and obtain less biased reliability estimates may impact on the estimation of examinee proficiency. The practical effects of the presence of LID on passage-based tests are discussed, as are issues regarding how to calibrate context-dependent item sets using item response theory.",Identification and evaluation of local item dependencies in the medical college admissions test,"Measurement specialists routinely assume examinee responses to test items are independent of one another. However, previous research has shown that many contemporary tests contain item dependencies and not accounting for these dependencies leads to misleading estimates of item, test, and ability parameters. The goals of the study were (a) to review methods for detecting local item dependence (LID), (b) to discuss the use of testlets to account for LID in context-dependent item sets, (c) to apply LID detection methods and testlet-based item calibrations to data from a large-scale, high-stakes admissions test, and (d) to evaluate the results with respect to test score reliability and examinee proficiency estimation. Item dependencies were found in the test and these were due to test speededness or context dependence (related to passage structure). Also, the results highlight that steps taken to correct for the presence of LID and obtain less biased reliability estimates may impact on the estimation of examinee proficiency. The practical effects of the presence of LID on passage-based tests are discussed, as are issues regarding how to calibrate context-dependent item sets using item response theory.","['specialist', 'routinely', 'assume', 'examinee', 'response', 'test', 'item', 'independent', 'previous', 'research', 'contemporary', 'test', 'contain', 'item', 'dependency', 'account', 'dependency', 'lead', 'misleading', 'estimate', 'item', 'test', 'ability', 'parameter', 'goal', 'study', 'review', 'method', 'detect', 'local', 'item', 'dependence', 'LID', 'b', 'discuss', 'testlet', 'account', 'LID', 'contextdependent', 'item', 'set', 'c', 'apply', 'LID', 'detection', 'method', 'testletbase', 'item', 'calibration', 'datum', 'largescale', 'highstake', 'admission', 'test', 'd', 'evaluate', 'result', 'respect', 'test', 'score', 'reliability', 'examinee', 'proficiency', 'estimation', 'Item', 'dependency', 'find', 'test', 'test', 'speededness', 'context', 'dependence', 'relate', 'passage', 'structure', 'result', 'highlight', 'step', 'correct', 'presence', 'LID', 'obtain', 'biased', 'reliability', 'estimate', 'impact', 'estimation', 'examinee', 'proficiency', 'practical', 'effect', 'presence', 'LID', 'passagebase', 'test', 'discuss', 'issue', 'regard', 'calibrate', 'contextdependent', 'item', 'set', 'item', 'response', 'theory']","['identification', 'evaluation', 'local', 'item', 'dependency', 'medical', 'college', 'admission', 'test']",specialist routinely assume examinee response test item independent previous research contemporary test contain item dependency account dependency lead misleading estimate item test ability parameter goal study review method detect local item dependence LID b discuss testlet account LID contextdependent item set c apply LID detection method testletbase item calibration datum largescale highstake admission test d evaluate result respect test score reliability examinee proficiency estimation Item dependency find test test speededness context dependence relate passage structure result highlight step correct presence LID obtain biased reliability estimate impact estimation examinee proficiency practical effect presence LID passagebase test discuss issue regard calibrate contextdependent item set item response theory,identification evaluation local item dependency medical college admission test,0.025879794088953134,0.025877764524440183,0.025878587581553064,0.8963792221080148,0.025984631697038886,0.00863831645753743,0.03235267258462189,0.0,0.09962661095045189,0.0
Walker C.M.; Beretvas S.N.,An empirical investigation demonstrating the multidimensional DIF paradigm: A cognitive explanation for DIF,2001,38,"Differential Item Functioning (DIF) is traditionally used to identify different item performance patterns between intact groups, most commonly involving race or sex comparisons. This study advocates expanding the utility of DIF as a step in construct validation. Rather than grouping examinees based on cultural differences, the reference and focal groups are chosen from two extremes along a distinct cognitive dimension that is hypothesized to supplement the dominant latent trait being measured. Specifically, this study investigates DIF between proficient and non-proficient fourth- and seventh-grade writers on open-ended mathematics test items that require students to communicate about mathematics. It is suggested that the occurrence of DIF in this situation actually enhances, rather than detracts from, the construct validity of the test because, according to the National Council of Teachers of Mathematics (NCTM), mathematical communication is an important component of mathematical ability, the dominant construct being assessed. However, the presence of DIF influences the validity of inferences that can be made from test scores and suggests that two scores should be reported, one for general mathematical ability and one for mathematical communication. The fact that currently only one test score is reported, a simple composite of scores on multiple-choice and open-ended items, may lead to incorrect decisions being made about examinees.",An empirical investigation demonstrating the multidimensional DIF paradigm: A cognitive explanation for DIF,"Differential Item Functioning (DIF) is traditionally used to identify different item performance patterns between intact groups, most commonly involving race or sex comparisons. This study advocates expanding the utility of DIF as a step in construct validation. Rather than grouping examinees based on cultural differences, the reference and focal groups are chosen from two extremes along a distinct cognitive dimension that is hypothesized to supplement the dominant latent trait being measured. Specifically, this study investigates DIF between proficient and non-proficient fourth- and seventh-grade writers on open-ended mathematics test items that require students to communicate about mathematics. It is suggested that the occurrence of DIF in this situation actually enhances, rather than detracts from, the construct validity of the test because, according to the National Council of Teachers of Mathematics (NCTM), mathematical communication is an important component of mathematical ability, the dominant construct being assessed. However, the presence of DIF influences the validity of inferences that can be made from test scores and suggests that two scores should be reported, one for general mathematical ability and one for mathematical communication. The fact that currently only one test score is reported, a simple composite of scores on multiple-choice and open-ended items, may lead to incorrect decisions being made about examinees.","['Differential', 'Item', 'Functioning', 'DIF', 'traditionally', 'identify', 'different', 'item', 'performance', 'pattern', 'intact', 'group', 'commonly', 'involve', 'race', 'sex', 'comparison', 'study', 'advocate', 'expand', 'utility', 'DIF', 'step', 'construct', 'validation', 'group', 'examinee', 'base', 'cultural', 'difference', 'reference', 'focal', 'group', 'choose', 'extreme', 'distinct', 'cognitive', 'dimension', 'hypothesize', 'supplement', 'dominant', 'latent', 'trait', 'measure', 'specifically', 'study', 'investigate', 'DIF', 'proficient', 'nonproficient', 'fourth', 'seventhgrade', 'writer', 'openende', 'mathematic', 'test', 'item', 'require', 'student', 'communicate', 'mathematic', 'suggest', 'occurrence', 'DIF', 'situation', 'actually', 'enhance', 'detract', 'construct', 'validity', 'test', 'accord', 'National', 'Council', 'Teachers', 'Mathematics', 'NCTM', 'mathematical', 'communication', 'important', 'component', 'mathematical', 'ability', 'dominant', 'construct', 'assess', 'presence', 'dif', 'influence', 'validity', 'inference', 'test', 'score', 'suggest', 'score', 'report', 'general', 'mathematical', 'ability', 'mathematical', 'communication', 'fact', 'currently', 'test', 'score', 'report', 'simple', 'composite', 'score', 'multiplechoice', 'openende', 'item', 'lead', 'incorrect', 'decision', 'examinee']","['empirical', 'investigation', 'demonstrate', 'multidimensional', 'DIF', 'paradigm', 'cognitive', 'explanation', 'DIF']",Differential Item Functioning DIF traditionally identify different item performance pattern intact group commonly involve race sex comparison study advocate expand utility DIF step construct validation group examinee base cultural difference reference focal group choose extreme distinct cognitive dimension hypothesize supplement dominant latent trait measure specifically study investigate DIF proficient nonproficient fourth seventhgrade writer openende mathematic test item require student communicate mathematic suggest occurrence DIF situation actually enhance detract construct validity test accord National Council Teachers Mathematics NCTM mathematical communication important component mathematical ability dominant construct assess presence dif influence validity inference test score suggest score report general mathematical ability mathematical communication fact currently test score report simple composite score multiplechoice openende item lead incorrect decision examinee,empirical investigation demonstrate multidimensional DIF paradigm cognitive explanation DIF,0.021517440029610743,0.021518450250720265,0.02151481124066749,0.9137146279820694,0.02173467049693208,0.09915540627084063,0.0353643853306383,0.0,0.0011395067936654313,0.016709496061171173
Buckendahl C.W.; Smith R.W.; Impara J.C.; Plake B.S.,A comparison of Angoff and Bookmark standard setting methods,2002,39,"This article presents a comparison of simplified variations on two prevalent methods, Angoff and Bookmark, for setting cut scores on educational assessments. The comparison is presented through an application with a Grade 7 Mathematics Assessment in a midwestern school district. Training and operational methods and procedures for each method are described in detail along with comparative results for the application. An alternative item ordering strategy for the Bookmark method that may increase its usability is also introduced. Although the Angoff method is more widely used, the Bookmark method has some promising features, specifically in educational settings. Teachers are able to focus on the expected performance of the ""barely proficient"" student without the additional challenge of estimating absolute item difficulty.",A comparison of Angoff and Bookmark standard setting methods,"This article presents a comparison of simplified variations on two prevalent methods, Angoff and Bookmark, for setting cut scores on educational assessments. The comparison is presented through an application with a Grade 7 Mathematics Assessment in a midwestern school district. Training and operational methods and procedures for each method are described in detail along with comparative results for the application. An alternative item ordering strategy for the Bookmark method that may increase its usability is also introduced. Although the Angoff method is more widely used, the Bookmark method has some promising features, specifically in educational settings. Teachers are able to focus on the expected performance of the ""barely proficient"" student without the additional challenge of estimating absolute item difficulty.","['article', 'present', 'comparison', 'simplified', 'variation', 'prevalent', 'method', 'Angoff', 'Bookmark', 'set', 'cut', 'score', 'educational', 'assessment', 'comparison', 'present', 'application', 'Grade', '7', 'Mathematics', 'Assessment', 'midwestern', 'school', 'district', 'training', 'operational', 'method', 'procedure', 'method', 'describe', 'detail', 'comparative', 'result', 'application', 'alternative', 'item', 'order', 'strategy', 'Bookmark', 'method', 'increase', 'usability', 'introduce', 'Angoff', 'method', 'widely', 'Bookmark', 'method', 'promising', 'feature', 'specifically', 'educational', 'setting', 'Teachers', 'able', 'focus', 'expect', 'performance', 'barely', 'proficient', 'student', 'additional', 'challenge', 'estimate', 'absolute', 'item', 'difficulty']","['comparison', 'Angoff', 'Bookmark', 'standard', 'setting', 'method']",article present comparison simplified variation prevalent method Angoff Bookmark set cut score educational assessment comparison present application Grade 7 Mathematics Assessment midwestern school district training operational method procedure method describe detail comparative result application alternative item order strategy Bookmark method increase usability introduce Angoff method widely Bookmark method promising feature specifically educational setting Teachers able focus expect performance barely proficient student additional challenge estimate absolute item difficulty,comparison Angoff Bookmark standard setting method,0.0268661721813439,0.0268661136062743,0.027653217462554378,0.5058722041391778,0.41274229261064976,0.0,0.03941220714698856,0.010141751142275631,0.03662863705044632,0.07653150636506476
Bielinski J.; Davison M.L.,A sex difference by item difficulty interaction in multiple-choice mathematics items administered to national probability samples,2001,38,"A 1998 study by Bielinski and Davison reported a sex difference by item difficulty interaction in which easy items tended to be easier for females than males, and hard items tended to be harder for females than males. To extend their research to nationally representative samples of students, this study used math achievement data from the 1992 NAEP, the TIMSS, and the NELS: 88. The data included students in grades 4, 8, 10, and 12. The interaction was assessed by correlating the item difficulty difference (bmale - bfemale) with item difficulty computed on the combined male/female sample. Using only the multiple-choice mathematics items, the predicted negative correlation was found for all eight populations and was significant in five. An argument is made that this phenomenon may help explain the greater variability in math achievement among males as compared to females and the emergence of higher performance of males in late adolescence.",A sex difference by item difficulty interaction in multiple-choice mathematics items administered to national probability samples,"A 1998 study by Bielinski and Davison reported a sex difference by item difficulty interaction in which easy items tended to be easier for females than males, and hard items tended to be harder for females than males. To extend their research to nationally representative samples of students, this study used math achievement data from the 1992 NAEP, the TIMSS, and the NELS: 88. The data included students in grades 4, 8, 10, and 12. The interaction was assessed by correlating the item difficulty difference (bmale - bfemale) with item difficulty computed on the combined male/female sample. Using only the multiple-choice mathematics items, the predicted negative correlation was found for all eight populations and was significant in five. An argument is made that this phenomenon may help explain the greater variability in math achievement among males as compared to females and the emergence of higher performance of males in late adolescence.","['1998', 'study', 'Bielinski', 'Davison', 'report', 'sex', 'difference', 'item', 'difficulty', 'interaction', 'easy', 'item', 'tend', 'easy', 'female', 'male', 'hard', 'item', 'tend', 'hard', 'female', 'male', 'extend', 'research', 'nationally', 'representative', 'sample', 'student', 'study', 'math', 'achievement', 'datum', '1992', 'naep', 'TIMSS', 'NELS', '88', 'datum', 'include', 'student', 'grade', '4', '8', '10', '12', 'interaction', 'assess', 'correlate', 'item', 'difficulty', 'difference', 'bmale', 'bfemale', 'item', 'difficulty', 'compute', 'combine', 'malefemale', 'sample', 'multiplechoice', 'mathematic', 'item', 'predict', 'negative', 'correlation', 'find', 'population', 'significant', 'argument', 'phenomenon', 'help', 'explain', 'great', 'variability', 'math', 'achievement', 'male', 'compare', 'female', 'emergence', 'high', 'performance', 'male', 'late', 'adolescence']","['sex', 'difference', 'item', 'difficulty', 'interaction', 'multiplechoice', 'mathematic', 'item', 'administer', 'national', 'probability', 'sample']",1998 study Bielinski Davison report sex difference item difficulty interaction easy item tend easy female male hard item tend hard female male extend research nationally representative sample student study math achievement datum 1992 naep TIMSS NELS 88 datum include student grade 4 8 10 12 interaction assess correlate item difficulty difference bmale bfemale item difficulty compute combine malefemale sample multiplechoice mathematic item predict negative correlation find population significant argument phenomenon help explain great variability math achievement male compare female emergence high performance male late adolescence,sex difference item difficulty interaction multiplechoice mathematic item administer national probability sample,0.025430566174131534,0.02540047829787253,0.02546808769411154,0.026630308950247702,0.8970705588836367,0.004708329683071039,0.0,0.0,0.03398093363262569,0.07168759376508442
Stocking M.L.; Lawrence I.; Feigenbaum M.; Jirele T.; Lewis C.; Van Essen T.,An empirical investigation of impact moderation in test construction,2002,39,"This investigation constructed four different kinds of test sections using three methods of test assembly that incorporate the goals of simultaneous moderation of three kinds of impact - gender impact, African-American impact, and Hispanic-American impact. The test sections were administered undetectably to random samples from the appropriate population. The results were evaluated by comparison of the characteristics of moderated sections with those of parallel operational sections. Almost all methods of test assembly produced either moderation of impact in the appropriate direction or no change in impact. Taking impact into account in test assembly tended to lower reliability slightly, raise concurrent validity slightly, and maintain the construct measured by the parallel operational section. It also reduced the relative efficiency for test takers in the middle score range while increasing efficiency for those with more extreme scores.",An empirical investigation of impact moderation in test construction,"This investigation constructed four different kinds of test sections using three methods of test assembly that incorporate the goals of simultaneous moderation of three kinds of impact - gender impact, African-American impact, and Hispanic-American impact. The test sections were administered undetectably to random samples from the appropriate population. The results were evaluated by comparison of the characteristics of moderated sections with those of parallel operational sections. Almost all methods of test assembly produced either moderation of impact in the appropriate direction or no change in impact. Taking impact into account in test assembly tended to lower reliability slightly, raise concurrent validity slightly, and maintain the construct measured by the parallel operational section. It also reduced the relative efficiency for test takers in the middle score range while increasing efficiency for those with more extreme scores.","['investigation', 'construct', 'different', 'kind', 'test', 'section', 'method', 'test', 'assembly', 'incorporate', 'goal', 'simultaneous', 'moderation', 'kind', 'impact', 'gender', 'impact', 'AfricanAmerican', 'impact', 'HispanicAmerican', 'impact', 'test', 'section', 'administer', 'undetectably', 'random', 'sample', 'appropriate', 'population', 'result', 'evaluate', 'comparison', 'characteristic', 'moderated', 'section', 'parallel', 'operational', 'section', 'method', 'test', 'assembly', 'produce', 'moderation', 'impact', 'appropriate', 'direction', 'change', 'impact', 'impact', 'account', 'test', 'assembly', 'tend', 'lower', 'reliability', 'slightly', 'raise', 'concurrent', 'validity', 'slightly', 'maintain', 'construct', 'measure', 'parallel', 'operational', 'section', 'reduce', 'relative', 'efficiency', 'test', 'taker', 'middle', 'score', 'range', 'increase', 'efficiency', 'extreme', 'score']","['empirical', 'investigation', 'impact', 'moderation', 'test', 'construction']",investigation construct different kind test section method test assembly incorporate goal simultaneous moderation kind impact gender impact AfricanAmerican impact HispanicAmerican impact test section administer undetectably random sample appropriate population result evaluate comparison characteristic moderated section parallel operational section method test assembly produce moderation impact appropriate direction change impact impact account test assembly tend lower reliability slightly raise concurrent validity slightly maintain construct measure parallel operational section reduce relative efficiency test taker middle score range increase efficiency extreme score,empirical investigation impact moderation test construction,0.030181074196735647,0.030182788721777924,0.2711788717615805,0.6379465005502247,0.030510764769681266,0.026126819794750768,0.028202122227142242,0.015556685806188759,0.010264455662818327,0.005364732762119745
Ma X.,Stability of school academic performance across subject areas,2001,38,"Recent studies on the stability of school academic performance across school subject areas show two weaknesses: a lack of research attention to elementary schools and a lack of adequate statistical adjustments for school characteristics. With data describing elementary students (N = 6,883 students in Grade 6 in 148 schools) from the New Brunswick School Climate Study (NBSCS), the current study examined correlates of academic performance across mathematics, science, reading, and writing among students and among schools, using a multivariate multilevel model with statistical adjustments for student characteristics and school context and climate characteristics. Results indicated that (a) students were differentially successful in different subject areas, (b) schools were differentially effective in different subject areas, and (c) the differential success was more obvious among students than among schools. Findings of this study call for a new type of school programs that aim to ensure that students progress equally in different subject areas and for stronger school policies that systematically coordinate classroom or department practices.",Stability of school academic performance across subject areas,"Recent studies on the stability of school academic performance across school subject areas show two weaknesses: a lack of research attention to elementary schools and a lack of adequate statistical adjustments for school characteristics. With data describing elementary students (N = 6,883 students in Grade 6 in 148 schools) from the New Brunswick School Climate Study (NBSCS), the current study examined correlates of academic performance across mathematics, science, reading, and writing among students and among schools, using a multivariate multilevel model with statistical adjustments for student characteristics and school context and climate characteristics. Results indicated that (a) students were differentially successful in different subject areas, (b) schools were differentially effective in different subject areas, and (c) the differential success was more obvious among students than among schools. Findings of this study call for a new type of school programs that aim to ensure that students progress equally in different subject areas and for stronger school policies that systematically coordinate classroom or department practices.","['recent', 'study', 'stability', 'school', 'academic', 'performance', 'school', 'subject', 'area', 'weakness', 'lack', 'research', 'attention', 'elementary', 'school', 'lack', 'adequate', 'statistical', 'adjustment', 'school', 'characteristic', 'datum', 'describe', 'elementary', 'student', 'n', '6883', 'student', 'Grade', '6', '148', 'school', 'New', 'Brunswick', 'School', 'Climate', 'Study', 'NBSCS', 'current', 'study', 'examine', 'correlate', 'academic', 'performance', 'mathematics', 'science', 'reading', 'write', 'student', 'school', 'multivariate', 'multilevel', 'statistical', 'adjustment', 'student', 'characteristic', 'school', 'context', 'climate', 'characteristic', 'result', 'indicate', 'student', 'differentially', 'successful', 'different', 'subject', 'area', 'b', 'school', 'differentially', 'effective', 'different', 'subject', 'area', 'c', 'differential', 'success', 'obvious', 'student', 'school', 'Findings', 'study', 'new', 'type', 'school', 'program', 'aim', 'ensure', 'student', 'progress', 'equally', 'different', 'subject', 'area', 'strong', 'school', 'policy', 'systematically', 'coordinate', 'classroom', 'department', 'practice']","['stability', 'school', 'academic', 'performance', 'subject', 'area']",recent study stability school academic performance school subject area weakness lack research attention elementary school lack adequate statistical adjustment school characteristic datum describe elementary student n 6883 student Grade 6 148 school New Brunswick School Climate Study NBSCS current study examine correlate academic performance mathematics science reading write student school multivariate multilevel statistical adjustment student characteristic school context climate characteristic result indicate student differentially successful different subject area b school differentially effective different subject area c differential success obvious student school Findings study new type school program aim ensure student progress equally different subject area strong school policy systematically coordinate classroom department practice,stability school academic performance subject area,0.028809740750081428,0.02876404362251976,0.028837549196561525,0.030617931863841735,0.8829707345669955,0.0,0.0,0.0,0.0,0.17462313122088055
Willingham W.W.; Pollack J.M.; Lewis C.,Grades and test scores: Accounting for observed differences,2002,39,"Why do grades and test scores often differ? A framework of possible differences is proposed in this article. An approximation of the framework was tested with data on 8,454 high school seniors from the National Education Longitudinal Study. Individual and group differences in grade versus test performance were substantially reduced by focusing the two measures on similar academic subjects, correcting for grading variations and unreliability, and adding teacher ratings and other information about students. Concurrent prediction of high school average was thus increased from 0.62 to 0.90; differential prediction in eight subgroups was reduced to 0.02 letter-grades. Grading variation was a major source of discrepancy between grades and test scores. Other major sources were teacher ratings and Scholastic Engagement, a promising organizing principle for understanding student achievement. Engagement was defined by three types of observable behavior: employing school skills, demonstrating initiative, and avoiding competing activities. While groups varied in average achievement, group performance was generally similar on grades and tests. Major factors in achievement were similarly constituted and similarly related from group to group. Differences between grades and tests give these measures complementary strengths in high-stakes assessment. If artifactual differences between the two measures are not corrected, common statistical estimates of validity and fairness are unduly conservative.",Grades and test scores: Accounting for observed differences,"Why do grades and test scores often differ? A framework of possible differences is proposed in this article. An approximation of the framework was tested with data on 8,454 high school seniors from the National Education Longitudinal Study. Individual and group differences in grade versus test performance were substantially reduced by focusing the two measures on similar academic subjects, correcting for grading variations and unreliability, and adding teacher ratings and other information about students. Concurrent prediction of high school average was thus increased from 0.62 to 0.90; differential prediction in eight subgroups was reduced to 0.02 letter-grades. Grading variation was a major source of discrepancy between grades and test scores. Other major sources were teacher ratings and Scholastic Engagement, a promising organizing principle for understanding student achievement. Engagement was defined by three types of observable behavior: employing school skills, demonstrating initiative, and avoiding competing activities. While groups varied in average achievement, group performance was generally similar on grades and tests. Major factors in achievement were similarly constituted and similarly related from group to group. Differences between grades and tests give these measures complementary strengths in high-stakes assessment. If artifactual differences between the two measures are not corrected, common statistical estimates of validity and fairness are unduly conservative.","['grade', 'test', 'score', 'differ', 'framework', 'possible', 'difference', 'propose', 'article', 'approximation', 'framework', 'test', 'datum', '8454', 'high', 'school', 'senior', 'National', 'Longitudinal', 'Study', 'Individual', 'group', 'difference', 'grade', 'versus', 'test', 'performance', 'substantially', 'reduce', 'focus', 'measure', 'similar', 'academic', 'subject', 'correct', 'grade', 'variation', 'unreliability', 'add', 'teacher', 'rating', 'information', 'student', 'concurrent', 'prediction', 'high', 'school', 'average', 'increase', '062', '090', 'differential', 'prediction', 'subgroup', 'reduce', '002', 'lettergrade', 'Grading', 'variation', 'major', 'source', 'discrepancy', 'grade', 'test', 'score', 'major', 'source', 'teacher', 'rating', 'Scholastic', 'Engagement', 'promising', 'organizing', 'principle', 'understand', 'student', 'achievement', 'Engagement', 'define', 'type', 'observable', 'behavior', 'employ', 'school', 'skill', 'demonstrate', 'initiative', 'avoid', 'compete', 'activity', 'group', 'vary', 'average', 'achievement', 'group', 'performance', 'generally', 'similar', 'grade', 'test', 'major', 'factor', 'achievement', 'similarly', 'constitute', 'similarly', 'relate', 'group', 'group', 'Differences', 'grade', 'test', 'measure', 'complementary', 'strength', 'highstake', 'assessment', 'artifactual', 'difference', 'measure', 'correct', 'common', 'statistical', 'estimate', 'validity', 'fairness', 'unduly', 'conservative']","['grade', 'test', 'score', 'Accounting', 'observed', 'difference']",grade test score differ framework possible difference propose article approximation framework test datum 8454 high school senior National Longitudinal Study Individual group difference grade versus test performance substantially reduce focus measure similar academic subject correct grade variation unreliability add teacher rating information student concurrent prediction high school average increase 062 090 differential prediction subgroup reduce 002 lettergrade Grading variation major source discrepancy grade test score major source teacher rating Scholastic Engagement promising organizing principle understand student achievement Engagement define type observable behavior employ school skill demonstrate initiative avoid compete activity group vary average achievement group performance generally similar grade test major factor achievement similarly constitute similarly relate group group Differences grade test measure complementary strength highstake assessment artifactual difference measure correct common statistical estimate validity fairness unduly conservative,grade test score Accounting observed difference,0.021982262933435852,0.02194755514247972,0.02197784579175809,0.21833881907205582,0.7157535170602705,0.007156697624095603,0.0,0.0,0.0,0.23170530606596296
Attali Y.; Fraenkel T.,The point-biserial as a discrimination index for distractors in multiple-choice items: Deficiencies in usage and an alternative,2000,37,We show that using the point-biserial as a discrimination index for distractors by differentiating between examinees who chose the distractor and examinees who did not choose the distractor is theoretically wrong and may lead to an incorrect rejection of items. We propose an alternative usage and present empirical evidence for its suitability.,The point-biserial as a discrimination index for distractors in multiple-choice items: Deficiencies in usage and an alternative,We show that using the point-biserial as a discrimination index for distractors by differentiating between examinees who chose the distractor and examinees who did not choose the distractor is theoretically wrong and may lead to an incorrect rejection of items. We propose an alternative usage and present empirical evidence for its suitability.,"['pointbiserial', 'discrimination', 'index', 'distractor', 'differentiate', 'examinee', 'choose', 'distractor', 'examine', 'choose', 'distractor', 'theoretically', 'wrong', 'lead', 'incorrect', 'rejection', 'item', 'propose', 'alternative', 'usage', 'present', 'empirical', 'evidence', 'suitability']","['pointbiserial', 'discrimination', 'index', 'distractor', 'multiplechoice', 'item', 'deficiency', 'usage', 'alternative']",pointbiserial discrimination index distractor differentiate examinee choose distractor examine choose distractor theoretically wrong lead incorrect rejection item propose alternative usage present empirical evidence suitability,pointbiserial discrimination index distractor multiplechoice item deficiency usage alternative,0.040376811728431036,0.040377648020947406,0.04039152465708274,0.8383575991465102,0.040496416447028526,0.011688539342578868,0.0,0.0,0.0257821578717597,0.002734425913015068
Attali Y.; Bar-Hillel M.,Guess where: The position of correct answers in multiple-choice test items as a psychometric variable,2003,40,"In this article, the authors show that test makers and test takers have a strong and systematic tendency for hiding correct answers - or, respectively, for seeking them - in middle positions. In single, isolated questions, both prefer middle positions to extreme ones in a ratio of up to 3 or 4 to 1. Because test makers routinely, deliberately, and excessively balance the answer key of operational tests, middle bias almost, though not quite, disappears in those keys. Examinees taking real tests also produce answer sequences that are more balanced than their single question tendencies but less balanced than the correct key. In a typical four-choice test, about 55% of erroneous answers are in the two central positions. The authors show that this bias is large enough to have real psychometric consequences, as questions with middle correct answers are easier and less discriminating than questions with extreme correct answers, a fact of which some implications are explored.",Guess where: The position of correct answers in multiple-choice test items as a psychometric variable,"In this article, the authors show that test makers and test takers have a strong and systematic tendency for hiding correct answers - or, respectively, for seeking them - in middle positions. In single, isolated questions, both prefer middle positions to extreme ones in a ratio of up to 3 or 4 to 1. Because test makers routinely, deliberately, and excessively balance the answer key of operational tests, middle bias almost, though not quite, disappears in those keys. Examinees taking real tests also produce answer sequences that are more balanced than their single question tendencies but less balanced than the correct key. In a typical four-choice test, about 55% of erroneous answers are in the two central positions. The authors show that this bias is large enough to have real psychometric consequences, as questions with middle correct answers are easier and less discriminating than questions with extreme correct answers, a fact of which some implications are explored.","['article', 'author', 'test', 'maker', 'test', 'taker', 'strong', 'systematic', 'tendency', 'hide', 'correct', 'answer', 'respectively', 'seek', 'middle', 'position', 'single', 'isolated', 'question', 'prefer', 'middle', 'position', 'extreme', 'ratio', '3', '4', '1', 'test', 'maker', 'routinely', 'deliberately', 'excessively', 'balance', 'answer', 'key', 'operational', 'test', 'middle', 'bias', 'disappear', 'key', 'examine', 'real', 'test', 'produce', 'answer', 'sequence', 'balanced', 'single', 'question', 'tendency', 'balanced', 'correct', 'key', 'typical', 'fourchoice', 'test', '55', 'erroneous', 'answer', 'central', 'position', 'author', 'bias', 'large', 'real', 'psychometric', 'consequence', 'question', 'middle', 'correct', 'answer', 'easy', 'discriminate', 'question', 'extreme', 'correct', 'answer', 'fact', 'implication', 'explore']","['guess', 'position', 'correct', 'answer', 'multiplechoice', 'test', 'item', 'psychometric', 'variable']",article author test maker test taker strong systematic tendency hide correct answer respectively seek middle position single isolated question prefer middle position extreme ratio 3 4 1 test maker routinely deliberately excessively balance answer key operational test middle bias disappear key examine real test produce answer sequence balanced single question tendency balanced correct key typical fourchoice test 55 erroneous answer central position author bias large real psychometric consequence question middle correct answer easy discriminate question extreme correct answer fact implication explore,guess position correct answer multiplechoice test item psychometric variable,0.029030525303024236,0.02903210441500316,0.029033215615025882,0.8838320578252639,0.029072096841682854,0.0058414103942004665,0.009700479509360958,0.0046376513378457015,0.018699482004856903,0.012653636911913622
Kane M.,Inferences about variance components and reliability-generalizability coefficients in the absence of random sampling,2002,39,"Generalizability theory (G theory) employs random-effects ANOVA to estimate the variance components included in generalizability coefficients, standard errors, and other indices of precision. The ANOVA models depend on random sampling assumptions, and the variance-component estimates are likely to be sensitive to violations of these assumptions. Yet, generalizability studies do not typically sample randomly. This kind of inconsistency between assumptions in statistical models and actual data collection procedures is not uncommon in science, but it does raise fundamental questions about the substantive inferences based on the statistical analyses. This article reviews criticisms of sampling assumptions in G theory (and in reliability theory) and examines the feasibility of using representative sampling, stratification, homogeneity assumptions, and replications to address these criticisms.",Inferences about variance components and reliability-generalizability coefficients in the absence of random sampling,"Generalizability theory (G theory) employs random-effects ANOVA to estimate the variance components included in generalizability coefficients, standard errors, and other indices of precision. The ANOVA models depend on random sampling assumptions, and the variance-component estimates are likely to be sensitive to violations of these assumptions. Yet, generalizability studies do not typically sample randomly. This kind of inconsistency between assumptions in statistical models and actual data collection procedures is not uncommon in science, but it does raise fundamental questions about the substantive inferences based on the statistical analyses. This article reviews criticisms of sampling assumptions in G theory (and in reliability theory) and examines the feasibility of using representative sampling, stratification, homogeneity assumptions, and replications to address these criticisms.","['generalizability', 'theory', 'g', 'theory', 'employ', 'randomeffect', 'ANOVA', 'estimate', 'variance', 'component', 'include', 'generalizability', 'coefficient', 'standard', 'error', 'index', 'precision', 'ANOVA', 'depend', 'random', 'sampling', 'assumption', 'variancecomponent', 'estimate', 'likely', 'sensitive', 'violation', 'assumption', 'generalizability', 'study', 'typically', 'sample', 'randomly', 'kind', 'inconsistency', 'assumption', 'statistical', 'actual', 'datum', 'collection', 'procedure', 'uncommon', 'science', 'raise', 'fundamental', 'question', 'substantive', 'inference', 'base', 'statistical', 'analysis', 'article', 'review', 'criticism', 'sample', 'assumption', 'G', 'theory', 'reliability', 'theory', 'examine', 'feasibility', 'representative', 'sample', 'stratification', 'homogeneity', 'assumption', 'replication', 'address', 'criticism']","['inference', 'variance', 'component', 'reliabilitygeneralizability', 'coefficient', 'absence', 'random', 'sampling']",generalizability theory g theory employ randomeffect ANOVA estimate variance component include generalizability coefficient standard error index precision ANOVA depend random sampling assumption variancecomponent estimate likely sensitive violation assumption generalizability study typically sample randomly kind inconsistency assumption statistical actual datum collection procedure uncommon science raise fundamental question substantive inference base statistical analysis article review criticism sample assumption G theory reliability theory examine feasibility representative sample stratification homogeneity assumption replication address criticism,inference variance component reliabilitygeneralizability coefficient absence random sampling,0.02762869022014868,0.027626128625531307,0.027737528268186415,0.6760740142869442,0.24093363859918934,0.002403013094079582,0.11190328533233354,0.002740639923197002,0.0,0.0
Lee G.,The influence of several factors on reliability for complex reading comprehension tests,2002,39,"The purpose of this study was to investigate the effects of items, passages, contents, themes, and types of passages on the reliability and standard errors of measurement for complex reading comprehension tests. Seven different generalizability theory models were used in the analyses. Results indicated that generalizability coefficients estimated using multivariate models incorporating content strata and types of passages were similar in size to reliability estimates based upon a model that did not include these factors. In contrast, incorporating passages and themes within univariate generalizability theory models produced non-negligible differences in the reliability estimates. This suggested that passages and themes be taken into account when evaluating the reliability of test scores for complex reading comprehension tests.",The influence of several factors on reliability for complex reading comprehension tests,"The purpose of this study was to investigate the effects of items, passages, contents, themes, and types of passages on the reliability and standard errors of measurement for complex reading comprehension tests. Seven different generalizability theory models were used in the analyses. Results indicated that generalizability coefficients estimated using multivariate models incorporating content strata and types of passages were similar in size to reliability estimates based upon a model that did not include these factors. In contrast, incorporating passages and themes within univariate generalizability theory models produced non-negligible differences in the reliability estimates. This suggested that passages and themes be taken into account when evaluating the reliability of test scores for complex reading comprehension tests.","['purpose', 'study', 'investigate', 'effect', 'item', 'passage', 'content', 'theme', 'type', 'passage', 'reliability', 'standard', 'error', 'complex', 'reading', 'comprehension', 'test', 'seven', 'different', 'generalizability', 'theory', 'analysis', 'result', 'indicate', 'generalizability', 'coefficient', 'estimate', 'multivariate', 'incorporate', 'content', 'strata', 'type', 'passage', 'similar', 'size', 'reliability', 'estimate', 'base', 'include', 'factor', 'contrast', 'incorporate', 'passage', 'theme', 'univariate', 'generalizability', 'theory', 'produce', 'nonnegligible', 'difference', 'reliability', 'estimate', 'suggest', 'passage', 'theme', 'account', 'evaluate', 'reliability', 'test', 'score', 'complex', 'reading', 'comprehension', 'test']","['influence', 'factor', 'reliability', 'complex', 'reading', 'comprehension', 'test']",purpose study investigate effect item passage content theme type passage reliability standard error complex reading comprehension test seven different generalizability theory analysis result indicate generalizability coefficient estimate multivariate incorporate content strata type passage similar size reliability estimate base include factor contrast incorporate passage theme univariate generalizability theory produce nonnegligible difference reliability estimate suggest passage theme account evaluate reliability test score complex reading comprehension test,influence factor reliability complex reading comprehension test,0.03334390700043056,0.0333445187020815,0.033351868769543325,0.8664886568231007,0.033471048704843784,0.003482189226791606,0.10273961555390793,0.00031908974572069905,0.013182941093260375,0.0
Yu F.; Nandakumar R.,Poly-detect for quantifying the degree of multidimensionality of item response data,2001,38,"This study investigated the validity of the Poly-Detect procedure to determine the approximate simple dimensionality structure and to quantify the degree of multidimensionality present in polytomous data through simulation and real data analysis. Both unidimensional and two-dimensional data were generated in the simulation study for the complete and the BIB data. The simulation results showed that the dimensionality structures were similar between the corresponding complete data and the BIB data as identified by the Poly-Detect procedure. Also, the simulation results indicated that the number of response categories of items did not affect the performance of the Poly-Detect procedure in quantifying the degree of multidimensionality of polytomous data. Based on the simulation results, a subjective scale of the degree of multidimensionality of a data set was developed. Application of the Poly-Detect procedure on the 1992 NAEP eighth-grade reading data indicated that the degree of multidimensionality of the entire 1992 NAEP eighth-grade reading item pool was moderate at most.",Poly-detect for quantifying the degree of multidimensionality of item response data,"This study investigated the validity of the Poly-Detect procedure to determine the approximate simple dimensionality structure and to quantify the degree of multidimensionality present in polytomous data through simulation and real data analysis. Both unidimensional and two-dimensional data were generated in the simulation study for the complete and the BIB data. The simulation results showed that the dimensionality structures were similar between the corresponding complete data and the BIB data as identified by the Poly-Detect procedure. Also, the simulation results indicated that the number of response categories of items did not affect the performance of the Poly-Detect procedure in quantifying the degree of multidimensionality of polytomous data. Based on the simulation results, a subjective scale of the degree of multidimensionality of a data set was developed. Application of the Poly-Detect procedure on the 1992 NAEP eighth-grade reading data indicated that the degree of multidimensionality of the entire 1992 NAEP eighth-grade reading item pool was moderate at most.","['study', 'investigate', 'validity', 'PolyDetect', 'procedure', 'determine', 'approximate', 'simple', 'dimensionality', 'structure', 'quantify', 'degree', 'multidimensionality', 'present', 'polytomous', 'datum', 'simulation', 'real', 'datum', 'analysis', 'unidimensional', 'twodimensional', 'datum', 'generate', 'simulation', 'study', 'complete', 'BIB', 'datum', 'simulation', 'result', 'dimensionality', 'structure', 'similar', 'correspond', 'complete', 'datum', 'BIB', 'datum', 'identify', 'PolyDetect', 'procedure', 'simulation', 'result', 'indicate', 'number', 'response', 'category', 'item', 'affect', 'performance', 'PolyDetect', 'procedure', 'quantify', 'degree', 'multidimensionality', 'polytomous', 'datum', 'base', 'simulation', 'result', 'subjective', 'scale', 'degree', 'multidimensionality', 'data', 'set', 'develop', 'application', 'PolyDetect', 'procedure', '1992', 'naep', 'eighthgrade', 'reading', 'datum', 'indicate', 'degree', 'multidimensionality', 'entire', '1992', 'naep', 'eighthgrade', 'read', 'item', 'pool', 'moderate']","['polydetect', 'quantify', 'degree', 'multidimensionality', 'item', 'response', 'datum']",study investigate validity PolyDetect procedure determine approximate simple dimensionality structure quantify degree multidimensionality present polytomous datum simulation real datum analysis unidimensional twodimensional datum generate simulation study complete BIB datum simulation result dimensionality structure similar correspond complete datum BIB datum identify PolyDetect procedure simulation result indicate number response category item affect performance PolyDetect procedure quantify degree multidimensionality polytomous datum base simulation result subjective scale degree multidimensionality data set develop application PolyDetect procedure 1992 naep eighthgrade reading datum indicate degree multidimensionality entire 1992 naep eighthgrade read item pool moderate,polydetect quantify degree multidimensionality item response datum,0.030057790358816786,0.030059286622905104,0.20424238370432118,0.7053786266949953,0.03026191261896164,0.009656329806557735,0.07334440188590416,0.0,0.022781992825533445,0.004971281719891455
Ban J.-E.; Hanson B.A.; Wang T.; Yi Q.; Harris D.J.,A comparative study of on-line pretest item-calibration/scaling methods in computerized adaptive testing,2001,38,"The purpose of this study was to compare and evaluate five on-line pretest item-calibration/scaling methods in computerized adaptive testing (CAT): marginal maximum likelihood estimate with one EM cycle (OEM), marginal maximum likelihood estimate with multiple EM cycles (MEM), Stocking's Method A, Stocking's Method B, and BILOG/Prior. The five methods were evaluated in terms of item-parameter recovery, using three different sample sizes (300, 1000 and 3000). The MEM method appeared to be the best choice among these, because it produced the smallest parameter-estimation errors for all sample size conditions. MEM and OEM are mathematically similar, although the OEM method produced larger errors. MEM also was preferable to OEM, unless the amount of time involved in iterative computation is a concern. Stocking's Method B also worked very well, but it required anchor items that either would increase test lengths or require larger sample sizes depending on test administration design. Until more appropriate ways of handling sparse data are devised, the BILOG/Prior method may not be a reasonable choice for small sample sizes. Stocking's Method A had the largest weighted total error, as well as a theoretical weakness (i.e., treating estimated ability as true ability); thus, there appeared to be little reason to use it.",A comparative study of on-line pretest item-calibration/scaling methods in computerized adaptive testing,"The purpose of this study was to compare and evaluate five on-line pretest item-calibration/scaling methods in computerized adaptive testing (CAT): marginal maximum likelihood estimate with one EM cycle (OEM), marginal maximum likelihood estimate with multiple EM cycles (MEM), Stocking's Method A, Stocking's Method B, and BILOG/Prior. The five methods were evaluated in terms of item-parameter recovery, using three different sample sizes (300, 1000 and 3000). The MEM method appeared to be the best choice among these, because it produced the smallest parameter-estimation errors for all sample size conditions. MEM and OEM are mathematically similar, although the OEM method produced larger errors. MEM also was preferable to OEM, unless the amount of time involved in iterative computation is a concern. Stocking's Method B also worked very well, but it required anchor items that either would increase test lengths or require larger sample sizes depending on test administration design. Until more appropriate ways of handling sparse data are devised, the BILOG/Prior method may not be a reasonable choice for small sample sizes. Stocking's Method A had the largest weighted total error, as well as a theoretical weakness (i.e., treating estimated ability as true ability); thus, there appeared to be little reason to use it.","['purpose', 'study', 'compare', 'evaluate', 'online', 'pret', 'itemcalibrationscaling', 'method', 'computerized', 'adaptive', 'testing', 'CAT', 'marginal', 'maximum', 'likelihood', 'estimate', 'EM', 'cycle', 'OEM', 'marginal', 'maximum', 'likelihood', 'estimate', 'multiple', 'EM', 'cycle', 'MEM', 'Stockings', 'Method', 'Stockings', 'Method', 'B', 'BILOGPrior', 'method', 'evaluate', 'term', 'itemparameter', 'recovery', 'different', 'sample', 'size', '300', '1000', '3000', 'MEM', 'method', 'appear', 'good', 'choice', 'produce', 'small', 'parameterestimation', 'error', 'sample', 'size', 'condition', 'MEM', 'OEM', 'mathematically', 'similar', 'OEM', 'method', 'produce', 'large', 'error', 'MEM', 'preferable', 'OEM', 'time', 'involve', 'iterative', 'computation', 'concern', 'Stockings', 'Method', 'B', 'work', 'require', 'anchor', 'item', 'increase', 'test', 'length', 'require', 'large', 'sample', 'size', 'depend', 'test', 'administration', 'design', 'appropriate', 'way', 'handle', 'sparse', 'datum', 'devise', 'BILOGPrior', 'method', 'reasonable', 'choice', 'small', 'sample', 'size', 'Stockings', 'Method', 'A', 'large', 'weight', 'total', 'error', 'theoretical', 'weakness', 'ie', 'treat', 'estimated', 'ability', 'true', 'ability', 'appear', 'little', 'reason']","['comparative', 'study', 'online', 'pret', 'itemcalibrationscaling', 'method', 'computerized', 'adaptive', 'testing']",purpose study compare evaluate online pret itemcalibrationscaling method computerized adaptive testing CAT marginal maximum likelihood estimate EM cycle OEM marginal maximum likelihood estimate multiple EM cycle MEM Stockings Method Stockings Method B BILOGPrior method evaluate term itemparameter recovery different sample size 300 1000 3000 MEM method appear good choice produce small parameterestimation error sample size condition MEM OEM mathematically similar OEM method produce large error MEM preferable OEM time involve iterative computation concern Stockings Method B work require anchor item increase test length require large sample size depend test administration design appropriate way handle sparse datum devise BILOGPrior method reasonable choice small sample size Stockings Method A large weight total error theoretical weakness ie treat estimated ability true ability appear little reason,comparative study online pret itemcalibrationscaling method computerized adaptive testing,0.0242274794701297,0.02422901657626986,0.303825482276527,0.6234057528910713,0.02431226878600224,0.0,0.0041505582902018456,0.03238088387188975,0.0832718835278837,0.0
Oshima T.C.; Davey T.C.; Lee K.,Multidimensional linking: Four practical approaches,2000,37,"This paper introduces and evaluates four practical multidimensional linking procedures that are based on the theoretical framework recently proposed by Davey, Oshima, and Lee (1996): (a) the Direct method, (b) the Equated Function method, (c) the Test Characteristic Function (TCF) method, and (d) the Item Characteristic Function (ICF) method. The evaluation was conducted using simulated data. As anticipated, the competing procedures yielded different linking parameter estimates. The TCF and ICF methods were found to be more stable and recovered the true linking parameters better than the other two methods. Furthermore, all procedures were found to be acceptable under almost any of the minimization criteria and offered dramatic improvement over not linking at all. It is recommended that the choice of a linking procedure should depend on the purpose of linking.",Multidimensional linking: Four practical approaches,"This paper introduces and evaluates four practical multidimensional linking procedures that are based on the theoretical framework recently proposed by Davey, Oshima, and Lee (1996): (a) the Direct method, (b) the Equated Function method, (c) the Test Characteristic Function (TCF) method, and (d) the Item Characteristic Function (ICF) method. The evaluation was conducted using simulated data. As anticipated, the competing procedures yielded different linking parameter estimates. The TCF and ICF methods were found to be more stable and recovered the true linking parameters better than the other two methods. Furthermore, all procedures were found to be acceptable under almost any of the minimization criteria and offered dramatic improvement over not linking at all. It is recommended that the choice of a linking procedure should depend on the purpose of linking.","['paper', 'introduce', 'evaluate', 'practical', 'multidimensional', 'link', 'procedure', 'base', 'theoretical', 'framework', 'recently', 'propose', 'Davey', 'Oshima', 'Lee', '1996', 'direct', 'method', 'b', 'Equated', 'Function', 'method', 'c', 'Test', 'Characteristic', 'Function', 'TCF', 'method', 'd', 'Item', 'Characteristic', 'Function', 'ICF', 'method', 'evaluation', 'conduct', 'simulated', 'datum', 'anticipate', 'compete', 'procedure', 'yield', 'different', 'linking', 'parameter', 'estimate', 'TCF', 'ICF', 'method', 'find', 'stable', 'recover', 'true', 'link', 'parameter', 'method', 'furthermore', 'procedure', 'find', 'acceptable', 'minimization', 'criterion', 'offer', 'dramatic', 'improvement', 'link', 'recommend', 'choice', 'link', 'procedure', 'depend', 'purpose', 'link']","['Multidimensional', 'link', 'practical', 'approach']",paper introduce evaluate practical multidimensional link procedure base theoretical framework recently propose Davey Oshima Lee 1996 direct method b Equated Function method c Test Characteristic Function TCF method d Item Characteristic Function ICF method evaluation conduct simulated datum anticipate compete procedure yield different linking parameter estimate TCF ICF method find stable recover true link parameter method furthermore procedure find acceptable minimization criterion offer dramatic improvement link recommend choice link procedure depend purpose link,Multidimensional link practical approach,0.02788420982228993,0.027882849372214795,0.027888878268568364,0.88834828368253,0.02799577885439687,0.0,0.026722101518938966,0.06623720908225543,0.053113050884913984,0.0
De Ayala R.J.; Plake B.S.; Impara J.C.,The impact of omitted responses on the accuracy of ability estimation in item response theory,2001,38,"Practitioners typically face situations in which examinees have not responded to all test items. This study investigated the effect on an examinee's ability estimate when an examinee is presented an item, has ample time to answer, but decides not to respond to the item. Three approaches to ability estimation (biweight estimation, expected a posteriori, and maximum likelihood estimation) were examined. A Monte Carlo study was performed and the effect of different levels of omissions on the simulee's ability estimates was determined. Results showed that the worst estimation occurred when omits were treated as incorrect. In contrast, substitution of 0.5 for omitted responses resulted in ability estimates that were almost as accurate as those using complete data. Implications for practitioners are discussed.",The impact of omitted responses on the accuracy of ability estimation in item response theory,"Practitioners typically face situations in which examinees have not responded to all test items. This study investigated the effect on an examinee's ability estimate when an examinee is presented an item, has ample time to answer, but decides not to respond to the item. Three approaches to ability estimation (biweight estimation, expected a posteriori, and maximum likelihood estimation) were examined. A Monte Carlo study was performed and the effect of different levels of omissions on the simulee's ability estimates was determined. Results showed that the worst estimation occurred when omits were treated as incorrect. In contrast, substitution of 0.5 for omitted responses resulted in ability estimates that were almost as accurate as those using complete data. Implications for practitioners are discussed.","['practitioner', 'typically', 'face', 'situation', 'examinee', 'respond', 'test', 'item', 'study', 'investigate', 'effect', 'examinees', 'ability', 'estimate', 'examinee', 'present', 'item', 'ample', 'time', 'answer', 'decide', 'respond', 'item', 'approach', 'ability', 'estimation', 'biweight', 'estimation', 'expect', 'posteriori', 'maximum', 'likelihood', 'estimation', 'examine', 'A', 'Monte', 'Carlo', 'study', 'perform', 'effect', 'different', 'level', 'omission', 'simulee', 'ability', 'estimate', 'determine', 'result', 'bad', 'estimation', 'occur', 'omit', 'treat', 'incorrect', 'contrast', 'substitution', '05', 'omit', 'response', 'result', 'ability', 'estimate', 'accurate', 'complete', 'datum', 'Implications', 'practitioner', 'discuss']","['impact', 'omit', 'response', 'accuracy', 'ability', 'estimation', 'item', 'response', 'theory']",practitioner typically face situation examinee respond test item study investigate effect examinees ability estimate examinee present item ample time answer decide respond item approach ability estimation biweight estimation expect posteriori maximum likelihood estimation examine A Monte Carlo study perform effect different level omission simulee ability estimate determine result bad estimation occur omit treat incorrect contrast substitution 05 omit response result ability estimate accurate complete datum Implications practitioner discuss,impact omit response accuracy ability estimation item response theory,0.027496902114932886,0.027497788189180646,0.02750906120635016,0.8899689121677137,0.027527336321822707,2.761019340118901e-06,0.02116522336347777,0.0,0.08761470497234133,0.0
Gallagher A.; Bridgeman B.; Cahalan C.,The effect of computer-based tests on racial-ethnic and gender groups,2002,39,"In this study data were examined from several national testing programs to determine whether the change from paper-based administration to computer-based tests (CBTs) influences group differences in performance. Performances by gender, racial, and ethnic groups on the Graduate Record Examination General Test, Graduate Management Admissions Test, SAT I: Reasoning Test, and Praxis: Professional Assessment for Beginning Teachers, were analyzed to determine whether the shift in testing format from paper-and-pencil tests to CBTs posed a disadvantage to any of these subgroups, beyond that already identified for paper-based test. Although all differences were quite small, some consistent patterns were found for some racial-ethnic and gender groups. African-American examinees and, to a lesser degree, Hispanic examinees appear to benefit from the CBT format. On some tests, female examinees' performance was relatively lower on the CBT version.",The effect of computer-based tests on racial-ethnic and gender groups,"In this study data were examined from several national testing programs to determine whether the change from paper-based administration to computer-based tests (CBTs) influences group differences in performance. Performances by gender, racial, and ethnic groups on the Graduate Record Examination General Test, Graduate Management Admissions Test, SAT I: Reasoning Test, and Praxis: Professional Assessment for Beginning Teachers, were analyzed to determine whether the shift in testing format from paper-and-pencil tests to CBTs posed a disadvantage to any of these subgroups, beyond that already identified for paper-based test. Although all differences were quite small, some consistent patterns were found for some racial-ethnic and gender groups. African-American examinees and, to a lesser degree, Hispanic examinees appear to benefit from the CBT format. On some tests, female examinees' performance was relatively lower on the CBT version.","['study', 'datum', 'examine', 'national', 'testing', 'program', 'determine', 'change', 'paperbase', 'administration', 'computerbase', 'test', 'CBTs', 'influence', 'group', 'difference', 'performance', 'Performances', 'gender', 'racial', 'ethnic', 'group', 'Graduate', 'Record', 'Examination', 'General', 'Test', 'Graduate', 'Management', 'Admissions', 'Test', 'SAT', 'I', 'Reasoning', 'Test', 'Praxis', 'Professional', 'Assessment', 'Beginning', 'Teachers', 'analyze', 'determine', 'shift', 'testing', 'format', 'paperandpencil', 'test', 'cbt', 'pose', 'disadvantage', 'subgroup', 'identify', 'paperbase', 'test', 'difference', 'small', 'consistent', 'pattern', 'find', 'racialethnic', 'gender', 'group', 'AfricanAmerican', 'examine', 'degree', 'hispanic', 'examinee', 'appear', 'benefit', 'CBT', 'format', 'test', 'female', 'examinee', 'performance', 'relatively', 'low', 'CBT', 'version']","['effect', 'computerbase', 'test', 'racialethnic', 'gender', 'group']",study datum examine national testing program determine change paperbase administration computerbase test CBTs influence group difference performance Performances gender racial ethnic group Graduate Record Examination General Test Graduate Management Admissions Test SAT I Reasoning Test Praxis Professional Assessment Beginning Teachers analyze determine shift testing format paperandpencil test cbt pose disadvantage subgroup identify paperbase test difference small consistent pattern find racialethnic gender group AfricanAmerican examine degree hispanic examinee appear benefit CBT format test female examinee performance relatively low CBT version,effect computerbase test racialethnic gender group,0.026763310662531917,0.026607384659928294,0.026708941071823995,0.4214630657518995,0.4984572978538164,0.024775289588758617,0.021112204885811284,0.002631757154574275,0.010775810878636338,0.05482216564480354
Bolt D.M.,A SIBTEST approach to testing DIF hypotheses using experimentally designed test items,2000,37,"This paper considers a modification of the DIF procedure SIBTEST for investigating the causes of differential item functioning (DIF). One way in which factors believed to be responsible for DIF can be investigated is by systematically manipulating them across multiple versions of an item using a randomized DIF study (Schmitt, Holland, & Dorans, 1993). In this paper, it is shown that the additivity of the index used for testing DIF in SIBTEST motivates a new extension of the method for statistically testing the effects of DIF factors. Because an important consideration is whether or not a studied DIF factor is consistent in its effects across items, a methodology for testing item × factor interactions is also presented. Using data from the mathematical sections of the Scholastic Assessment Test (SAT), the effects of two potential DIP factors - item formal (multiple-choice versus open-ended) and problem type (abstract versus concrete) - are investigated for gender. Results suggest a small but statistically significant and consistent effect of item format (favoring males for multiple-choice items) across items, and a larger but less consistent effect due to problem type.",A SIBTEST approach to testing DIF hypotheses using experimentally designed test items,"This paper considers a modification of the DIF procedure SIBTEST for investigating the causes of differential item functioning (DIF). One way in which factors believed to be responsible for DIF can be investigated is by systematically manipulating them across multiple versions of an item using a randomized DIF study (Schmitt, Holland, & Dorans, 1993). In this paper, it is shown that the additivity of the index used for testing DIF in SIBTEST motivates a new extension of the method for statistically testing the effects of DIF factors. Because an important consideration is whether or not a studied DIF factor is consistent in its effects across items, a methodology for testing item × factor interactions is also presented. Using data from the mathematical sections of the Scholastic Assessment Test (SAT), the effects of two potential DIP factors - item formal (multiple-choice versus open-ended) and problem type (abstract versus concrete) - are investigated for gender. Results suggest a small but statistically significant and consistent effect of item format (favoring males for multiple-choice items) across items, and a larger but less consistent effect due to problem type.","['paper', 'consider', 'modification', 'DIF', 'procedure', 'sibtest', 'investigate', 'cause', 'differential', 'item', 'function', 'dif', 'way', 'factor', 'believe', 'responsible', 'DIF', 'investigate', 'systematically', 'manipulate', 'multiple', 'version', 'item', 'randomize', 'dif', 'study', 'Schmitt', 'Holland', 'Dorans', '1993', 'paper', 'additivity', 'index', 'test', 'DIF', 'SIBTEST', 'motivate', 'new', 'extension', 'method', 'statistically', 'test', 'effect', 'dif', 'factor', 'important', 'consideration', 'study', 'DIF', 'factor', 'consistent', 'effect', 'item', 'methodology', 'testing', 'item', '×', 'factor', 'interaction', 'present', 'datum', 'mathematical', 'section', 'Scholastic', 'Assessment', 'Test', 'sit', 'effect', 'potential', 'DIP', 'factor', 'item', 'formal', 'multiplechoice', 'versus', 'openende', 'problem', 'type', 'abstract', 'versus', 'concrete', 'investigate', 'gender', 'result', 'suggest', 'small', 'statistically', 'significant', 'consistent', 'effect', 'item', 'format', 'favor', 'male', 'multiplechoice', 'item', 'item', 'large', 'consistent', 'effect', 'problem', 'type']","['sibtest', 'approach', 'test', 'DIF', 'hypothesis', 'experimentally', 'design', 'test', 'item']",paper consider modification DIF procedure sibtest investigate cause differential item function dif way factor believe responsible DIF investigate systematically manipulate multiple version item randomize dif study Schmitt Holland Dorans 1993 paper additivity index test DIF SIBTEST motivate new extension method statistically test effect dif factor important consideration study DIF factor consistent effect item methodology testing item × factor interaction present datum mathematical section Scholastic Assessment Test sit effect potential DIP factor item formal multiplechoice versus openende problem type abstract versus concrete investigate gender result suggest small statistically significant consistent effect item format favor male multiplechoice item item large consistent effect problem type,sibtest approach test DIF hypothesis experimentally design test item,0.025669890416312086,0.025672534574487243,0.027335659104651463,0.8944343825101436,0.02688753339440555,0.13599255416334782,0.0,0.0,0.014748383897666485,0.0
Hoskens M.; Wilson M.,Real-time feedback on rater drift in constructed-response items: An example from the golden state examination,2001,38,"In this study, patterns of variation in severities of a group of raters over time or so-called ""rater drift"" was examined when raters scored an essay written under examination conditions. At the same time feedback was given to rater leaders (called ""table leaders"") who then interpreted the feedback and reported to the raters. Rater severities in five successive periods were estimated using a modified linear logistic test model (LLTM, Fischer, 1973) approach. It was found that the raters did indeed drift towards the mean, but a planned comparision of the feedback with a control condition was not successful; it was believed that this was due to contamination at the table leader level. A series of models was also estimated designed to detect other types of rater effects beyond severity: a tendency to use extreme scores, and tendency to prefer certain categories. The models for these effects were found to be showing significant improvement in fit, implying that these effects were indeed present, although they were difficult to detect in relatively short time periods.",Real-time feedback on rater drift in constructed-response items: An example from the golden state examination,"In this study, patterns of variation in severities of a group of raters over time or so-called ""rater drift"" was examined when raters scored an essay written under examination conditions. At the same time feedback was given to rater leaders (called ""table leaders"") who then interpreted the feedback and reported to the raters. Rater severities in five successive periods were estimated using a modified linear logistic test model (LLTM, Fischer, 1973) approach. It was found that the raters did indeed drift towards the mean, but a planned comparision of the feedback with a control condition was not successful; it was believed that this was due to contamination at the table leader level. A series of models was also estimated designed to detect other types of rater effects beyond severity: a tendency to use extreme scores, and tendency to prefer certain categories. The models for these effects were found to be showing significant improvement in fit, implying that these effects were indeed present, although they were difficult to detect in relatively short time periods.","['study', 'pattern', 'variation', 'severity', 'group', 'rater', 'time', 'socalle', 'rater', 'drift', 'examine', 'rater', 'score', 'essay', 'write', 'examination', 'condition', 'time', 'feedback', 'rater', 'leader', 'table', 'leader', 'interpret', 'feedback', 'report', 'rater', 'Rater', 'severitie', 'successive', 'period', 'estimate', 'modify', 'linear', 'logistic', 'test', 'lltm', 'Fischer', '1973', 'approach', 'find', 'rater', 'drift', 'mean', 'plan', 'comparision', 'feedback', 'control', 'condition', 'successful', 'believe', 'contamination', 'table', 'leader', 'level', 'series', 'estimate', 'design', 'detect', 'type', 'rater', 'effect', 'severity', 'tendency', 'extreme', 'score', 'tendency', 'prefer', 'certain', 'category', 'effect', 'find', 'significant', 'improvement', 'fit', 'imply', 'effect', 'present', 'difficult', 'detect', 'relatively', 'short', 'time', 'period']","['Realtime', 'feedback', 'rater', 'drift', 'constructedresponse', 'item', 'example', 'golden', 'state', 'examination']",study pattern variation severity group rater time socalle rater drift examine rater score essay write examination condition time feedback rater leader table leader interpret feedback report rater Rater severitie successive period estimate modify linear logistic test lltm Fischer 1973 approach find rater drift mean plan comparision feedback control condition successful believe contamination table leader level series estimate design detect type rater effect severity tendency extreme score tendency prefer certain category effect find significant improvement fit imply effect present difficult detect relatively short time period,Realtime feedback rater drift constructedresponse item example golden state examination,0.028783643515461727,0.0287852611736154,0.028779583591952207,0.7222486777645291,0.19140283395444163,0.0021236342582216976,0.0040004668632505624,0.0,0.018398932572746107,0.07547691820618248
Kamata A.,Item analysis by the hierarchical generalized linear model,2001,38,"The hierarchical generalized linear model (HGLM) is presented as an explicit, two-level formulation of a multilevel item response model. In this paper, it is shown that the HGLM is equivalent to the Rasch model and that, characteristic of the HGLM, person ability can be expressed in the form of random effects rather than parameters. The two-level item analysis model is presented as a latent regression model with person-characteristic variables. Furthermore, it is shown that the two-level HGLM model can be extended to a three-level latent regression model that permits investigation of the variation of students' performance across groups, such as is found in classrooms and schools, and of the interactive effect of person-and group-characteristic variables.",Item analysis by the hierarchical generalized linear model,"The hierarchical generalized linear model (HGLM) is presented as an explicit, two-level formulation of a multilevel item response model. In this paper, it is shown that the HGLM is equivalent to the Rasch model and that, characteristic of the HGLM, person ability can be expressed in the form of random effects rather than parameters. The two-level item analysis model is presented as a latent regression model with person-characteristic variables. Furthermore, it is shown that the two-level HGLM model can be extended to a three-level latent regression model that permits investigation of the variation of students' performance across groups, such as is found in classrooms and schools, and of the interactive effect of person-and group-characteristic variables.","['hierarchical', 'generalize', 'linear', 'HGLM', 'present', 'explicit', 'twolevel', 'formulation', 'multilevel', 'item', 'response', 'paper', 'HGLM', 'equivalent', 'Rasch', 'characteristic', 'HGLM', 'person', 'ability', 'express', 'form', 'random', 'effect', 'parameter', 'twolevel', 'item', 'analysis', 'present', 'latent', 'regression', 'personcharacteristic', 'variable', 'furthermore', 'twolevel', 'HGLM', 'extend', 'threelevel', 'latent', 'regression', 'permit', 'investigation', 'variation', 'student', 'performance', 'group', 'find', 'classroom', 'school', 'interactive', 'effect', 'personand', 'groupcharacteristic', 'variable']","['item', 'analysis', 'hierarchical', 'generalize', 'linear']",hierarchical generalize linear HGLM present explicit twolevel formulation multilevel item response paper HGLM equivalent Rasch characteristic HGLM person ability express form random effect parameter twolevel item analysis present latent regression personcharacteristic variable furthermore twolevel HGLM extend threelevel latent regression permit investigation variation student performance group find classroom school interactive effect personand groupcharacteristic variable,item analysis hierarchical generalize linear,0.03127855786236983,0.031279427898366505,0.03133008754555617,0.22839337284840758,0.6777185538452999,0.01272068109522692,0.0,0.0,0.02076016208095488,0.0611608857740898
Yi Q.; Wang T.; Ban J.-C.,Effects of scale transformation and test-termination rule on the precision of ability estimation in computerized adaptive testing,2001,38,"Error indices (bias, standard error of estimation, and root mean squared error) obtained on different measurement scales under different test-termination rules in computerized adaptive testing (CAT) were examined. Four ability estimation methods (maximum likelihood estimation, weighted likelihood estimation, expected a posterior, and maximum a posterior), three measurement scales (θ, number-correct score, and ACT score), and three test-termination rules (fixed length, fixed standard error, and target information) were studied for a real and a generated item pool. The findings indicated that the amount and direction of bias, standard error of estimation, and root mean squared error obtained under different ability estimation methods were influenced both by scale transformations and by test-termination rules in a CAT environment. The implications of these effects for testing programs are discussed.",Effects of scale transformation and test-termination rule on the precision of ability estimation in computerized adaptive testing,"Error indices (bias, standard error of estimation, and root mean squared error) obtained on different measurement scales under different test-termination rules in computerized adaptive testing (CAT) were examined. Four ability estimation methods (maximum likelihood estimation, weighted likelihood estimation, expected a posterior, and maximum a posterior), three measurement scales (θ, number-correct score, and ACT score), and three test-termination rules (fixed length, fixed standard error, and target information) were studied for a real and a generated item pool. The findings indicated that the amount and direction of bias, standard error of estimation, and root mean squared error obtained under different ability estimation methods were influenced both by scale transformations and by test-termination rules in a CAT environment. The implications of these effects for testing programs are discussed.","['error', 'indice', 'bias', 'standard', 'error', 'estimation', 'root', 'mean', 'squared', 'error', 'obtain', 'different', 'scale', 'different', 'testtermination', 'rule', 'computerized', 'adaptive', 'testing', 'CAT', 'examine', 'ability', 'estimation', 'method', 'maximum', 'likelihood', 'estimation', 'weight', 'likelihood', 'estimation', 'expect', 'posterior', 'maximum', 'posterior', 'scale', 'θ', 'numbercorrect', 'score', 'act', 'score', 'testtermination', 'rule', 'fix', 'length', 'fix', 'standard', 'error', 'target', 'information', 'study', 'real', 'generate', 'item', 'pool', 'finding', 'indicate', 'direction', 'bias', 'standard', 'error', 'estimation', 'root', 'mean', 'squared', 'error', 'obtain', 'different', 'ability', 'estimation', 'method', 'influence', 'scale', 'transformation', 'testtermination', 'rule', 'CAT', 'environment', 'implication', 'effect', 'testing', 'program', 'discuss']","['effect', 'scale', 'transformation', 'testtermination', 'rule', 'precision', 'ability', 'estimation', 'computerized', 'adaptive', 'testing']",error indice bias standard error estimation root mean squared error obtain different scale different testtermination rule computerized adaptive testing CAT examine ability estimation method maximum likelihood estimation weight likelihood estimation expect posterior maximum posterior scale θ numbercorrect score act score testtermination rule fix length fix standard error target information study real generate item pool finding indicate direction bias standard error estimation root mean squared error obtain different ability estimation method influence scale transformation testtermination rule CAT environment implication effect testing program discuss,effect scale transformation testtermination rule precision ability estimation computerized adaptive testing,0.02891408176546696,0.028914646294319078,0.02908661209937274,0.8840703098631467,0.029014349977694528,0.0,0.0441624139438985,0.002588799807924374,0.065373701435442,0.0
Kane M.T.,Current concerns in validity theory,2001,38,"We are at the end of the first century of work on models of educational and psychological measurement and into a new millennium. This certainly seems like an appropriate time for looking backward and looking forward in assessment. Furthermore, a new edition of the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 1999) has been published, and the previous editions of the Standards have served as benchmarks in the development of measurement theory. This backward glance will be just that, a glance. After a brief historical review focusing mainly on construct validity, the current state of validity theory will be summarized, with an emphasis on the role of arguments in validation. Then how an argument-based approach might be applied will be examined in regards to two issues in validity theory: the distinction between performance-based and theory-based interpretations, and the role of consequences in validation.",,"We are at the end of the first century of work on models of educational and psychological measurement and into a new millennium. This certainly seems like an appropriate time for looking backward and looking forward in assessment. Furthermore, a new edition of the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 1999) has been published, and the previous editions of the Standards have served as benchmarks in the development of measurement theory. This backward glance will be just that, a glance. After a brief historical review focusing mainly on construct validity, the current state of validity theory will be summarized, with an emphasis on the role of arguments in validation. Then how an argument-based approach might be applied will be examined in regards to two issues in validity theory: the distinction between performance-based and theory-based interpretations, and the role of consequences in validation.","['end', 'century', 'work', 'educational', 'psychological', 'new', 'millennium', 'certainly', 'like', 'appropriate', 'time', 'look', 'backward', 'look', 'forward', 'assessment', 'furthermore', 'new', 'edition', 'Standards', 'Educational', 'Psychological', 'Testing', 'AERA', 'APA', 'NCME', '1999', 'publish', 'previous', 'edition', 'Standards', 'serve', 'benchmark', 'development', 'theory', 'backward', 'glance', 'glance', 'brief', 'historical', 'review', 'focus', 'mainly', 'construct', 'validity', 'current', 'state', 'validity', 'theory', 'summarize', 'emphasis', 'role', 'argument', 'validation', 'argumentbased', 'approach', 'apply', 'examine', 'regard', 'issue', 'validity', 'theory', 'distinction', 'performancebase', 'theorybase', 'interpretation', 'role', 'consequence', 'validation']",,end century work educational psychological new millennium certainly like appropriate time look backward look forward assessment furthermore new edition Standards Educational Psychological Testing AERA APA NCME 1999 publish previous edition Standards serve benchmark development theory backward glance glance brief historical review focus mainly construct validity current state validity theory summarize emphasis role argument validation argumentbased approach apply examine regard issue validity theory distinction performancebase theorybase interpretation role consequence validation,,0.35164446896058665,0.025505287116622935,0.025498123492284672,0.5717691174503158,0.02558300298018988,0.0,0.05419306625383127,0.0,0.00405128928332985,0.007879364269289844
Pitkin A.K.; Vispoel W.P.,Differences between self-adapted and computerized adaptive tests: A meta-analysis,2001,38,"Self-adapted testing has been described as a variation of computerized adaptive testing that reduces test anxiety and thereby enhances test performance. The purpose of this study was to gain a better understanding of these proposed effects of self-adapted tests (SATs); meta-analysis procedures were used to estimate differences between SATs and computerized adaptive tests (CATs) in proficiency estimates and post-test anxiety levels across studies in which these two types of tests have been compared. After controlling for measurement error, the results showed that SATs yielded proficiency estimates that were 0.12 standard deviation units higher and post-test anxiety levels that were 0.19 standard deviation units lower than those yielded by CATs. We speculate about possible reasons for these differences and discuss advantages and disadvantages of using SATs in operational settings.",Differences between self-adapted and computerized adaptive tests: A meta-analysis,"Self-adapted testing has been described as a variation of computerized adaptive testing that reduces test anxiety and thereby enhances test performance. The purpose of this study was to gain a better understanding of these proposed effects of self-adapted tests (SATs); meta-analysis procedures were used to estimate differences between SATs and computerized adaptive tests (CATs) in proficiency estimates and post-test anxiety levels across studies in which these two types of tests have been compared. After controlling for measurement error, the results showed that SATs yielded proficiency estimates that were 0.12 standard deviation units higher and post-test anxiety levels that were 0.19 standard deviation units lower than those yielded by CATs. We speculate about possible reasons for these differences and discuss advantages and disadvantages of using SATs in operational settings.","['selfadapted', 'testing', 'describe', 'variation', 'computerized', 'adaptive', 'testing', 'reduce', 'test', 'anxiety', 'enhance', 'test', 'performance', 'purpose', 'study', 'gain', 'understanding', 'propose', 'effect', 'selfadapte', 'test', 'sats', 'metaanalysis', 'procedure', 'estimate', 'difference', 'sats', 'computerized', 'adaptive', 'test', 'cat', 'proficiency', 'estimate', 'postt', 'anxiety', 'level', 'study', 'type', 'test', 'compare', 'control', 'error', 'result', 'SATs', 'yield', 'proficiency', 'estimate', '012', 'standard', 'deviation', 'unit', 'high', 'postt', 'anxiety', 'level', '019', 'standard', 'deviation', 'unit', 'low', 'yield', 'cat', 'speculate', 'possible', 'reason', 'difference', 'discuss', 'advantage', 'disadvantage', 'sats', 'operational', 'setting']","['difference', 'selfadapted', 'computerized', 'adaptive', 'test', 'metaanalysis']",selfadapted testing describe variation computerized adaptive testing reduce test anxiety enhance test performance purpose study gain understanding propose effect selfadapte test sats metaanalysis procedure estimate difference sats computerized adaptive test cat proficiency estimate postt anxiety level study type test compare control error result SATs yield proficiency estimate 012 standard deviation unit high postt anxiety level 019 standard deviation unit low yield cat speculate possible reason difference discuss advantage disadvantage sats operational setting,difference selfadapted computerized adaptive test metaanalysis,0.02949025631978657,0.02949125156349786,0.029486254181129812,0.8818459461842653,0.02968629175132049,0.004761657259356622,0.026273941404903762,0.0007749543218957698,0.04478498653428347,0.024059450209278965
Hau K.-T.; Chang H.-H.,Item selection in computerized adaptive testing: Should more discriminating be used first?,2001,38,"During computerized adaptive testing (CAT), items are selected continuously according to the test-taker's estimated ability. The traditional method of attaining the highest efficiency in ability estimation is to select items of maximum Fisher information at the currently estimated ability. Test security has become a problem because high-discrimination items are more likely to be selected and become overexposed. So, there seems to be a tradeoff between high efficiency in ability estimations and balanced usage of items. This series of four studies with simulated data addressed the dilemma by focusing on the notion of whether more or less discriminating items should be used first in CAT. The first study demonstrated that the common maximum information method with Sympson and Hetter (1985) control resulted in the use of more discriminating items first. The remaining studies showed that using items in the reverse order (i.e., less discriminating items first), as described in Chang and Ying's (1999) stratified method had potential advantages: (a) a more balanced item usage and (b) a relatively stable resultant item pool structure with easy and inexpensive management: This stratified method may have ability-estimation efficiency better than or close to that of other methods, particularly for operational item pools when retired items cannot be totally replenished with similar highly discriminating items. It is argued that the judicious selection of items, as in the stratified method, is a more active control of item exposure, which can successfully even out the usage of all items.",Item selection in computerized adaptive testing: Should more discriminating be used first?,"During computerized adaptive testing (CAT), items are selected continuously according to the test-taker's estimated ability. The traditional method of attaining the highest efficiency in ability estimation is to select items of maximum Fisher information at the currently estimated ability. Test security has become a problem because high-discrimination items are more likely to be selected and become overexposed. So, there seems to be a tradeoff between high efficiency in ability estimations and balanced usage of items. This series of four studies with simulated data addressed the dilemma by focusing on the notion of whether more or less discriminating items should be used first in CAT. The first study demonstrated that the common maximum information method with Sympson and Hetter (1985) control resulted in the use of more discriminating items first. The remaining studies showed that using items in the reverse order (i.e., less discriminating items first), as described in Chang and Ying's (1999) stratified method had potential advantages: (a) a more balanced item usage and (b) a relatively stable resultant item pool structure with easy and inexpensive management: This stratified method may have ability-estimation efficiency better than or close to that of other methods, particularly for operational item pools when retired items cannot be totally replenished with similar highly discriminating items. It is argued that the judicious selection of items, as in the stratified method, is a more active control of item exposure, which can successfully even out the usage of all items.","['computerized', 'adaptive', 'testing', 'CAT', 'item', 'select', 'continuously', 'accord', 'testtaker', 'estimate', 'ability', 'traditional', 'method', 'attain', 'high', 'efficiency', 'ability', 'estimation', 'select', 'item', 'maximum', 'Fisher', 'information', 'currently', 'estimate', 'ability', 'test', 'security', 'problem', 'highdiscrimination', 'item', 'likely', 'select', 'overexposed', 'tradeoff', 'high', 'efficiency', 'ability', 'estimation', 'balanced', 'usage', 'item', 'series', 'study', 'simulated', 'datum', 'address', 'dilemma', 'focus', 'notion', 'discriminating', 'item', 'CAT', 'study', 'demonstrate', 'common', 'maximum', 'information', 'method', 'Sympson', 'Hetter', '1985', 'control', 'result', 'discriminating', 'item', 'remain', 'study', 'item', 'reverse', 'order', 'ie', 'discriminating', 'item', 'describe', 'Chang', 'Yings', '1999', 'stratify', 'method', 'potential', 'advantage', 'balanced', 'item', 'usage', 'b', 'relatively', 'stable', 'resultant', 'item', 'pool', 'structure', 'easy', 'inexpensive', 'management', 'stratify', 'method', 'abilityestimation', 'efficiency', 'close', 'method', 'particularly', 'operational', 'item', 'pool', 'retire', 'item', 'totally', 'replenish', 'similar', 'highly', 'discriminate', 'item', 'argue', 'judicious', 'selection', 'item', 'stratified', 'method', 'active', 'control', 'item', 'exposure', 'successfully', 'usage', 'item']","['item', 'selection', 'computerized', 'adaptive', 'testing', 'discriminating']",computerized adaptive testing CAT item select continuously accord testtaker estimate ability traditional method attain high efficiency ability estimation select item maximum Fisher information currently estimate ability test security problem highdiscrimination item likely select overexposed tradeoff high efficiency ability estimation balanced usage item series study simulated datum address dilemma focus notion discriminating item CAT study demonstrate common maximum information method Sympson Hetter 1985 control result discriminating item remain study item reverse order ie discriminating item describe Chang Yings 1999 stratify method potential advantage balanced item usage b relatively stable resultant item pool structure easy inexpensive management stratify method abilityestimation efficiency close method particularly operational item pool retire item totally replenish similar highly discriminate item argue judicious selection item stratified method active control item exposure successfully usage item,item selection computerized adaptive testing discriminating,0.02278604106121524,0.02278570386433074,0.022781412943643525,0.9088110078814867,0.022835834249323884,0.0,0.0,0.0,0.1433354527069083,0.0
Clauser B.E.; Swanson D.B.; Harik P.,Multivariate generalizability analysis of the impact of training and examinee performance information on judgments made in an Angoff-style standard-setting procedure,2002,39,"Cut scores, estimated using the Angoff procedure, are routinely used to make high-stakes classification decisions based on examinee scores. Precision is necessary in estimation of cut scores because of the importance of these decisions. Although much has been written about how these procedures should be implemented, there is relatively little literature providing empirical support for specific approaches to providing training and feedback to standard-setting judges. This article presents a multivariate generalizability analysis designed to examine the impact of training and feedback on various sources of error in estimation of cut scores for a standard-setting procedure in which multiple independent groups completed the judgments. The results indicate that after training, there was little improvement in the ability of judges to rank order items by difficulty but there was a substantial improvement in inter-judge consistency in centering ratings. The results also show a substantial group effect. Consistent with this result, the direction of change for the estimated cut score was shown to be group dependent.",Multivariate generalizability analysis of the impact of training and examinee performance information on judgments made in an Angoff-style standard-setting procedure,"Cut scores, estimated using the Angoff procedure, are routinely used to make high-stakes classification decisions based on examinee scores. Precision is necessary in estimation of cut scores because of the importance of these decisions. Although much has been written about how these procedures should be implemented, there is relatively little literature providing empirical support for specific approaches to providing training and feedback to standard-setting judges. This article presents a multivariate generalizability analysis designed to examine the impact of training and feedback on various sources of error in estimation of cut scores for a standard-setting procedure in which multiple independent groups completed the judgments. The results indicate that after training, there was little improvement in the ability of judges to rank order items by difficulty but there was a substantial improvement in inter-judge consistency in centering ratings. The results also show a substantial group effect. Consistent with this result, the direction of change for the estimated cut score was shown to be group dependent.","['cut', 'score', 'estimate', 'Angoff', 'procedure', 'routinely', 'highstake', 'classification', 'decision', 'base', 'examinee', 'score', 'Precision', 'necessary', 'estimation', 'cut', 'score', 'importance', 'decision', 'write', 'procedure', 'implement', 'relatively', 'little', 'literature', 'provide', 'empirical', 'support', 'specific', 'approach', 'provide', 'training', 'feedback', 'standardsette', 'judge', 'article', 'present', 'multivariate', 'generalizability', 'analysis', 'design', 'examine', 'impact', 'training', 'feedback', 'source', 'error', 'estimation', 'cut', 'score', 'standardsette', 'procedure', 'multiple', 'independent', 'group', 'complete', 'judgment', 'result', 'indicate', 'training', 'little', 'improvement', 'ability', 'judge', 'rank', 'order', 'item', 'difficulty', 'substantial', 'improvement', 'interjudge', 'consistency', 'center', 'rating', 'result', 'substantial', 'group', 'effect', 'Consistent', 'result', 'direction', 'change', 'estimate', 'cut', 'score', 'group', 'dependent']","['multivariate', 'generalizability', 'analysis', 'impact', 'training', 'examinee', 'performance', 'information', 'judgment', 'Angoffstyle', 'standardsetting', 'procedure']",cut score estimate Angoff procedure routinely highstake classification decision base examinee score Precision necessary estimation cut score importance decision write procedure implement relatively little literature provide empirical support specific approach provide training feedback standardsette judge article present multivariate generalizability analysis design examine impact training feedback source error estimation cut score standardsette procedure multiple independent group complete judgment result indicate training little improvement ability judge rank order item difficulty substantial improvement interjudge consistency center rating result substantial group effect Consistent result direction change estimate cut score group dependent,multivariate generalizability analysis impact training examinee performance information judgment Angoffstyle standardsetting procedure,0.025302017796537096,0.025303031611340496,0.17887762992337766,0.7448628746570513,0.025654446011693477,0.004671936195083507,0.12157935736752797,0.000744952204030768,0.004790674375096747,0.022654933907168554
Gierl M.J.; Khaliq S.N.,Identifying sources of differential item and bundle functioning on translated achievement tests: A confirmatory analysis,2001,38,"Increasingly, tests are being translated and adapted into different languages. Differential item functioning (DIF) analyses are often used to identify non-equivalent items across language groups. However, few studies have focused on understanding why some translated items produce DIF. The purpose of the current study is to identify sources of differential item and bundle functioning on translated achievement tests using substantive and statistical analyses. A substantive analysis of existing DIF items was conducted by an 11-member committee of testing specialists. In their review, four sources of translation DIF were identified. Two certified translators used these four sources to categorize a new set of DIF items from Grade 6 and 9 Mathematics and Social Studies Achievement Tests. Each item was associated with a specific source of translation DIF and each item was anticipated to favor a specific group of examinees. Then, a statistical analysis was conducted on the items in each category using SIBTEST. The translators sorted the mathematics DIF items into three sources, and they correctly predicted the group that would he favored for seven of the eight items or bundles of items across two grade levels. The translators sorted the social studies DIF items into four sources, and they correctly predicted the group that would be favored for eight of the 13 items or bundles of items across two grade levels. The majority of items in mathematics and social studies were associated with differences in the words, expressions, or sentence structure of items that are not inherent to the language and/or culture. By combining substantive and statistical DIF analyses, researchers can study the sources of DIF and create a body of confirmed DIF hypotheses that may be used to develop guidelines and test construction principles for reducing DIF on translated tests.",Identifying sources of differential item and bundle functioning on translated achievement tests: A confirmatory analysis,"Increasingly, tests are being translated and adapted into different languages. Differential item functioning (DIF) analyses are often used to identify non-equivalent items across language groups. However, few studies have focused on understanding why some translated items produce DIF. The purpose of the current study is to identify sources of differential item and bundle functioning on translated achievement tests using substantive and statistical analyses. A substantive analysis of existing DIF items was conducted by an 11-member committee of testing specialists. In their review, four sources of translation DIF were identified. Two certified translators used these four sources to categorize a new set of DIF items from Grade 6 and 9 Mathematics and Social Studies Achievement Tests. Each item was associated with a specific source of translation DIF and each item was anticipated to favor a specific group of examinees. Then, a statistical analysis was conducted on the items in each category using SIBTEST. The translators sorted the mathematics DIF items into three sources, and they correctly predicted the group that would he favored for seven of the eight items or bundles of items across two grade levels. The translators sorted the social studies DIF items into four sources, and they correctly predicted the group that would be favored for eight of the 13 items or bundles of items across two grade levels. The majority of items in mathematics and social studies were associated with differences in the words, expressions, or sentence structure of items that are not inherent to the language and/or culture. By combining substantive and statistical DIF analyses, researchers can study the sources of DIF and create a body of confirmed DIF hypotheses that may be used to develop guidelines and test construction principles for reducing DIF on translated tests.","['increasingly', 'test', 'translate', 'adapt', 'different', 'language', 'differential', 'item', 'function', 'dif', 'analysis', 'identify', 'nonequivalent', 'item', 'language', 'group', 'study', 'focus', 'understand', 'translate', 'item', 'produce', 'DIF', 'purpose', 'current', 'study', 'identify', 'source', 'differential', 'item', 'bundle', 'function', 'translate', 'achievement', 'test', 'substantive', 'statistical', 'analysis', 'substantive', 'analysis', 'exist', 'dif', 'item', 'conduct', '11member', 'committee', 'testing', 'specialist', 'review', 'source', 'translation', 'DIF', 'identify', 'certify', 'translator', 'source', 'categorize', 'new', 'set', 'dif', 'item', 'Grade', '6', '9', 'Mathematics', 'Social', 'Studies', 'Achievement', 'test', 'item', 'associate', 'specific', 'source', 'translation', 'DIF', 'item', 'anticipate', 'favor', 'specific', 'group', 'examine', 'statistical', 'analysis', 'conduct', 'item', 'category', 'SIBTEST', 'translator', 'sort', 'mathematic', 'DIF', 'item', 'source', 'correctly', 'predict', 'group', 'favor', 'seven', 'item', 'bundle', 'item', 'grade', 'level', 'translator', 'sort', 'social', 'study', 'DIF', 'item', 'source', 'correctly', 'predict', 'group', 'favor', '13', 'item', 'bundle', 'item', 'grade', 'level', 'majority', 'item', 'mathematic', 'social', 'study', 'associate', 'difference', 'word', 'expression', 'sentence', 'structure', 'item', 'inherent', 'language', 'andor', 'culture', 'combine', 'substantive', 'statistical', 'dif', 'analyse', 'researcher', 'study', 'source', 'DIF', 'create', 'body', 'confirmed', 'DIF', 'hypothesis', 'develop', 'guideline', 'test', 'construction', 'principle', 'reduce', 'DIF', 'translate', 'test']","['identify', 'source', 'differential', 'item', 'bundle', 'function', 'translate', 'achievement', 'test', 'confirmatory', 'analysis']",increasingly test translate adapt different language differential item function dif analysis identify nonequivalent item language group study focus understand translate item produce DIF purpose current study identify source differential item bundle function translate achievement test substantive statistical analysis substantive analysis exist dif item conduct 11member committee testing specialist review source translation DIF identify certify translator source categorize new set dif item Grade 6 9 Mathematics Social Studies Achievement test item associate specific source translation DIF item anticipate favor specific group examine statistical analysis conduct item category SIBTEST translator sort mathematic DIF item source correctly predict group favor seven item bundle item grade level translator sort social study DIF item source correctly predict group favor 13 item bundle item grade level majority item mathematic social study associate difference word expression sentence structure item inherent language andor culture combine substantive statistical dif analyse researcher study source DIF create body confirmed DIF hypothesis develop guideline test construction principle reduce DIF translate test,identify source differential item bundle function translate achievement test confirmatory analysis,0.025630121020673434,0.025628500027620234,0.025659356907747396,0.3094381622995046,0.6136438597444542,0.13991005798364098,0.0,0.0,0.003087309745675656,0.029572995501382936
Vispoel W.P.; Hendrickson A.B.; Bleiler T.,Limiting answer review and change on computerized adaptive vocabulary tests: Psychometric and attitudinal results,2000,37,"Previous simulation studies of computerized adaptive tests (CATs) have revealed that the validity and precision of proficiency estimates can be maintained when review opportunities are limited to items within successive blocks. Our purpose in this study was to evaluate the effectiveness of CATs with such restricted review options in a live testing setting. Vocabulary CATs were compared under four conditions: (a) no item review allowed, (b) review allowed only within successive 5-item blocks, (c) review allowed only within successive 10-item blocks, and (d) review allowed only after answering all 40 items. Results revealed no trustworthy differences among conditions in vocabulary proficiency estimates, measurement error, or testing time. Within each review condition, ability estimates and number correct scores increased slightly after review, more answers were changed from wrong to right than from right to wrong, most examinees who changed answers improved proficiency estimates by doing so, and nearly all examinees indicated that they had an adequate opportunity to review their previous answers. These results suggest that restricting review opportunities on CATs may provide a viable way to satisfy examinee desires, maintain validity and measurement precision, and keep testing time at acceptable levels.",Limiting answer review and change on computerized adaptive vocabulary tests: Psychometric and attitudinal results,"Previous simulation studies of computerized adaptive tests (CATs) have revealed that the validity and precision of proficiency estimates can be maintained when review opportunities are limited to items within successive blocks. Our purpose in this study was to evaluate the effectiveness of CATs with such restricted review options in a live testing setting. Vocabulary CATs were compared under four conditions: (a) no item review allowed, (b) review allowed only within successive 5-item blocks, (c) review allowed only within successive 10-item blocks, and (d) review allowed only after answering all 40 items. Results revealed no trustworthy differences among conditions in vocabulary proficiency estimates, measurement error, or testing time. Within each review condition, ability estimates and number correct scores increased slightly after review, more answers were changed from wrong to right than from right to wrong, most examinees who changed answers improved proficiency estimates by doing so, and nearly all examinees indicated that they had an adequate opportunity to review their previous answers. These results suggest that restricting review opportunities on CATs may provide a viable way to satisfy examinee desires, maintain validity and measurement precision, and keep testing time at acceptable levels.","['previous', 'simulation', 'study', 'computerized', 'adaptive', 'test', 'cat', 'reveal', 'validity', 'precision', 'proficiency', 'estimate', 'maintain', 'review', 'opportunity', 'limit', 'item', 'successive', 'block', 'purpose', 'study', 'evaluate', 'effectiveness', 'cat', 'restricted', 'review', 'option', 'live', 'testing', 'set', 'Vocabulary', 'cat', 'compare', 'condition', 'item', 'review', 'allow', 'b', 'review', 'allow', 'successive', '5item', 'block', 'c', 'review', 'allow', 'successive', '10item', 'block', 'd', 'review', 'allow', 'answer', '40', 'item', 'result', 'reveal', 'trustworthy', 'difference', 'condition', 'vocabulary', 'proficiency', 'estimate', 'error', 'testing', 'time', 'review', 'condition', 'ability', 'estimate', 'number', 'correct', 'score', 'increase', 'slightly', 'review', 'answer', 'change', 'wrong', 'right', 'right', 'wrong', 'examinee', 'change', 'answer', 'improved', 'proficiency', 'estimate', 'nearly', 'examinee', 'indicate', 'adequate', 'opportunity', 'review', 'previous', 'answer', 'result', 'suggest', 'restrict', 'review', 'opportunity', 'cat', 'provide', 'viable', 'way', 'satisfy', 'examinee', 'desire', 'maintain', 'validity', 'precision', 'test', 'time', 'acceptable', 'level']","['limit', 'answer', 'review', 'change', 'computerized', 'adaptive', 'vocabulary', 'test', 'psychometric', 'attitudinal', 'result']",previous simulation study computerized adaptive test cat reveal validity precision proficiency estimate maintain review opportunity limit item successive block purpose study evaluate effectiveness cat restricted review option live testing set Vocabulary cat compare condition item review allow b review allow successive 5item block c review allow successive 10item block d review allow answer 40 item result reveal trustworthy difference condition vocabulary proficiency estimate error testing time review condition ability estimate number correct score increase slightly review answer change wrong right right wrong examinee change answer improved proficiency estimate nearly examinee indicate adequate opportunity review previous answer result suggest restrict review opportunity cat provide viable way satisfy examinee desire maintain validity precision test time acceptable level,limit answer review change computerized adaptive vocabulary test psychometric attitudinal result,0.02769382516544209,0.027682096979641007,0.027685268642201492,0.8891520259048038,0.027786783307911594,0.0,0.004009568748300222,0.0,0.07790697326827643,0.005966760901966359
Caruso J.C.; Witkiewitz K.,Increasing the reliability of ability-achievement difference scores: An example using the Kaufman assessment battery for children,2002,39,"In this study, we focused on increasing the reliability of ability-achievement difference scores using the Kaufman Assessment Battery for Children (KABC) as an example. Ability-achievement difference scores are often used as indicators of learning disabilities, but when they are derived from traditional equally weighted ability and achievement scores, they have suboptimal psychometric properties because of the high correlations between the scores. As an alternative to equally weighted difference scores, we examined an orthogonal reliable component analysis (RCA) solution and an oblique principal component analysis (PCA) solution for the standardization sample of the KABC (among 5- to 12-year-olds). The components were easily identifiable as the simultaneous processing, sequential processing, and achievement constructs assessed by the KABC. As judged via the score intercorrelations, all three types of scores had adequate convergent validity, while the orthogonal RCA scores had superior discriminant validity, followed by the oblique PCA scores. Differences between the orthogonal RCA scores were more reliable than differences between the oblique PCA scores, which were in turn more reliable than differences between the traditional equally weighted scores. The increased reliability with which the KABC differences are assessed with the orthogonal RCA method has important practical implications, including narrower confidence intervals around difference scores used in individual administrations of the KABC.",Increasing the reliability of ability-achievement difference scores: An example using the Kaufman assessment battery for children,"In this study, we focused on increasing the reliability of ability-achievement difference scores using the Kaufman Assessment Battery for Children (KABC) as an example. Ability-achievement difference scores are often used as indicators of learning disabilities, but when they are derived from traditional equally weighted ability and achievement scores, they have suboptimal psychometric properties because of the high correlations between the scores. As an alternative to equally weighted difference scores, we examined an orthogonal reliable component analysis (RCA) solution and an oblique principal component analysis (PCA) solution for the standardization sample of the KABC (among 5- to 12-year-olds). The components were easily identifiable as the simultaneous processing, sequential processing, and achievement constructs assessed by the KABC. As judged via the score intercorrelations, all three types of scores had adequate convergent validity, while the orthogonal RCA scores had superior discriminant validity, followed by the oblique PCA scores. Differences between the orthogonal RCA scores were more reliable than differences between the oblique PCA scores, which were in turn more reliable than differences between the traditional equally weighted scores. The increased reliability with which the KABC differences are assessed with the orthogonal RCA method has important practical implications, including narrower confidence intervals around difference scores used in individual administrations of the KABC.","['study', 'focus', 'increase', 'reliability', 'abilityachievement', 'difference', 'score', 'Kaufman', 'Assessment', 'Battery', 'Children', 'KABC', 'example', 'Abilityachievement', 'difference', 'score', 'indicator', 'learn', 'disability', 'derive', 'traditional', 'equally', 'weight', 'ability', 'achievement', 'score', 'suboptimal', 'psychometric', 'property', 'high', 'correlation', 'score', 'alternative', 'equally', 'weight', 'difference', 'score', 'examine', 'orthogonal', 'reliable', 'component', 'analysis', 'rca', 'solution', 'oblique', 'principal', 'component', 'analysis', 'pca', 'solution', 'standardization', 'sample', 'KABC', '5', '12yearolds', 'component', 'easily', 'identifiable', 'simultaneous', 'processing', 'sequential', 'processing', 'achievement', 'construct', 'assess', 'KABC', 'judge', 'score', 'intercorrelation', 'type', 'score', 'adequate', 'convergent', 'validity', 'orthogonal', 'RCA', 'score', 'superior', 'discriminant', 'validity', 'follow', 'oblique', 'PCA', 'score', 'difference', 'orthogonal', 'RCA', 'score', 'reliable', 'difference', 'oblique', 'PCA', 'score', 'turn', 'reliable', 'difference', 'traditional', 'equally', 'weight', 'score', 'increase', 'reliability', 'KABC', 'difference', 'assess', 'orthogonal', 'RCA', 'method', 'important', 'practical', 'implication', 'include', 'narrow', 'confidence', 'interval', 'difference', 'score', 'individual', 'administration', 'KABC']","['increase', 'reliability', 'abilityachievement', 'difference', 'score', 'example', 'Kaufman', 'assessment', 'battery', 'child']",study focus increase reliability abilityachievement difference score Kaufman Assessment Battery Children KABC example Abilityachievement difference score indicator learn disability derive traditional equally weight ability achievement score suboptimal psychometric property high correlation score alternative equally weight difference score examine orthogonal reliable component analysis rca solution oblique principal component analysis pca solution standardization sample KABC 5 12yearolds component easily identifiable simultaneous processing sequential processing achievement construct assess KABC judge score intercorrelation type score adequate convergent validity orthogonal RCA score superior discriminant validity follow oblique PCA score difference orthogonal RCA score reliable difference oblique PCA score turn reliable difference traditional equally weight score increase reliability KABC difference assess orthogonal RCA method important practical implication include narrow confidence interval difference score individual administration KABC,increase reliability abilityachievement difference score example Kaufman assessment battery child,0.027007690558811817,0.02700459695428904,0.502476183410417,0.41611788660018617,0.02739364247629594,0.0,0.07329577058245752,0.01251329586108407,0.0,0.0367883909637838
Clauser B.E.; Harik P.; Clyman S.G.,The generalizability of scores for a performance assessment scored with a computer-automated scoring system,2000,37,"When performance assessments are delivered and scored by computer, the costs of scoring may be substantially lower than those of scoring the same assessment based on expert review of the individual performances. Computerized scoring algorithms also ensure that the scoring rules are implemented precisely and uniformly. Such computerized algorithms represent an effort to encode the scoring policies of experts. This raises the question, would a different group of experts have produced a meaningfully different algorithm? The research reported in this paper uses generalizability theory to assess the impact of using independent, randomly equivalent groups of experts to develop the scoring algorithms for a set of computer-simulation tasks designed to measure physicians' patient management skills. The results suggest that the impact of this ""expert group"" effect may be significant but that it can be controlled with appropriate test development strategies. The appendix presents multivariate generalizability analysis to examine the stability of the assessed proficiency across scores representing the scoring policies of different groups of experts.",The generalizability of scores for a performance assessment scored with a computer-automated scoring system,"When performance assessments are delivered and scored by computer, the costs of scoring may be substantially lower than those of scoring the same assessment based on expert review of the individual performances. Computerized scoring algorithms also ensure that the scoring rules are implemented precisely and uniformly. Such computerized algorithms represent an effort to encode the scoring policies of experts. This raises the question, would a different group of experts have produced a meaningfully different algorithm? The research reported in this paper uses generalizability theory to assess the impact of using independent, randomly equivalent groups of experts to develop the scoring algorithms for a set of computer-simulation tasks designed to measure physicians' patient management skills. The results suggest that the impact of this ""expert group"" effect may be significant but that it can be controlled with appropriate test development strategies. The appendix presents multivariate generalizability analysis to examine the stability of the assessed proficiency across scores representing the scoring policies of different groups of experts.","['performance', 'assessment', 'deliver', 'score', 'computer', 'cost', 'scoring', 'substantially', 'low', 'score', 'assessment', 'base', 'expert', 'review', 'individual', 'performance', 'computerized', 'scoring', 'algorithm', 'ensure', 'scoring', 'rule', 'implement', 'precisely', 'uniformly', 'computerized', 'algorithm', 'represent', 'effort', 'encode', 'scoring', 'policy', 'expert', 'raise', 'question', 'different', 'group', 'expert', 'produce', 'meaningfully', 'different', 'algorithm', 'research', 'report', 'paper', 'generalizability', 'theory', 'assess', 'impact', 'independent', 'randomly', 'equivalent', 'group', 'expert', 'develop', 'scoring', 'algorithm', 'set', 'computersimulation', 'task', 'design', 'measure', 'physician', 'patient', 'management', 'skill', 'result', 'suggest', 'impact', 'expert', 'group', 'effect', 'significant', 'control', 'appropriate', 'test', 'development', 'strategy', 'appendix', 'present', 'multivariate', 'generalizability', 'analysis', 'examine', 'stability', 'assessed', 'proficiency', 'score', 'represent', 'scoring', 'policy', 'different', 'group', 'expert']","['generalizability', 'score', 'performance', 'assessment', 'score', 'computerautomated', 'scoring', 'system']",performance assessment deliver score computer cost scoring substantially low score assessment base expert review individual performance computerized scoring algorithm ensure scoring rule implement precisely uniformly computerized algorithm represent effort encode scoring policy expert raise question different group expert produce meaningfully different algorithm research report paper generalizability theory assess impact independent randomly equivalent group expert develop scoring algorithm set computersimulation task design measure physician patient management skill result suggest impact expert group effect significant control appropriate test development strategy appendix present multivariate generalizability analysis examine stability assessed proficiency score represent scoring policy different group expert,generalizability score performance assessment score computerautomated scoring system,0.02679060213507162,0.026775067563137026,0.026780654957790168,0.892739503252941,0.026914172091060253,0.004077758550677271,0.11116682662462887,0.0,0.0,0.024355123029739804
Spray J.A.; Huang C.-Y.,Obtaining test blueprint weights from job analysis surveys,2000,37,"A method for combining multiple scale responses from job or task surveys based on a hierarchical ranking scheme is presented. A rationale for placing the resulting ordinal information onto an interval scale of measurement using the Rasch Rating Scale Model is also provided. After a simple linear transformation, the item or task parameter estimates can be used to obtain item weights to be used in constructing test blueprints. Prior weights can then be used to modify the item weights after data collection, based either on content balancing requirements or Bayesian prior content weights from SMEs (subject matter experts). Finally, a method is suggested to link two or more surveys, again using the Rasch Rating Scale Model and the computer program, Bigsteps, when it is desirable to shorten the length of the typical job or task survey.",Obtaining test blueprint weights from job analysis surveys,"A method for combining multiple scale responses from job or task surveys based on a hierarchical ranking scheme is presented. A rationale for placing the resulting ordinal information onto an interval scale of measurement using the Rasch Rating Scale Model is also provided. After a simple linear transformation, the item or task parameter estimates can be used to obtain item weights to be used in constructing test blueprints. Prior weights can then be used to modify the item weights after data collection, based either on content balancing requirements or Bayesian prior content weights from SMEs (subject matter experts). Finally, a method is suggested to link two or more surveys, again using the Rasch Rating Scale Model and the computer program, Bigsteps, when it is desirable to shorten the length of the typical job or task survey.","['method', 'combine', 'multiple', 'scale', 'response', 'job', 'task', 'survey', 'base', 'hierarchical', 'rank', 'scheme', 'present', 'rationale', 'place', 'result', 'ordinal', 'information', 'interval', 'scale', 'Rasch', 'Rating', 'Scale', 'provide', 'simple', 'linear', 'transformation', 'item', 'task', 'parameter', 'estimate', 'obtain', 'item', 'weight', 'construct', 'test', 'blueprint', 'Prior', 'weight', 'modify', 'item', 'weight', 'datum', 'collection', 'base', 'content', 'balancing', 'requirement', 'bayesian', 'prior', 'content', 'weight', 'sme', 'subject', 'matter', 'expert', 'finally', 'method', 'suggest', 'link', 'survey', 'Rasch', 'Rating', 'Scale', 'computer', 'program', 'bigstep', 'desirable', 'shorten', 'length', 'typical', 'job', 'task', 'survey']","['obtain', 'test', 'blueprint', 'weight', 'job', 'analysis', 'survey']",method combine multiple scale response job task survey base hierarchical rank scheme present rationale place result ordinal information interval scale Rasch Rating Scale provide simple linear transformation item task parameter estimate obtain item weight construct test blueprint Prior weight modify item weight datum collection base content balancing requirement bayesian prior content weight sme subject matter expert finally method suggest link survey Rasch Rating Scale computer program bigstep desirable shorten length typical job task survey,obtain test blueprint weight job analysis survey,0.027483814789175477,0.027485142955659108,0.33908089534094366,0.5781589487652314,0.02779119814899026,0.0,0.031641904067885016,0.0017003120062647806,0.04955539893577392,0.012274153562279904
Sykes R.C.; Yen W.M.,The scaling of mixed-item-format tests with the one-parameter and two-parameter partial credit models,2000,37,"Item response theory scalings were conducted for six tests with mixed item formats. These tests differed in their proportions of constructed response (c.r.) and multiple choice (m.c.) items and in overall difficulty. The scalings included those based on scores for the c.r. items that had maintained the number of levels as the item rubrics, either produced from single ratings or multiple ratings that were averaged and rounded to the nearest integer, as well as scalings for a single form of c.r. items obtained by summing multiple ratings. A one-parameter (1PPC) or two-parameter (2PPC) partial credit model was used for the c.r. items and the one-parameter logistic (1PL) or three-parameter logistic (3PL) model for the m.c. items. Item fit was substantially worse with the combination 1PL/1PPC model than the 3PL/2PPC model due to the former's restrictive assumptions that there would be no guessing on the m.c. items and equal item discrimination across items and item types. The presence of varying item discriminations resulted in the 1PL/1PPC model producing estimates of item information that could be spuriously inflated for c.r. items that had three or more score levels. Information for some items with summed ratings were usually overestimated by 300% or more for the 1PL/1PPC model. These inflated information values resulted in under-estimated standard errors of ability estimates. The constraints posed by the restricted model suggests limitations on the testing contexts in which the 1PL/1PPC model can be accurately applied.",The scaling of mixed-item-format tests with the one-parameter and two-parameter partial credit models,"Item response theory scalings were conducted for six tests with mixed item formats. These tests differed in their proportions of constructed response (c.r.) and multiple choice (m.c.) items and in overall difficulty. The scalings included those based on scores for the c.r. items that had maintained the number of levels as the item rubrics, either produced from single ratings or multiple ratings that were averaged and rounded to the nearest integer, as well as scalings for a single form of c.r. items obtained by summing multiple ratings. A one-parameter (1PPC) or two-parameter (2PPC) partial credit model was used for the c.r. items and the one-parameter logistic (1PL) or three-parameter logistic (3PL) model for the m.c. items. Item fit was substantially worse with the combination 1PL/1PPC model than the 3PL/2PPC model due to the former's restrictive assumptions that there would be no guessing on the m.c. items and equal item discrimination across items and item types. The presence of varying item discriminations resulted in the 1PL/1PPC model producing estimates of item information that could be spuriously inflated for c.r. items that had three or more score levels. Information for some items with summed ratings were usually overestimated by 300% or more for the 1PL/1PPC model. These inflated information values resulted in under-estimated standard errors of ability estimates. The constraints posed by the restricted model suggests limitations on the testing contexts in which the 1PL/1PPC model can be accurately applied.","['item', 'response', 'theory', 'scaling', 'conduct', 'test', 'mixed', 'item', 'format', 'test', 'differ', 'proportion', 'construct', 'response', 'cr', 'multiple', 'choice', 'mc', 'item', 'overall', 'difficulty', 'scaling', 'include', 'base', 'score', 'cr', 'item', 'maintain', 'number', 'level', 'item', 'rubric', 'produce', 'single', 'rating', 'multiple', 'rating', 'average', 'round', 'near', 'integer', 'scaling', 'single', 'form', 'cr', 'item', 'obtain', 'sum', 'multiple', 'rating', 'oneparameter', '1PPC', 'twoparameter', '2PPC', 'partial', 'credit', 'cr', 'item', 'oneparameter', 'logistic', '1pl', 'threeparameter', 'logistic', '3pl', 'mc', 'item', 'Item', 'fit', 'substantially', 'bad', 'combination', '1pl1ppc', '3pl2ppc', 'restrictive', 'assumption', 'guessing', 'mc', 'item', 'equal', 'item', 'discrimination', 'item', 'item', 'type', 'presence', 'vary', 'item', 'discrimination', 'result', '1pl1ppc', 'produce', 'estimate', 'item', 'information', 'spuriously', 'inflate', 'cr', 'item', 'score', 'level', 'Information', 'item', 'sum', 'rating', 'usually', 'overestimate', '300', '1pl1ppc', 'inflated', 'information', 'value', 'result', 'underestimated', 'standard', 'error', 'ability', 'estimate', 'constraint', 'pose', 'restricted', 'suggest', 'limitation', 'testing', 'context', '1pl1ppc', 'accurately', 'apply']","['scaling', 'mixeditemformat', 'test', 'oneparameter', 'twoparameter', 'partial', 'credit']",item response theory scaling conduct test mixed item format test differ proportion construct response cr multiple choice mc item overall difficulty scaling include base score cr item maintain number level item rubric produce single rating multiple rating average round near integer scaling single form cr item obtain sum multiple rating oneparameter 1PPC twoparameter 2PPC partial credit cr item oneparameter logistic 1pl threeparameter logistic 3pl mc item Item fit substantially bad combination 1pl1ppc 3pl2ppc restrictive assumption guessing mc item equal item discrimination item item type presence vary item discrimination result 1pl1ppc produce estimate item information spuriously inflate cr item score level Information item sum rating usually overestimate 300 1pl1ppc inflated information value result underestimated standard error ability estimate constraint pose restricted suggest limitation testing context 1pl1ppc accurately apply,scaling mixeditemformat test oneparameter twoparameter partial credit,0.024399581364452777,0.024400914869162925,0.02442224475867786,0.9022142896140818,0.024562969393624724,0.0011389486739409387,0.0,0.0,0.12119708415156014,0.0012998620541304156
Sotaridona L.S.; Meijer R.R.,Statistical properties of the K-index for detecting answer copying,2002,39,"We investigated the statistical properties of the K-index (Holland, 1996) that can be used to detect copying behavior on a test. A simulation study was conducted to investigate the applicability of the K-index for small, medium, and large datasets. Furthermore, the Type I error rate and the detection rate of this index were compared with the copying index, ω (Wollack, 1997). Several approximations were used to calculate the K-index. Results showed that all approximations were able to hold the Type I error rates below the nominal level. Results further showed that using co resulted in higher detection rates than the K-indices for small and medium sample sizes (100 and 500 simulees).",Statistical properties of the K-index for detecting answer copying,"We investigated the statistical properties of the K-index (Holland, 1996) that can be used to detect copying behavior on a test. A simulation study was conducted to investigate the applicability of the K-index for small, medium, and large datasets. Furthermore, the Type I error rate and the detection rate of this index were compared with the copying index, ω (Wollack, 1997). Several approximations were used to calculate the K-index. Results showed that all approximations were able to hold the Type I error rates below the nominal level. Results further showed that using co resulted in higher detection rates than the K-indices for small and medium sample sizes (100 and 500 simulees).","['investigate', 'statistical', 'property', 'Kindex', 'Holland', '1996', 'detect', 'copy', 'behavior', 'test', 'simulation', 'study', 'conduct', 'investigate', 'applicability', 'Kindex', 'small', 'medium', 'large', 'dataset', 'furthermore', 'Type', 'I', 'error', 'rate', 'detection', 'rate', 'index', 'compare', 'copy', 'index', 'ω', 'Wollack', '1997', 'approximation', 'calculate', 'Kindex', 'Results', 'approximation', 'able', 'hold', 'type', 'I', 'error', 'rate', 'nominal', 'level', 'result', 'far', 'co', 'result', 'high', 'detection', 'rate', 'Kindices', 'small', 'medium', 'sample', 'size', '100', '500', 'simulee']","['statistical', 'property', 'Kindex', 'detect', 'answer', 'copying']",investigate statistical property Kindex Holland 1996 detect copy behavior test simulation study conduct investigate applicability Kindex small medium large dataset furthermore Type I error rate detection rate index compare copy index ω Wollack 1997 approximation calculate Kindex Results approximation able hold type I error rate nominal level result far co result high detection rate Kindices small medium sample size 100 500 simulee,statistical property Kindex detect answer copying,0.029030335939663823,0.029032147778411035,0.1868697824183419,0.7255242361641837,0.029543497699399564,0.06280020352306387,0.0,0.008385733590591747,0.007224768847297255,0.003349431029112196
Plake B.S.; Impara J.C.; Irwin P.M.,Consistency of Angoff-based predictions of item performance: Evidence of technical quality of results from the angoff standard setting method,2000,37,"Judgmental standard-setting methods, such as the Angoff (1971) method, use item performance estimates as the basis for determining the minimum passing score (MPS).Therefore, the accuracy of these item performance estimates is crucial to the validity of the resulting MPS. Recent researchers (Shepard, 1995: Impara & Plake, 1998; National Research Council, 1999) have called into question the ability of judges to make accurate item performance estimates for target subgroups of candidates, such as minimally competent candidates. The purpose of this study was to examine the intra-and inter-rater consistency of item performance estimates from an Angoff standard setting. Results provide evidence that item perforperformance estimates were consistent within and across panels within and across years. Factors that might have influenced this high degree of reliability in the item performance estimates in a standard setting study are discussed.",Consistency of Angoff-based predictions of item performance: Evidence of technical quality of results from the angoff standard setting method,"Judgmental standard-setting methods, such as the Angoff (1971) method, use item performance estimates as the basis for determining the minimum passing score (MPS).Therefore, the accuracy of these item performance estimates is crucial to the validity of the resulting MPS. Recent researchers (Shepard, 1995: Impara & Plake, 1998; National Research Council, 1999) have called into question the ability of judges to make accurate item performance estimates for target subgroups of candidates, such as minimally competent candidates. The purpose of this study was to examine the intra-and inter-rater consistency of item performance estimates from an Angoff standard setting. Results provide evidence that item perforperformance estimates were consistent within and across panels within and across years. Factors that might have influenced this high degree of reliability in the item performance estimates in a standard setting study are discussed.","['judgmental', 'standardsetting', 'method', 'Angoff', '1971', 'method', 'item', 'performance', 'estimate', 'basis', 'determine', 'minimum', 'pass', 'score', 'MPSTherefore', 'accuracy', 'item', 'performance', 'estimate', 'crucial', 'validity', 'result', 'MPS', 'Recent', 'researcher', 'Shepard', '1995', 'Impara', 'Plake', '1998', 'National', 'Research', 'Council', '1999', 'question', 'ability', 'judge', 'accurate', 'item', 'performance', 'estimate', 'target', 'subgroup', 'candidate', 'minimally', 'competent', 'candidate', 'purpose', 'study', 'examine', 'intraand', 'interrater', 'consistency', 'item', 'performance', 'estimate', 'Angoff', 'standard', 'set', 'result', 'provide', 'evidence', 'item', 'perforperformance', 'estimate', 'consistent', 'panel', 'year', 'factor', 'influence', 'high', 'degree', 'reliability', 'item', 'performance', 'estimate', 'standard', 'setting', 'study', 'discuss']","['consistency', 'Angoffbased', 'prediction', 'item', 'performance', 'Evidence', 'technical', 'quality', 'result', 'angoff', 'standard', 'setting', 'method']",judgmental standardsetting method Angoff 1971 method item performance estimate basis determine minimum pass score MPSTherefore accuracy item performance estimate crucial validity result MPS Recent researcher Shepard 1995 Impara Plake 1998 National Research Council 1999 question ability judge accurate item performance estimate target subgroup candidate minimally competent candidate purpose study examine intraand interrater consistency item performance estimate Angoff standard set result provide evidence item perforperformance estimate consistent panel year factor influence high degree reliability item performance estimate standard setting study discuss,consistency Angoffbased prediction item performance Evidence technical quality result angoff standard setting method,0.025076488335485547,0.025077871926034244,0.3480915195554783,0.5765587532068535,0.02519536697614836,0.0,0.06748074709505891,0.00085435846063034,0.06516451299415923,0.015843833555720065
Cohen J.; Snow S.,Impact of changing difficulty on inferences from the national assessment of educational progress,2002,39,"The U.S. Department of Education measures student achievement through the National Assessment of Educational Progress (NAEP). NAEP estimates of population proficiency quantiles are based on a Bayesian multiple-imputation procedure. This article shows (a) that the resulting estimates depend directly on the mix of item difficulties on the test, and (b) the difficulty of items on the NAEP mathematics exam has increased over time. Does the increasing difficulty of the exam lead to observable changes in student performance over time? This study compared the simulated performance of 1990 examinees on the easier 1990 exam and the more difficult 1996 exam. No significant differences were found. While our results instill confidence that these changes have not impacted the NAEP trend line, our findings are both data-specific and limited in scope, and NAEP should carefully evaluate future adjustments to the test in this manner.",Impact of changing difficulty on inferences from the national assessment of educational progress,"The U.S. Department of Education measures student achievement through the National Assessment of Educational Progress (NAEP). NAEP estimates of population proficiency quantiles are based on a Bayesian multiple-imputation procedure. This article shows (a) that the resulting estimates depend directly on the mix of item difficulties on the test, and (b) the difficulty of items on the NAEP mathematics exam has increased over time. Does the increasing difficulty of the exam lead to observable changes in student performance over time? This study compared the simulated performance of 1990 examinees on the easier 1990 exam and the more difficult 1996 exam. No significant differences were found. While our results instill confidence that these changes have not impacted the NAEP trend line, our findings are both data-specific and limited in scope, and NAEP should carefully evaluate future adjustments to the test in this manner.","['US', 'Department', 'measure', 'student', 'achievement', 'National', 'Assessment', 'Educational', 'Progress', 'naep', 'naep', 'estimate', 'population', 'proficiency', 'quantile', 'base', 'bayesian', 'multipleimputation', 'procedure', 'article', 'result', 'estimate', 'depend', 'directly', 'mix', 'item', 'difficulty', 'test', 'b', 'difficulty', 'item', 'naep', 'mathematic', 'exam', 'increase', 'time', 'increase', 'difficulty', 'exam', 'lead', 'observable', 'change', 'student', 'performance', 'time', 'study', 'compare', 'simulated', 'performance', '1990', 'examine', 'easy', '1990', 'exam', 'difficult', '1996', 'exam', 'significant', 'difference', 'find', 'result', 'instill', 'confidence', 'change', 'impact', 'naep', 'trend', 'line', 'finding', 'dataspecific', 'limit', 'scope', 'NAEP', 'carefully', 'evaluate', 'future', 'adjustment', 'test', 'manner']","['impact', 'change', 'difficulty', 'inference', 'national', 'assessment', 'educational', 'progress']",US Department measure student achievement National Assessment Educational Progress naep naep estimate population proficiency quantile base bayesian multipleimputation procedure article result estimate depend directly mix item difficulty test b difficulty item naep mathematic exam increase time increase difficulty exam lead observable change student performance time study compare simulated performance 1990 examine easy 1990 exam difficult 1996 exam significant difference find result instill confidence change impact naep trend line finding dataspecific limit scope NAEP carefully evaluate future adjustment test manner,impact change difficulty inference national assessment educational progress,0.027440106148616753,0.027439379371090335,0.027438408383233474,0.8898313711435558,0.027850734953503688,0.0,0.01796117641839677,0.0,0.03241035952414677,0.07628786570151504
Wainer H.; Sheehan K.M.; Wang X.,Some paths toward making praxis scores more useful,2000,37,"In this study we describe an analytic method for aiding in the generation of subscales that characterize the deep structure of tests. In addition we also derive a procedure for estimating scores for these scales that are much more statistically stable than subscores computed solely from the items that are contained on that scale. These scores achieve their stability through augmentation with information from other related information on the test. These methods were used to complement each other on a data set obtained from a Praxis administration. We found that the deep structure of the test yielded ten subscales and that, because the test was essentially unidimensional, ten subscores could be computed, all with very high reliability. This result was contrasted with the calculation of six traditional subscales based on surface features of the items. These subscales also yielded augmented subscores of high reliability.",Some paths toward making praxis scores more useful,"In this study we describe an analytic method for aiding in the generation of subscales that characterize the deep structure of tests. In addition we also derive a procedure for estimating scores for these scales that are much more statistically stable than subscores computed solely from the items that are contained on that scale. These scores achieve their stability through augmentation with information from other related information on the test. These methods were used to complement each other on a data set obtained from a Praxis administration. We found that the deep structure of the test yielded ten subscales and that, because the test was essentially unidimensional, ten subscores could be computed, all with very high reliability. This result was contrasted with the calculation of six traditional subscales based on surface features of the items. These subscales also yielded augmented subscores of high reliability.","['study', 'describe', 'analytic', 'method', 'aid', 'generation', 'subscale', 'characterize', 'deep', 'structure', 'test', 'addition', 'derive', 'procedure', 'estimate', 'score', 'scale', 'statistically', 'stable', 'subscore', 'compute', 'solely', 'item', 'contain', 'scale', 'score', 'achieve', 'stability', 'augmentation', 'information', 'related', 'information', 'test', 'method', 'complement', 'data', 'set', 'obtain', 'Praxis', 'administration', 'find', 'deep', 'structure', 'test', 'yield', 'subscale', 'test', 'essentially', 'unidimensional', 'subscore', 'compute', 'high', 'reliability', 'result', 'contrast', 'calculation', 'traditional', 'subscale', 'base', 'surface', 'feature', 'item', 'subscale', 'yield', 'augmented', 'subscore', 'high', 'reliability']","['path', 'praxis', 'score', 'useful']",study describe analytic method aid generation subscale characterize deep structure test addition derive procedure estimate score scale statistically stable subscore compute solely item contain scale score achieve stability augmentation information related information test method complement data set obtain Praxis administration find deep structure test yield subscale test essentially unidimensional subscore compute high reliability result contrast calculation traditional subscale base surface feature item subscale yield augmented subscore high reliability,path praxis score useful,0.028882804363157176,0.02888010509831007,0.028919396938361593,0.4545153901327246,0.45880230346744655,0.0,0.07168914045733686,0.008075635985182943,0.0333521061931587,0.007272664661084971
Dorans N.J.; Holland P.W.,Population invariance and the equatability of tests: Basic theory and the linear case,2000,37,"How does the fact that two tests should not be equated manifest itself? This paper addresses this question through the study of the degree to which equating functions fail to exhibit population invariance across subpopulations. Equating functions are supposed to be population invariant by definition. But, when two tests are not equatable, it is possible that the linking functions, used to connect the scores of one to the scores of the other, are not invariant across different populations of examinees. While no acceptable equating function is ever completely population invariant, in the situations where equating is usually performed we believe that the dependence of the equating function on the population used to compute it is usually small enough to be ignored. We introduce two root-mean-square difference measures of the degree to which the functions used to link two tests computed on different subpopulations differ from the linking function computed for the whole population. We also introduce the system of ""parallel-linear"" linking functions for multiple subpopulations and show that, for this system, our measure of population invariance can be computed easily from the standardized mean differences between the scores of the subpopulations on the two tests. For the parallel-linear case, we develop a correlation-based upper bound on our measure that holds for all systems of subpopulations. We illustrate these ideas using data from the SAT I and from a concordance study of several combinations of ACT and SAT I scores. In the appendices, we give some theoretical results bearing on the other equating ""requirements"" of ""same construct,"" ""same reliability"" and one aspect of Lord's concept of equity.",Population invariance and the equatability of tests: Basic theory and the linear case,"How does the fact that two tests should not be equated manifest itself? This paper addresses this question through the study of the degree to which equating functions fail to exhibit population invariance across subpopulations. Equating functions are supposed to be population invariant by definition. But, when two tests are not equatable, it is possible that the linking functions, used to connect the scores of one to the scores of the other, are not invariant across different populations of examinees. While no acceptable equating function is ever completely population invariant, in the situations where equating is usually performed we believe that the dependence of the equating function on the population used to compute it is usually small enough to be ignored. We introduce two root-mean-square difference measures of the degree to which the functions used to link two tests computed on different subpopulations differ from the linking function computed for the whole population. We also introduce the system of ""parallel-linear"" linking functions for multiple subpopulations and show that, for this system, our measure of population invariance can be computed easily from the standardized mean differences between the scores of the subpopulations on the two tests. For the parallel-linear case, we develop a correlation-based upper bound on our measure that holds for all systems of subpopulations. We illustrate these ideas using data from the SAT I and from a concordance study of several combinations of ACT and SAT I scores. In the appendices, we give some theoretical results bearing on the other equating ""requirements"" of ""same construct,"" ""same reliability"" and one aspect of Lord's concept of equity.","['fact', 'test', 'equate', 'manifest', 'paper', 'address', 'question', 'study', 'degree', 'equate', 'function', 'fail', 'exhibit', 'population', 'invariance', 'subpopulation', 'Equating', 'function', 'suppose', 'population', 'invariant', 'definition', 'test', 'equatable', 'possible', 'link', 'function', 'connect', 'score', 'score', 'invariant', 'different', 'population', 'examinee', 'acceptable', 'equating', 'function', 'completely', 'population', 'invariant', 'situation', 'equating', 'usually', 'perform', 'believe', 'dependence', 'equate', 'function', 'population', 'compute', 'usually', 'small', 'ignore', 'introduce', 'rootmeansquare', 'difference', 'measure', 'degree', 'function', 'link', 'test', 'compute', 'different', 'subpopulation', 'differ', 'link', 'function', 'compute', 'population', 'introduce', 'system', 'parallellinear', 'link', 'function', 'multiple', 'subpopulation', 'system', 'measure', 'population', 'invariance', 'compute', 'easily', 'standardized', 'mean', 'difference', 'score', 'subpopulation', 'test', 'parallellinear', 'case', 'develop', 'correlationbase', 'upper', 'bind', 'measure', 'hold', 'system', 'subpopulation', 'illustrate', 'idea', 'datum', 'SAT', 'I', 'concordance', 'study', 'combination', 'ACT', 'SAT', 'I', 'score', 'appendix', 'theoretical', 'result', 'bear', 'equate', 'requirement', 'construct', 'reliability', 'aspect', 'Lords', 'concept', 'equity']","['population', 'invariance', 'equatability', 'test', 'basic', 'theory', 'linear', 'case']",fact test equate manifest paper address question study degree equate function fail exhibit population invariance subpopulation Equating function suppose population invariant definition test equatable possible link function connect score score invariant different population examinee acceptable equating function completely population invariant situation equating usually perform believe dependence equate function population compute usually small ignore introduce rootmeansquare difference measure degree function link test compute different subpopulation differ link function compute population introduce system parallellinear link function multiple subpopulation system measure population invariance compute easily standardized mean difference score subpopulation test parallellinear case develop correlationbase upper bind measure hold system subpopulation illustrate idea datum SAT I concordance study combination ACT SAT I score appendix theoretical result bear equate requirement construct reliability aspect Lords concept equity,population invariance equatability test basic theory linear case,0.025859390675600134,0.025861269954438867,0.026111650810235146,0.5790798605565651,0.34308782800316073,0.009335268648528626,0.0023943743626972467,0.12769363266808972,0.0,0.01336415142166448
Congdon P.J.; McQueen J.,The stability of rater severity in large-scale assessment programs,2000,37,"The purpose of this study was to investigate the stability of rater severity over an extended rating period. Multifaceted Rasch analysis was applied to ratings of 16 raters on writing performances of 8,285 elementary school students. Each performance was rated by two trained raters over a period of seven rating days. Performances rated on the first day were re-rated at the end of the rating period. Statistically significant differences between raters were found within each day and in all days combined. Daily estimates of the relative severity of individual raters were found to differ significantly from single, on-average estimates for the whole rating period. For 10 raters, severity estimates on the last day were significantly different from estimates on the first day. These findings cast doubt on the practice of using a single calibration of rater severity as the basis for adjustment of person measures.",The stability of rater severity in large-scale assessment programs,"The purpose of this study was to investigate the stability of rater severity over an extended rating period. Multifaceted Rasch analysis was applied to ratings of 16 raters on writing performances of 8,285 elementary school students. Each performance was rated by two trained raters over a period of seven rating days. Performances rated on the first day were re-rated at the end of the rating period. Statistically significant differences between raters were found within each day and in all days combined. Daily estimates of the relative severity of individual raters were found to differ significantly from single, on-average estimates for the whole rating period. For 10 raters, severity estimates on the last day were significantly different from estimates on the first day. These findings cast doubt on the practice of using a single calibration of rater severity as the basis for adjustment of person measures.","['purpose', 'study', 'investigate', 'stability', 'rater', 'severity', 'extended', 'rating', 'period', 'Multifaceted', 'Rasch', 'analysis', 'apply', 'rating', '16', 'rater', 'writing', 'performance', '8285', 'elementary', 'school', 'student', 'performance', 'rate', 'train', 'rater', 'period', 'seven', 'rating', 'day', 'Performances', 'rate', 'day', 'rerate', 'end', 'rating', 'period', 'statistically', 'significant', 'difference', 'rater', 'find', 'day', 'day', 'combine', 'Daily', 'estimate', 'relative', 'severity', 'individual', 'rater', 'find', 'differ', 'significantly', 'single', 'onaverage', 'estimate', 'rating', 'period', '10', 'rater', 'severity', 'estimate', 'day', 'significantly', 'different', 'estimate', 'day', 'finding', 'cast', 'doubt', 'practice', 'single', 'calibration', 'rater', 'severity', 'basis', 'adjustment', 'person', 'measure']","['stability', 'rater', 'severity', 'largescale', 'assessment', 'program']",purpose study investigate stability rater severity extended rating period Multifaceted Rasch analysis apply rating 16 rater writing performance 8285 elementary school student performance rate train rater period seven rating day Performances rate day rerate end rating period statistically significant difference rater find day day combine Daily estimate relative severity individual rater find differ significantly single onaverage estimate rating period 10 rater severity estimate day significantly different estimate day finding cast doubt practice single calibration rater severity basis adjustment person measure,stability rater severity largescale assessment program,0.03314308542137402,0.03312354134573444,0.03313551305961873,0.03531568176228229,0.8652821784109905,0.0,0.0,0.0,0.010604745854767176,0.0933249793512499
Jodoin M.G.,Measurement efficiency of innovative item formats in computer-based testing,2003,40,"The psychometric literature provides little empirical evaluation of examinee test data to assess essential psychometric properties of innovative items. In this study, examinee responses to conventional (e.g., multiple choice) and innovative item formats in a computer-based testing program were analyzed for IRT information with the three-parameter and graded response models. The innovative item types considered in this study provided more information across all levels of ability than multiple-choice items. In addition, accurate timing data captured via computer administration were analyzed to consider the relative efficiency of the multiple choice and innovative item types. As with previous research, multiple-choice items provide more information per unit time. Implications for balancing policy, psychometric, and pragmatic factors in selecting item formats are also discussed.",Measurement efficiency of innovative item formats in computer-based testing,"The psychometric literature provides little empirical evaluation of examinee test data to assess essential psychometric properties of innovative items. In this study, examinee responses to conventional (e.g., multiple choice) and innovative item formats in a computer-based testing program were analyzed for IRT information with the three-parameter and graded response models. The innovative item types considered in this study provided more information across all levels of ability than multiple-choice items. In addition, accurate timing data captured via computer administration were analyzed to consider the relative efficiency of the multiple choice and innovative item types. As with previous research, multiple-choice items provide more information per unit time. Implications for balancing policy, psychometric, and pragmatic factors in selecting item formats are also discussed.","['psychometric', 'literature', 'provide', 'little', 'empirical', 'evaluation', 'examinee', 'test', 'datum', 'assess', 'essential', 'psychometric', 'property', 'innovative', 'item', 'study', 'examinee', 'response', 'conventional', 'eg', 'multiple', 'choice', 'innovative', 'item', 'format', 'computerbased', 'testing', 'program', 'analyze', 'IRT', 'information', 'threeparameter', 'grade', 'response', 'innovative', 'item', 'type', 'consider', 'study', 'provide', 'information', 'level', 'ability', 'multiplechoice', 'item', 'addition', 'accurate', 'timing', 'datum', 'capture', 'computer', 'administration', 'analyze', 'consider', 'relative', 'efficiency', 'multiple', 'choice', 'innovative', 'item', 'type', 'previous', 'research', 'multiplechoice', 'item', 'provide', 'information', 'unit', 'time', 'Implications', 'balance', 'policy', 'psychometric', 'pragmatic', 'factor', 'select', 'item', 'format', 'discuss']","['efficiency', 'innovative', 'item', 'format', 'computerbase', 'testing']",psychometric literature provide little empirical evaluation examinee test datum assess essential psychometric property innovative item study examinee response conventional eg multiple choice innovative item format computerbased testing program analyze IRT information threeparameter grade response innovative item type consider study provide information level ability multiplechoice item addition accurate timing datum capture computer administration analyze consider relative efficiency multiple choice innovative item type previous research multiplechoice item provide information unit time Implications balance policy psychometric pragmatic factor select item format discuss,efficiency innovative item format computerbase testing,0.027338033659774837,0.02733454168911057,0.02736543479697928,0.8903923612634431,0.02756962859069224,0.005595754121032884,0.02643506609971533,0.0,0.08411992771805346,0.009812697682395557
Sotaridona L.S.; Meijer R.R.,Two new statistics to detect answer copying,2003,40,"Two new indices to detect answer copying on a multiple-choice test - S1 and S2 - were proposed. The S1 index is similar to the K index (Holland, 1996) and the K̄2 index (Sotaridona & Meijer, 2002) but the distribution of the number of matching incorrect answers of the source and the copier is modeled by the Poisson distribution instead of the binomial distribution to improve the detection rate of K and K̄2. The S2 index was proposed to overcome a limitation of the K and K̄2 index, namely, their insensitiveness to correct answers copying. The S2 index incorporates the matching correct answers in addition to the matching incorrect answers. A simulation study was conducted to investigate the usefulness of S1 and S2 for 40- and 80-item tests, 100 and 500 sample sizes, and 10%, 20%, 30%, and 40% answer copying. The Type I errors and detection rates of S1 and S2 were compared with those of the K̄2 and the ω copying index (Wollack, 1997). Results showed that all four indices were able to maintain their Type I errors, with S1 and K̄2 being slightly conservative compared to S2 and ω. Furthermore, S1 had higher detection rates than K̄2. The S2 index showed a significant improvement in detection rate compared to K and K̄2.",,"Two new indices to detect answer copying on a multiple-choice test - S1 and S2 - were proposed. The S1 index is similar to the K index (Holland, 1996) and the K̄2 index (Sotaridona & Meijer, 2002) but the distribution of the number of matching incorrect answers of the source and the copier is modeled by the Poisson distribution instead of the binomial distribution to improve the detection rate of K and K̄2. The S2 index was proposed to overcome a limitation of the K and K̄2 index, namely, their insensitiveness to correct answers copying. The S2 index incorporates the matching correct answers in addition to the matching incorrect answers. A simulation study was conducted to investigate the usefulness of S1 and S2 for 40- and 80-item tests, 100 and 500 sample sizes, and 10%, 20%, 30%, and 40% answer copying. The Type I errors and detection rates of S1 and S2 were compared with those of the K̄2 and the ω copying index (Wollack, 1997). Results showed that all four indices were able to maintain their Type I errors, with S1 and K̄2 being slightly conservative compared to S2 and ω. Furthermore, S1 had higher detection rates than K̄2. The S2 index showed a significant improvement in detection rate compared to K and K̄2.","['new', 'index', 'detect', 'answer', 'copying', 'multiplechoice', 'test', 'S1', 'S2', 'propose', 'S1', 'index', 'similar', 'K', 'index', 'Holland', '1996', 'K̄2', 'index', 'Sotaridona', 'Meijer', '2002', 'distribution', 'number', 'match', 'incorrect', 'answer', 'source', 'copier', 'Poisson', 'distribution', 'instead', 'binomial', 'distribution', 'improve', 'detection', 'rate', 'K', 'K̄2', 'S2', 'index', 'propose', 'overcome', 'limitation', 'K', 'K̄2', 'index', 'insensitiveness', 'correct', 'answer', 'copy', 'S2', 'index', 'incorporate', 'match', 'correct', 'answer', 'addition', 'matching', 'incorrect', 'answer', 'simulation', 'study', 'conduct', 'investigate', 'usefulness', 'S1', 's2', '40', '80item', 'test', '100', '500', 'sample', 'size', '10', '20', '30', '40', 'answer', 'copy', 'Type', 'I', 'error', 'detection', 'rate', 'S1', 'S2', 'compare', 'K̄2', 'ω', 'copy', 'index', 'Wollack', '1997', 'result', 'index', 'able', 'maintain', 'type', 'I', 'error', 'S1', 'K̄2', 'slightly', 'conservative', 'compare', 'S2', 'ω', 'furthermore', 'S1', 'high', 'detection', 'rate', 'K̄2', 'S2', 'index', 'significant', 'improvement', 'detection', 'rate', 'compare', 'K', 'K̄2']",,new index detect answer copying multiplechoice test S1 S2 propose S1 index similar K index Holland 1996 K̄2 index Sotaridona Meijer 2002 distribution number match incorrect answer source copier Poisson distribution instead binomial distribution improve detection rate K K̄2 S2 index propose overcome limitation K K̄2 index insensitiveness correct answer copy S2 index incorporate match correct answer addition matching incorrect answer simulation study conduct investigate usefulness S1 s2 40 80item test 100 500 sample size 10 20 30 40 answer copy Type I error detection rate S1 S2 compare K̄2 ω copy index Wollack 1997 result index able maintain type I error S1 K̄2 slightly conservative compare S2 ω furthermore S1 high detection rate K̄2 S2 index significant improvement detection rate compare K K̄2,,0.030934762412913288,0.030936189585119206,0.3248921277524561,0.5819265681804157,0.03131035206909568,0.04650262879789822,0.0,0.0008394424264172521,0.005951859976389641,0.0008764154531369989
Wang T.; Kolen M.J.; Harris D.J.,Psychometric properties of scale scores and performance levels for performance assessments using polytomous IRT,2000,37,"With a focus on performance assessments, this paper describes procedures for calculating conditional standard error of measurement (CSEM) and reliability of scale scores and classification consistency of performance levels. Scale scores that are transformations of total raw scores are the focus of these procedures, although other types of raw scores are considered as well. Polytomous IRT models provide the psychometric foundation for the procedures that are described. The procedures are applied using test data from ACT's Work Keys Writing Assessment to demonstrate their usefulness. Two polytomous IRT models were compared, as were two different procedures for calculating scores. One simulation study was done using one of the models to evaluate the accuracy of the proposed procedures. The results suggest that the procedures provide quite stable estimates and have the potential to be useful in a variety of performance assessment situations.",Psychometric properties of scale scores and performance levels for performance assessments using polytomous IRT,"With a focus on performance assessments, this paper describes procedures for calculating conditional standard error of measurement (CSEM) and reliability of scale scores and classification consistency of performance levels. Scale scores that are transformations of total raw scores are the focus of these procedures, although other types of raw scores are considered as well. Polytomous IRT models provide the psychometric foundation for the procedures that are described. The procedures are applied using test data from ACT's Work Keys Writing Assessment to demonstrate their usefulness. Two polytomous IRT models were compared, as were two different procedures for calculating scores. One simulation study was done using one of the models to evaluate the accuracy of the proposed procedures. The results suggest that the procedures provide quite stable estimates and have the potential to be useful in a variety of performance assessment situations.","['focus', 'performance', 'assessment', 'paper', 'describe', 'procedure', 'calculate', 'conditional', 'standard', 'error', 'CSEM', 'reliability', 'scale', 'score', 'classification', 'consistency', 'performance', 'level', 'Scale', 'score', 'transformation', 'total', 'raw', 'score', 'focus', 'procedure', 'type', 'raw', 'score', 'consider', 'polytomous', 'IRT', 'provide', 'psychometric', 'foundation', 'procedure', 'describe', 'procedure', 'apply', 'test', 'datum', 'act', 'work', 'Keys', 'write', 'Assessment', 'demonstrate', 'usefulness', 'polytomous', 'IRT', 'compare', 'different', 'procedure', 'calculate', 'score', 'simulation', 'study', 'evaluate', 'accuracy', 'propose', 'procedure', 'result', 'suggest', 'procedure', 'provide', 'stable', 'estimate', 'potential', 'useful', 'variety', 'performance', 'assessment', 'situation']","['psychometric', 'property', 'scale', 'score', 'performance', 'level', 'performance', 'assessment', 'polytomous', 'IRT']",focus performance assessment paper describe procedure calculate conditional standard error CSEM reliability scale score classification consistency performance level Scale score transformation total raw score focus procedure type raw score consider polytomous IRT provide psychometric foundation procedure describe procedure apply test datum act work Keys write Assessment demonstrate usefulness polytomous IRT compare different procedure calculate score simulation study evaluate accuracy propose procedure result suggest procedure provide stable estimate potential useful variety performance assessment situation,psychometric property scale score performance level performance assessment polytomous IRT,0.028095516938923502,0.028095877704508273,0.028111547153243592,0.8873112184819726,0.028385839721352054,0.0033111296257287825,0.19903295078444855,0.0023986776195029824,0.0,0.0022281662988067404
Tate R.,Performance of a proposed method for the linking of mixed format tests with constructed response and multiple choice items,2000,37,"The error associated with a proposed linking method for tests consisting of both constructed response and multiple choice items was investigated in a simulation study. Study factors that were varied included the relative proportion of constructed response items in the test, the size of the year-to-year change in the ability metric, the number of anchor items, the number of linking papers to be reassessed, and the presence of guessing. The results supported the use of the proposed linking method. In addition, simulations were used to illustrate possible linking bias resulting from (a) the use of the traditional linking method and (b) the use of only multiple choice anchor items in the presence of test multidimensionality.",Performance of a proposed method for the linking of mixed format tests with constructed response and multiple choice items,"The error associated with a proposed linking method for tests consisting of both constructed response and multiple choice items was investigated in a simulation study. Study factors that were varied included the relative proportion of constructed response items in the test, the size of the year-to-year change in the ability metric, the number of anchor items, the number of linking papers to be reassessed, and the presence of guessing. The results supported the use of the proposed linking method. In addition, simulations were used to illustrate possible linking bias resulting from (a) the use of the traditional linking method and (b) the use of only multiple choice anchor items in the presence of test multidimensionality.","['error', 'associate', 'propose', 'linking', 'method', 'test', 'consist', 'construct', 'response', 'multiple', 'choice', 'item', 'investigate', 'simulation', 'study', 'study', 'factor', 'vary', 'include', 'relative', 'proportion', 'construct', 'response', 'item', 'test', 'size', 'yeartoyear', 'change', 'ability', 'metric', 'number', 'anchor', 'item', 'number', 'link', 'paper', 'reassess', 'presence', 'guess', 'result', 'support', 'propose', 'linking', 'method', 'addition', 'simulation', 'illustrate', 'possible', 'link', 'bias', 'result', 'traditional', 'linking', 'method', 'b', 'multiple', 'choice', 'anchor', 'item', 'presence', 'test', 'multidimensionality']","['performance', 'propose', 'method', 'linking', 'mixed', 'format', 'test', 'construct', 'response', 'multiple', 'choice', 'item']",error associate propose linking method test consist construct response multiple choice item investigate simulation study study factor vary include relative proportion construct response item test size yeartoyear change ability metric number anchor item number link paper reassess presence guess result support propose linking method addition simulation illustrate possible link bias result traditional linking method b multiple choice anchor item presence test multidimensionality,performance propose method linking mixed format test construct response multiple choice item,0.02902694499054413,0.029022365249013034,0.0296280752746139,0.8760599980476144,0.036262616438214566,0.00647792999230388,0.0,0.03950241682931235,0.10181589942623424,0.0
Wang N.,Use of the Rasch IRT Model in Standard Setting: An Item-mapping Method,2003,40,"This article provides both logical and empirical evidence to justify the use of an item-mapping method for establishing passing scores for multiple-choice licensure and certification examinations. After describing the item-mapping standard-setting process, the rationale and theoretical basis for this method are discussed, and the similarities and differences between the item-mapping and the Bookmark methods are also provided. Empirical evidence supporting use of the item-mapping method is provided by comparing results from four standard-setting studies for diverse licensure and certification examinations. The four cut score studies were conducted using both the item-mapping and the Angoff methods. Rating data from the four standard-setting studies, using each of the two methods, were analyzed using item-by-rater random effects generalizability and dependability studies to examine which method yielded higher inter-judge consistency. Results indicated that the item-mapping method produced higher inter-judge consistency and achieved greater rater agreement than the Angoff method.",Use of the Rasch IRT Model in Standard Setting: An Item-mapping Method,"This article provides both logical and empirical evidence to justify the use of an item-mapping method for establishing passing scores for multiple-choice licensure and certification examinations. After describing the item-mapping standard-setting process, the rationale and theoretical basis for this method are discussed, and the similarities and differences between the item-mapping and the Bookmark methods are also provided. Empirical evidence supporting use of the item-mapping method is provided by comparing results from four standard-setting studies for diverse licensure and certification examinations. The four cut score studies were conducted using both the item-mapping and the Angoff methods. Rating data from the four standard-setting studies, using each of the two methods, were analyzed using item-by-rater random effects generalizability and dependability studies to examine which method yielded higher inter-judge consistency. Results indicated that the item-mapping method produced higher inter-judge consistency and achieved greater rater agreement than the Angoff method.","['article', 'provide', 'logical', 'empirical', 'evidence', 'justify', 'itemmappe', 'method', 'establish', 'pass', 'score', 'multiplechoice', 'licensure', 'certification', 'examination', 'describe', 'itemmappe', 'standardsetting', 'process', 'rationale', 'theoretical', 'basis', 'method', 'discuss', 'similarity', 'difference', 'itemmapping', 'Bookmark', 'method', 'provide', 'empirical', 'evidence', 'support', 'itemmappe', 'method', 'provide', 'compare', 'result', 'standardsette', 'study', 'diverse', 'licensure', 'certification', 'examination', 'cut', 'score', 'study', 'conduct', 'itemmapping', 'Angoff', 'method', 'rating', 'datum', 'standardsette', 'study', 'method', 'analyze', 'itembyrater', 'random', 'effect', 'generalizability', 'dependability', 'study', 'examine', 'method', 'yield', 'high', 'interjudge', 'consistency', 'result', 'indicate', 'itemmappe', 'method', 'produce', 'high', 'interjudge', 'consistency', 'achieve', 'great', 'rater', 'agreement', 'Angoff', 'method']","['Use', 'Rasch', 'IRT', 'Standard', 'Setting', 'Itemmapping', 'Method']",article provide logical empirical evidence justify itemmappe method establish pass score multiplechoice licensure certification examination describe itemmappe standardsetting process rationale theoretical basis method discuss similarity difference itemmapping Bookmark method provide empirical evidence support itemmappe method provide compare result standardsette study diverse licensure certification examination cut score study conduct itemmapping Angoff method rating datum standardsette study method analyze itembyrater random effect generalizability dependability study examine method yield high interjudge consistency result indicate itemmappe method produce high interjudge consistency achieve great rater agreement Angoff method,Use Rasch IRT Standard Setting Itemmapping Method,0.030406471688044636,0.030408129511959477,0.3992730448193609,0.5049867180512319,0.03492563592940321,0.0,0.03996006233212173,0.021350863572339646,0.03320893756658881,0.018904596984413238
Lee G.; Fitzpatrick A.R.,The effects of a student sampling plan on estimates of the standard errors for student passing rates,2003,40,"Examined in this study were three procedures for estimating the standard errors of school passing rates using a generalizability theory model. Also examined was how these procedures behaved for student samples that differed in size. The procedures differed in terms of their assumptions about the populations from which students were sampled, and it was found that student sample size generally had a notable effect on the size of the standard error estimates they produced. Also the three procedures produced markedly different standard error estimates when student sample size was small.",The effects of a student sampling plan on estimates of the standard errors for student passing rates,"Examined in this study were three procedures for estimating the standard errors of school passing rates using a generalizability theory model. Also examined was how these procedures behaved for student samples that differed in size. The procedures differed in terms of their assumptions about the populations from which students were sampled, and it was found that student sample size generally had a notable effect on the size of the standard error estimates they produced. Also the three procedures produced markedly different standard error estimates when student sample size was small.","['examine', 'study', 'procedure', 'estimate', 'standard', 'error', 'school', 'pass', 'rate', 'generalizability', 'theory', 'examine', 'procedure', 'behave', 'student', 'sample', 'differ', 'size', 'procedure', 'differ', 'term', 'assumption', 'population', 'student', 'sample', 'find', 'student', 'sample', 'size', 'generally', 'notable', 'effect', 'size', 'standard', 'error', 'estimate', 'produce', 'procedure', 'produce', 'markedly', 'different', 'standard', 'error', 'estimate', 'student', 'sample', 'size', 'small']","['effect', 'student', 'sample', 'plan', 'estimate', 'standard', 'error', 'student', 'pass', 'rate']",examine study procedure estimate standard error school pass rate generalizability theory examine procedure behave student sample differ size procedure differ term assumption population student sample find student sample size generally notable effect size standard error estimate produce procedure produce markedly different standard error estimate student sample size small,effect student sample plan estimate standard error student pass rate,0.0359714342771466,0.035971847018820204,0.03597192443757664,0.8555837920363676,0.036501002230089084,0.0036204700823044885,0.11552652696165643,0.018770877969733004,0.0,0.03354232715387924
Smits N.; Mellenbergh G.J.; Vorst H.C.M.,Alternative missing data techniques to grade point average: Imputing unavailable grades,2002,39,"In this article, grade point average (GPA) is considered a missing data technique for unavailable grades in school grade records. In Study 1, theoretical and empirical differences between GPA and seven alternative missing grade techniques were considered. These seven techniques are subject mean substitution, corrected subject mean, subject correlation substitution, regression imputation, expectation maximization algorithm imputation and two multiple imputation methods - stochastic regression imputation and data augmentation. The missing grade techniques differ greatly. Data augmentation and stochastic regression imputation appear to be superior as missing grade techniques. In Study 2, the completed grade records (observed and imputed values) were used in two prediction analyses of academic achievement. One analysis was based on unweighed grades, the other on weighed grades. In both analyses, alternative missing grade methods produced better and more consistent predictions. It is concluded that some alternative missing grade methods are superior to GPA.",Alternative missing data techniques to grade point average: Imputing unavailable grades,"In this article, grade point average (GPA) is considered a missing data technique for unavailable grades in school grade records. In Study 1, theoretical and empirical differences between GPA and seven alternative missing grade techniques were considered. These seven techniques are subject mean substitution, corrected subject mean, subject correlation substitution, regression imputation, expectation maximization algorithm imputation and two multiple imputation methods - stochastic regression imputation and data augmentation. The missing grade techniques differ greatly. Data augmentation and stochastic regression imputation appear to be superior as missing grade techniques. In Study 2, the completed grade records (observed and imputed values) were used in two prediction analyses of academic achievement. One analysis was based on unweighed grades, the other on weighed grades. In both analyses, alternative missing grade methods produced better and more consistent predictions. It is concluded that some alternative missing grade methods are superior to GPA.","['article', 'grade', 'point', 'average', 'GPA', 'consider', 'miss', 'data', 'technique', 'unavailable', 'grade', 'school', 'grade', 'record', 'Study', '1', 'theoretical', 'empirical', 'difference', 'GPA', 'seven', 'alternative', 'miss', 'grade', 'technique', 'consider', 'seven', 'technique', 'subject', 'mean', 'substitution', 'correct', 'subject', 'mean', 'subject', 'correlation', 'substitution', 'regression', 'imputation', 'expectation', 'maximization', 'algorithm', 'imputation', 'multiple', 'imputation', 'method', 'stochastic', 'regression', 'imputation', 'data', 'augmentation', 'missing', 'grade', 'technique', 'differ', 'greatly', 'Data', 'augmentation', 'stochastic', 'regression', 'imputation', 'appear', 'superior', 'miss', 'grade', 'technique', 'study', '2', 'complete', 'grade', 'record', 'observe', 'impute', 'value', 'prediction', 'analysis', 'academic', 'achievement', 'analysis', 'base', 'unweighed', 'grade', 'weigh', 'grade', 'analysis', 'alternative', 'miss', 'grade', 'method', 'produce', 'consistent', 'prediction', 'conclude', 'alternative', 'miss', 'grade', 'method', 'superior', 'GPA']","['alternative', 'miss', 'datum', 'technique', 'grade', 'point', 'average', 'impute', 'unavailable', 'grade']",article grade point average GPA consider miss data technique unavailable grade school grade record Study 1 theoretical empirical difference GPA seven alternative miss grade technique consider seven technique subject mean substitution correct subject mean subject correlation substitution regression imputation expectation maximization algorithm imputation multiple imputation method stochastic regression imputation data augmentation missing grade technique differ greatly Data augmentation stochastic regression imputation appear superior miss grade technique study 2 complete grade record observe impute value prediction analysis academic achievement analysis base unweighed grade weigh grade analysis alternative miss grade method produce consistent prediction conclude alternative miss grade method superior GPA,alternative miss datum technique grade point average impute unavailable grade,0.031088564557095525,0.031052996055832012,0.22437356997958274,0.680036699002543,0.03344817040494674,0.0,0.0,0.0,0.0011357461076290415,0.15470466386497356
Ban J.-C.; Hanson B.A.; Yi Q.; Harris D.J.,Data sparseness and on-line pretest item calibration-scaling methods in CAT,2002,39,"The purpose of this study was to compare and evaluate three on-line pretest item calibration-scaling methods (the marginal maximum likelihood estimate with one expectation maximization [EM] cycle [OEM] method, the marginal maximum likelihood estimate with multiple EM cycles [MEM] method, and Stocking's Method B) in terms of item parameter recovery when the item responses to the pretest items in the pool are sparse. Simulations of computerized adaptive tests were used to evaluate the results yielded by the three methods. The MEM method produced the smallest average total error in parameter estimation, and the OEM method yielded the largest total error.",Data sparseness and on-line pretest item calibration-scaling methods in CAT,"The purpose of this study was to compare and evaluate three on-line pretest item calibration-scaling methods (the marginal maximum likelihood estimate with one expectation maximization [EM] cycle [OEM] method, the marginal maximum likelihood estimate with multiple EM cycles [MEM] method, and Stocking's Method B) in terms of item parameter recovery when the item responses to the pretest items in the pool are sparse. Simulations of computerized adaptive tests were used to evaluate the results yielded by the three methods. The MEM method produced the smallest average total error in parameter estimation, and the OEM method yielded the largest total error.","['purpose', 'study', 'compare', 'evaluate', 'online', 'pret', 'item', 'calibrationscale', 'method', 'marginal', 'maximum', 'likelihood', 'estimate', 'expectation', 'maximization', 'EM', 'cycle', 'OEM', 'method', 'marginal', 'maximum', 'likelihood', 'estimate', 'multiple', 'EM', 'cycle', 'MEM', 'method', 'Stockings', 'Method', 'B', 'term', 'item', 'parameter', 'recovery', 'item', 'response', 'pret', 'item', 'pool', 'sparse', 'simulation', 'computerized', 'adaptive', 'test', 'evaluate', 'result', 'yield', 'method', 'MEM', 'method', 'produce', 'small', 'average', 'total', 'error', 'parameter', 'estimation', 'OEM', 'method', 'yield', 'large', 'total', 'error']","['datum', 'sparseness', 'online', 'pret', 'item', 'calibrationscale', 'method', 'CAT']",purpose study compare evaluate online pret item calibrationscale method marginal maximum likelihood estimate expectation maximization EM cycle OEM method marginal maximum likelihood estimate multiple EM cycle MEM method Stockings Method B term item parameter recovery item response pret item pool sparse simulation computerized adaptive test evaluate result yield method MEM method produce small average total error parameter estimation OEM method yield large total error,datum sparseness online pret item calibrationscale method CAT,0.030260044647423073,0.030256844395819583,0.28809993636008907,0.6210559895003414,0.030327185096326946,0.0,0.0,0.016768597136169565,0.11509921701091905,0.0
Embretson S.; Gorin J.,Improving construct validity with cognitive psychology principles,2001,38,"Cognitive psychology principles have been heralded as possibly central to construct validity. In this paper, testing practices are examined in three stages: (a) the past, in which the traditional testing research paradigm left little role for cognitive psychology principles, (b) the present, in which testing research is enhanced by cognitive psychology principles, and (c) the future, for which we predict that cognitive psychology's potential will be fully realized through item design. An extended example of item design by cognitive theory is given to illustrate the principles. A spatial ability test that consists of an object assembly task highlights how cognitive design principles can lead to item generation.",Improving construct validity with cognitive psychology principles,"Cognitive psychology principles have been heralded as possibly central to construct validity. In this paper, testing practices are examined in three stages: (a) the past, in which the traditional testing research paradigm left little role for cognitive psychology principles, (b) the present, in which testing research is enhanced by cognitive psychology principles, and (c) the future, for which we predict that cognitive psychology's potential will be fully realized through item design. An extended example of item design by cognitive theory is given to illustrate the principles. A spatial ability test that consists of an object assembly task highlights how cognitive design principles can lead to item generation.","['cognitive', 'psychology', 'principle', 'herald', 'possibly', 'central', 'construct', 'validity', 'paper', 'testing', 'practice', 'examine', 'stage', 'past', 'traditional', 'testing', 'research', 'paradigm', 'leave', 'little', 'role', 'cognitive', 'psychology', 'principle', 'b', 'present', 'testing', 'research', 'enhance', 'cognitive', 'psychology', 'principle', 'c', 'future', 'predict', 'cognitive', 'psychologys', 'potential', 'fully', 'realize', 'item', 'design', 'extended', 'example', 'item', 'design', 'cognitive', 'theory', 'illustrate', 'principle', 'spatial', 'ability', 'test', 'consist', 'object', 'assembly', 'task', 'highlight', 'cognitive', 'design', 'principle', 'lead', 'item', 'generation']","['improve', 'construct', 'validity', 'cognitive', 'psychology', 'principle']",cognitive psychology principle herald possibly central construct validity paper testing practice examine stage past traditional testing research paradigm leave little role cognitive psychology principle b present testing research enhance cognitive psychology principle c future predict cognitive psychologys potential fully realize item design extended example item design cognitive theory illustrate principle spatial ability test consist object assembly task highlight cognitive design principle lead item generation,improve construct validity cognitive psychology principle,0.03189830549612985,0.03188458368059063,0.032526484253886,0.8716640181967451,0.032026608372648535,0.0,0.07849627937291065,0.0,0.014020144524115616,0.0
Dorans N.J.,Recentering and realigning the SAT score distributions: How and why,2002,39,"The process employed to produce the conversions that take scores from the original SAT scales to recentered scales, in which reference group scores are centered near the midpoint of the score-reporting range, is laid out. For the purposes of this article, SAT Verbal and SAT Mathematical scores were placed on recentered scales, which have reporting ranges of 920 to 980, means of 950, and standard deviations of 11. (The 920-to-980 scale is used in this article to highlight the distinction between it and the old 200-to-800 scale. In actuality, recentered scores were reported on a 200-to-800 scale.) Recentering was accomplished via a linear transformation of normally distributed scores that were obtained from a continuized, smoothed frequency distribution of original SAT scores that were originally on augmented two-digit scales (i.e., discrete scores rounded to either 0 or 5 in the third decimal place). These discrete scores were obtained for all students in the 1990 Reference Group using 35 different editions of the SAT spanning October 1988 to June 1990. The performance of this 1990 Reference Group on the original and recentered scales is described. The effects of recentering on scores of individuals and the 1990 Reference Group are also examined. Finally, recentering did not occur solely on the basis of its technical merit. Issues associated with converting recentering from a possibility into a reality are discussed.",Recentering and realigning the SAT score distributions: How and why,"The process employed to produce the conversions that take scores from the original SAT scales to recentered scales, in which reference group scores are centered near the midpoint of the score-reporting range, is laid out. For the purposes of this article, SAT Verbal and SAT Mathematical scores were placed on recentered scales, which have reporting ranges of 920 to 980, means of 950, and standard deviations of 11. (The 920-to-980 scale is used in this article to highlight the distinction between it and the old 200-to-800 scale. In actuality, recentered scores were reported on a 200-to-800 scale.) Recentering was accomplished via a linear transformation of normally distributed scores that were obtained from a continuized, smoothed frequency distribution of original SAT scores that were originally on augmented two-digit scales (i.e., discrete scores rounded to either 0 or 5 in the third decimal place). These discrete scores were obtained for all students in the 1990 Reference Group using 35 different editions of the SAT spanning October 1988 to June 1990. The performance of this 1990 Reference Group on the original and recentered scales is described. The effects of recentering on scores of individuals and the 1990 Reference Group are also examined. Finally, recentering did not occur solely on the basis of its technical merit. Issues associated with converting recentering from a possibility into a reality are discussed.","['process', 'employ', 'produce', 'conversion', 'score', 'original', 'SAT', 'scale', 'recentere', 'scale', 'reference', 'group', 'score', 'center', 'near', 'midpoint', 'scorereporting', 'range', 'lay', 'purpose', 'article', 'SAT', 'Verbal', 'SAT', 'Mathematical', 'score', 'place', 'recentere', 'scale', 'report', 'range', '920', '980', 'mean', '950', 'standard', 'deviation', '11', '920to980', 'scale', 'article', 'highlight', 'distinction', 'old', '200to800', 'scale', 'actuality', 'recentere', 'score', 'report', '200to800', 'scale', 'recentere', 'accomplish', 'linear', 'transformation', 'normally', 'distribute', 'score', 'obtain', 'continuize', 'smoothed', 'frequency', 'distribution', 'original', 'SAT', 'score', 'originally', 'augmented', 'twodigit', 'scale', 'ie', 'discrete', 'score', 'round', '0', '5', 'decimal', 'place', 'discrete', 'score', 'obtain', 'student', '1990', 'Reference', 'Group', '35', 'different', 'edition', 'SAT', 'span', 'October', '1988', 'June', '1990', 'performance', '1990', 'Reference', 'Group', 'original', 'recentered', 'scale', 'describe', 'effect', 'recentere', 'score', 'individual', '1990', 'Reference', 'Group', 'examine', 'finally', 'recentere', 'occur', 'solely', 'basis', 'technical', 'merit', 'Issues', 'associate', 'convert', 'recentere', 'possibility', 'reality', 'discuss']","['recentere', 'realign', 'SAT', 'score', 'distribution']",process employ produce conversion score original SAT scale recentere scale reference group score center near midpoint scorereporting range lay purpose article SAT Verbal SAT Mathematical score place recentere scale report range 920 980 mean 950 standard deviation 11 920to980 scale article highlight distinction old 200to800 scale actuality recentere score report 200to800 scale recentere accomplish linear transformation normally distribute score obtain continuize smoothed frequency distribution original SAT score originally augmented twodigit scale ie discrete score round 0 5 decimal place discrete score obtain student 1990 Reference Group 35 different edition SAT span October 1988 June 1990 performance 1990 Reference Group original recentered scale describe effect recentere score individual 1990 Reference Group examine finally recentere occur solely basis technical merit Issues associate convert recentere possibility reality discuss,recentere realign SAT score distribution,0.026229009154485165,0.02621740582411132,0.026211674783421523,0.8948740912278261,0.026467819010156005,0.0,0.05425451307532205,0.008446982152830825,0.0,0.038789066616014986
Wainer H.; Wang X.,Using a new statistical model for testlets to score TOEFL,2000,37,"Standard item response theory (IRT) models fit to examination responses ignore the fact that sets of items (testlets) often are matched with a single common stimulus (e.g., a reading comprehension passage). In this setting, all items given to an examinee are unlikely to be conditionally independent (given examinee proficiency). Models that assume conditional independence will overestimate the precision with which examinee proficiency is measured. Overstatement of precision may lead to inaccurate inferences as well as prematurely ended examinations in which the stopping rule is based on the estimated standard error of examinee proficiency (e.g., an adaptive test). The standard three parameter IRT model was modified to include an additional random effect for items nested within the same testlet (Wainer, Bradlow, & Du, 2000). This parameter, γ, characterizes the amount of local dependence in a testlet. We fit 86 TOEFL testlets (50 reading comprehension and 36 listening comprehension) with the new model, and obtained a value for the variance of γ for each testlet. We compared the standard parameters (discrimination (a), difficulty (b) and guessing (c)) with what is obtained through traditional modeling. We found that difficulties were well estimated either way, but estimates of both a and c were biased if conditional independence is incorrectly assumed. Of greater import, we found that test information was substantially over-estimated when conditional independence was incorrectly assumed.",Using a new statistical model for testlets to score TOEFL,"Standard item response theory (IRT) models fit to examination responses ignore the fact that sets of items (testlets) often are matched with a single common stimulus (e.g., a reading comprehension passage). In this setting, all items given to an examinee are unlikely to be conditionally independent (given examinee proficiency). Models that assume conditional independence will overestimate the precision with which examinee proficiency is measured. Overstatement of precision may lead to inaccurate inferences as well as prematurely ended examinations in which the stopping rule is based on the estimated standard error of examinee proficiency (e.g., an adaptive test). The standard three parameter IRT model was modified to include an additional random effect for items nested within the same testlet (Wainer, Bradlow, & Du, 2000). This parameter, γ, characterizes the amount of local dependence in a testlet. We fit 86 TOEFL testlets (50 reading comprehension and 36 listening comprehension) with the new model, and obtained a value for the variance of γ for each testlet. We compared the standard parameters (discrimination (a), difficulty (b) and guessing (c)) with what is obtained through traditional modeling. We found that difficulties were well estimated either way, but estimates of both a and c were biased if conditional independence is incorrectly assumed. Of greater import, we found that test information was substantially over-estimated when conditional independence was incorrectly assumed.","['standard', 'item', 'response', 'theory', 'IRT', 'fit', 'examination', 'response', 'ignore', 'fact', 'set', 'item', 'testlet', 'match', 'single', 'common', 'stimulus', 'eg', 'reading', 'comprehension', 'passage', 'set', 'item', 'examinee', 'unlikely', 'conditionally', 'independent', 'examinee', 'proficiency', 'Models', 'assume', 'conditional', 'independence', 'overestimate', 'precision', 'examinee', 'proficiency', 'measure', 'Overstatement', 'precision', 'lead', 'inaccurate', 'inference', 'prematurely', 'end', 'examination', 'stopping', 'rule', 'base', 'estimate', 'standard', 'error', 'examinee', 'proficiency', 'eg', 'adaptive', 'test', 'standard', 'parameter', 'IRT', 'modify', 'include', 'additional', 'random', 'effect', 'item', 'nest', 'testlet', 'Wainer', 'Bradlow', 'Du', '2000', 'parameter', 'γ', 'characterize', 'local', 'dependence', 'testlet', 'fit', '86', 'toefl', 'testlet', '50', 'reading', 'comprehension', '36', 'listening', 'comprehension', 'new', 'obtain', 'value', 'variance', 'γ', 'testlet', 'compare', 'standard', 'parameter', 'discrimination', 'difficulty', 'b', 'guess', 'c', 'obtain', 'traditional', 'modeling', 'find', 'difficulty', 'estimate', 'way', 'estimate', 'c', 'bias', 'conditional', 'independence', 'incorrectly', 'assume', 'great', 'import', 'find', 'test', 'information', 'substantially', 'overestimate', 'conditional', 'independence', 'incorrectly', 'assume']","['new', 'statistical', 'testlet', 'score', 'toefl']",standard item response theory IRT fit examination response ignore fact set item testlet match single common stimulus eg reading comprehension passage set item examinee unlikely conditionally independent examinee proficiency Models assume conditional independence overestimate precision examinee proficiency measure Overstatement precision lead inaccurate inference prematurely end examination stopping rule base estimate standard error examinee proficiency eg adaptive test standard parameter IRT modify include additional random effect item nest testlet Wainer Bradlow Du 2000 parameter γ characterize local dependence testlet fit 86 toefl testlet 50 reading comprehension 36 listening comprehension new obtain value variance γ testlet compare standard parameter discrimination difficulty b guess c obtain traditional modeling find difficulty estimate way estimate c bias conditional independence incorrectly assume great import find test information substantially overestimate conditional independence incorrectly assume,new statistical testlet score toefl,0.02239545884990994,0.0223947592501254,0.022413547515518523,0.9103573582999974,0.022438876084448802,0.0,0.03131717553179234,0.0,0.08003924336315918,0.0
Scrams D.J.; McLeod L.D.,An expected response function approach to graphical differential item functioning,2000,37,"In this paper a new approach to graphical differential item functioning (DIF) is offered. The methodology is based on a sampling-theory approach to expected response functions (Lewis, 1985; Mislevy, Wingersky, & Sheehan, 1994). Essentially, error in item calibrations is modeled explicitly, and repeated samples are taken from the posterior distributions of the item parameters. Sampled parameter values are used to estimate the posterior distribution of the difference in item characteristic curves (ICCs) for two groups. A point-wise expectation is taken as an estimate of the true difference between the ICCs, and the sampled-difference functions indicate uncertainty in the estimate. The approach is applied to a set of pretest items, and the results are compared to traditional Mantel-Haenszel DIF statistics. The expected-response-function approach is contrasted with Pashley's (1992) graphical DIF approach.",An expected response function approach to graphical differential item functioning,"In this paper a new approach to graphical differential item functioning (DIF) is offered. The methodology is based on a sampling-theory approach to expected response functions (Lewis, 1985; Mislevy, Wingersky, & Sheehan, 1994). Essentially, error in item calibrations is modeled explicitly, and repeated samples are taken from the posterior distributions of the item parameters. Sampled parameter values are used to estimate the posterior distribution of the difference in item characteristic curves (ICCs) for two groups. A point-wise expectation is taken as an estimate of the true difference between the ICCs, and the sampled-difference functions indicate uncertainty in the estimate. The approach is applied to a set of pretest items, and the results are compared to traditional Mantel-Haenszel DIF statistics. The expected-response-function approach is contrasted with Pashley's (1992) graphical DIF approach.","['paper', 'new', 'approach', 'graphical', 'differential', 'item', 'function', 'DIF', 'offer', 'methodology', 'base', 'samplingtheory', 'approach', 'expect', 'response', 'function', 'Lewis', '1985', 'Mislevy', 'Wingersky', 'Sheehan', '1994', 'Essentially', 'error', 'item', 'calibration', 'explicitly', 'repeat', 'sample', 'posterior', 'distribution', 'item', 'parameter', 'sample', 'parameter', 'value', 'estimate', 'posterior', 'distribution', 'difference', 'item', 'characteristic', 'curve', 'icc', 'group', 'pointwise', 'expectation', 'estimate', 'true', 'difference', 'icc', 'sampleddifference', 'function', 'indicate', 'uncertainty', 'estimate', 'approach', 'apply', 'set', 'pret', 'item', 'result', 'compare', 'traditional', 'MantelHaenszel', 'DIF', 'statistic', 'expectedresponsefunction', 'approach', 'contrast', 'Pashleys', '1992', 'graphical', 'DIF', 'approach']","['expected', 'response', 'function', 'approach', 'graphical', 'differential', 'item', 'function']",paper new approach graphical differential item function DIF offer methodology base samplingtheory approach expect response function Lewis 1985 Mislevy Wingersky Sheehan 1994 Essentially error item calibration explicitly repeat sample posterior distribution item parameter sample parameter value estimate posterior distribution difference item characteristic curve icc group pointwise expectation estimate true difference icc sampleddifference function indicate uncertainty estimate approach apply set pret item result compare traditional MantelHaenszel DIF statistic expectedresponsefunction approach contrast Pashleys 1992 graphical DIF approach,expected response function approach graphical differential item function,0.026065912693964147,0.026065055438420453,0.026066635765924728,0.89558933696064,0.02621305914105061,0.08683578764945167,0.024211752105079122,0.002179799209614252,0.02596741115205848,0.0
Bassiri D.; Schulz E.M.,Constructing a universal scale of high school course difficulty,2003,40,"This study examined the usefulness of applying the Rasch rating scale model (Andrich, 1978) to high school grade data. ACT Assessment test scores (English, Mathematics, Reading, and Science Reasoning) were used as ""common items"" to adjust for different grading standards in individual high school courses both within and across schools. This scaling approach yielded an ACT Assessment-adjusted high school grade point average (AA-HSGPA) on a common scale across high schools and cohorts within a large public university. AA-HSGPA was a better predictor of first-year college grade point average (CGPA) than the regular high school grade point average. The best model for predicting CGPA included both the ACT composite score and AA-HSGPA.",Constructing a universal scale of high school course difficulty,"This study examined the usefulness of applying the Rasch rating scale model (Andrich, 1978) to high school grade data. ACT Assessment test scores (English, Mathematics, Reading, and Science Reasoning) were used as ""common items"" to adjust for different grading standards in individual high school courses both within and across schools. This scaling approach yielded an ACT Assessment-adjusted high school grade point average (AA-HSGPA) on a common scale across high schools and cohorts within a large public university. AA-HSGPA was a better predictor of first-year college grade point average (CGPA) than the regular high school grade point average. The best model for predicting CGPA included both the ACT composite score and AA-HSGPA.","['study', 'examine', 'usefulness', 'apply', 'Rasch', 'rating', 'scale', 'Andrich', '1978', 'high', 'school', 'grade', 'datum', 'ACT', 'Assessment', 'test', 'score', 'English', 'Mathematics', 'Reading', 'Science', 'Reasoning', 'common', 'item', 'adjust', 'different', 'grade', 'standard', 'individual', 'high', 'school', 'course', 'school', 'scaling', 'approach', 'yield', 'ACT', 'Assessmentadjusted', 'high', 'school', 'grade', 'point', 'average', 'AAHSGPA', 'common', 'scale', 'high', 'school', 'cohort', 'large', 'public', 'university', 'AAHSGPA', 'predictor', 'firstyear', 'college', 'grade', 'point', 'average', 'CGPA', 'regular', 'high', 'school', 'grade', 'point', 'average', 'good', 'predict', 'CGPA', 'include', 'ACT', 'composite', 'score', 'aahsgpa']","['construct', 'universal', 'scale', 'high', 'school', 'course', 'difficulty']",study examine usefulness apply Rasch rating scale Andrich 1978 high school grade datum ACT Assessment test score English Mathematics Reading Science Reasoning common item adjust different grade standard individual high school course school scaling approach yield ACT Assessmentadjusted high school grade point average AAHSGPA common scale high school cohort large public university AAHSGPA predictor firstyear college grade point average CGPA regular high school grade point average good predict CGPA include ACT composite score aahsgpa,construct universal scale high school course difficulty,0.03012513565235975,0.030122428655635195,0.03034708068000888,0.03189998504521516,0.877505369966781,0.0,0.0,0.0,0.0,0.19882681199694988
Penfield R.D.; Algina J.,Applying the Liu-Agresti Estimator of the Cumulative Common Odds Ratio to DIF Detection in Polytomous Items,2003,40,"Liu and Agresti (1996) proposed a Mantel and Haenszel-type (1959) estimator of a common odds ratio for several 2 × J tables, where the J columns are ordinal levels of a response variable. This article applies the Liu-Agresti estimator to the case of assessing differential item functioning (DIF) in items having an ordinal response variable. A simulation study was conducted to investigate the accuracy of the Liu-Agresti estimator in relation to other statistical DIF detection procedures. The results of the simulation study indicate that the Liu-Agresti estimator is a viable alternative to other DIF detection statistics.",Applying the Liu-Agresti Estimator of the Cumulative Common Odds Ratio to DIF Detection in Polytomous Items,"Liu and Agresti (1996) proposed a Mantel and Haenszel-type (1959) estimator of a common odds ratio for several 2 × J tables, where the J columns are ordinal levels of a response variable. This article applies the Liu-Agresti estimator to the case of assessing differential item functioning (DIF) in items having an ordinal response variable. A simulation study was conducted to investigate the accuracy of the Liu-Agresti estimator in relation to other statistical DIF detection procedures. The results of the simulation study indicate that the Liu-Agresti estimator is a viable alternative to other DIF detection statistics.","['Liu', 'Agresti', '1996', 'propose', 'Mantel', 'Haenszeltype', '1959', 'estimator', 'common', 'odd', 'ratio', '2', '×', 'J', 'table', 'J', 'column', 'ordinal', 'level', 'response', 'variable', 'article', 'apply', 'LiuAgresti', 'estimator', 'case', 'assess', 'differential', 'item', 'function', 'DIF', 'item', 'ordinal', 'response', 'variable', 'simulation', 'study', 'conduct', 'investigate', 'accuracy', 'LiuAgresti', 'estimator', 'relation', 'statistical', 'dif', 'detection', 'procedure', 'result', 'simulation', 'study', 'indicate', 'LiuAgresti', 'estimator', 'viable', 'alternative', 'DIF', 'detection', 'statistic']","['apply', 'LiuAgresti', 'Estimator', 'Cumulative', 'Common', 'Odds', 'ratio', 'DIF', 'Detection', 'Polytomous', 'Items']",Liu Agresti 1996 propose Mantel Haenszeltype 1959 estimator common odd ratio 2 × J table J column ordinal level response variable article apply LiuAgresti estimator case assess differential item function DIF item ordinal response variable simulation study conduct investigate accuracy LiuAgresti estimator relation statistical dif detection procedure result simulation study indicate LiuAgresti estimator viable alternative DIF detection statistic,apply LiuAgresti Estimator Cumulative Common Odds ratio DIF Detection Polytomous Items,0.031027579664413166,0.031028514299554985,0.031066040193044674,0.03518804981895322,0.8716898160240338,0.11935231210323419,0.0,0.0,0.0,0.0
Cole N.S.; Zieky M.J.,The new faces of fairness,2001,38,"For most of the 20th century, measurement professionals paid little interest to item and test fairness. A confluence of events in the late 1960s and early 1970s led to an intense interest in fairness issues among measurement professionals. In spite of more than 30 years of effort, there is still no generally accepted definition of fairness with respect to testing and no measure that can prove or disprove the fairness of a test. To advance the fairness of tests, measurement professionals must pay more attention to reducing group differences at the design stage of test development, to providing all examinees an opportunity to demonstrate their knowledge and skills, to deterring test misuse, and to accommodating differences among individuals.",,"For most of the 20th century, measurement professionals paid little interest to item and test fairness. A confluence of events in the late 1960s and early 1970s led to an intense interest in fairness issues among measurement professionals. In spite of more than 30 years of effort, there is still no generally accepted definition of fairness with respect to testing and no measure that can prove or disprove the fairness of a test. To advance the fairness of tests, measurement professionals must pay more attention to reducing group differences at the design stage of test development, to providing all examinees an opportunity to demonstrate their knowledge and skills, to deterring test misuse, and to accommodating differences among individuals.","['20th', 'century', 'professional', 'pay', 'little', 'interest', 'item', 'test', 'fairness', 'confluence', 'event', 'late', '1960', 'early', '1970', 'lead', 'intense', 'interest', 'fairness', 'issue', 'professional', 'spite', '30', 'year', 'effort', 'generally', 'accept', 'definition', 'fairness', 'respect', 'testing', 'measure', 'prove', 'disprove', 'fairness', 'test', 'advance', 'fairness', 'test', 'professional', 'pay', 'attention', 'reduce', 'group', 'difference', 'design', 'stage', 'test', 'development', 'provide', 'examine', 'opportunity', 'demonstrate', 'knowledge', 'skill', 'deter', 'test', 'misuse', 'accommodate', 'difference', 'individual']",,20th century professional pay little interest item test fairness confluence event late 1960 early 1970 lead intense interest fairness issue professional spite 30 year effort generally accept definition fairness respect testing measure prove disprove fairness test advance fairness test professional pay attention reduce group difference design stage test development provide examine opportunity demonstrate knowledge skill deter test misuse accommodate difference individual,,0.4979319946688072,0.02943935703231839,0.029447158455725965,0.41353424028176966,0.029647249561378687,0.013023563873844676,0.012087103628416373,0.002977891752083038,0.006933979975095527,0.0294349188782535
Stone C.A.,Monte Carlo based null distribution for an alternative goodness-of-fit test statistic in IRT models,2000,37,"Assessing the correspondence between model predictions and observed data is a recommended procedure for justifying the application of an IRT model. However, with shorter tests, current goodness-of-fit procedures that assume precise point estimates of ability, are inappropriate. The present paper describes a goodness-of-fit statistic that considers the imprecision with which ability is estimated and involves constructing item fit tables based on each examinee's posterior distribution of ability, given the likelihood of their response pattern and an assumed marginal ability distribution. However, the posterior expectations that are computed are dependent and the distribution of the goodness-of-fit statistic is unknown. The present paper also describes a Monte Carlo resampling procedure that can be used to assess the significance of the fit statistic and compares this method with a previously used method. The results indicate that the method described herein is an effective and reasonably simple procedure for assessing the validity of applying IRT models when ability estimates are imprecise.",Monte Carlo based null distribution for an alternative goodness-of-fit test statistic in IRT models,"Assessing the correspondence between model predictions and observed data is a recommended procedure for justifying the application of an IRT model. However, with shorter tests, current goodness-of-fit procedures that assume precise point estimates of ability, are inappropriate. The present paper describes a goodness-of-fit statistic that considers the imprecision with which ability is estimated and involves constructing item fit tables based on each examinee's posterior distribution of ability, given the likelihood of their response pattern and an assumed marginal ability distribution. However, the posterior expectations that are computed are dependent and the distribution of the goodness-of-fit statistic is unknown. The present paper also describes a Monte Carlo resampling procedure that can be used to assess the significance of the fit statistic and compares this method with a previously used method. The results indicate that the method described herein is an effective and reasonably simple procedure for assessing the validity of applying IRT models when ability estimates are imprecise.","['assess', 'correspondence', 'prediction', 'observed', 'datum', 'recommend', 'procedure', 'justify', 'application', 'IRT', 'short', 'test', 'current', 'goodnessoffit', 'procedure', 'assume', 'precise', 'point', 'estimate', 'ability', 'inappropriate', 'present', 'paper', 'describe', 'goodnessoffit', 'statistic', 'consider', 'imprecision', 'ability', 'estimate', 'involve', 'construct', 'item', 'fit', 'table', 'base', 'examinee', 'posterior', 'distribution', 'ability', 'likelihood', 'response', 'pattern', 'assume', 'marginal', 'ability', 'distribution', 'posterior', 'expectation', 'compute', 'dependent', 'distribution', 'goodnessoffit', 'statistic', 'unknown', 'present', 'paper', 'describe', 'Monte', 'Carlo', 'resampling', 'procedure', 'assess', 'significance', 'fit', 'statistic', 'compare', 'method', 'previously', 'method', 'result', 'indicate', 'method', 'describe', 'effective', 'reasonably', 'simple', 'procedure', 'assess', 'validity', 'apply', 'IRT', 'ability', 'estimate', 'imprecise']","['Monte', 'Carlo', 'base', 'null', 'distribution', 'alternative', 'goodnessoffit', 'test', 'statistic', 'IRT']",assess correspondence prediction observed datum recommend procedure justify application IRT short test current goodnessoffit procedure assume precise point estimate ability inappropriate present paper describe goodnessoffit statistic consider imprecision ability estimate involve construct item fit table base examinee posterior distribution ability likelihood response pattern assume marginal ability distribution posterior expectation compute dependent distribution goodnessoffit statistic unknown present paper describe Monte Carlo resampling procedure assess significance fit statistic compare method previously method result indicate method describe effective reasonably simple procedure assess validity apply IRT ability estimate imprecise,Monte Carlo base null distribution alternative goodnessoffit test statistic IRT,0.02584703476962687,0.025847759857225323,0.025871269814631847,0.8963131541366818,0.026120781421834178,0.0,0.09158428652863611,0.0013404017858676908,0.05492098556881954,0.0
Cicmanec K.M.,Standards-based scoring and traditional grading practices,2001,38,"Reviews the book 'Standards-Based Scoring and Traditional Grading Practices,' edited by E. Trumbull and B. Farr.",Standards-based scoring and traditional grading practices,"Reviews the book 'Standards-Based Scoring and Traditional Grading Practices,' edited by E. Trumbull and B. Farr.","['review', 'book', 'StandardsBased', 'Scoring', 'Traditional', 'Grading', 'Practices', 'edit', 'E', 'Trumbull', 'B', 'Farr']","['standardsbased', 'scoring', 'traditional', 'grade', 'practice']",review book StandardsBased Scoring Traditional Grading Practices edit E Trumbull B Farr,standardsbased scoring traditional grade practice,0.8129514790101017,0.04573475078133247,0.04587133903965022,0.047901147396160484,0.047541283772755086,0.0,0.0004317117891638773,0.0,0.00011511487739643378,0.052982344742148614
