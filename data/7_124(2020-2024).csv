Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
Domingue B.W.; Kanopka K.; Stenhaug B.; Soland J.; Kuhfeld M.; Wise S.; Piech C.,"Domingue, Benjamin W. (37103720900); Kanopka, Klint (57207731590); Stenhaug, Ben (57214128465); Soland, James (55960263300); Kuhfeld, Megan (56326201500); Wise, Steve (7202622563); Piech, Chris (55142243300)",37103720900; 57207731590; 57214128465; 55960263300; 56326201500; 7202622563; 55142243300,Variation in Respondent Speed and its Implications: Evidence from an Adaptive Testing Scenario,2021,Journal of Educational Measurement,58,3,,335,363,28,7,10.1111/jedm.12291,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108352505&doi=10.1111%2fjedm.12291&partnerID=40&md5=a7d8c01d12e0a1099d94faf597839181,"The more frequent collection of response time data is leading to an increased need for an understanding of how such data can be included in measurement models. Models for response time have been advanced, but relatively limited large-scale empirical investigations have been conducted. We take advantage of a large data set from the adaptive NWEA MAP Growth Reading Assessment to shed light on emergent features of response time behavior. We identify two behaviors in particular. The first, response acceleration, is a reduction in response time for responses that occur later in the assessment. We note that such reductions are heterogeneous as a function of estimated ability (lower ability estimates are associated with larger increases in acceleration) and that reductions in response time lead to lower accuracy relative to expectation for lower ability students. The second is within-person variation in the association between time usage and accuracy. Idiosyncratic within-person changes in response time have inconsistent implications for accuracy; in some cases additional response time predicts higher accuracy but in other cases additional response time predicts declines in accuracy. These findings have implications for models that incorporate response time and accuracy. Our approach may be useful in other studies of adaptive testing data. © 2021 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85108352505
Baldwin P.; Yaneva V.; Mee J.; Clauser B.E.; Ha L.A.,"Baldwin, Peter (34867881100); Yaneva, Victoria (57003253500); Mee, Janet (23498092200); Clauser, Brian E. (7003595460); Ha, Le An (13612155200)",34867881100; 57003253500; 23498092200; 7003595460; 13612155200,Using Natural Language Processing to Predict Item Response Times and Improve Test Construction,2021,Journal of Educational Measurement,58,1,,4,30,26,9,10.1111/jedm.12264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102484910&doi=10.1111%2fjedm.12264&partnerID=40&md5=c607a7ea8212680fd8196c00e6031865,"In this article, it is shown how item text can be represented by (a) 113 features quantifying the text's linguistic characteristics, (b) 16 measures of the extent to which an information-retrieval-based automatic question-answering system finds an item challenging, and (c) through dense word representations (word embeddings). Using a random forests algorithm, these data then are used to train a prediction model for item response times and predicted response times then are used to assemble test forms. Using empirical data from the United States Medical Licensing Examination, we show that timing demands are more consistent across these specially assembled forms than across forms comprising randomly-selected items. Because an exam's timing conditions affect examinee performance, this result has implications for exam fairness whenever examinees are compared with each other or against a common standard. © 2020 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85102484910
Lim H.; Choe E.M.; Han K.T.,"Lim, Hwanggyu (57216896756); Choe, Edison M. (57197771137); Han, Kyung T. (19638651100)",57216896756; 57197771137; 19638651100,A Residual-Based Differential Item Functioning Detection Framework in Item Response Theory,2022,Journal of Educational Measurement,59,1,,80,104,24,6,10.1111/jedm.12313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127265688&doi=10.1111%2fjedm.12313&partnerID=40&md5=251186c900e2a4bd14808c1c170b54bc,"Differential item functioning (DIF) of test items should be evaluated using practical methods that can produce accurate and useful results. Among a plethora of DIF detection techniques, we introduce the new Residual DIF (RDIF) framework, which stands out for its accessibility without sacrificing efficacy. This framework consists of three item response theory (IRT) residual statistics: (Formula presented.), (Formula presented.), and (Formula presented.). We conducted a simulation study with a 40-item test to assess the performance of RDIF in comparison with the Mantel-Haenszel, logistic regression, and IRT-based likelihood ratio test methods. Even when analyzing small sample sizes, the results revealed (Formula presented.) to be the most robust DIF detection statistic with strict control of Type I error across all simulated conditions when paired with the purification procedure. Also, (Formula presented.) and (Formula presented.) proved to be powerful indicators of uniform and nonuniform DIF, respectively. Therefore, (Formula presented.) should serve as the primary flagging criterion, whereas (Formula presented.) and (Formula presented.) best serve as indicators of DIF type. An empirical DIF study also showed that the RDIF framework could perform satisfactorily with real data from a large-scale assessment. Overall, the RDIF framework demonstrated its potential as a new standard for IRT-based DIF detection methodology. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85127265688
Liao X.; Bolt D.M.; Kim J.-S.,"Liao, Xiangyi (57222607143); Bolt, Daniel M. (57223443138); Kim, Jee-Seon (8849255200)",57222607143; 57223443138; 8849255200,Curvilinearity in the Reference Composite and Practical Implications for Measurement,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195021316&doi=10.1111%2fjedm.12402&partnerID=40&md5=ce40732070e1bd260a120c9bc1d1bb2f,"Item difficulty and dimensionality often correlate, implying that unidimensional IRT approximations to multidimensional data (i.e., reference composites) can take a curvilinear form in the multidimensional space. Although this issue has been previously discussed in the context of vertical scaling applications, we illustrate how such a phenomenon can also easily occur within individual tests. Measures of reading proficiency, for example, often use different task types within a single assessment, a feature that may not only lead to multidimensionality, but also an association between item difficulty and dimensionality. Using a latent regression strategy, we demonstrate through simulations and empirical analysis how associations between dimensionality and difficulty yield a nonlinear reference composite where the weights of the underlying dimensions change across the scale continuum according to the difficulties of the items associated with the dimensions. We further show how this form of curvilinearity produces systematic forms of misspecification in traditional unidimensional IRT models (e.g., 2PL) and can be better accommodated by models such as monotone-polynomial or asymmetric IRT models. Simulations and a real-data example from the Early Childhood Longitudinal Study—Kindergarten are provided for demonstration. Some implications for measurement modeling and for understanding the effects of 2PL misspecification on measurement metrics are discussed. © 2024 The Author(s). Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85195021316
Kim S.Y.; Lee W.-C.,"Kim, Stella Y. (57207794797); Lee, Won-Chan (57203094500)",57207794797; 57203094500,Several Variations of Simple-Structure MIRT Equating,2023,Journal of Educational Measurement,60,1,,76,105,29,0,10.1111/jedm.12341,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135020339&doi=10.1111%2fjedm.12341&partnerID=40&md5=ccc23cd04dccb9171f8b3016736d8604,"The current study proposed several variants of simple-structure multidimensional item response theory equating procedures. Four distinct sets of data were used to demonstrate feasibility of proposed equating methods for two different equating designs: a random groups design and a common-item nonequivalent groups design. Findings indicated some notable differences between the multidimensional and unidimensional approaches when data exhibited evidence for multidimensionality. In addition, some of the proposed methods were successful in providing equating results for both section-level and composite-level scores, which has not been achieved by most of the existing methodologies. The traditional method of using a set of quadrature points and weights for equating turned out to be computationally intensive, particularly for the data with higher dimensions. The study suggested an alternative way of using the Monte-Carlo approach for such data. This study also proposed a simple-structure true-score equating procedure that does not rely on a multivariate observed-score distribution. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85135020339
Li D.,"Li, Dongmei (57770170000)",57770170000,Assessing the Impact of Equating Error on Group Means and Group Mean Differences,2022,Journal of Educational Measurement,59,1,,62,79,17,0,10.1111/jedm.12311,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126300847&doi=10.1111%2fjedm.12311&partnerID=40&md5=e13a8087c6a3cd50fb9012cf4441baf5,"Equating error is usually small relative to the magnitude of measurement error, but it could be one of the major sources of error contributing to mean scores of large groups in educational measurement, such as the year-to-year state mean score fluctuations. Though testing programs may routinely calculate the standard error of equating (SEE), the commonly used summary statistics of SEE are not direct quantifications of the impact of equating error on group means. This article proposed summary statistics that directly quantify the impact of equating error on group means or group mean differences and provided empirical and analytical methods to estimate these statistics. Examples based on empirical data were used to illustrate practical applications of these statistics. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85126300847
Wise S.L.; Kuhfeld M.R.,"Wise, Steven L. (7202622563); Kuhfeld, Megan R. (56326201500)",7202622563; 56326201500,Using Retest Data to Evaluate and Improve Effort-Moderated Scoring,2021,Journal of Educational Measurement,58,1,,130,149,19,30,10.1111/jedm.12275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087159385&doi=10.1111%2fjedm.12275&partnerID=40&md5=3bc153b2a207298c59ead878d17ac293,"There has been a growing research interest in the identification and management of disengaged test taking, which poses a validity threat that is particularly prevalent with low-stakes tests. This study investigated effort-moderated (E-M) scoring, in which item responses classified as rapid guesses are identified and excluded from scoring. Using achievement test data composed of test takers who were quickly retested and showed differential degrees of disengagement, three basic findings emerged. First, standard E-M scoring accounted for roughly one-third of the score distortion due to differential disengagement. Second, a modified E-M scoring method that used more liberal time thresholds performed better—accounting for two-thirds or more of the distortion. Finally, the inability of E-M scoring to account for all of the score distortion suggests the additional presence of nonrapid item responses that reflect less-than-full engagement by some test takers. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85087159385
Setzer J.C.; Cheng Y.; Liu C.,"Setzer, J. Carl (26039416900); Cheng, Ying (36107474600); Liu, Cheng (56124182300)",26039416900; 36107474600; 56124182300,Classification Accuracy and Consistency of Compensatory Composite Test Scores,2023,Journal of Educational Measurement,60,3,,501,519,18,0,10.1111/jedm.12357,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147310697&doi=10.1111%2fjedm.12357&partnerID=40&md5=3be60d2e8595b93946e8f70144d65ffb,"Test scores are often used to make decisions about examinees, such as in licensure and certification testing, as well as in many educational contexts. In some cases, these decisions are based upon compensatory scores, such as those from multiple sections or components of an exam. Classification accuracy and classification consistency are two psychometric characteristics of test scores that are often reported when decisions are based on those scores, and several techniques currently exist for estimating both accuracy and consistency. However, research on classification accuracy and consistency on compensatory test scores is scarce. This study demonstrates two techniques that can be used to estimate classification accuracy and consistency when test scores are used in a compensatory manner. First, a simulation study demonstrates that both methods provide very similar results under the studied conditions. Second, we demonstrate how the two methods could be used with a high-stakes licensure exam. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85147310697
Goldhammer F.; Kroehne U.; Hahnel C.; Naumann J.; De Boeck P.,"Goldhammer, Frank (9274883100); Kroehne, Ulf (55849736300); Hahnel, Carolin (56904315700); Naumann, Johannes (57023455400); De Boeck, Paul (7005323510)",9274883100; 55849736300; 56904315700; 57023455400; 7005323510,Does Timed Testing Affect the Interpretation of Efficiency Scores?—A GLMM Analysis of Reading Components,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192748578&doi=10.1111%2fjedm.12393&partnerID=40&md5=ba4d27c077f41f91a4925c77e5078af5,"The efficiency of cognitive component skills is typically assessed with speeded performance tests. Interpreting only effective ability or effective speed as efficiency may be challenging because of the within-person dependency between both variables (speed-ability tradeoff, SAT). The present study measures efficiency as effective ability conditional on speed by controlling speed experimentally. Item-level time limits control the stimulus presentation time and the time window for responding (timed condition). The overall goal was to examine the construct validity of effective ability scores obtained from untimed and timed condition by comparing the effects of theory-based item properties on item difficulty. If such effects exist, the scores reflect how well the test-takers were able to cope with the theory-based requirements. A German subsample from PISA 2012 completed two reading component skills tasks (i.e., word recognition and semantic integration) with and without item-level time limits. Overall, the included linguistic item properties showed stronger effects on item difficulty in the timed than the untimed condition. In the semantic integration task, item properties explained the time required in the untimed condition. The results suggest that effective ability scores in the timed condition better reflect how well test-takers were able to cope with the theoretically relevant task demands. © 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85192748578
Madison M.J.; Wind S.A.; Maas L.; Yamaguchi K.; Haab S.,"Madison, Matthew J. (56670984400); Wind, Stefanie A. (55616798300); Maas, Lientje (57216944455); Yamaguchi, Kazuhiro (57200510912); Haab, Sergio (58545967800)",56670984400; 55616798300; 57216944455; 57200510912; 58545967800,A One-Parameter Diagnostic Classification Model with Familiar Measurement Properties,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12390,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191353513&doi=10.1111%2fjedm.12390&partnerID=40&md5=a945c232a865ba9574a3ad0d43e6da91,"Diagnostic classification models (DCMs) are psychometric models designed to classify examinees according to their proficiency or nonproficiency of specified latent characteristics. These models are well suited for providing diagnostic and actionable feedback to support intermediate and formative assessment efforts. Several DCMs have been developed and applied in different settings. This study examines a DCM with functional form similar to the 1-parameter logistic item response theory model. Using data from a large-scale mathematics education research study, we demonstrate and prove that the proposed DCM has measurement properties akin to the Rasch and one-parameter logistic item response theory models, including sum score sufficiency, item-free and person-free measurement, and invariant item and person ordering. We introduce some potential applications for this model, and discuss the implications and limitations of these developments, as well as directions for future research. © 2024 by the National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Green Open Access,Scopus,2-s2.0-85191353513
Gerasimova D.,"Gerasimova, Daria (57192558862)",57192558862,Argument-Based Approach to Validity: Developing a Living Document and Incorporating Preregistration,2024,Journal of Educational Measurement,61,2,,252,273,21,0,10.1111/jedm.12385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195096718&doi=10.1111%2fjedm.12385&partnerID=40&md5=0f53369e23a451fc5d466f2d1e51cdd4,"I propose two practical advances to the argument-based approach to validity: developing a living document and incorporating preregistration. First, I present a potential structure for the living document that includes an up-to-date summary of the validity argument. As the validation process may span across multiple studies, the living document allows future users of the instrument to access the entire validity argument in one place. Second, I describe how preregistration can be incorporated in the argument-based approach. Specifically, I distinguish between two types of preregistration: preregistration of the argument and preregistration of validation studies. Preregistration of the argument is a single preregistration that is specified for the entire validation process. Here, the developer specifies interpretations, uses, and claims before collecting validity evidence. Preregistration of a validation study refers to preregistering a single validation study that aims to evaluate a set of claims. Here, the developer describes study components (e.g., research design, data collection, data analysis, etc.), before collecting data. Both preregistration types have the potential to reduce the risk of bias (e.g., hindsight and confirmation biases), as well as to allow others to evaluate the risk of bias and, hence, calibrate confidence, in the developer's evaluation of the validity argument. © 2024 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85195096718
Fu J.; Tan X.; Kyllonen P.C.,"Fu, Jianbin (57205675462); Tan, Xuan (58667903100); Kyllonen, Patrick C. (6602515483)",57205675462; 58667903100; 6602515483,Information Functions of Rank-2PL Models for Forced-Choice Questionnaires,2024,Journal of Educational Measurement,61,1,,125,149,24,0,10.1111/jedm.12379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175099272&doi=10.1111%2fjedm.12379&partnerID=40&md5=d1e3f9c39fcb9c5468beb8dee15d6100,"This paper presents the item and test information functions of the Rank two-parameter logistic models (Rank-2PLM) for items with two (pair) and three (triplet) statements in forced-choice questionnaires. The Rank-2PLM model for pairs is the MUPP-2PLM (Multi-Unidimensional Pairwise Preference) and, for triplets, is the Triplet-2PLM. Fisher's information and directional information are described, and the test information for Maximum Likelihood (ML), Maximum A Posterior (MAP), and Expected A Posterior (EAP) trait score estimates is distinguished. Expected item/test information indexes at various levels are proposed and plotted to provide diagnostic information on items and tests. The expected test information indexes for EAP scores may be difficult to compute due to a typical test's vast number of item response patterns. The relationships of item/test information with discrimination parameters of statements, standard error, and reliability estimates of trait score estimates are discussed and demonstrated using real data. Practical suggestions for checking the various expected item/test information indexes and plots are provided. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85175099272
Castellano K.E.; McCaffrey D.F.; Lockwood J.R.,"Castellano, Katherine E. (55633157900); McCaffrey, Daniel F. (7005049755); Lockwood, J.R. (7102841578)",55633157900; 7005049755; 7102841578,An Exploration of an Improved Aggregate Student Growth Measure Using Data from Two States,2023,Journal of Educational Measurement,60,2,,173,201,28,0,10.1111/jedm.12354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147370896&doi=10.1111%2fjedm.12354&partnerID=40&md5=6dbf9c81ff4927da29a2aac4ee527f76,"The simple average of student growth scores is often used in accountability systems, but it can be problematic for decision making. When computed using a small/moderate number of students, it can be sensitive to the sample, resulting in inaccurate representations of growth of the students, low year-to-year stability, and inequities for low-incidence groups. An alternative designed to address these issues is to use an Empirical Best Linear Prediction (EBLP), which is a weighted average of growth score data from other years and/or subjects. We apply both approaches to two statewide datasets to answer empirical questions about their performance. The EBLP outperforms the simple average in accuracy and cross-year stability with the exception that accuracy was not necessarily improved for very large districts in one of the states. In such exceptions, we show a beneficial alternative may be to use a hybrid approach in which very large districts receive the simple average and all others receive the EBLP. We find that adding more growth score data to the computation of the EBLP can improve accuracy, but not necessarily for larger schools/districts. We review key decision points in aggregate growth reporting and in specifying an EBLP weighted average in practice. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85147370896
Baldwin P.; Clauser B.E.,"Baldwin, Peter (34867881100); Clauser, Brian E. (7003595460)",34867881100; 7003595460,Historical Perspectives on Score Comparability Issues Raised by Innovations in Testing,2022,Journal of Educational Measurement,59,2,,140,160,20,4,10.1111/jedm.12318,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129722485&doi=10.1111%2fjedm.12318&partnerID=40&md5=9ad8500b4c0c053f6f7e9e5ccbd70a33,"While score comparability across test forms typically relies on common (or randomly equivalent) examinees or items, innovations in item formats, test delivery, and efforts to extend the range of score interpretation may require a special data collection before examinees or items can be used in this way—or may be incompatible with common examinee or item designs altogether. When comparisons are necessary under these nonroutine conditions, forms still must be connected by something and this article focuses on these form-invariant connective somethings. A conceptual framework for thinking about the problem of score comparability in this way is given followed by a description of three classes of connectives. Examples from the history of innovations in testing are given for each class. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85129722485
Huggins-Manley C.; Raborn A.W.; Jones P.K.; Myers T.,"Huggins-Manley, Corinne (57657610600); Raborn, Anthony W. (57193504520); Jones, Peggy K. (59123588400); Myers, Ted (57224971189)",57657610600; 57193504520; 59123588400; 57224971189,A Nonparametric Composite Group DIF Index for Focal Groups Stemming from Multicategorical Variables,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12394,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192825724&doi=10.1111%2fjedm.12394&partnerID=40&md5=0f6aea1b7bb98c442bbdbc33a990e0ad,"The purpose of this study is to develop a nonparametric DIF method that (a) compares focal groups directly to the composite group that will be used to develop the reported test score scale, and (b) allows practitioners to explore for DIF related to focal groups stemming from multicategorical variables that constitute a small proportion of the overall testing population. We propose the nonparametric root expected proportion squared difference (REPSD) index that evaluates the statistical significance of composite group DIF for relatively small focal groups stemming from multicategorical focal variables, with decisions of statistical significance based on quasi-exact p values obtained from Monte Carlo permutations of the DIF statistic under the null distribution. We conduct a simulation to evaluate conditions under which the index produces acceptable Type I error and power rates, as well as an application to a school district assessment. Practitioners can calculate the REPSD index in a freely available package we created in the R environment. © 2024 by the National Council on Measurement in Education.",,,Article,Article in press,,Scopus,2-s2.0-85192825724
Deribo T.; Kroehne U.; Goldhammer F.,"Deribo, Tobias (57219188156); Kroehne, Ulf (55849736300); Goldhammer, Frank (9274883100)",57219188156; 55849736300; 9274883100,Model-Based Treatment of Rapid Guessing,2021,Journal of Educational Measurement,58,2,,281,303,22,21,10.1111/jedm.12290,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105793358&doi=10.1111%2fjedm.12290&partnerID=40&md5=2c4f0b8ce682de6362de7888c3f5e4d5,"The increased availability of time-related information as a result of computer-based assessment has enabled new ways to measure test-taking engagement. One of these ways is to distinguish between solution and rapid guessing behavior. Prior research has recommended response-level filtering to deal with rapid guessing. Response-level filtering can lead to parameter bias if rapid guessing depends on the measured trait or (un-)observed covariates. Therefore, a model based on Mislevy and Wu (1996) was applied to investigate the assumption of ignorable missing data underlying response-level filtering. The model allowed us to investigate different approaches to treating response-level filtered responses in a single framework through model parameterization. The study found that lower-ability test-takers tend to rapidly guess more frequently and are more likely to be unable to solve an item they guessed on, indicating a violation of the assumption of ignorable missing data underlying response-level filtering. Further ability estimation seemed sensitive to different approaches to treating response-level filtered responses. Moreover, model-based approaches exhibited better model fit and higher convergent validity evidence compared to more naïve treatments of rapid guessing. The results illustrate the need to thoroughly investigate the assumptions underlying specific treatments of rapid guessing as well as the need for robust methods. © 2021 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85105793358
Yuan L.; Huang Y.; Li S.; Chen P.,"Yuan, Lu (57209690044); Huang, Yingshi (57216828276); Li, Shuhang (58003594500); Chen, Ping (56975615100)",57209690044; 57216828276; 58003594500; 56975615100,Online Calibration in Multidimensional Computerized Adaptive Testing with Polytomously Scored Items,2023,Journal of Educational Measurement,60,3,,476,500,24,0,10.1111/jedm.12353,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143906001&doi=10.1111%2fjedm.12353&partnerID=40&md5=3c9bae82ab7573df11f48bb857bfab12,"Online calibration is a key technology for item calibration in computerized adaptive testing (CAT) and has been widely used in various forms of CAT, including unidimensional CAT, multidimensional CAT (MCAT), CAT with polytomously scored items, and cognitive diagnostic CAT. However, as multidimensional and polytomous assessment data become more common, only a few published reports focus on online calibration in MCAT with polytomously scored items (P-MCAT). Therefore, standing on the shoulders of the existing online calibration methods/designs, this study proposes four new P-MCAT online calibration methods and two new P-MCAT online calibration designs and conducts two simulation studies to evaluate their performance under varying conditions (i.e., different calibration sample sizes and correlations between dimensions). Results show that all of the newly proposed methods can accurately recover item parameters, and the adaptive designs outperform the random design in most cases. In the end, this paper provides practical guidance based on simulation results. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85143906001
Gong T.; Shuai L.; Mislevy R.J.,"Gong, Tao (35177507200); Shuai, Lan (55078175800); Mislevy, Robert J. (6701800690)",35177507200; 55078175800; 6701800690,Sociocognitive Processes and Item Response Models: A Didactic Example,2024,Journal of Educational Measurement,61,1,,150,173,23,1,10.1111/jedm.12376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171289534&doi=10.1111%2fjedm.12376&partnerID=40&md5=0efb057277c2e5cb54a71d9020583070,"The usual interpretation of the person and task variables in between-persons measurement models such as item response theory (IRT) is as attributes of persons and tasks, respectively. They can be viewed instead as ensemble descriptors of patterns of interactions among persons and situations that arise from sociocognitive complex adaptive system (CASs). This view offers insights for interpreting and using between-persons measurement models and connecting with sociocognitive research. In this article, we use data generated from an agent-based model to illustrate relations between “social” and “cognitive” features of a simple underlying CAS and the variables of an IRT model fit to resulting data. We note how the ideas connect to explanatory item response modeling and briefly comment on implications for score interpretations and uses in practice. © 2023 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85171289534
Park S.; Kim K.Y.; Lee W.-C.,"Park, Seohee (57218993787); Kim, Kyung Yong (57205130600); Lee, Won-Chan (57203094500)",57218993787; 57205130600; 57203094500,Estimating Classification Accuracy and Consistency Indices for Multiple Measures with the Simple Structure MIRT Model,2023,Journal of Educational Measurement,60,1,,106,125,19,1,10.1111/jedm.12338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132587863&doi=10.1111%2fjedm.12338&partnerID=40&md5=588de3cf3763adced5f6eb6734c7b025,"Multiple measures, such as multiple content domains or multiple types of performance, are used in various testing programs to classify examinees for screening or selection. Despite the popular usages of multiple measures, there is little research on classification consistency and accuracy of multiple measures. Accordingly, this study introduces an approach to estimate classification consistency and accuracy indices for multiple measures under four possible decision rules: (1) complementary, (2) conjunctive, (3) compensatory, and (4) pairwise combinations of the three. The current study uses the IRT-recursive-based approach with the simple-structure multidimensional IRT model (SS-MIRT) to estimate the classification consistency and accuracy for multiple measures. Theoretical formulations of the four decision rules with a binary decision (Pass/Fail) are presented. The estimation procedures are illustrated using an empirical data example based on SS-MIRT. In addition, this study applies the estimation procedures to the unidimensional IRT (UIRT) context, considering that UIRT is practically used more. This application shows that the proposed procedure of classification consistency and accuracy could be used with a UIRT model for individual measures as an alternative method of SS-MIRT. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85132587863
Lim H.; Choe E.M.,"Lim, Hwanggyu (57216896756); Choe, Edison M. (57197771137)",57216896756; 57197771137,Detecting Differential Item Functioning in CAT Using IRT Residual DIF Approach,2023,Journal of Educational Measurement,60,4,,626,650,24,0,10.1111/jedm.12366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154060275&doi=10.1111%2fjedm.12366&partnerID=40&md5=8cf513ad3a8280f6a7ee9f2ac3d55707,"The residual differential item functioning (RDIF) detection framework was developed recently under a linear testing context. To explore the potential application of this framework to computerized adaptive testing (CAT), the present study investigated the utility of the RDIFR statistic both as an index for detecting uniform DIF of pretest items in CAT and as a direct measure of the effect size of uniform DIF. Extensive CAT simulations revealed RDIFR to have well-controlled Type I error and slightly higher power to detect uniform DIF compared with CATSIB, especially when pretest items were calibrated using fixed-item parameter calibration. Moreover, RDIFR accurately estimated the amount of uniform DIF irrespective of the presence of impact. Therefore, RDIFR demonstrates its potential as a useful tool for evaluating both the statistical and practical significance of uniform DIF in CAT. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85154060275
Ranger J.; Kuhn J.-T.; Wolgast A.,"Ranger, Jochen (37049459900); Kuhn, Jörg-Tobias (7201531071); Wolgast, Anett (24478136600)",37049459900; 7201531071; 24478136600,Robust Estimation of Ability and Mental Speed Employing the Hierarchical Model for Responses and Response Times,2021,Journal of Educational Measurement,58,3,,308,334,26,2,10.1111/jedm.12284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092512634&doi=10.1111%2fjedm.12284&partnerID=40&md5=fd97b927f5a1ff62715cf2ef036c2de5,"Van der Linden's hierarchical model for responses and response times can be used in order to infer the ability and mental speed of test takers from their responses and response times in an educational test. A standard approach for this is maximum likelihood estimation. In real-world applications, the data of some test takers might be partly irregular, resulting from rapid guessing or item preknowledge. The maximum likelihood estimator is not robust against contamination with irregular data. In this article, we propose a robust estimator of ability and mental speed. The estimator consists of two steps. In the first step, the mental speed is estimated with the estimator of Gervini and Yohai that ignores outlying response times. In the second step, the ability is estimated with an M-estimator that down weights unusual responses given at unusual response times. This is achieved by combining the hard-rejection weights of Gervini and Yohai with the M-estimator suggested by Croux and Haesbroeck for the logistic regression model. The proposed estimator is consistent, almost as efficient as the maximum likelihood estimator in uncontaminated data and robust in contaminated data. The performance of the estimator is analyzed in a simulation study and an empirical example. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85092512634
Tijmstra J.; Bolsinova M.; Liaw Y.-L.; Rutkowski L.; Rutkowski D.,"Tijmstra, Jesper (36859659000); Bolsinova, Maria (56162730900); Liaw, Yuan-Ling (57194495624); Rutkowski, Leslie (30567756500); Rutkowski, David (30567720500)",36859659000; 56162730900; 57194495624; 30567756500; 30567720500,Sensitivity of the RMSD for Detecting Item-Level Misfit in Low-Performing Countries,2020,Journal of Educational Measurement,57,4,,566,583,17,13,10.1111/jedm.12263,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077165122&doi=10.1111%2fjedm.12263&partnerID=40&md5=bb465c2939093126a9202be4e60b2126,"Although the root-mean squared deviation (RMSD) is a popular statistical measure for evaluating country-specific item-level misfit (i.e., differential item functioning [DIF]) in international large-scale assessment, this paper shows that its sensitivity to detect misfit may depend strongly on the proficiency distribution of the considered countries. Specifically, items for which most respondents in a country have a very low (or high) probability of providing a correct answer will rarely be flagged by the RMSD as showing misfit, even if very strong DIF is present. With many international large-scale assessment initiatives moving toward covering a more heterogeneous group of countries, this raises issues for the ability of the RMSD to detect item-level misfit, especially in low-performing countries that are not well-aligned with the overall difficulty level of the test. This may put one at risk of incorrectly assuming measurement invariance to hold, and may also inflate estimated between-country difference in proficiency. The degree to which the RMSD is able to detect DIF in low-performing countries is studied using both an empirical example from PISA 2015 and a simulation study. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85077165122
Henninger M.,"Henninger, Mirka (56191224100)",56191224100,A Novel Partial Credit Extension Using Varying Thresholds to Account for Response Tendencies,2021,Journal of Educational Measurement,58,1,,104,129,25,6,10.1111/jedm.12268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082752541&doi=10.1111%2fjedm.12268&partnerID=40&md5=b2e535495b04421f4c3f3ac0896d2e7a,"Item Response Theory models with varying thresholds are essential tools to account for unknown types of response tendencies in rating data. However, in order to separate constructs to be measured and response tendencies, specific constraints have to be imposed on varying thresholds and their interrelations. In this article, a multidimensional extension of a Partial Credit Model using a sum-to-zero constraint for varying thresholds is proposed. The new model allows us to flexibly account for response tendencies and to model covariations between varying thresholds that are commonly found in empirical data. The model's ability to estimate different types of response tendencies under various data structures is shown in a simulation study. An illustrative multicountry analysis demonstrates that differences between respondents in terms of their response tendencies exist and can be captured by the new model. Furthermore, it is well suited to account for extreme and mid response styles, but also to accommodate unknown, previously unmodeled, response tendencies. Therewith, the sum-to-zero model can be considered a suitable candidate to examine the types response tendencies in rating data and to account for biases in construct measures due to response tendencies. © 2020 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85082752541
Chalmers R.P.,"Chalmers, R. Philip (55279941900)",55279941900,A Unified Comparison of IRT-Based Effect Sizes for DIF Investigations,2023,Journal of Educational Measurement,60,2,,318,350,32,2,10.1111/jedm.12347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141482918&doi=10.1111%2fjedm.12347&partnerID=40&md5=cd657f95a476179e01193f47e63f9fea,"Several marginal effect size (ES) statistics suitable for quantifying the magnitude of differential item functioning (DIF) have been proposed in the area of item response theory; for instance, the Differential Functioning of Items and Tests (DFIT) statistics, signed and unsigned item difference in the sample statistics (SIDS, UIDS, NSIDS, and NUIDS), the standardized indices of impact, and the differential response functioning (DRF) statistics. However, the relationship between these proposed statistics has not been fully discussed, particularly with respect to population parameter definitions and recovery performance across independent samples. To address these issues, this article provides a unified presentation of competing DIF ES definitions and estimators, and evaluates the recovery efficacy of these competing estimators using a set of Monte Carlo simulation experiments. Statistical and inferential properties of the estimators are discussed, as well as future areas of research in this model-based area of bias quantification. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85141482918
Lu J.; Wang C.,"Lu, Jing (57197818229); Wang, Chun (15924354400)",57197818229; 15924354400,A Response Time Process Model for Not-Reached and Omitted Items,2020,Journal of Educational Measurement,57,4,,584,620,36,13,10.1111/jedm.12270,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085113097&doi=10.1111%2fjedm.12270&partnerID=40&md5=cb2240373ece8397b8644361bfb0253c,"Item nonresponses are prevalent in standardized testing. They happen either when students fail to reach the end of a test due to a time limit or quitting, or when students choose to omit some items strategically. Oftentimes, item nonresponses are nonrandom, and hence, the missing data mechanism needs to be properly modeled. In this paper, we proposed to use an innovative item response time model as a cohesive missing data model to account for the two most common item nonresponses: not-reached items and omitted items. In particular, the new model builds on a behavior process interpretation: a person chooses to skip an item if the required effort exceeds the implicit time the person allocates to the item (Lee & Ying, 2015; Wolf, Smith, & Birnbaum, 1995), whereas a person fails to reach the end of the test due to lack of time. This assumption was verified by analyzing the 2015 PISA computer-based mathematics data. Simulation studies were conducted to further evaluate the performance of the proposed Bayesian estimation algorithm for the new model and to compare the new model with a recently proposed “speed-accuracy + omission” model (Ulitzsch, von Davier, & Pohl, 2019). Results revealed that all model parameters could recover properly, and inadequately accounting for missing data caused biased item and person parameter estimates. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85085113097
Liu J.; Becker K.,"Liu, Jinghua (57680217800); Becker, Kirk (55059895600)",57680217800; 55059895600,The Impact of Cheating on Score Comparability via Pool-Based IRT Pre-equating,2022,Journal of Educational Measurement,59,2,,208,230,22,4,10.1111/jedm.12321,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129944518&doi=10.1111%2fjedm.12321&partnerID=40&md5=62f4f96513bc11bc3efd12b6f3f7c0b5,"For any testing programs that administer multiple forms across multiple years, maintaining score comparability via equating is essential. With continuous testing and high-stakes results, especially with less secure online administrations, testing programs must consider the potential for cheating on their exams. This study used empirical and simulated data to examine the impact of item exposure and prior knowledge on the estimation of item difficulty and test taker's ability via pool-based IRT preequating. Raw-to-theta transformations were derived from two groups of test takers with and without possible prior knowledge of exposed items, and these were compared to a criterion raw to theta transformation. Results indicated that item exposure has a large impact on item difficulty, not only altering the difficulty of exposed items, but also altering the difficulty of unexposed items. Item exposure makes test takers with prior knowledge appear more able. Further, theta estimation bias for test takers without prior knowledge increases when more test takers with possible prior knowledge are in the calibration population. Score inflation occurs for test takers with and without prior knowledge, especially for those with lower abilities. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85129944518
Ma W.; Sorrel M.A.; Zhai X.; Ge Y.,"Ma, Wenchao (57188813352); Sorrel, Miguel A. (56499358600); Zhai, Xiaoming (57192683367); Ge, Yuan (58794452000)",57188813352; 56499358600; 57192683367; 58794452000,A Dual-Purpose Model for Binary Data: Estimating Ability and Misconceptions,2024,Journal of Educational Measurement,61,2,,179,197,18,0,10.1111/jedm.12383,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181447025&doi=10.1111%2fjedm.12383&partnerID=40&md5=af17b95b3112ddeb086457fc1b003cc7,"Most existing diagnostic models are developed to detect whether students have mastered a set of skills of interest, but few have focused on identifying what scientific misconceptions students possess. This article developed a general dual-purpose model for simultaneously estimating students' overall ability and the presence and absence of misconceptions. The expectation-maximization algorithm was developed to estimate the model parameters. A simulation study was conducted to evaluate to what extent the parameters can be accurately recovered under varied conditions. A set of real data in science education was also analyzed to examine the viability of the proposed model in practice. © 2024 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85181447025
Wyse A.E.; McBride J.R.,"Wyse, Adam E. (36999095900); McBride, James R. (7202910224)",36999095900; 7202910224,A Framework for Measuring the Amount of Adaptation of Rasch-based Computerized Adaptive Tests,2021,Journal of Educational Measurement,58,1,,83,103,20,3,10.1111/jedm.12267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079703857&doi=10.1111%2fjedm.12267&partnerID=40&md5=3b0fc3c4fd104c245a1a3ea28166980e,A key consideration when giving any computerized adaptive test (CAT) is how much adaptation is present when the test is used in practice. This study introduces a new framework to measure the amount of adaptation of Rasch-based CATs based on looking at the differences between the selected item locations (Rasch item difficulty parameters) of the administered items and target item locations determined from provisional ability estimates at the start of each item. Several new indices based on this framework are introduced and compared to previously suggested measures of adaptation using simulated and real test data. Results from the simulation indicate that some previously suggested indices are not as sensitive to changes in item pool size and the use of constraints as the new indices and may not work as well under different item selection rules. The simulation study and real data example also illustrate the utility of using the new indices to measure adaptation at both a group and individual level. Discussion is provided on how one may use several of the indices to measure adaptation of Rasch-based CATs in practice. © 2020 by the National Council on Measurement in Education,,,Article,Final,,Scopus,2-s2.0-85079703857
Huang S.; Chung S.; Falk C.F.,"Huang, Sijia (57849790200); Chung, Seungwon (57207122800); Falk, Carl F. (26659012900)",57849790200; 57207122800; 26659012900,Modeling Response Styles in Cross-Classified Data Using a Cross-Classified Multidimensional Nominal Response Model,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12401,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194559531&doi=10.1111%2fjedm.12401&partnerID=40&md5=8633080d2f750987c7ae0cbd87d9e9f6,"In this study, we introduced a cross-classified multidimensional nominal response model (CC-MNRM) to account for various response styles (RS) in the presence of cross-classified data. The proposed model allows slopes to vary across items and can explore impacts of observed covariates on latent constructs. We applied a recently developed variant of the Metropolis-Hastings Robbins-Monro (MH-RM) algorithm to address the computational challenge of estimating the proposed model. To demonstrate our new approach, we analyzed empirical student evaluation of teaching (SET) data collected from a large public university with three models: a CC-MNRM with RS, a CC-MNRM with no RS, and a multilevel MNRM with RS. Results indicated that the three models led to different inferences regarding the observed covariates. Additionally, in the example, ignoring/incorporating RS led to changes in student substantive scores, while the instructor substantive scores were less impacted. Misspecifying the cross-classified data structure resulted in apparent changes on instructor scores. To further evaluate the proposed modeling approach, we conducted a preliminary simulation study and observed good parameter and score recovery. We concluded this study with discussions of limitations and future research directions. © 2024 by the National Council on Measurement in Education.",,,Article,Article in press,,Scopus,2-s2.0-85194559531
Qiao X.; Jiao H.; He Q.,"Qiao, Xin (57201984849); Jiao, Hong (55155258600); He, Qiwei (55027328300)",57201984849; 55155258600; 55027328300,"Multiple-Group Joint Modeling of Item Responses, Response Times, and Action Counts with the Conway-Maxwell-Poisson Distribution",2023,Journal of Educational Measurement,60,2,,255,281,26,3,10.1111/jedm.12349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143308079&doi=10.1111%2fjedm.12349&partnerID=40&md5=d4f9c03cabec26c18602edef7f6ab798,"Multiple group modeling is one of the methods to address the measurement noninvariance issue. Traditional studies on multiple group modeling have mainly focused on item responses. In computer-based assessments, joint modeling of response times and action counts with item responses helps estimate the latent speed and action levels in addition to latent ability. These two new data sources can also be used to further address the measurement noninvariance issue. One challenge, however, is to correctly model action counts which can be underdispersed, overdispersed, or equidispersed in real data sets. To address this, we adopted the Conway-Maxwell-Poisson distribution that accounts for different types of dispersion in action counts and incorporated it in the multiple group joint modeling of item responses, response times, and action counts. Bayesian Markov Chain Monte Carlo method was used for model parameter estimation. To illustrate an application of the proposed model, an empirical data analysis was conducted using the Programme for International Student Assessment (PISA) 2015 collaborative problem-solving items where potential measurement noninvariance issue existed between gender groups. Results indicated that Conway-Maxwell-Poisson model yielded better model fit than alternative count data models such as negative binomial and Poisson models. In addition, response times and action counts provided further information on performance differences between groups. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85143308079
Jones P.; Tong Y.; Liu J.; Borglum J.; Primoli V.,"Jones, Paul (57679299800); Tong, Ye (9249492600); Liu, Jinghua (57680217800); Borglum, Joshua (57679909100); Primoli, Vince (58332233400)",57679299800; 9249492600; 57680217800; 57679909100; 58332233400,Score Comparability between Online Proctored and In-Person Credentialing Exams,2022,Journal of Educational Measurement,59,2,,180,207,27,6,10.1111/jedm.12320,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129863021&doi=10.1111%2fjedm.12320&partnerID=40&md5=538e63920d3ed9cf32f708b793c55b12,"This article studied two methods to detect mode effects in two credentialing exams. In Study 1, we used a “modal scale comparison approach,” where the same pool of items was calibrated separately, without transformation, within two TC cohorts (TC1 and TC2) and one OP cohort (OP1) matched on their pool-based scale score distributions. The calibrations from all three groups were used to score the TC2 cohort, designated the validation sample. The TC1 item parameters and TC1-based thetas and pass rates were more like the native TC2 values than the OP1-based values, indicating mode effects, but the score and pass/fail decision differences were small. In Study 2, we used a “cross-modal repeater approach” in which test takers who failed their first attempt in one modality took the test again in either the same or different modality. The two pairs of repeater groups (TC → TC: TC → OP, and OP → OP: OP → TC) were matched exactly on their first attempt scores. Results showed increased pass rate and greater score variability in all conditions involving OP, with mode effects noticeable in both the TC → OP condition and less-strongly in the OP → TC condition. Limitations of the study and implications for exam developers were discussed. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85129863021
Chen C.-W.; Andersson B.; Zhu J.,"Chen, Chia-Wen (55633283000); Andersson, Björn (7401941749); Zhu, Jinxin (56468215300)",55633283000; 7401941749; 56468215300,A Factor Mixture Model for Item Responses and Certainty of Response Indices to Identify Student Knowledge Profiles,2023,Journal of Educational Measurement,60,1,,28,51,23,0,10.1111/jedm.12344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139650896&doi=10.1111%2fjedm.12344&partnerID=40&md5=a79016662eb51be8779e96301fb1f1ce,"The certainty of response index (CRI) measures respondents' confidence level when answering an item. In conjunction with the answers to the items, previous studies have used descriptive statistics and arbitrary thresholds to identify student knowledge profiles with the CRIs. Whereas this approach overlooked the measurement error of the observed item responses and indices, we address this by proposing a factor mixture model that integrates a latent class model to detect student subgroups and a measurement model to control for student ability and confidence level. Applying the model to 773 seventh graders' responses to an algebra test, where some items were related to new material that had not been taught in class, we found two subgroups: (1) students who had high confidence in answering items involving the new material; and (2) students who had low confidence in answering items involving the new material but higher general self-confidence than the first group. We regressed the posterior probability of the group membership on gender, prior achievement, and preview behavior and found preview behavior a significant factor associated with the membership. Finally, we discussed the implications of the current study for teaching practices and future research. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85139650896
Becker B.; Weirich S.; Goldhammer F.; Debeer D.,"Becker, Benjamin (57219514845); Weirich, Sebastian (54790322500); Goldhammer, Frank (9274883100); Debeer, Dries (55765500900)",57219514845; 54790322500; 9274883100; 55765500900,Controlling the Speededness of Assembled Test Forms: A Generalization to the Three-Parameter Lognormal Response Time Model,2023,Journal of Educational Measurement,60,4,,551,574,23,0,10.1111/jedm.12364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156198966&doi=10.1111%2fjedm.12364&partnerID=40&md5=d2541e2d53972aff713d84cfdf556200,"When designing or modifying a test, an important challenge is controlling its speededness. To achieve this, van der Linden (2011a, 2011b) proposed using a lognormal response time model, more specifically the two-parameter lognormal model, and automated test assembly (ATA) via mixed integer linear programming. However, this approach has a severe limitation, in that the two-parameter lognormal model lacks a slope parameter. This means that the model assumes that all items are equally speed sensitive. From a conceptual perspective, this assumption seems very restrictive. Furthermore, various other empirical studies and new data analyses performed by us show that this assumption almost never holds in practice. To overcome this shortcoming, we bring together the already frequently used three-parameter lognormal model for response times, which contains a slope parameter, and the ATA approach for controlling speededness by van der Linden. Using multiple empirically based illustrations, the proposed extension is illustrated, including complete and documented R code. Both the original van der Linden approach and our newly proposed approach are available to practitioners in the freely available R package eatATA. © 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85156198966
DeCarlo L.T.; Zhou X.,"DeCarlo, Lawrence T. (6701664430); Zhou, Xiaoliang (57215022516)",6701664430; 57215022516,A Latent Class Signal Detection Model for Rater Scoring with Ordered Perceptual Distributions,2021,Journal of Educational Measurement,58,1,,31,53,22,1,10.1111/jedm.12265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079718700&doi=10.1111%2fjedm.12265&partnerID=40&md5=461bbef1753ba1343e060ba1dda01d1a,"In signal detection rater models for constructed response (CR) scoring, it is assumed that raters discriminate equally well between different latent classes defined by the scoring rubric. An extended model that relaxes this assumption is introduced; the model recognizes that a rater may not discriminate equally well between some of the scoring classes. The extension recognizes a different type of rater effect and is shown to offer useful tests and diagnostic plots of the equal discrimination assumption, along with ways to assess rater accuracy and various rater effects. The approach is illustrated with an application to a large-scale language test. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85079718700
Hurtz G.M.; Mucino R.,"Hurtz, Gregory M. (6602249619); Mucino, Regi (59123521100)",6602249619; 59123521100,Expanding the Lognormal Response Time Model Using Profile Similarity Metrics to Improve the Detection of Anomalous Testing Behavior,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12395,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192790635&doi=10.1111%2fjedm.12395&partnerID=40&md5=d5eaaffbd7c7c9612b7980818a7b18d5,"The Lognormal Response Time (LNRT) model measures the speed of test-takers relative to the normative time demands of items on a test. The resulting speed parameters and model residuals are often analyzed for evidence of anomalous test-taking behavior associated with fast and poorly fitting response time patterns. Extending this model, we demonstrate the connection between the existing LNRT model parameters and the “level” component of profile similarity, and we define two new parameters for the LNRT model representing profile “dispersion” and “shape.” We show that while the LNRT model measures level (speed), profile dispersion and shape are conflated in model residuals, and that distinguishing them provides meaningful and useful parameters for identifying anomalous testing behavior. Results from data in a situation where many test-takers gained preknowledge of test items revealed that profile shape, not currently measured in the LNRT model, was the most sensitive response time index to the abnormal test-taking behavior patterns. Results strongly support expanding the LNRT model to measure not only each test-taker's level of speed, but also the dispersion and shape of their response time profiles. © 2024 by the National Council on Measurement in Education.",,,Article,Article in press,,Scopus,2-s2.0-85192790635
Pan Y.; Wollack J.A.,"Pan, Yiqin (57207947064); Wollack, James A. (6701334965)",57207947064; 6701334965,An Unsupervised-Learning-Based Approach to Compromised Items Detection,2021,Journal of Educational Measurement,58,3,,413,433,20,6,10.1111/jedm.12299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113615199&doi=10.1111%2fjedm.12299&partnerID=40&md5=ad6b1b76a6b307ca63854942bf621ed2,"As technologies have been improved, item preknowledge has become a common concern in the test security area. The present study proposes an unsupervised-learning-based approach to detect compromised items. The unsupervised-learning-based compromised item detection approach contains three steps: (1) classify responses of each examinee as either normal or aberrant based on both the item response and the response time; (2) use a recursive algorithm to cluster examinees into groups based on their response similarity; (3) identify the group with strongest preknowledge signal and report questionable items as compromised. Results show that under the conditions studied, provided the amount of preknowledge is not overwhelming and aberrance effect is at least moderate, the approach controls the false-negative rate at a relatively low level and the false-positive rate at an extremely low level. © 2021 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85113615199
Mutak A.; Krause R.; Ulitzsch E.; Much S.; Ranger J.; Pohl S.,"Mutak, Augustin (57474384700); Krause, Robert (57213790200); Ulitzsch, Esther (57192248606); Much, Sören (57224058292); Ranger, Jochen (37049459900); Pohl, Steffi (23398048100)",57474384700; 57213790200; 57192248606; 57224058292; 37049459900; 23398048100,Modeling the Intraindividual Relation of Ability and Speed within a Test,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12391,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190770841&doi=10.1111%2fjedm.12391&partnerID=40&md5=6dfc7f1de6fbfe9f8f542c7606967e7c,"Understanding the intraindividual relation between an individual's speed and ability in testing scenarios is essential to assure a fair assessment. Different approaches exist for estimating this relationship, that either rely on specific study designs or on specific assumptions. This paper aims to add to the toolbox of approaches for estimating this relationship. We propose the intraindividual speed-ability-relation (ISAR) model, which relies on nonstationarity of speed and ability over the course of the test. The ISAR model explicitly models intraindividual change in ability and speed within a test and assesses the intraindividual relation of speed and ability by evaluating the relationship of both latent change variables. Model estimation is good, when there are interindividual differences in speed and ability changes in the data. In empirical data from PISA, we found that the intraindividual relationship between speed and ability is not universally negative for all individuals and varies across different competence domains and countries. We discuss possible explanations for this relationship. © 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85190770841
Gorney K.; Wollack J.A.,"Gorney, Kylie (57481973300); Wollack, James A. (6701334965)",57481973300; 6701334965,Generating Models for Item Preknowledge,2022,Journal of Educational Measurement,59,1,,22,42,20,5,10.1111/jedm.12309,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126030783&doi=10.1111%2fjedm.12309&partnerID=40&md5=eaa253e51f222269c787bf262bd351fb,"Detection methods for item preknowledge are often evaluated in simulation studies where models are used to generate the data. To ensure the reliability of such methods, it is crucial that these models are able to accurately represent situations that are encountered in practice. The purpose of this article is to provide a critical analysis of common models that have been used to simulate preknowledge. Both response accuracy (RA) and response time (RT) models are considered. The justifications and supporting evidence for each model are evaluated using three real data sets, and the impact of generating model on detection power is examined in two simulation studies. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85126030783
Hassan M.U.; Miller F.,"Hassan, Mahmood Ul (57209311288); Miller, Frank (55419701000)",57209311288; 55419701000,Optimal Calibration of Items for Multidimensional Achievement Tests,2024,Journal of Educational Measurement,61,2,,274,302,28,0,10.1111/jedm.12386,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187667942&doi=10.1111%2fjedm.12386&partnerID=40&md5=b5dfc89ad3ce681f7f21ec5eb9c73679,"Multidimensional achievement tests are recently gaining more importance in educational and psychological measurements. For example, multidimensional diagnostic tests can help students to determine which particular domain of knowledge they need to improve for better performance. To estimate the characteristics of candidate items (calibration) for future multidimensional achievement tests, we use optimal design theory. We generalize a previously developed exchange algorithm for optimal design computation to the multidimensional setting. We also develop an asymptotic theorem saying which item should be calibrated by examinees with extreme abilities. For several examples, we compute the optimal design numerically with the exchange algorithm. We see clear structures in these results and explain them using the asymptotic theorem. Moreover, we investigate the performance of the optimal design in a simulation study. © 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85187667942
Yavuz Temel G.,"Yavuz Temel, Güler (57613528600)",57613528600,Detecting Multidimensional DIF in Polytomous Items with IRT Methods and Estimation Approaches,2024,Journal of Educational Measurement,61,1,,69,98,29,1,10.1111/jedm.12377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174145245&doi=10.1111%2fjedm.12377&partnerID=40&md5=1440cdeb3d5ac027849125a9a90e3eba,"The purpose of this study was to investigate multidimensional DIF with a simple and nonsimple structure in the context of multidimensional Graded Response Model (MGRM). This study examined and compared the performance of the IRT-LR and Wald test using MML-EM and MHRM estimation approaches with different test factors and test structures in simulation studies and applying real data sets. When the test structure included two dimensions, the IRT-LR (MML-EM) generally performed better than the Wald test and provided higher power rates. If the test included three dimensions, the methods provided similar performance in DIF detection. In contrast to these results, when the number of dimensions in the test was four, MML-EM estimation completely lost precision in estimating the nonuniform DIF, even with large sample sizes. The Wald with MHRM estimation approaches outperformed the Wald test (MML-EM) and IRT-LR (MML-EM). The Wald test had higher power rate and acceptable type I error rates for nonuniform DIF with the MHRM estimation approach.The small and/or unbalanced sample sizes, small DIF magnitudes, unequal ability distributions between groups, number of dimensions, estimation methods and test structure were evaluated as important test factors for detecting multidimensional DIF. © 2023 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85174145245
Liu J.; Meng X.; Xu G.; Gao W.; Shi N.,"Liu, Jia (58607916800); Meng, Xiangbin (55932465300); Xu, Gongjun (57200611437); Gao, Wei (15848238700); Shi, Ningzhong (7004451232)",58607916800; 55932465300; 57200611437; 15848238700; 7004451232,MSAEM Estimation for Confirmatory Multidimensional Four-Parameter Normal Ogive Models,2024,Journal of Educational Measurement,61,1,,99,124,25,0,10.1111/jedm.12378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173732880&doi=10.1111%2fjedm.12378&partnerID=40&md5=4ee17b33833cb3908282e96c2436fd7f,"In this paper, we develop a mixed stochastic approximation expectation-maximization (MSAEM) algorithm coupled with a Gibbs sampler to compute the marginalized maximum a posteriori estimate (MMAPE) of a confirmatory multidimensional four-parameter normal ogive (M4PNO) model. The proposed MSAEM algorithm not only has the computational advantages of the stochastic approximation expectation-maximization (SAEM) algorithm for multidimensional data, but it also alleviates the potential instability caused by label-switching, and then improved the estimation accuracy. Simulation studies are conducted to illustrate the good performance of the proposed MSAEM method, where MSAEM consistently performs better than SAEM and some other existing methods in multidimensional item response theory. Moreover, the proposed method is applied to a real data set from the 2018 Programme for International Student Assessment (PISA) to demonstrate the usefulness of the 4PNO model as well as MSAEM in practice. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85173732880
Dorsey D.W.; Michaels H.R.,"Dorsey, David W. (7004925510); Michaels, Hillary R. (57216874594)",7004925510; 57216874594,Validity Arguments Meet Artificial Intelligence in Innovative Educational Assessment: A Discussion and Look Forward,2022,Journal of Educational Measurement,59,3,,389,394,5,0,10.1111/jedm.12330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131509648&doi=10.1111%2fjedm.12330&partnerID=40&md5=1bbf8faed2aca2d8810f9261e02ece36,"In this concluding article of the special issue, we provide an overall discussion and point to future emerging trends in AI that might shape our approach to validity and building validity arguments. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85131509648
Kim R.Y.; Yoo Y.J.,"Kim, Rae Yeong (57778109800); Yoo, Yun Joo (7201926465)",57778109800; 7201926465,Cognitive Diagnostic Multistage Testing by Partitioning Hierarchically Structured Attributes,2023,Journal of Educational Measurement,60,1,,126,147,21,1,10.1111/jedm.12339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133399391&doi=10.1111%2fjedm.12339&partnerID=40&md5=d0ec190436cd3ac9384fd570bb320bfc,"In cognitive diagnostic models (CDMs), a set of fine-grained attributes is required to characterize complex problem solving and provide detailed diagnostic information about an examinee. However, it is challenging to ensure reliable estimation and control computational complexity when The test aims to identify the examinee's attribute profile in a large-scale map of attributes. To address this problem, this study proposes a cognitive diagnostic multistage testing by partitioning hierarchically structured attributes (CD-MST-PH) as a multistage testing for CDM. In CD-MST-PH, multiple testlets can be constructed based on separate attribute groups before testing occurs, which retains the advantages of multistage testing over fully adaptive testing or the on-the-fly approach. Moreover, testlets are offered sequentially and adaptively, thus improving test accuracy and efficiency. An item information measure is proposed to compute the discrimination power of an item for each attribute, and a module assembly method is presented to construct modules anchored at each separate attribute group. Several module selection indices for CD-MST-PH are also proposed by modifying the item selection indices used in cognitive diagnostic computerized adaptive testing. The results of simulation study show that CD-MST-PH can improve test accuracy and efficiency relative to the conventional test without adaptive stages. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85133399391
Bengs D.; Kroehne U.; Brefeld U.,"Bengs, Daniel (56556818200); Kroehne, Ulf (55849736300); Brefeld, Ulf (8279934000)",56556818200; 55849736300; 8279934000,Simultaneous Constrained Adaptive Item Selection for Group-Based Testing,2021,Journal of Educational Measurement,58,2,,236,261,25,1,10.1111/jedm.12285,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092653620&doi=10.1111%2fjedm.12285&partnerID=40&md5=993b912e0ab26e0f00038e7f9efa0b7d,"By tailoring test forms to the test-taker's proficiency, Computerized Adaptive Testing (CAT) enables substantial increases in testing efficiency over fixed forms testing. When used for formative assessment, the alignment of task difficulty with proficiency increases the chance that teachers can derive useful feedback from assessment data. The application of CAT to formative assessment in the classroom, however, is hindered by the large number of different items used for the whole class; the required familiarization with a large number of test items puts a significant burden on teachers. An improved CAT procedure for group-based testing is presented, which uses simultaneous automated test assembly to impose a limit on the number of items used per group. The proposed linear model for simultaneous adaptive item selection allows for full adaptivity and the accommodation of constraints on test content. The effectiveness of the group-based CAT is demonstrated with real-world items in a simulated adaptive test of 3,000 groups of test-takers, under different assumptions on group composition. Results show that the group-based CAT maintained the efficiency of CAT, while a reduction in the number of used items by one half to two-thirds was achieved, depending on the within-group variance of proficiencies. © 2020 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85092653620
Cho S.-J.; Goodwin A.; Naveiras M.; Salas J.,"Cho, Sun-Joo (36459479300); Goodwin, Amanda (36448949500); Naveiras, Matthew (57209395112); Salas, Jorge (57219433500)",36459479300; 36448949500; 57209395112; 57219433500,Differential and Functional Response Time Item Analysis: An Application to Understanding Paper versus Digital Reading Processes,2024,Journal of Educational Measurement,61,2,,219,251,32,0,10.1111/jedm.12389,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190292133&doi=10.1111%2fjedm.12389&partnerID=40&md5=193e0d469a4f55b154f48d21b3c370f8,"Despite the growing interest in incorporating response time data into item response models, there has been a lack of research investigating how the effect of speed on the probability of a correct response varies across different groups (e.g., experimental conditions) for various items (i.e., differential response time item analysis). Furthermore, previous research has shown a complex relationship between response time and accuracy, necessitating a functional analysis to understand the patterns that manifest from this relationship. In this study, response time data are incorporated into an item response model for two purposes: (a) to examine how individuals' speed within an experimental condition affects their response accuracy on an item, and (b) to detect the differences in individuals' speed between conditions in the presence of within-condition effects. For these two purposes, by-variable smooth functions are employed to model differential and functional response time effects by experimental condition for each item. This model is illustrated using an empirical data set to describe the effect of individuals' speed on their reading comprehension ability in two experimental conditions of reading medium (paper vs. digital) by item. A simulation study showed that the recovery of parameters and by-variable smooth functions of response time was satisfactory, and that the type I error rate and power of the test for the by-variable smooth function of response time were acceptable in conditions similar to the empirical data set. In addition, the proposed method correctly identified the range of response time where between-condition differences in the effect of response time on the probability of a correct response were accurate. © 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85190292133
Kasli M.; Zopluoglu C.; Toton S.L.,"Kasli, Murat (57218920017); Zopluoglu, Cengiz (55115824500); Toton, Sarah L. (57219288952)",57218920017; 55115824500; 57219288952,A Deterministic Gated Lognormal Response Time Model to Identify Examinees with Item Preknowledge,2023,Journal of Educational Measurement,60,1,,148,169,21,0,10.1111/jedm.12340,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133541952&doi=10.1111%2fjedm.12340&partnerID=40&md5=ec4946d0be999ccfc5bb6165e5afabab,"Response times (RTs) have recently attracted a significant amount of attention in the literature as they may provide meaningful information about item preknowledge. In this study, a new model, the Deterministic Gated Lognormal Response Time (DG-LNRT) model, is proposed to identify examinees with item preknowledge using RTs. The proposed model was applied to two different data sets and performance was assessed with false-positive rates, true-positive rates, and precision. The results were compared with another recently proposed Z-statistic. Follow-up simulation studies were also conducted to examine model performance in settings similar to the real data sets. The results indicate that the proposed model is viable and can help detect item preknowledge under certain conditions. However, its performance is highly dependent on the correct specification of the compromised items. © 2022 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85133541952
Wolkowitz A.A.,"Wolkowitz, Amanda A. (36524389200)",36524389200,A Computationally Simple Method for Estimating Decision Consistency,2021,Journal of Educational Measurement,58,3,,388,412,24,0,10.1111/jedm.12297,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107879434&doi=10.1111%2fjedm.12297&partnerID=40&md5=2b312114a0add70ab4ec2a5224b5e1ef,"Decision consistency (DC) is the reliability of a classification decision based on a test score. In professional credentialing, the decision is often a high-stakes pass/fail decision. The current methods for estimating DC are computationally complex. The purpose of this research is to provide a computationally and conceptually simple method for estimating DC that produces results comparable to, and at times potentially better than, the widely used Livingston-Lewis method. © 2021 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85107879434
Huggins-Manley A.C.; Booth B.M.; D'Mello S.K.,"Huggins-Manley, A. Corinne (56479235800); Booth, Brandon M. (57191862448); D'Mello, Sidney K. (14053463100)",56479235800; 57191862448; 14053463100,Toward Argument-Based Fairness with an Application to AI-Enhanced Educational Assessments,2022,Journal of Educational Measurement,59,3,,362,388,26,5,10.1111/jedm.12334,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131011002&doi=10.1111%2fjedm.12334&partnerID=40&md5=f23218b68d21ec145ae3247db1adb8e4,"The field of educational measurement places validity and fairness as central concepts of assessment quality. Prior research has proposed embedding fairness arguments within argument-based validity processes, particularly when fairness is conceived as comparability in assessment properties across groups. However, we argue that a more flexible approach to fairness arguments that occurs outside of and complementary to validity arguments is required to address many of the views on fairness that a set of assessment stakeholders may hold. Accordingly, we focus this manuscript on two contributions: (a) introducing the argument-based fairness approach to complement argument-based validity for both traditional and artificial intelligence (AI)-enhanced assessments and (b) applying it in an illustrative AI assessment of perceived hireability in automated video interviews used to prescreen job candidates. We conclude with recommendations for further advancing argument-based fairness approaches. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85131011002
Briggs D.C.,"Briggs, Derek C. (7202334195)",7202334195,NCME Presidential Address 2022: Turning the Page to the Next Chapter of Educational Measurement,2022,Journal of Educational Measurement,59,4,,398,417,19,3,10.1111/jedm.12350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141710396&doi=10.1111%2fjedm.12350&partnerID=40&md5=3abdd5f4291cc2f7c1826d6f91be0f1c,"Objective
Given theoretical and methodological advances that propose hypothesis about change in one or multiple processes, analytical methods for longitudinal data have been developed that provide researchers with various options for analyzing change over time. In this paper, we revisit several latent growth curve models that may be considered to answer questions about repeated measures of continuous variables, which may be operationalised as time varying covariates or outcomes.

Study design and setting
To illustrate each of the models discussed and how to interpret parameter estimates, we present examples of each method discussed using cognitive and blood pressure measures from a longitudinal study of ageing, the OCTO Twin Study.

Result and Conclusion
Although statistical models are helpful tools to test theoretical hypotheses about the dynamics between multiple processes, the choice of model and its specification will influence results and conclusions made.

Keywords: latent growth model, time varying covariates, bivariate latent growth model, longitudinal models",,,Article,Final,,Scopus,2-s2.0-85141710396
Puhan G.; Kim S.,"Puhan, Gautam (12445769400); Kim, Sooyeon (56668293600)",12445769400; 56668293600,Score Comparability Issues with At-Home Testing and How to Address Them,2022,Journal of Educational Measurement,59,2,,161,179,18,4,10.1111/jedm.12324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129301654&doi=10.1111%2fjedm.12324&partnerID=40&md5=71ccbae4fc923f6b21096e1e6adfe64e,"As a result of the COVID-19 pandemic, at-home testing has become a popular delivery mode in many testing programs. When programs offer at-home testing to expand their service, the score comparability between test takers testing remotely and those testing in a test center is critical. This article summarizes statistical procedures that could be used to evaluate potential mode effects at both the item level and the total score levels. Using operational data from a licensure test, we also compared linking relationships between the test center and at-home testing groups to determine the reporting score conversion from a subpopulation invariance perspective. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85129301654
van Laar S.; Braeken J.,"van Laar, Saskia (57204907206); Braeken, Johan (23011766400)",57204907206; 23011766400,Random Responders in the TIMSS 2015 Student Questionnaire: A Threat to Validity?,2022,Journal of Educational Measurement,59,4,,470,501,31,12,10.1111/jedm.12317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128786455&doi=10.1111%2fjedm.12317&partnerID=40&md5=46f733794e788427895d752a9907e5f0,"The low-stakes character of international large-scale educational assessments implies that a participating student might at times provide unrelated answers as if s/he was not even reading the items and choosing a response option randomly throughout. Depending on the severity of this invalid response behavior, interpretations of the assessment results are at risk of being invalidated. Not much is known about the prevalence nor impact of such random responders in the context of international large-scale educational assessments. Following a mixture item response theory (IRT) approach, an initial investigation of both issues is conducted for the Confidence in and Value of Mathematics/Science (VoM/VoS) scales in the Trends in International Mathematics and Science Study (TIMSS) 2015 student questionnaire. We end with a call to facilitate further mapping of invalid response behavior in this context by the inclusion of instructed response items and survey completion speed indicators in the assessments and a habit of sensitivity checks in all secondary data studies. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85128786455
Han S.; Kang H.-A.,"Han, Suhwa (57226382686); Kang, Hyeon-Ah (57202316196)",57226382686; 57202316196,Online Monitoring of Test-Taking Behavior Based on Item Responses and Response Times,2023,Journal of Educational Measurement,60,4,,651,675,24,0,10.1111/jedm.12367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153318850&doi=10.1111%2fjedm.12367&partnerID=40&md5=72255d59ce3c5cc61bb3e6e48304b7e5,"The study presents multivariate sequential monitoring procedures for examining test-taking behaviors online. The procedures monitor examinee's responses and response times and signal aberrancy as soon as significant change is identifieddetected in the test-taking behavior. The study in particular proposes three schemes to track different indicators of a test-taking mode—the observable manifest variables, latent trait variables, and measurement likelihood. For each procedure, sequential sampling strategies are presented to implement online monitoring. Numerical experimentation based on simulated data suggests that the proposed procedures demonstrate adequate performance. The procedures identified examinees with aberrant behaviors with high detection power and timeliness, while maintaining error rates reasonably small. Experimental application to real data also suggested that the procedures have practical relevance to real assessments. Based on the observations from the experiential analysis, the study discusses implications and guidelines for practical use. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85153318850
Bulut O.; Gorgun G.; Karamese H.,"Bulut, Okan (42561052700); Gorgun, Guher (57216341290); Karamese, Hacer (58686431300)",42561052700; 57216341290; 58686431300,Incorporating Test-Taking Engagement into Multistage Adaptive Testing Design for Large-Scale Assessments,2023,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12380,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176093177&doi=10.1111%2fjedm.12380&partnerID=40&md5=0b399cc76658fc0ce84f1b6f242e1946,"The use of multistage adaptive testing (MST) has gradually increased in large-scale testing programs as MST achieves a balanced compromise between linear test design and item-level adaptive testing. MST works on the premise that each examinee gives their best effort when attempting the items, and their responses truly reflect what they know or can do. However, research shows that large-scale assessments may suffer from a lack of test-taking engagement, especially if they are low stakes. Examinees with low test-taking engagement are likely to show noneffortful responding (e.g., answering the items very rapidly without reading the item stem or response options). To alleviate the impact of noneffortful responses on the measurement accuracy of MST, test-taking engagement can be operationalized as a latent trait based on response times and incorporated into the on-the-fly module assembly procedure. To demonstrate the proposed approach, a Monte-Carlo simulation study was conducted based on item parameters from an international large-scale assessment. The results indicated that the on-the-fly module assembly considering both ability and test-taking engagement could minimize the impact of noneffortful responses, yielding more accurate ability estimates and classifications. Implications for practice and directions for future research were discussed. © 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85176093177
Moses T.,"Moses, Tim (12243516500)",12243516500,Linking and Comparability across Conditions of Measurement: Established Frameworks and Proposed Updates,2022,Journal of Educational Measurement,59,2,,231,250,19,5,10.1111/jedm.12322,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130885411&doi=10.1111%2fjedm.12322&partnerID=40&md5=abeb04e00ec447351a0dad31e8a2a5dd,"One result of recent changes in testing is that previously established linking frameworks may not adequately address challenges in current linking situations. Test linking through equating, concordance, vertical scaling or battery scaling may not represent linkings for the scores of tests developed to measure constructs differently for different examinees, or tests that are administered in different modes and data collection designs. This article considers how previously proposed linking frameworks might be updated to address more recent testing situations. The first section summarizes the definitions and frameworks described in previous test linking discussions. Additional sections consider some sources of more disparate approaches to test development and administrations, as well as the implications of these for test linking. Possibilities for reflecting these features in an expanded test linking framework are proposed that encourage limited comparability, such as comparability that is restricted to subgroups or to the conditions of a linking study when a linking is produced, or within, but not across tests or test forms when an empirical linking based on examinee data is not produced. The implications of an updated framework of previously established linking approaches are further described in a final discussion. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85130885411
Karatoprak Ersen R.; Lee W.-C.,"Karatoprak Ersen, Rabia (57225186034); Lee, Won-Chan (57203094500)",57225186034; 57203094500,Pretest Item Calibration in Computerized Multistage Adaptive Testing,2023,Journal of Educational Measurement,60,3,,379,401,22,1,10.1111/jedm.12361,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150647249&doi=10.1111%2fjedm.12361&partnerID=40&md5=831c92cc3c513c08547c98a5a10c8ad5,"The purpose of this study was to compare calibration and linking methods for placing pretest item parameter estimates on the item pool scale in a 1-3 computerized multistage adaptive testing design in terms of item parameter recovery. Two models were used: embedded-section, in which pretest items were administered within a separate module, and embedded-items, in which pretest items were distributed across operational modules. The calibration methods were separate calibration with linking (SC) and fixed calibration (FC) with three parallel approaches under each (FC-1 and SC-1; FC-2 and SC-2; and FC-3 and SC-3). The FC-1 and SC-1 used only operational items in the routing module to link pretest items. The FC-2 and SC-2 also used only operational items in the routing module for linking, but in addition, the operational items in second stage modules were freely estimated. The FC-3 and SC-3 used operational items in all modules to link pretest items. The third calibration approach (i.e., FC-3 and SC-3) yielded the best results. For all three approaches, SC outperformed FC in all study conditions which were module length, sample size and examinee distributions. © 2023 National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85150647249
Joo S.-H.; Lee P.,"Joo, Seang-Hwane (57190023153); Lee, Philseok (56594010500)",57190023153; 56594010500,Detecting Differential Item Functioning Using Posterior Predictive Model Checking: A Comparison of Discrepancy Statistics,2022,Journal of Educational Measurement,59,4,,442,469,27,4,10.1111/jedm.12316,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128865252&doi=10.1111%2fjedm.12316&partnerID=40&md5=500622a83fae57034c099f296561384a,"This study proposes a new Bayesian differential item functioning (DIF) detection method using posterior predictive model checking (PPMC). Item fit measures including infit, outfit, observed score distribution (OSD), and Q1 were considered as discrepancy statistics for the PPMC DIF methods. The performance of the PPMC DIF method was evaluated via a Monte Carlo simulation manipulating sample size, DIF size, DIF type, DIF percentage, and subpopulation trait distribution. Parametric DIF methods, such as Lord's chi-square and Raju's area approaches, were also included in the simulation design in order to compare the performance of the proposed PPMC DIF methods to those previously existing. Based on Type I error and power analysis, we found that PPMC DIF methods showed better-controlled Type I error rates than the existing methods and comparable power to detect uniform DIF. The implications and recommendations for applied researchers are discussed. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85128865252
Thompson W.J.; Nash B.; Clark A.K.; Hoover J.C.,"Thompson, W. Jake (57192008519); Nash, Brooke (54791242100); Clark, Amy K. (57074333100); Hoover, Jeffrey C. (57201583346)",57192008519; 54791242100; 57074333100; 57201583346,Using Simulated Retests to Estimate the Reliability of Diagnostic Assessment Systems,2023,Journal of Educational Measurement,60,3,,455,475,20,0,10.1111/jedm.12359,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148060719&doi=10.1111%2fjedm.12359&partnerID=40&md5=580fa4da42eab4c732f4018c81aaadb6,"As diagnostic classification models become more widely used in large-scale operational assessments, we must give consideration to the methods for estimating and reporting reliability. Researchers must explore alternatives to traditional reliability methods that are consistent with the design, scoring, and reporting levels of diagnostic assessment systems. In this article, we describe and evaluate a method for simulating retests to summarize reliability evidence at multiple reporting levels. We evaluate how the performance of reliability estimates from simulated retests compares to other measures of classification consistency and accuracy for diagnostic assessments that have previously been described in the literature, but which limit the level at which reliability can be reported. Overall, the findings show that reliability estimates from simulated retests are an accurate measure of reliability and are consistent with other measures of reliability for diagnostic assessments. We then apply this method to real data from the Examination for the Certificate of Proficiency in English to demonstrate the method in practice and compare reliability estimates from observed data. Finally, we discuss implications for the field and possible next directions. © 2023 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85148060719
Kim H.J.; Lee W.-C.,"Kim, Hyung Jin (57211830150); Lee, Won-Chan (57203094500)",57211830150; 57203094500,Evaluation of Factors Affecting the Performance of the S−X2 Item-Fit Index,2022,Journal of Educational Measurement,59,1,,105,133,28,0,10.1111/jedm.12312,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127168107&doi=10.1111%2fjedm.12312&partnerID=40&md5=2fb0458efe45cee426193ad589166aae,"Orlando and Thissen (2000) introduced the (Formula presented.) item-fit index for testing goodness-of-fit with dichotomous item response theory (IRT) models. This study considers and evaluates an alternative approach for computing (Formula presented.) values and other factors associated with collapsing tables of observed and expected numbers (OE tables), which can affect flagging items. Results suggest that collapsing OE tables requires careful consideration of a trade-off between power and empirical type I error rate. Concurrent collapsing of score categories would be preferred over separate collapsing for its procedural simplicity, minimal effect of choice of a minimum cell value on empirical type I error rates, and reasonable type I error rates even for the most sparse condition in the study. For separate collapsing, a smaller minimum cell value is recommended as OE tables possess more sparseness (e.g., longer test lengths and smaller sample sizes) if inflated type I error rates are more of a concern in detecting items for misfit based on the (Formula presented.) index. If it is more important to identify misfit items, the study results recommend using a larger minimum cell value for collapsing. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85127168107
Jin K.-Y.; Siu W.-L.; Huang X.,"Jin, Kuan-Yu (35388583700); Siu, Wai-Lok (57486423100); Huang, Xiaoting (57839708900)",35388583700; 57486423100; 57839708900,Exploring the Impact of Random Guessing in Distractor Analysis,2022,Journal of Educational Measurement,59,1,,43,61,18,4,10.1111/jedm.12310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126232746&doi=10.1111%2fjedm.12310&partnerID=40&md5=14575e4d6ae49cbeef6efa5481124ca9,"Multiple-choice (MC) items are widely used in educational tests. Distractor analysis, an important procedure for checking the utility of response options within an MC item, can be readily implemented in the framework of item response theory (IRT). Although random guessing is a popular behavior of test-takers when answering MC items, none of the existing IRT models for distractor analysis have considered the influence of random guessing in this process. In this article, we propose a new IRT model to distinguish the influence of random guessing from response option functioning. A brief simulation study was conducted to examine the parameter recovery of the proposed model. To demonstrate its effectiveness, the new model was applied to the mathematics tests of the Hong Kong Diploma of Secondary Education Examination (HKDSE) from 2015 to 2019. The results of empirical analyses suggest that the complexity of item contents is a key factor in inducing students’ random guessing. The implications and applications of the new model to other testing situations are also discussed. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85126232746
DeCarlo L.T.,"DeCarlo, Lawrence T. (6701664430)",6701664430,On Joining a Signal Detection Choice Model with Response Time Models,2021,Journal of Educational Measurement,58,4,,438,464,26,4,10.1111/jedm.12300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114365768&doi=10.1111%2fjedm.12300&partnerID=40&md5=22167d5b86cc48650d9fbbc217a4d629,"In a signal detection theory (SDT) approach to multiple choice exams, examinees are viewed as choosing, for each item, the alternative that is perceived as being the most plausible, with perceived plausibility depending in part on whether or not an item is known. The SDT model is a process model and provides measures of item difficulty, item discrimination, and the relative plausibility of each alternative. It is shown how to incorporate information from response times into the model, which has potential benefits for estimation and also offers a way to study underlying processes. The SDT model is joined with a lognormal response time (RT) model in a manner similar to that used in hierarchical models. In addition, a mixture extension of the RT model is joined in a novel way with the SDT choice model, using the idea that the probability of “knowing” in the SDT model might be related to the probability of working in one of two speed states in the mixture RT model. A semiparametric version of the mixture RT model is also used to assess robustness. The fused SDT/RT models are examined with science items from the 2015 Program for International Student Assessment. © 2021 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85114365768
van der Linden W.J.; Belov D.I.,"van der Linden, Wim J. (55409657500); Belov, Dmitry I. (8355735900)",55409657500; 8355735900,A Statistical Test for the Detection of Item Compromise Combining Responses and Response Times,2023,Journal of Educational Measurement,60,2,,235,254,19,3,10.1111/jedm.12346,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141410324&doi=10.1111%2fjedm.12346&partnerID=40&md5=888c48cb2534f5dae38a34f4df7591ab,"A test of item compromise is presented which combines the test takers' responses and response times (RTs) into a statistic defined as the number of correct responses on the item for test takers with RTs flagged as suspicious. The test has null and alternative distributions belonging to the well-known family of compound binomial distributions, is simple to calculate, and has results that are easy to interpret. It also demonstrated nearly perfect power for the detection of compromise with no more than 10 test takers with preknowledge of the more difficult and discriminating items in a set of empirical examples. For the easier and less discriminating items, the presence of some 20 test takers with preknowledge still sufficed. A test based on the reverse statistic of the total time by test takers with responses flagged as suspicious may seem a natural alternative but misses the property of a monotone likelihood ratio necessary to decide between a test that should be left or right sided. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85141410324
Kwon T.Y.; Huggins-Manley A.C.; Templin J.; Zheng M.,"Kwon, Tae Yeon (58960789000); Huggins-Manley, A. Corinne (56479235800); Templin, Jonathan (23010693000); Zheng, Mingying (58150400700)",58960789000; 56479235800; 23010693000; 58150400700,Modeling Hierarchical Attribute Structures in Diagnostic Classification Models with Multiple Attempts,2024,Journal of Educational Measurement,61,2,,198,218,20,0,10.1111/jedm.12387,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188968888&doi=10.1111%2fjedm.12387&partnerID=40&md5=36e22c98c7dff2c9ecb5e6050bb56a3c,"In classroom assessments, examinees can often answer test items multiple times, resulting in sequential multiple-attempt data. Sequential diagnostic classification models (DCMs) have been developed for such data. As student learning processes may be aligned with a hierarchy of measured traits, this study aimed to develop a sequential hierarchical DCM (sequential HDCM), which combines a sequential DCM with the HDCM, and investigate classification accuracy of the model in the presence of hierarchies when multiple attempts are allowed in dynamic assessment. We investigated the model's impact on classification accuracy when hierarchical structures are correctly specified, misspecified, or overspecified. The results indicate that (1) a sequential HDCM accurately classified students as masters and nonmasters when the data had a hierarchical structure; (2) a sequential HDCM produced similar or slightly higher classification accuracy than nonhierarchical sequential LCDM when the data had hierarchical structures; and (3) the misspecification of the hierarchical structure of the data resulted in lower classification accuracy when the misspecified model had fewer attribute profiles than the true model. We discuss limitations and make recommendations on using the proposed model in practice. This study provides practitioners with information about the possibilities for psychometric modeling of dynamic classroom assessment data. © 2024 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85188968888
Grochowalski J.H.; Hendrickson A.,"Grochowalski, Joseph H. (57022208700); Hendrickson, Amy (7006207129)",57022208700; 7006207129,Detecting Group Collaboration Using Multiple Correspondence Analysis,2023,Journal of Educational Measurement,60,3,,402,427,25,0,10.1111/jedm.12363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150653919&doi=10.1111%2fjedm.12363&partnerID=40&md5=08fa042910b8bea23aaeaa2bac4749d6,"Test takers wishing to gain an unfair advantage often share answers with other test takers, either sharing all answers (a full key) or some (a partial key). Detecting key sharing during a tight testing window requires an efficient, easily interpretable, and rich form of analysis that is descriptive and inferential. We introduce a detection method based on multiple correspondence analysis (MCA) that identifies test takers with unusual response similarities. The method simultaneously detects multiple shared keys (partial or full), plots results, and is computationally efficient as it requires only matrix operations. We describe the method, evaluate its detection accuracy under various simulation conditions, and demonstrate the procedure on a real data set with known test-taking misbehavior. The simulation results showed that the MCA method had reasonably high power under realistic conditions and maintained the nominal false-positive level, except when the group size was very large or partial shared keys had more than 50% of the items. The real data analysis illustrated visual detection procedures and inference about the item responses possibly shared in the key, which was likely shared among 91 test takers, many of whom were confirmed by nonstatistical investigation to have engaged in test-taking misconduct. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85150653919
Ferrara S.; Qunbar S.,"Ferrara, Steve (35333949500); Qunbar, Saed (57732370800)",35333949500; 57732370800,Validity Arguments for AI-Based Automated Scores: Essay Scoring as an Illustration,2022,Journal of Educational Measurement,59,3,,288,313,25,11,10.1111/jedm.12333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131509749&doi=10.1111%2fjedm.12333&partnerID=40&md5=bf6e30fe83e3dbcf14a1888069eaeeb5,"In this article, we argue that automated scoring engines should be transparent and construct relevant—that is, as much as is currently feasible. Many current automated scoring engines cannot achieve high degrees of scoring accuracy without allowing in some features that may not be easily explained and understood and may not be obviously and directly relevant to the target assessment construct. We address the current limitations on evidence and validity arguments for scores from automated scoring engines from the points of view of the Standards for Educational and Psychological Testing (i.e., construct relevance, construct representation, and fairness) and emerging principles in Artificial Intelligence (e.g., explainable AI, an examinee's right to explanations, and principled AI). We illustrate these concepts and arguments for automated essay scores. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85131509749
Sinharay S.; Johnson M.S.,"Sinharay, Sandip (6602980064); Johnson, Matthew S. (55723899700)",6602980064; 55723899700,Computation and Accuracy Evaluation of Comparable Scores on Culturally Responsive Assessments,2024,Journal of Educational Measurement,61,1,,5,46,41,0,10.1111/jedm.12381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176914720&doi=10.1111%2fjedm.12381&partnerID=40&md5=84f3381ad993d41dbb323379b7b842ae,"Culturally responsive assessments have been proposed as potential tools to ensure equity and fairness for examinees from all backgrounds including those from traditionally underserved or minoritized groups. However, these assessments are relatively new and, with few exceptions, are yet to be implemented in large scale. Consequently, there is a lack of guidance on how one can compute comparable scores on various versions of these assessments. In this paper, the multigroup multidimensional Rasch model is repurposed for modeling data originating from various versions of a culturally responsive assessment and for analyzing such data to compute comparable scores. Two simulation studies are performed to evaluate the performance of the model for data simulated from hypothetical culturally responsive assessments and to find the conditions under which the computed scores are accurate. Recommendations are made for measurement practitioners interested in culturally responsive assessments. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85176914720
Guo J.; Xu X.; Xin T.,"Guo, Jinxin (57212453708); Xu, Xin (57221066152); Xin, Tao (7003499953)",57212453708; 57221066152; 7003499953,A Note on Latent Traits Estimates under IRT Models with Missingness,2023,Journal of Educational Measurement,60,4,,575,625,50,1,10.1111/jedm.12365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153360564&doi=10.1111%2fjedm.12365&partnerID=40&md5=b349a2e0804bdc51e76b2b4e6583f49e,"Missingness due to not-reached items and omitted items has received much attention in the recent psychometric literature. Such missingness, if not handled properly, would lead to biased parameter estimation, as well as inaccurate inference of examinees, and further erode the validity of the test. This paper reviews some commonly used IRT based models allowing missingness, followed by three popular examinee scoring methods, including maximum likelihood estimation, maximum a posteriori, and expected a posteriori. Simulation studies were conducted to compare these examinee scoring methods across these commonly used models in the presence of missingness. Results showed that all the methods could infer examinees' ability accurately when the missingness is ignorable. If the missingness is nonignorable, incorporating those missing responses would improve the precision in estimating abilities for examinees with missingness, especially when the test length is short. In terms of examinee scoring methods, expected a posteriori method performed better for evaluating latent traits under models allowing missingness. An empirical study based on the PISA 2015 Science Test was further performed. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85153360564
Johnson M.S.; Liu X.; McCaffrey D.F.,"Johnson, Matthew S. (55723899700); Liu, Xiang (57191903375); McCaffrey, Daniel F. (7005049755)",55723899700; 57191903375; 7005049755,Psychometric Methods to Evaluate Measurement and Algorithmic Bias in Automated Scoring,2022,Journal of Educational Measurement,59,3,,338,361,23,9,10.1111/jedm.12335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131007650&doi=10.1111%2fjedm.12335&partnerID=40&md5=4916295855ef63cc1b6cd4fdd101457b,"With the increasing use of automated scores in operational testing settings comes the need to understand the ways in which they can yield biased and unfair results. In this paper, we provide a brief survey of some of the ways in which the predictive methods used in automated scoring can lead to biased, and thus unfair automated scores. After providing definitions of fairness from machine learning and a psychometric framework to study them, we demonstrate how modeling decisions, like omitting variables, using proxy measures or confounded variables, and even the optimization criterion in estimation can lead to biased and unfair automated scores. We then introduce two simple methods for evaluating bias, evaluate their statistical properties through simulation, and apply to an item from a large-scale reading assessment. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85131007650
Qiao X.; Jiao H.,"Qiao, Xin (57201984849); Jiao, Hong (55155258600)",57201984849; 55155258600,Explanatory Cognitive Diagnostic Modeling Incorporating Response Times,2021,Journal of Educational Measurement,58,4,,564,585,21,1,10.1111/jedm.12306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125066020&doi=10.1111%2fjedm.12306&partnerID=40&md5=c4070896679f2baab0e540967b2509b2,"This study proposes explanatory cognitive diagnostic model (CDM) jointly incorporating responses and response times (RTs) with the inclusion of item covariates related to both item responses and RTs. The joint modeling of item responses and RTs intends to provide more information for cognitive diagnosis while item covariates can be used to predict item parameters when item calibration is not feasible in diagnostic assessments or item parameter estimation errors could be too large due to small sample sizes for calibration. In addition, the inclusion of the item covariates allows the evaluation of cognitive theories underlying the test design in item development. Model parameter estimation is explored using the Bayesian Markov chain Monte Carlo (MCMC) method. A Monte Carlo simulation study is conducted to examine the parameter recovery of the proposed model under different simulated conditions in comparison to alternative competing models. Further, the application of the proposed model is illustrated using the Programme for International Student Assessment (PISA) 2012 problem-solving items modeling both item response and RT data. The study results indicate that model parameters can be well recovered using the MCMC algorithm and the explanatory CDM jointly incorporating item responses and RTs with item covariates holds promising applications in digital-based diagnostic assessments. © 2022 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85125066020
Lim H.; Davey T.; Wells C.S.,"Lim, Hwanggyu (57216896756); Davey, Tim (36785089100); Wells, Craig S. (24167495500)",57216896756; 36785089100; 24167495500,A Recursion-Based Analytical Approach to Evaluate the Performance of MST,2021,Journal of Educational Measurement,58,2,,154,178,24,3,10.1111/jedm.12276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087157451&doi=10.1111%2fjedm.12276&partnerID=40&md5=0e018c461cdf09bc5a8b882bcae48e66,"This study proposed a recursion-based analytical approach to assess measurement precision of ability estimation and classification accuracy in multistage adaptive tests (MSTs). A simulation study was conducted to compare the proposed recursion-based analytical method with an analytical method proposed by Park, Kim, Chung, and Dodd and with the more commonly used Monte Carlo (MC) simulation approaches. The results from the simulation study indicate that the recursion-based analytical method produced more stable measurement properties in MST than the MC-based simulation method. It also produced more credible measurement performance than the analytical approach of Park, Kim, Chung, and Dodd. We expect that the recursion-based analytical method would be especially efficient in instances when multiple MST designs need to be compared to determine which one has better measurement performance. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85087157451
Gorney K.; Wollack J.A.,"Gorney, Kylie (57481973300); Wollack, James A. (6701334965)",57481973300; 6701334965,Using Item Scores and Distractors in Person-Fit Assessment,2023,Journal of Educational Measurement,60,1,,3,27,24,6,10.1111/jedm.12345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138189880&doi=10.1111%2fjedm.12345&partnerID=40&md5=9f40e68bf445a1e1eb8618caa3790301,"In order to detect a wide range of aberrant behaviors, it can be useful to incorporate information beyond the dichotomous item scores. In this paper, we extend the (Formula presented.) and (Formula presented.) person-fit statistics so that unusual behavior in item scores and unusual behavior in item distractors can be used as indicators of aberrance. Through detailed simulations, we show that the new statistics are more powerful than existing statistics in detecting several types of aberrant behavior, and that they are able to control the Type I error rate in instances where the model does not exactly fit the data. A real data example is also provided to demonstrate the utility of the new statistics in an operational setting. © 2022 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85138189880
Yamaguchi K.; Zhang J.,"Yamaguchi, Kazuhiro (57200510912); Zhang, Jihong (57219453191)",57200510912; 57219453191,Fully Gibbs Sampling Algorithms for Bayesian Variable Selection in Latent Regression Models,2023,Journal of Educational Measurement,60,2,,202,234,32,0,10.1111/jedm.12348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140398095&doi=10.1111%2fjedm.12348&partnerID=40&md5=33a291c79b914a3a8cbf06f9d15c3ddb,"This study proposed Gibbs sampling algorithms for variable selection in a latent regression model under a unidimensional two-parameter logistic item response theory model. Three types of shrinkage priors were employed to obtain shrinkage estimates: double-exponential (i.e., Laplace), horseshoe, and horseshoe+ priors. These shrinkage priors were compared to a uniform prior case in both simulation and real data analysis. The simulation study revealed that two types of horseshoe priors had a smaller root mean square errors and shorter 95% credible interval lengths than double-exponential or uniform priors. In addition, the horseshoe+ prior was slightly more stable than the horseshoe prior. The real data example successfully proved the utility of horseshoe and horseshoe+ priors in selecting effective predictive covariates for math achievement. © 2022 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85140398095
Tang X.; Zheng Y.; Wu T.; Hau K.-T.; Chang H.-H.,"Tang, Xiuxiu (57988841000); Zheng, Yi (55713171300); Wu, Tong (59164864300); Hau, Kit-Tai (7006812724); Chang, Hua-Hua (7407524642)",57988841000; 55713171300; 59164864300; 7006812724; 7407524642,Utilizing Response Time for Item Selection in On-the-Fly Multistage Adaptive Testing for PISA Assessment,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195480974&doi=10.1111%2fjedm.12403&partnerID=40&md5=bcf73aa67ff5ca42b0cc5e8b84bd3617,"Multistage adaptive testing (MST) has been recently adopted for international large-scale assessments such as Programme for International Student Assessment (PISA). MST offers improved measurement efficiency over traditional nonadaptive tests and improved practical convenience over single-item-adaptive computerized adaptive testing (CAT). As a third alternative adaptive test design to MST and CAT, Zheng and Chang proposed the “on-the-fly multistage adaptive testing” (OMST), which combines the benefits of MST and CAT and offsets their limitations. In this study, we adopted the OMST design while also incorporating response time (RT) in item selection. Via simulations emulating the PISA 2018 reading test, including using the real item attributes and replicating PISA 2018 reading test's MST design, we compared the performance of our OMST designs against the simulated MST design in (1) measurement accuracy of test takers’ ability, (2) test time efficiency and consistency, and (3) expected gains in precision by design. We also investigated the performance of OMST in item bank usage and constraints management. Results show great potential for the proposed RT-incorporated OMST designs to be used for PISA and potentially other international large-scale assessments. © 2024 by the National Council on Measurement in Education.",,,Article,Article in press,,Scopus,2-s2.0-85195480974
Feng T.; Cai L.,"Feng, Tianying (57211690478); Cai, Li (59155077000)",57211690478; 59155077000,Sensemaking of Process Data from Evaluation Studies of Educational Games: An Application of Cross-Classified Item Response Theory Modeling,2024,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195029738&doi=10.1111%2fjedm.12396&partnerID=40&md5=5a46f940496c031c64c3ecb45061d65f,"Process information collected from educational games can illuminate how students approach interactive tasks, complementing assessment outcomes routinely examined in evaluation studies. However, the two sources of information are historically analyzed and interpreted separately, and diagnostic process information is often underused. To tackle these issues, we present a new application of cross-classified item response theory modeling, using indicators of knowledge misconceptions and item-level assessment data collected from a multisite game-based randomized controlled trial. This application addresses (a) the joint modeling of students' pretest and posttest item responses and game-based processes described by indicators of misconceptions; (b) integration of gameplay information when gauging the intervention effect of an educational game; (c) relationships among game-based misconception, pretest initial status, and pre-to-post change; and (d) nesting of students within schools, a common aspect in multisite research. We also demonstrate how to structure the data and set up the model to enable our proposed application, and how our application compares to three other approaches to analyzing gameplay and assessment data. Lastly, we note the implications for future evaluation studies and for using analytic results to inform learning and instruction. © 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85195029738
Xu L.; Wang S.; Cai Y.; Tu D.,"Xu, Lingling (57220777583); Wang, Shiyu (55940206100); Cai, Yan (57196027339); Tu, Dongbo (57196026901)",57220777583; 55940206100; 57196027339; 57196026901,The Automated Test Assembly and Routing Rule for Multistage Adaptive Testing with Multidimensional Item Response Theory,2021,Journal of Educational Measurement,58,4,,538,563,25,2,10.1111/jedm.12305,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123506143&doi=10.1111%2fjedm.12305&partnerID=40&md5=fc7e94e54d03e0b5d69fe01220e71f04,"Designing a multidimensional adaptive test (M-MST) based on a multidimensional item response theory (MIRT) model is critical to make full use of the advantages of both MST and MIRT in implementing multidimensional assessments. This study proposed two types of automated test assembly (ATA) algorithms and one set of routing rules that can facilitate the development of an M-MST. Different M-MST designs were developed based on the proposed ATA algorithms and routing rules and were evaluated through two sets of simulation studies. Study 1 used simulated item banks and considered a variety of testing factors, and results from which can inform us the theoretical performance of the proposed M-MST designs. Study 2 was designed based on a real multidimensional assessment. In addition, a MCAT was simulated as a baseline design to demonstrate the advantage of the proposed M-MST design in each study. Our simulation results indicate that the proposed ATA algorithms and routing rule can generate M-MSTs with a good-quality control and the same or even better ability estimation results than the MCAT given the same condition. This demonstrates the advantage of using M-MST for multidimensional assessment especially for the one that needs to satisfy many nonstatistical constraints. © 2022 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85123506143
Hong M.; Rebouças D.A.; Cheng Y.,"Hong, Maxwell (57204481125); Rebouças, Daniella A. (57210212875); Cheng, Ying (36107474600)",57204481125; 57210212875; 36107474600,Robust Estimation for Response Time Modeling,2021,Journal of Educational Measurement,58,2,,262,280,18,4,10.1111/jedm.12286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096667683&doi=10.1111%2fjedm.12286&partnerID=40&md5=b5b195032d764321251948f90244bf9d,"Response time has started to play an increasingly important role in educational and psychological testing, which prompts many response time models to be proposed in recent years. However, response time modeling can be adversely impacted by aberrant response behavior. For example, test speededness can cause response time to certain items to deviate from the hypothesized model. In this article, we introduce a robust estimation approach when estimating a respondent's working speed under the log-normal model by down-weighting aberrant response times. A simulation study is carried out to compare the performance of two weighting schemes and a real data example is provided to showcase the use of the new robust estimation method. Limitations and future directions are also discussed. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85096667683
Lee S.; Han S.; Choi S.W.,"Lee, Sooyong (57226382978); Han, Suhwa (57226382686); Choi, Seung W. (7408120005)",57226382978; 57226382686; 7408120005,A Bayesian Moderated Nonlinear Factor Analysis Approach for DIF Detection under Violation of the Equal Variance Assumption,2024,Journal of Educational Measurement,61,2,,303,324,21,0,10.1111/jedm.12388,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188441164&doi=10.1111%2fjedm.12388&partnerID=40&md5=9463158ab569b59454fe492c2337a8bb,"Research has shown that multiple-indicator multiple-cause (MIMIC) models can result in inflated Type I error rates in detecting differential item functioning (DIF) when the assumption of equal latent variance is violated. This study explains how the violation of the equal variance assumption adversely impacts the detection of nonuniform DIF and how it can be addressed through moderated nonlinear factor analysis (MNLFA) model via Bayesian estimation approach to overcome limitations from the restrictive assumption. The Bayesian MNLFA approach suggested in this study better control Type I errors by freely estimating latent factor variances across different groups. Our experimentation with simulated data demonstrates that the BMNFA models outperform the existing MIMIC models, in terms of Type I error control as well as parameter recovery. The results suggest that the MNLFA models have the potential to be a superior choice to the existing MIMIC models, especially in situations where the assumption of equal latent variance assumption is not likely to hold. © 2024 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85188441164
Jin K.-Y.; Eckes T.,"Jin, Kuan-Yu (35388583700); Eckes, Thomas (6701846398)",35388583700; 6701846398,Measuring the Impact of Peer Interaction in Group Oral Assessments with an Extended Many-Facet Rasch Model,2024,Journal of Educational Measurement,61,1,,47,68,21,1,10.1111/jedm.12375,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171265834&doi=10.1111%2fjedm.12375&partnerID=40&md5=e42eab1c04ae70e60ec9b7cec4975e9d,"Many language proficiency tests include group oral assessments involving peer interaction. In such an assessment, examinees discuss a common topic with others. Human raters score each examinee's spoken performance on specially designed criteria. However, measurement models for analyzing group assessment data usually assume local person independence and thus fail to consider the impact of peer interaction on the assessment outcomes. This research advances an extended many-facet Rasch model for group assessments (MFRM-GA), accounting for local person dependence. In a series of simulations, we examined the MFRM-GA's parameter recovery and the consequences of ignoring peer interactions under the traditional modeling approach. We also used a real dataset from the English-speaking test of the Language Proficiency Assessment for Teachers (LPAT) routinely administered in Hong Kong to illustrate the efficiency of the new model. The discussion focuses on the model's usefulness for measuring oral language proficiency, practical implications, and future research perspectives. © 2023 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85171265834
Wang S.; Zhang M.; Lee W.-C.; Huang F.; Li Z.; Li Y.; Yu S.,"Wang, Shaojie (57204061760); Zhang, Minqiang (56535581900); Lee, Won-Chan (57203094500); Huang, Feifei (57203749422); Li, Zonglong (56921102700); Li, Yixing (57222736282); Yu, Sufang (57222173557)",57204061760; 56535581900; 57203094500; 57203749422; 56921102700; 57222736282; 57222173557,Two IRT Characteristic Curve Linking Methods Weighted by Information,2022,Journal of Educational Measurement,59,4,,423,441,18,0,10.1111/jedm.12315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129342095&doi=10.1111%2fjedm.12315&partnerID=40&md5=119416eb361a6ec5630e29ee21b6af57,"Traditional IRT characteristic curve linking methods ignore parameter estimation errors, which may undermine the accuracy of estimated linking constants. Two new linking methods are proposed that take into account parameter estimation errors. The item- (IWCC) and test-information-weighted characteristic curve (TWCC) methods employ weighting components in the loss function from traditional methods by their corresponding item and test information, respectively. Monte Carlo simulation was conducted to evaluate the performances of the new linking methods and compare them with traditional ones. Ability difference between linking groups, sample size, and test length were manipulated under the common-item nonequivalent groups design. Results showed that the two information-weighted characteristic curve methods outperformed traditional methods, in general. TWCC was found to be more accurate and stable than IWCC. A pseudo-form pseudo-group analysis was also performed, and similar results were observed. Finally, guidelines for practice and future directions are discussed. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85129342095
Binici S.; Cuhadar I.,"Binici, Salih (24503138100); Cuhadar, Ismail (57223002334)",24503138100; 57223002334,Validating Performance Standards via Latent Class Analysis,2022,Journal of Educational Measurement,59,4,,502,516,14,1,10.1111/jedm.12325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132656961&doi=10.1111%2fjedm.12325&partnerID=40&md5=1063ab99fa63ac4ace8291c605c943e6,"Validity of performance standards is a key element for the defensibility of standard setting results, and validating performance standards requires collecting multiple pieces of evidence at every step during the standard setting process. This study employs a statistical procedure, latent class analysis, to set performance standards and compares latent class analysis results with previously established performance standards via the modified-Angoff method for cross-validation. The context of the study is an operational large-scale science assessment administered in one of the southern states in the United States. Results show that the number of classes that emerged in the latent class analysis concurs with the number of existing performance levels. In addition, there is a substantial level of agreement between latent class analysis results and modified-Angoff method in terms of classifying students into the same performance levels. Overall, the findings establish evidence for the validity of the performance standards identified via the modified-Angoff method. Practical implications of the study findings are discussed. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85132656961
Ercikan K.; McCaffrey D.F.,"Ercikan, Kadriye (6603174172); McCaffrey, Daniel F. (7005049755)",6603174172; 7005049755,Optimizing Implementation of Artificial-Intelligence-Based Automated Scoring: An Evidence Centered Design Approach for Designing Assessments for AI-based Scoring,2022,Journal of Educational Measurement,59,3,,272,287,15,12,10.1111/jedm.12332,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131687968&doi=10.1111%2fjedm.12332&partnerID=40&md5=fda7ba6ae17d22a1131a7c6d4281d8c0,"Artificial-intelligence-based automated scoring is often an afterthought and is considered after assessments have been developed, resulting in nonoptimal possibility of implementing automated scoring solutions. In this article, we provide a review of Artificial intelligence (AI)-based methodologies for scoring in educational assessments. We then propose an evidence-centered design framework for developing assessments to align conceptualization, scoring, and ultimate assessment interpretation and use with the advantages and limitations of AI-based scoring in mind. We provide recommendations for defining construct, task, and evidence models to guide task and assessment design that optimize the development and implementation of AI-based automated scoring of constructed response items and support the validity of inferences from and uses of scores. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85131687968
Luo X.,"Luo, Xiao (57688238400)",57688238400,Automated Test Assembly with Mixed-Integer Programming: The Effects of Modeling Approaches and Solvers,2020,Journal of Educational Measurement,57,4,,547,565,18,9,10.1111/jedm.12262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075192471&doi=10.1111%2fjedm.12262&partnerID=40&md5=9850e01aa3619a8ec6ee4ce0d6f3cfcb,"Automated test assembly (ATA) is a modern approach to test assembly that applies advanced optimization algorithms on computers to build test forms automatically. ATA greatly improves the efficiency and accuracy of the test assembly. This study investigated the effects of the modeling methods and solvers in the mixed-integer programming (MIP) approach to ATA in the context of assembling parallel linear test forms and multistage testing (MST) panels. The results of two simulation studies indicated that the newly proposed maximin modeling method significantly improved the parallelism of the test information functions (TIFs) among assembled test forms while maintaining relatively high overall TIFs, and the newly proposed binary minimax method considerably reduced the overall discrepancies from the targets. A comparison of four freely available noncommercial MIP solvers from the utilitarian, as opposed to the benchmarking, perspective was also included in this study. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85075192471
Chen Y.; Zhang J.; Yang Y.; Lee Y.-S.,"Chen, Yi (57735499300); Zhang, Jingru (57223109472); Yang, Yi (57218548428); Lee, Young-Sun (57203798701)",57735499300; 57223109472; 57218548428; 57203798701,Latent Space Model for Process Data,2022,Journal of Educational Measurement,59,4,,517,535,18,4,10.1111/jedm.12337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131713436&doi=10.1111%2fjedm.12337&partnerID=40&md5=fdb02ab0107100c9251752f9d5dc6c56,"The development of human-computer interactive items in educational assessments provides opportunities to extract useful process information for problem-solving. However, the complex, intensive, and noisy nature of process data makes it challenging to model with the traditional psychometric methods. Social network methods have been applied to visualize and analyze process data. Nonetheless, research about statistical modeling of process information using social network methods is still limited. This article explored the application of the latent space model (LSM) for analyzing process data in educational assessment. The adjacent matrix of transitions between actions was created based on the weighted and directed network of action sequences and related auxiliary information. Then, the adjacent matrix was modeled with LSM to identify the lower-dimensional latent positions of actions. Three applications based on the results from LSM were introduced: action clustering, error analysis, and performance measurement. The simulation study showed that LSM can cluster actions from the same problem-solving strategy and measure students’ performance by comparing their action sequences with the optimal strategy. Finally, we analyzed the empirical data from PISA 2012 as a real case scenario to illustrate how to use LSM. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85131713436
Li L.; Becker B.J.,"Li, Lanrong (57306011500); Becker, Betsy Jane (7401580470)",57306011500; 7401580470,Assessing Differential Bundle Functioning Using Meta-Analysis,2021,Journal of Educational Measurement,58,4,,492,514,22,0,10.1111/jedm.12303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117717065&doi=10.1111%2fjedm.12303&partnerID=40&md5=3dbd4ab935d0b6f4d12f8920d4058491,"Differential bundle functioning (DBF) has been proposed to quantify the accumulated amount of differential item functioning (DIF) in an item cluster/bundle (Douglas, Roussos, and Stout). The simultaneous item bias test (SIBTEST, Shealy and Stout) has been used to test for DBF (e.g., Walker, Zhang, and Surber). Research on DBF may have the potential to reveal the mechanism underlying DIF. However, an unresolved issue is the lack of an effect size for DBF, making it difficult to assess and compare the amounts of DBF within and between tests. We propose using meta-analysis techniques to study DBF. By meta-analyzing DIF indices, we can examine the heterogeneity of DIF across items, using the weighted average of DIF indices in an item bundle as a measure of effect size for DBF. A Monte Carlo simulation study compared the performance of our proposed effect size for DBF and a new test of nonzero average DIF in an item bundle with that of a DBF test using SIBTEST. When the primary and secondary dimensions were moderately correlated, our proposed effect size for DBF had little bias; the test of nonzero average DIF also showed power and Type I error levels comparable to those of the DBF test. © 2021 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85117717065
Casabianca J.M.; Donoghue J.R.; Shin H.J.; Chao S.-F.; Choi I.,"Casabianca, Jodi M. (55839149700); Donoghue, John R. (7102202534); Shin, Hyo Jeong (56594032400); Chao, Szu-Fu (57201914594); Choi, Ikkyu (57200083603)",55839149700; 7102202534; 56594032400; 57201914594; 57200083603,Using Linkage Sets to Improve Connectedness in Rater Response Model Estimation,2023,Journal of Educational Measurement,60,3,,428,454,26,0,10.1111/jedm.12360,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148094145&doi=10.1111%2fjedm.12360&partnerID=40&md5=1c923108d57348448e3bd42a73163bbb,"Using item-response theory to model rater effects provides an alternative solution for rater monitoring and diagnosis, compared to using standard performance metrics. In order to fit such models, the ratings data must be sufficiently connected in order to estimate rater effects. Due to popular rating designs used in large-scale testing scenarios, there tends to be a large proportion of missing data, yielding sparse matrices and estimation issues. In this article, we explore the impact of different types of connectedness, or linkage, brought about by using a linkage set—a collection of responses scored by most or all raters. We also explore the impact of the properties and composition of the linkage set, the different connectedness yielded from different rating designs, and the role of scores from automated scoring engines. In designing monitoring systems using the rater response version of the generalized partial credit model, the study results suggest use of a linkage set, especially a large one that is comprised of responses representing the full score scale. Results also show that a double-human-scoring design provides more connectedness than a design with one human and an automated scoring engine. Furthermore, scores from automated scoring engines do not provide adequate connectedness. We discuss considerations for operational implementation and further study. © 2023 by the National Council on Measurement in Education.",,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85148094145
Bolt D.M.; Liao X.,"Bolt, Daniel M. (57223443138); Liao, Xiangyi (57222607143)",57223443138; 57222607143,On the Positive Correlation between DIF and Difficulty: A New Theory on the Correlation as Methodological Artifact,2021,Journal of Educational Measurement,58,4,,465,491,26,3,10.1111/jedm.12302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118507684&doi=10.1111%2fjedm.12302&partnerID=40&md5=f1f424aebf9472c7ffeef55e2be8edb0,"We revisit the empirically observed positive correlation between DIF and difficulty studied by Freedle and commonly seen in tests of verbal proficiency when comparing populations of different mean latent proficiency levels. It is shown that a positive correlation between DIF and difficulty estimates is actually an expected result (absent any true DIF) in the presence of systematically negative ICC asymmetry. Under such conditions, conditional upon sum score, correct responses to easier items are indicative of higher latent proficiency than correct responses to more difficult items. Negative ICC asymmetry, which can occur as a result of disjunctively interacting latent subprocesses (including disjunctive combinations of proficiency-based guessing and problem-solving processes, for example), is suggested to be widely present among items used in many verbal tests due to their general lack of problem-solving complexity and the anticipated presence of proficiency-related guessing. When systematic, negative ICC asymmetries result in a shrinkage of units at the lower end of the IRT metric when traditional symmetric IRT models are applied. We demonstrate by simulation an ensuing DIF-difficulty correlation artifact in application of a model-based DIF approach, as well as for the Mantel-Haenszel and Standardization DIF detection methods. A sensitivity analysis illustration demonstrates how assuming a fixed level of negative asymmetry in the ICCs effectively makes an observed positive correlation between DIF and difficulty seen when fitting symmetric ICCs go away. © 2021 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85118507684
Guo W.; Wind S.A.,"Guo, Wenjing (57205765777); Wind, Stefanie A. (55616798300)",57205765777; 55616798300,Examining the Impacts of Ignoring Rater Effects in Mixed-Format Tests,2021,Journal of Educational Measurement,58,3,,364,387,23,6,10.1111/jedm.12292,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108285686&doi=10.1111%2fjedm.12292&partnerID=40&md5=b7d15ba22ec0296e21a0951a25f5df96,"The use of mixed-format tests made up of multiple-choice (MC) items and constructed response (CR) items is popular in large-scale testing programs, including the National Assessment of Educational Progress (NAEP) and many district- and state-level assessments in the United States. Rater effects, or raters’ scoring tendencies that result in performances receiving different scores than are warranted given their quality, are concerns for the interpretation of scores on CR items. However, there are few published studies in which researchers have systematically considered the impact of ignoring rater effects when they are present on estimates of student ability using large-scale mixed-format assessments. Using results from an analysis of NAEP data, we systematically explored the impacts of rater effects on student achievement estimates. Our results suggest that in conditions that reflect many large-scale mixed-format assessments, directly modeling rater effects yields more accurate student achievement estimates than estimation procedures that do not incorporate raters. We consider the implications of our findings for research and practice. © 2021 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85108285686
Choe E.M.; Han K.C.T.,"Choe, Edison M. (57197771137); Han, Kyung (Chris) T. (57469659900)",57197771137; 57469659900,Constructing a Robust Score Scale from IRT Scores with Informed Boundaries,2022,Journal of Educational Measurement,59,1,,4,21,17,0,10.1111/jedm.12307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125380522&doi=10.1111%2fjedm.12307&partnerID=40&md5=5ffe3c303b62435ba239deee01c0c068,"In operational testing, item response theory (IRT) models for dichotomous responses are popular for measuring a single latent construct (Formula presented.), such as cognitive ability in a content domain. Estimates of (Formula presented.), also called IRT scores or (Formula presented.), can be computed using estimators based on the likelihood function, such as maximum likelihood (ML), weighted likelihood (WL), maximum a posteriori (MAP), and expected a posteriori (EAP). Although the parameter space of (Formula presented.) is theoretically unrestricted, the range of finite (Formula presented.) is constrained by the estimator and test form properties, which is important to consider but often overlooked when developing a score scale for reporting purposes. Irrespective of the estimator or test forms at hand, a common practice is to fix arbitrary points symmetric about zero (e.g., −4 and 4) as anchors for deriving a score transformation, possibly resulting in unintended gaps or truncations at the extremes. Therefore, a systematic framework is proposed for using IRT scores to construct a robust score scale with informed boundaries that are logical and consistent across test forms. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85125380522
Frey A.; König C.; Fink A.,"Frey, Andreas (56227779100); König, Christoph (57004477600); Fink, Aron (57209774957)",56227779100; 57004477600; 57209774957,A Highly Adaptive Testing Design for PISA,2023,Journal of Educational Measurement,,,,,,,0,10.1111/jedm.12382,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178412756&doi=10.1111%2fjedm.12382&partnerID=40&md5=1a22efbe1879f7aa9ecd6ff03bdb7717,"The highly adaptive testing (HAT) design is introduced as an alternative test design for the Programme for International Student Assessment (PISA). The principle of HAT is to be as adaptive as possible when selecting items while accounting for PISA's nonstatistical constraints and addressing issues concerning PISA such as item position effects. HAT combines established methods from the field of computerized adaptive testing. It is implemented in R and code is provided. HAT was compared to the PISA 2018 multistage design (MST) in a simulation study based on a factorial design with the independent variables response probability (RP;.50,.62), item pool optimality (PISA 2018, optimal), and ability level (low, medium, high). PISA-specific conditions regarding sample size, missing responses, and nonstatistical constraints were implemented. HAT clearly outperformed MST regarding test information, RMSE, and constraint management across ability groups but it showed slightly weaker item exposure. Raising RP to.62 did not decrease test information much and is therefore a viable option to foster students’ test-taking experience with HAT. Test information for HAT was up to three times higher than for MST when using a hypothetical optimal item pool. Summarizing, HAT proved to be a promising and applicable test design for PISA. © 2023 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Article in press,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85178412756
Sinharay S.,"Sinharay, Sandip (6602980064)",6602980064,Measuring the Uncertainty of Imputed Scores,2023,Journal of Educational Measurement,60,2,,351,375,24,0,10.1111/jedm.12352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143905573&doi=10.1111%2fjedm.12352&partnerID=40&md5=2f17585da57b29175bf7424fd8c46e81,"Technical difficulties and other unforeseen events occasionally lead to incomplete data on educational tests, which necessitates the reporting of imputed scores to some examinees. While there exist several approaches for reporting imputed scores, there is a lack of any guidance on the reporting of the uncertainty of imputed scores. In this paper, several approaches are suggested for quantifying the uncertainty of imputed scores using measures that are similar in spirit to estimates of reliability and standard error of measurement. A simulation study is performed to examine the properties of the approaches. The approaches are then applied to data from a state test on which some examinees' scores had to be imputed following computer problems. Several recommendations are made for practice. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85143905573
Shear B.R.,"Shear, Benjamin R. (55838529800)",55838529800,"Gender Bias in Test Item Formats: Evidence from PISA 2009, 2012, and 2015 Math and Reading Tests",2023,Journal of Educational Measurement,60,4,,676,696,20,0,10.1111/jedm.12372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161464012&doi=10.1111%2fjedm.12372&partnerID=40&md5=35ddc72076c42a0e2754f9460775f1c7,"Large-scale standardized tests are regularly used to measure student achievement overall and for student subgroups. These uses assume tests provide comparable measures of outcomes across student subgroups, but prior research suggests score comparisons across gender groups may be complicated by the type of test items used. This paper presents evidence that among nationally representative samples of 15-year-olds in the United States participating in the 2009, 2012, and 2015 PISA math and reading tests, there are consistent item format by gender differences. On average, male students answer multiple-choice items correctly relatively more often and female students answer constructed-response items correctly relatively more often. These patterns were consistent across 34 additional participating PISA jurisdictions, although the size of the format differences varied and were larger on average in reading than math. The average magnitude of the format differences is not large enough to be flagged in routine differential item functioning analyses intended to detect test bias but is large enough to raise questions about the validity of inferences based on comparisons of scores across gender groups. Researchers and other test users should account for test item format, particularly when comparing scores across gender groups. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85161464012
He Y.,"He, Yinhong (57205923904)",57205923904,An Exponentially Weighted Moving Average Procedure for Detecting Back Random Responding Behavior,2023,Journal of Educational Measurement,60,2,,282,317,35,1,10.1111/jedm.12351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143506174&doi=10.1111%2fjedm.12351&partnerID=40&md5=ae1282a1c70cd3e614c1af8b02810216,"Back random responding (BRR) behavior is one of the commonly observed careless response behaviors. Accurately detecting BRR behavior can improve test validities. Yu and Cheng (2019) showed that the change point analysis (CPA) procedure based on weighted residual (CPA-WR) performed well in detecting BRR. Compared with the CPA procedure, the exponentially weighted moving average (EWMA) obtains more detailed information. This study equipped the weighted residual statistic with EWMA, and proposed the EWMA-WR method to detect BRR. To make the critical values adaptive to the ability levels, this study proposed the Monte Carlo simulation with ability stratification (MC-stratification) method for calculating critical values. Compared to the original Monte Carlo simulation (MC) method, the newly proposed MC-stratification method generated a larger number of satisfactory results. The performances of CPA-WR and EWMA-WR were evaluated under different conditions that varied in the test lengths, abnormal proportions, critical values and smoothing constants used in the EWMA-WR method. The results showed that EWMA-WR was more powerful than CPA-WR in detecting BRR. Moreover, an empirical study was conducted to illustrate the utility of EWMA-WR for detecting BRR. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85143506174
Combs A.,"Combs, Adam (57219794920)",57219794920,A New Bayesian Person-Fit Analysis Method Using Pivotal Discrepancy Measures,2023,Journal of Educational Measurement,60,1,,52,75,23,0,10.1111/jedm.12342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138163974&doi=10.1111%2fjedm.12342&partnerID=40&md5=74e183820bac6f13cb5f6c3ac532af1c,"A common method of checking person-fit in Bayesian item response theory (IRT) is the posterior-predictive (PP) method. In recent years, more powerful approaches have been proposed that are based on resampling methods using the popular (Formula presented.) statistic. There has also been proposed a new Bayesian model checking method based on pivotal discrepancy measures (PDMs). A PDM T is a discrepancy measure that is a pivotal quantity with a known reference distribution. A posterior sample of T can be generated using standard Markov chain Monte Carlo output, and a p-value is obtained from probability bounds computed on order statistics of the sample. In this paper, we propose a general procedure to apply this PDM method to person-fit checking in IRT models. We illustrate this using the (Formula presented.) and (Formula presented.) measures. Simulation studies are done comparing these with the PP method and one of the more recent resampling methods. The results show that the PDM method is more powerful than the PP method. Under certain conditions, it is more powerful than the resampling method, while in others, it is less. The PDM method is also applied to a real data set. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85138163974
He Y.; Qi Y.,"He, Yinhong (57205923904); Qi, Yuanyuan (58475800100)",57205923904; 58475800100,Using Response Time in Multidimensional Computerized Adaptive Testing,2023,Journal of Educational Measurement,60,4,,697,738,41,0,10.1111/jedm.12373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164141980&doi=10.1111%2fjedm.12373&partnerID=40&md5=2610f10ed6f108cb8ddbe47eb969e51e,"In multidimensional computerized adaptive testing (MCAT), item selection strategies are generally constructed based on responses, and they do not consider the response times required by items. This study constructed two new criteria (referred to as DT-inc and DT) for MCAT item selection by utilizing information from response times. The new designs maximize the amount of information per unit time. Furthermore, these two new designs were extended to the DTS-inc and DTS designs to efficiently estimate intentional abilities. Moreover, the EAP method for ability estimation was also equipped with response time. The performances of the response-time-based EAP (RT-based EAP) and the new designs were evaluated in simulation and empirical studies. The results showed that the RT-based EAP significantly improved the ability estimation precision compared with the EAP without using response time, and the new designs dramatically saved testing times for examinees with a small sacrifice of ability estimation precision and item pool usage. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85164141980
DeCarlo L.T.,"DeCarlo, Lawrence T. (6701664430)",6701664430,Classical Item Analysis from a Signal Detection Perspective,2023,Journal of Educational Measurement,60,3,,520,547,27,1,10.1111/jedm.12358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148951720&doi=10.1111%2fjedm.12358&partnerID=40&md5=a87017f8c5d0ac0a5f35e6e0cbd4875d,"A conceptualization of multiple-choice exams in terms of signal detection theory (SDT) leads to simple measures of item difficulty and item discrimination that are closely related to, but also distinct from, those used in classical item analysis (CIA). The theory defines a “true split,” depending on whether or not examinees know an item, and so it provides a basis for using total scores to split item tables, as done in CIA, while also clarifying benefits and limitations of the approach. The SDT item difficulty and discrimination measures differ from those used in CIA in that they explicitly consider the role of distractors and avoid limitations due to range restrictions. A new screening measure is also introduced. The measures are theoretically well-grounded and are simple to compute by hand calculations or with standard software for choice models; simulations show that they offer advantages over traditional measures. © 2023 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85148951720
Yaneva V.; Clauser B.E.; Morales A.; Paniagua M.,"Yaneva, Victoria (57003253500); Clauser, Brian E. (7003595460); Morales, Amy (56076750600); Paniagua, Miguel (7005203348)",57003253500; 7003595460; 56076750600; 7005203348,Using Eye-Tracking Data as Part of the Validity Argument for Multiple-Choice Questions: A Demonstration,2021,Journal of Educational Measurement,58,4,,515,537,22,5,10.1111/jedm.12304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121333040&doi=10.1111%2fjedm.12304&partnerID=40&md5=d893b1048f74a7b7fe8775b39a9f1f28,"Eye-tracking technology can create a record of the location and duration of visual fixations as a test-taker reads test questions. Although the cognitive process the test-taker is using cannot be directly observed, eye-tracking data can support inferences about these unobserved cognitive processes. This type of information has the potential to support improved test design and to contribute to an overall validity argument for the inferences and uses made based on test scores. Although several authors have referred to the potential usefulness of eye-tracking data, there are relatively few published studies that provide examples of that use. In this paper, we report the results an eye-tracking study designed to evaluate how the presence of the options in multiple-choice questions impacts the way medical students responded to questions designed to evaluate clinical reasoning. Examples of the types of data that can be extracted are presented. We then discuss the implications of these results for evaluating the validity of inferences made based on the type of items used in this study. © 2021 National Board of Medical Examiners. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85121333040
Strachan T.; Cho U.H.; Kim K.Y.; Willse J.T.; Chen S.-H.; Ip E.H.; Ackerman T.A.; Weeks J.P.,"Strachan, Tyler (57207758268); Cho, Uk Hyun (57211849118); Kim, Kyung Yong (57205130600); Willse, John T. (15064571600); Chen, Shyh-Huei (56999278200); Ip, Edward H. (36917734200); Ackerman, Terry A. (16404476400); Weeks, Jonathan P. (35225618200)",57207758268; 57211849118; 57205130600; 15064571600; 56999278200; 36917734200; 16404476400; 35225618200,Using a Projection IRT Method for Vertical Scaling When Construct Shift Is Present,2021,Journal of Educational Measurement,58,2,,211,235,24,1,10.1111/jedm.12278,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089398902&doi=10.1111%2fjedm.12278&partnerID=40&md5=9660be5c74380ce8eb8d86ff505de320,"In vertical scaling, results of tests from several different grade levels are placed on a common scale. Most vertical scaling methodologies rely heavily on the assumption that the construct being measured is unidimensional. In many testing situations, however, such an assumption could be problematic. For instance, the construct measured at one grade level may differ from that measured in another grade (e.g., construct shift). On the other hand, dimensions that involve low-level skills are usually mastered by almost all students as they progress to higher grades. These types of changes in the multidimensional structure, within and across grades, create challenges for developing a vertical scale. In this article, we propose the use of projective IRT (PIRT) as a potential solution to the problem. Assuming that a test measures a primary dimension of substantive interest as well as some peripheral dimensions, the idea underlying PIRT is to integrate out the secondary dimensions such that the model provides both item parameters and ability estimates for the primary dimension. A simulation study was conducted to evaluate the effectiveness of the PIRT as a method for vertical scaling. An example using empirical data from a measure of foundational reading skills is also presented. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85089398902
Almehrizi R.S.,"Almehrizi, Rashid S. (55822272900)",55822272900,"Standard Errors of Variance Components, Measurement Errors and Generalizability Coefficients for Crossed Designs",2021,Journal of Educational Measurement,58,2,,179,210,31,3,10.1111/jedm.12277,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089036907&doi=10.1111%2fjedm.12277&partnerID=40&md5=96ea85bba2c4da6e352786aa0194ad32,"Estimates of various variance components, universe score variance, measurement error variances, and generalizability coefficients, like all statistics, are subject to sampling variability, particularly in small samples. Such variability is quantified traditionally through estimated standard errors and/or confidence intervals. The paper derived new standard errors for all estimated statistics for two crossed designs (single-facet design and two-facet design) in generalizability theory. The derivation was based on the assumption of multivariate normal distribution for observation scores using delta method. The derivation was differentiating between fixed and random facets. The adequacy of the derived standard errors was examined using Mont Carlo simulation for the two designs under several test conditions and compared to the traditional existing methods as well as implementation on real data. Results showed that the derived standard errors for all estimators are converging to the empirical standard errors for all simulation conditions with the two designs for both normal and non-normal continuous data. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85089036907
Lee Y.-H.; Haberman S.J.,"Lee, Yi-Hsuan (37054334600); Haberman, Shelby J. (14048312500)",37054334600; 14048312500,Studying Score Stability with a Harmonic Regression Family: A Comparison of Three Approaches to Adjustment of Examinee-Specific Demographic Data,2021,Journal of Educational Measurement,58,1,,54,82,28,4,10.1111/jedm.12266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079706361&doi=10.1111%2fjedm.12266&partnerID=40&md5=a48ef48815beb40ff1ae9301d12d17a0,"For assessments that use different forms in different administrations, equating methods are applied to ensure comparability of scores over time. Ideally, a score scale is well maintained throughout the life of a testing program. In reality, instability of a score scale can result from a variety of causes, some are expected while others may be unforeseen. The situation is more challenging for assessments that assemble many different forms and deliver frequent administrations per year. Harmonic regression, a seasonal-adjustment method, has been found useful in achieving the goal of differentiating between possible known sources of variability and unknown sources so as to study score stability for such assessments. As an extension, this paper presents a family of three approaches that incorporate examinees' demographic data into harmonic regression in different ways. A generic evaluation method based on jackknifing is developed to compare the approaches within the family. The three approaches are compared using real data from an international language assessment. Results suggest that all approaches perform similarly and are effective in meeting the goal. The paper also discusses the properties and limitations of the three approaches, along with inferences about score (in)stability based on the harmonic regression results. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85079706361
Shermis M.D.,"Shermis, Mark D. (6701764497)",6701764497,Anchoring Validity Evidence for Automated Essay Scoring,2022,Journal of Educational Measurement,59,3,,314,337,23,4,10.1111/jedm.12336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130492447&doi=10.1111%2fjedm.12336&partnerID=40&md5=472708bd1689ed6b24b66957d6b3965c,"One of the challenges of discussing validity arguments for machine scoring of essays centers on the absence of a commonly held definition and theory of good writing. At best, the algorithms attempt to measure select attributes of writing and calibrate them against human ratings with the goal of accurate prediction of scores for new essays. Sometimes these attributes are based on the fundamentals of writing (e.g., fluency), but quite often they are based on locally developed rubrics that may be confounded with specific content coverage expectations. This lack of transparency makes it difficult to provide systematic evidence that machine scoring is assessing writing, but slices or correlates of writing performance. © 2022 by the National Council on Measurement in Education.",,,Article,Final,,Scopus,2-s2.0-85130492447
Köhler C.; Khorramdel L.; Pokropek A.; Hartig J.,"Köhler, Carmen (56819742200); Khorramdel, Lale (37120568100); Pokropek, Artur (45561420300); Hartig, Johannes (18037163300)",56819742200; 37120568100; 45561420300; 18037163300,DIF Detection for Multiple Groups: Comparing Three-Level GLMMs and Multiple-Group IRT Models,2024,Journal of Educational Measurement,61,2,,325,344,19,1,10.1111/jedm.12384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185490986&doi=10.1111%2fjedm.12384&partnerID=40&md5=4381870f9cfbc4c1c05a33338980d511,"For assessment scales applied to different groups (e.g., students from different states; patients in different countries), multigroup differential item functioning (MG-DIF) needs to be evaluated in order to ensure that respondents with the same trait level but from different groups have equal response probabilities on a particular item. The current study compares two approaches for DIF detection: a multiple-group item response theory (MG-IRT) model and a generalized linear mixed model (GLMM). In the MG-IRT model approach, item parameters are constrained to be equal across groups and DIF is evaluated for each item in each group. In the GLMM, groups are treated as random, and item difficulties are modeled as correlated random effects with a joint multivariate normal distribution. Its nested structure allows the estimation of item difficulty variances and covariances at the group level. We use an excerpt from the PISA 2015 reading domain as an exemplary empirical investigation, and conduct a simulation study to compare the performance of the two approaches. Results from the empirical investigation show that the detection of countries with DIF is similar in both approaches. Results from the simulation study confirm this finding and indicate slight advantages of the MG-IRT model approach. © 2024 The Authors. Journal of Educational Measurement published by Wiley Periodicals LLC on behalf of National Council on Measurement in Education.",,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85185490986
Kaufmann E.; Budescu D.V.,"Kaufmann, Esther (26642560900); Budescu, David V. (7003588697)",26642560900; 7003588697,Do Teachers Consider Advice? On the Acceptance of Computerized Expert Models,2020,Journal of Educational Measurement,57,2,,311,342,31,10,10.1111/jedm.12251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071922967&doi=10.1111%2fjedm.12251&partnerID=40&md5=2594bd6b971b4896debfa822c43d97cf,"The literature suggests that simple expert (mathematical) models can improve the quality of decisions, but people are not always eager to accept and endorse such models. We ran three online experiments to test the receptiveness to advice from computerized expert models. Middle- and high-school teachers (N = 435) evaluated student profiles that varied in several personal and task relevant factors. They were offered (Studies I and II), or could ask for (Study III), advice from either expert models or human advisors. Overall, teachers requested and followed advice of expert models less frequently than advice from humans. Task-relevant factors (task difficulty) seem to be more salient than personal factors for teachers’ willingness to receive advice. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85071922967
Wang C.; Chen P.; Jiang S.,"Wang, Chun (15924354400); Chen, Ping (56975615100); Jiang, Shengyu (57210894572)",15924354400; 56975615100; 57210894572,Item Calibration Methods With Multiple Subscale Multistage Testing,2020,Journal of Educational Measurement,57,1,,3,28,25,7,10.1111/jedm.12241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071748611&doi=10.1111%2fjedm.12241&partnerID=40&md5=a516342eba3f5e1371ce30fe5457b29d,"Many large-scale educational surveys have moved from linear form design to multistage testing (MST) design. One advantage of MST is that it can provide more accurate latent trait (θ) estimates using fewer items than required by linear tests. However, MST generates incomplete response data by design; hence, questions remain as to how to calibrate items using the incomplete data from MST design. Further complication arises when there are multiple correlated subscales per test, and when items from different subscales need to be calibrated according to their respective score reporting metric. The current calibration-per-subscale method produced biased item parameters, and there is no available method for resolving the challenge. Deriving from the missing data principle, we showed when calibrating all items together the Rubin's ignorability assumption is satisfied such that the traditional single-group calibration is sufficient. When calibrating items per subscale, we proposed a simple modification to the current calibration-per-subscale method that helps reinstate the missing-at-random assumption and therefore corrects for the estimation bias that is otherwise existent. Three mainstream calibration methods are discussed in the context of MST, they are the marginal maximum likelihood estimation, the expectation maximization method, and the fixed parameter calibration. An extensive simulation study is conducted and a real data example from NAEP is analyzed to provide convincing empirical evidence. © 2019 by the National Council on Measurement in Education",EM; marginal maximum likelihood; missing data; multistage testing,,Article,Final,,Scopus,2-s2.0-85071748611
Fujimoto K.A.,"Fujimoto, Ken A. (55595763600)",55595763600,A More Flexible Bayesian Multilevel Bifactor Item Response Theory Model,2020,Journal of Educational Measurement,57,2,,255,285,30,3,10.1111/jedm.12249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074357118&doi=10.1111%2fjedm.12249&partnerID=40&md5=58456ae55a3f97e080a264ce65c24255,"Multilevel bifactor item response theory (IRT) models are commonly used to account for features of the data that are related to the sampling and measurement processes used to gather those data. These models conventionally make assumptions about the portions of the data structure that represent these features. Unfortunately, when data violate these models' assumptions but these models are used anyway, incorrect conclusions about the cluster effects could be made and potentially relevant dimensions could go undetected. To address the limitations of these conventional models, a more flexible multilevel bifactor IRT model that does not make these assumptions is presented, and this model is based on the generalized partial credit model. Details of a simulation study demonstrating this model outperforming competing models and showing the consequences of using conventional multilevel bifactor IRT models to analyze data that violate these models' assumptions are reported. Additionally, the model's usefulness is illustrated through the analysis of the Program for International Student Assessment data related to interest in science. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85074357118
Kim H.J.; Brennan R.L.; Lee W.-C.,"Kim, Hyung Jin (57211830150); Brennan, Robert L. (34975092300); Lee, Won-Chan (57203094500)",57211830150; 34975092300; 57203094500,A New Statistic to Assess Fitness of Cubic-Spline Postsmoothing,2020,Journal of Educational Measurement,57,1,,124,144,20,0,10.1111/jedm.12244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073802169&doi=10.1111%2fjedm.12244&partnerID=40&md5=f2450b8576d52103d45bebd65e841b39,"In equating, smoothing techniques are frequently used to diminish sampling error. There are typically two types of smoothing: presmoothing and postsmoothing. For polynomial log-linear presmoothing, an optimum smoothing degree can be determined statistically based on the Akaike information criterion or Chi-square difference criterion. For cubic-spline postsmoothing, visual inspection has been an important tool in choosing such optimum degrees in operational settings. This study introduces a new statistic for assessing the fitness of the cubic-spline postsmoothing method, which accommodates three conditions: (1) one standard error band, (2) deviation from unsmoothed equivalents, and (3) smoothness. A principal advantage of the new statistic proposed in this study is that an optimum degree of smoothing can be selected automatically by giving consistent amount of attention to deviation and smoothness across multiple equatings, whereas visual inspection may not be consistent. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85073802169
Kim Y.,"Kim, Yongnam (55026008200)",55026008200,Partial Identification of Answer Reviewing Effects in Multiple-Choice Exams,2020,Journal of Educational Measurement,57,4,,511,526,15,4,10.1111/jedm.12259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074054315&doi=10.1111%2fjedm.12259&partnerID=40&md5=90a625d8e032fd054abcd0572908557f,"Does reviewing previous answers during multiple-choice exams help examinees increase their final score? This article formalizes the question using a rigorous causal framework, the potential outcomes framework. Viewing examinees’ reviewing status as a treatment and their final score as an outcome, the article first explains the challenges of identifying the causal effect of answer reviewing in regular exam-taking settings. In addition to the incapability of randomizing the treatment selection (reviewing status) and the lack of other information to make this selection process ignorable, the treatment variable itself is not fully known to researchers. Looking at examinees’ answer sheet data, it is unclear whether an examinee who did not change his or her answer on a specific item reviewed it but retained the initial answer (treatment condition) or chose not to review it (control condition). Despite such challenges, however, the article develops partial identification strategies and shows that the sign of the answer reviewing effect can be reasonably inferred. By analyzing a statewide math assessment data set, the article finds that reviewing initial answers is generally beneficial for examinees. © 2019 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85074054315
Thissen D.,"Thissen, David (7003712685)",7003712685,Comments on Shelby Haberman's NCME Career Award Address: Statistical Theory and Assessment Practice,2020,Journal of Educational Measurement,57,3,,397,402,5,0,10.1111/jedm.12279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089597045&doi=10.1111%2fjedm.12279&partnerID=40&md5=841aeb9570dbc4bfc4425c836427b1a1,"When I was a graduate student,1 Howard Wainer quoted his own graduate mentor, Harold Gulliksen, saying ""The difference between basic and applied research is that basic research has so many more applications.""2 Shelby Haberman illustrates that truth in his career award address, in which he summarizes some of the applied contributions he made to educational measurement during his years at ETS, based on his basic statistical research that continued from earlier decades. The breadth of application of Haberman's statistical research is illustrated by the range of examples in his address: the ""effects of model error in the analysis of item responses by use of latent-structure models, weighting to attempt to correct for sampling bias, and the effects of excessively complex models"" (p. 374). Those three topics do not include his seminal work on subscores (e.g., Haberman, 2008) or much of his work on item fit in IRT models (e.g., Haberman, Sinharay, and Chon, 2013); both of those lines of research were recognized with the NCME Annual Award for Exceptional Achieve-
ment in Educational Measurement in 2009 and 2015. Haberman includes pithy lines in his address that should be memorized by all students of educational measurement. Examples are: (referring to likelihood ratio tests between models) ""This kind of test typically suffices to demonstrate that any given IRT model is not valid"" (p. 375) followed a bit later by ""At this point, looking at the size of the error in a model appears more helpful than looking at the existence of error"" (p. 376). And the greatest piece of wisdom in the entire address: ""One challenge is that software produces numbers whether or not the numbers are meaningful"" (p. 381). Students told those things in classes on educational measurement might subsequently make better use of assessment data. ",,,Article,Final,,Scopus,2-s2.0-85089597045
Lee S.,"Lee, Sunbok (57192521363)",57192521363,Logistic Regression Procedure Using Penalized Maximum Likelihood Estimation for Differential Item Functioning,2020,Journal of Educational Measurement,57,3,,443,457,14,5,10.1111/jedm.12253,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071946439&doi=10.1111%2fjedm.12253&partnerID=40&md5=c2b07715d75eda0a95d14b1746206c51,"In the logistic regression (LR) procedure for differential item functioning (DIF), the parameters of LR have often been estimated using maximum likelihood (ML) estimation. However, ML estimation suffers from the finite-sample bias. Furthermore, ML estimation for LR can be substantially biased in the presence of rare event data. The bias of ML estimation due to small samples and rare event data can degrade the performance of the LR procedure, especially when testing the DIF of difficult items in small samples. Penalized ML (PML) estimation was originally developed to reduce the finite-sample bias of conventional ML estimation and also was known to reduce the bias in the estimation of LR for the rare events data. The goal of this study is to compare the performances of the LR procedures based on the ML and PML estimation in terms of the statistical power and Type I error. In a simulation study, Swaminathan and Rogers's Wald test based on PML estimation (PSR) showed the highest statistical power in most of the simulation conditions, and LRT based on conventional PML estimation (PLRT) showed the most robust and stable Type I error. The discussion about the trade-off between bias and variance is presented in the discussion section. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85071946439
Castellano K.E.; McCaffrey D.F.,"Castellano, Katherine E. (55633157900); McCaffrey, Daniel F. (7005049755)",55633157900; 7005049755,Comparing the Accuracy of Student Growth Measures,2020,Journal of Educational Measurement,57,1,,71,91,20,5,10.1111/jedm.12242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073954192&doi=10.1111%2fjedm.12242&partnerID=40&md5=6da48b0c7985f87e2d530fd0e791e0a6,"Testing programs are often interested in using a student growth measure. This article presents analytic derivations of the accuracy of common student growth measures on both the raw scale of the test and the percentile rank scale in terms of the proportional reduction in mean squared error and the squared correlation between the estimator and target. The study contrasts the accuracy of the growth measures against that of current status measures—current test scores and their percentile ranks. Key findings include the extent that status measures are more accurate than any of the growth measures and that alternative methods to estimate growth could be more accurate than the currently used methods. Our findings highlight the importance for evaluating the statistical properties of growth measures along with other concerns for states that are debating the reporting of growth. Our results also point out that assessing the accuracy of growth measures requires the specification of quantities of interest in terms of latent achievement rather than observed test scores, which is common practice for developing status measures but essentially never done by testing programs for growth measures. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85073954192
Guo H.; Dorans N.J.,"Guo, Hongwen (35762848700); Dorans, Neil J. (6602289148)",35762848700; 6602289148,Using Weighted Sum Scores to Close the Gap Between DIF Practice and Theory,2020,Journal of Educational Measurement,57,4,,484,510,26,8,10.1111/jedm.12258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074354815&doi=10.1111%2fjedm.12258&partnerID=40&md5=f06edccf15581ffbfe05f79e73fc3b04,"We make a distinction between the operational practice of using an observed score to assess differential item functioning (DIF) and the concept of departure from measurement invariance (DMI) that conditions on a latent variable. DMI and DIF indices of effect sizes, based on the Mantel-Haenszel test of common odds ratio, converge under restricted conditions if a simple sum score is used as the matching or conditioning variable in a DIF analysis. Based on theoretical results, we demonstrate analytically that matching on a weighted sum score can significantly reduce the difference between DIF and DMI measures over what can be achieved with a simple sum score. We also examine the utility of binning methods that could facilitate potential operational use of DIF with weighted sum scores. A real data application was included to show this feasibility. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85074354815
Hong S.E.; Monroe S.; Falk C.F.,"Hong, Seong Eun (57210187845); Monroe, Scott (55874822900); Falk, Carl F. (26659012900)",57210187845; 55874822900; 26659012900,Performance of Person-Fit Statistics Under Model Misspecification,2020,Journal of Educational Measurement,57,3,,423,442,19,2,10.1111/jedm.12207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069824082&doi=10.1111%2fjedm.12207&partnerID=40&md5=594f3dee1a521160028508bdd198571a,"In educational and psychological measurement, a person-fit statistic (PFS) is designed to identify aberrant response patterns. For parametric PFSs, valid inference depends on several assumptions, one of which is that the item response theory (IRT) model is correctly specified. Previous studies have used empirical data sets to explore the effects of model misspecification on PFSs. We further this line of research by using a simulation study, which allows us to explore issues that may be of interest to practitioners. Results show that, depending on the generating and analysis item models, Type I error rates at fixed values of the latent variable may be greatly inflated, even when the aggregate rates are relatively accurate. Results also show that misspecification is most likely to affect PFSs for examinees with extreme latent variable scores. Two empirical data analyses are used to illustrate the importance of model specification. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85069824082
Lee W.-C.; Kim S.Y.; Choi J.; Kang Y.,"Lee, Won-Chan (57203094500); Kim, Stella Y. (57207794797); Choi, Jiwon (57211408636); Kang, Yujin (57211409246)",57203094500; 57207794797; 57211408636; 57211409246,IRT Approaches to Modeling Scores on Mixed-Format Tests,2020,Journal of Educational Measurement,57,2,,230,254,24,6,10.1111/jedm.12248,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073776768&doi=10.1111%2fjedm.12248&partnerID=40&md5=74c0e3be6fdaeb21a7480dd5cb53ca6b,"This article considers psychometric properties of composite raw scores and transformed scale scores on mixed-format tests that consist of a mixture of multiple-choice and free-response items. Test scores on several mixed-format tests are evaluated with respect to conditional and overall standard errors of measurement, score reliability, and classification consistency and accuracy under three item response theory (IRT) frameworks: unidimensional IRT (UIRT), simple structure multidimensional IRT (SS-MIRT), and bifactor multidimensional IRT (BF-MIRT) models. Illustrative examples are presented using data from three mixed-format exams with various levels of format effects. In general, the two MIRT models produced similar results, while the UIRT model resulted in consistently lower estimates of reliability and classification consistency/accuracy indices compared to the MIRT models. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85073776768
Chen C.-W.; Wang W.-C.; Chiu M.M.; Ro S.,"Chen, Chia-Wen (55633283000); Wang, Wen-Chung (7501757876); Chiu, Ming Ming (7101865518); Ro, Sage (57190583283)",55633283000; 7501757876; 7101865518; 57190583283,Item Selection and Exposure Control Methods for Computerized Adaptive Testing with Multidimensional Ranking Items,2020,Journal of Educational Measurement,57,2,,343,369,26,6,10.1111/jedm.12252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073801474&doi=10.1111%2fjedm.12252&partnerID=40&md5=ebaf246fad02071df8dee0f660940606,"The use of computerized adaptive testing algorithms for ranking items (e.g., college preferences, career choices) involves two major challenges: unacceptably high computation times (selecting from a large item pool with many dimensions) and biased results (enhanced preferences or intensified examinee responses because of repeated statements across items). To address these issues, we introduce subpool partition strategies for item selection and within-person statement exposure control procedures. Simulations showed that the multinomial method reduces computation time while maintaining measurement precision. Both the freeze and revised Sympson-Hetter online (RSHO) methods controlled the statement exposure rate; RSHO sacrificed some measurement precision but increased pool use. Furthermore, preventing a statement's repetition on consecutive items neither hindered the effectiveness of the freeze or RSHO method nor reduced measurement precision. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85073801474
Castellano K.E.; McCaffrey D.F.,"Castellano, Katherine E. (55633157900); McCaffrey, Daniel F. (7005049755)",55633157900; 7005049755,Estimating the Accuracy of Relative Growth Measures Using Empirical Data,2020,Journal of Educational Measurement,57,1,,92,123,31,4,10.1111/jedm.12243,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073973884&doi=10.1111%2fjedm.12243&partnerID=40&md5=a98ca6a9382961fafd582c653fc67d2e,"The residual gain score has been of historical interest, and its percentile rank has been of interest more recently given its close correspondence to the popular Student Growth Percentile. However, these estimators suffer from low accuracy and systematic bias (bias conditional on prior latent achievement). This article explores three alternatives—using the expected a posterior (EAP), conditioning on an additional lagged score, and correcting for measurement error bias from the prior score (Corrected-Observed)—evaluated in terms of their systematic bias, squared correlation with their target (R2), and proportional reduction in mean squared error (PRMSE). Both analytic results (under model assumptions) and empirical results (found using item response data to calculate the growth estimators) reveal that the EAP estimators are the most accurate, whereas the Corrected-Observed removes systematic bias, but reduces overall accuracy. Adding another prior year often decreases accuracy but only slightly reduces systematic bias at realistic test reliabilities. For all estimators, R2 and PRMSE are substantially below levels that are considered necessary for reporting educational measurements with moderate to high stakes. For all but the EAP, the raw residual gain estimators have negative PRMSE, indicating that inferences about a student's latent growth would be more accurate if students were assigned the average residual rather than estimating their residual. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85073973884
Liu C.; Kolen M.J.,"Liu, Chunyan (57204854890); Kolen, Michael J. (6603925839)",57204854890; 6603925839,A New Statistic for Selecting the Smoothing Parameter for Polynomial Loglinear Equating Under the Random Groups Design,2020,Journal of Educational Measurement,57,3,,458,479,21,0,10.1111/jedm.12257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074106743&doi=10.1111%2fjedm.12257&partnerID=40&md5=e3319c5932f97bcdbee18729d610d019,"Smoothing is designed to yield smoother equating results that can reduce random equating error without introducing very much systematic error. The main objective of this study is to propose a new statistic and to compare its performance to the performance of the Akaike information criterion and likelihood ratio chi-square difference statistics in selecting the smoothing parameter for polynomial loglinear equating under the random groups design. These model selection statistics were compared for four sample sizes (500, 1,000, 2,000, and 3,000) and eight simulated equating conditions, including both conditions where equating is not needed and conditions where equating is needed. The results suggest that all model selection statistics tend to improve the equating accuracy by reducing the total equating error. The new statistic tended to have less overall error than the other two methods. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85074106743
van der Linden W.J.; Choi S.W.,"van der Linden, Wim J. (55409657500); Choi, Seung W. (7408120005)",55409657500; 7408120005,Improving Item-Exposure Control in Adaptive Testing,2020,Journal of Educational Measurement,57,3,,405,422,17,11,10.1111/jedm.12254,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073979843&doi=10.1111%2fjedm.12254&partnerID=40&md5=71c9be76178a4caa08e7426c82ce6415,"One of the methods of controlling test security in adaptive testing is imposing random item-ineligibility constraints on the selection of the items with probabilities automatically updated to maintain a predetermined upper bound on the exposure rates. Three major improvements of the method are presented. First, a few modifications to improve the initialization of the method and accelerate the impact of its feedback mechanism on the observed item-exposure rates are introduced. Second, the case of conditional item-exposure control given the uncertainty of examinee's ability parameter is addressed. Third, although rare for a well-designed item pool, when applied in combination with the shadow-test approach to adaptive testing the method may meet occasional infeasibility of the shadow-test model. A big M method is proposed that resolves the issue. The practical advantages of the improvements are illustrated using simulated adaptive testing from a real-world item pool under a variety of conditions. © 2019 by the National Council on Measurement in Education",,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85073979843
Huelmann T.; Debelak R.; Strobl C.,"Huelmann, Thorben (57202468572); Debelak, Rudolf (55053599000); Strobl, Carolin (15924367800)",57202468572; 55053599000; 15924367800,A Comparison of Aggregation Rules for Selecting Anchor Items in Multigroup DIF Analysis,2020,Journal of Educational Measurement,57,2,,185,215,30,3,10.1111/jedm.12246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071863150&doi=10.1111%2fjedm.12246&partnerID=40&md5=9991ea7af2cef7ab3a622dd521b6bb9b,"This study addresses the topic of how anchoring methods for differential item functioning (DIF) analysis can be used in multigroup scenarios. The direct approach would be to combine anchoring methods developed for two-group scenarios with multigroup DIF-detection methods. Alternatively, multiple tests could be carried out. The results of these tests need to be aggregated to determine the anchor for the final DIF analysis. In this study, the direct approach and three aggregation rules are investigated. All approaches are combined with a variety of anchoring methods, such as the “all-other purified” and “mean p-value threshold” methods, in two simulation studies based on the Rasch model. Our results indicate that the direct approach generally does not lead to more accurate or even to inferior results than the aggregation rules. The min rule overall shows the best trade-off between low false alarm rate and medium to high hit rate. However, it might be too sensitive when the number of groups is large. In this case, the all rule may be a good compromise. We also take a closer look at the anchor selection method “next candidate,” which performed rather poorly, and suggest possible improvements. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85071863150
Haberman S.J.,"Haberman, Shelby J. (14048312500)",14048312500,Statistical Theory and Assessment Practice,2020,Journal of Educational Measurement,57,3,,374,385,11,0,10.1111/jedm.12282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089458734&doi=10.1111%2fjedm.12282&partnerID=40&md5=1cc8a69f51ce571bc464193ebda8c085,Examples of the impact of statistical theory on assessment practice are provided from the perspective of a statistician trained in theoretical statistics who began to work on assessments. Goodness of fit of item-response models is examined in terms of restricted likelihood-ratio tests and generalized residuals. Minimum discriminant information adjustment is used for linking with no anchors or problematic anchors and for repeater analysis. Assessment issues are examined in cases in which the number of parameters is large relative to the number of observations. © 2020 by the National Council on Measurement in Education,,,Article,Final,,Scopus,2-s2.0-85089458734
Kim S.Y.; Lee W.-C.,"Kim, Stella Y. (57207794797); Lee, Won-Chan (57203094500)",57207794797; 57203094500,Classification Consistency and Accuracy With Atypical Score Distributions,2020,Journal of Educational Measurement,57,2,,286,310,24,2,10.1111/jedm.12250,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071844645&doi=10.1111%2fjedm.12250&partnerID=40&md5=c88bb32e788e9f8e2131b183a23af9f2,"The current study aims to evaluate the performance of three non-IRT procedures (i.e., normal approximation, Livingston-Lewis, and compound multinomial) for estimating classification indices when the observed score distribution shows atypical patterns: (a) bimodality, (b) structural (i.e., systematic) bumpiness, or (c) structural zeros (i.e., no frequencies). Under a bimodal distribution, the normal approximation procedure produced substantially large bias. For a distribution with structural bumpiness, the compound multinomial procedure tended to introduce larger bias. Under a distribution with structural zeroes, the relative performance of selected estimation procedures depended on cut score location and the sample-size conditions. In general, the differences in estimation errors among the three procedures were not substantially large. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85071844645
Kim K.Y.,"Kim, Kyung Yong (57205130600)",57205130600,Two IRT Fixed Parameter Calibration Methods for the Bifactor Model,2020,Journal of Educational Measurement,57,1,,29,50,21,1,10.1111/jedm.12230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070087528&doi=10.1111%2fjedm.12230&partnerID=40&md5=fe55daedf9d9453aa9b463e4191763ca,"New items are often evaluated prior to their operational use to obtain item response theory (IRT) item parameter estimates for quality control purposes. Fixed parameter calibration is one linking method that is widely used to estimate parameters for new items and place them on the desired scale. This article provides detailed descriptions of two fixed parameter calibration methods for the bifactor model and compares their relative performance through simulation. The two methods, which were natural generalizations of their counterparts in the unidimensional context, are the one prior weights updating and multiple expectation-maximization (EM) cycles (OWU-MEM) and multiple prior weights updating and multiple EM cycles (MWU-MEM) methods. In addition, for comparison purposes, the separate calibration method with Haebara linking was included in the simulation. In general, the MWU-MEM method recovered item parameters well for both equivalent and nonequivalent groups, whereas the OWU-MEM method worked well only for equivalent groups. With a few exceptions, the MWU-MEM and Haebara methods showed comparable item parameter recovery. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85070087528
Pokropek A.; Borgonovi F.,"Pokropek, Artur (45561420300); Borgonovi, Francesca (35613910500)",45561420300; 35613910500,Linking via Pseudo-Equivalent Group Design: Methodological Considerations and an Application to the PISA and PIAAC Assessments,2020,Journal of Educational Measurement,57,4,,527,546,19,2,10.1111/jedm.12261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075123129&doi=10.1111%2fjedm.12261&partnerID=40&md5=567da9ae5d5fa161be42610084fffd1e,This article presents the pseudo-equivalent group approach and discusses how it can enhance the quality of linking in the presence of nonequivalent groups. The pseudo-equivalent group approach allows to achieve pseudo-equivalence using propensity score reweighting techniques. We use it to perform linking to establish scale concordance between two assessments. The article presents Monte-Carlo simulations and a real data application based on data from the Survey of Adult Skills (PIAAC) and the Programme for International Student Assessment (PISA). Monte-Carlo simulations suggest that the pseudo-equivalent group design is particularly useful whenever there is a large overlap across the two groups with respect to balancing variables and when the correlation between such variables and ability is medium or high. The example based on PISA and PIAAC data indicates that the approach can provide reasonable accurate linking that can be used for group-level comparisons. © 2019 by the National Council on Measurement in Education,,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85075123129
Langenfeld T.; Thomas J.; Zhu R.; Morris C.A.,"Langenfeld, Thomas (57065600600); Thomas, Jay (57208681800); Zhu, Rongchun (57210931914); Morris, Carrie A. (57190971692)",57065600600; 57208681800; 57210931914; 57190971692,Integrating Multiple Sources of Validity Evidence for an Assessment-Based Cognitive Model,2020,Journal of Educational Measurement,57,2,,159,184,25,4,10.1111/jedm.12245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071956618&doi=10.1111%2fjedm.12245&partnerID=40&md5=32839f60eb85efa8052e28851d767cdb,An assessment of graphic literacy was developed by articulating and subsequently validating a skills-based cognitive model intended to substantiate the plausibility of score interpretations. Model validation involved use of multiple sources of evidence derived from large-scale field testing and cognitive labs studies. Data from large-scale field testing were evaluated using traditional psychometric methods. The psychometric analyses were augmented using eye tracking technology to perform gaze pattern and pupillometry analyses to gain better understanding of problem-solving strategies and cognitive load. Findings from the data sources were integrated to provide strong evidence supporting the model and score interpretations. Implications for using gaze pattern and pupillometry analyses to enhance learning and assessment are discussed. © 2019 by the National Council on Measurement in Education,,,Article,Final,,Scopus,2-s2.0-85071956618
Mislevy R.J.,"Mislevy, Robert J. (6701800690)",6701800690,Statistical Theoreticians and Educational Assessment: Comments on Shelby Haberman's NCME Career Contributions Award,2020,Journal of Educational Measurement,57,3,,386,396,10,0,10.1111/jedm.12280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089743682&doi=10.1111%2fjedm.12280&partnerID=40&md5=078107d895433a19d67103a29693e222,"In his 2019 NCME Career Contributions Award address, Dr. Shelby Haberman uses examples of three kinds to illustrate how his training in theoretical statistics influenced his contributions to educational measurement. I bracket my comments on his address, and his contributions more generally, by considering two questions: Why might any theoretical statisticians receive this award? Why aren't all recipients theoretical statisticians? Through the course of the discussion emerges the answer to a third, easier, question: Why Shelby Haberman?. © 2020 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85089743682
Clauser B.E.; Kane M.; Clauser J.C.,"Clauser, Brian E. (7003595460); Kane, Michael (36088969800); Clauser, Jerome C. (56002327600)",7003595460; 36088969800; 56002327600,Examining the Precision of Cut Scores Within a Generalizability Theory Framework: A Closer Look at the Item Effect,2020,Journal of Educational Measurement,57,2,,216,229,13,2,10.1111/jedm.12247,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071933741&doi=10.1111%2fjedm.12247&partnerID=40&md5=f26c06518de3af40039be627d3d8d5a4,"An Angoff standard setting study generally yields judgments on a number of items by a number of judges (who may or may not be nested in panels). Variability associated with judges (and possibly panels) contributes error to the resulting cut score. The variability associated with items plays a more complicated role. To the extent that the mean item judgments directly reflect empirical item difficulties, the variability in Angoff judgments over items would not add error to the cut score, but to the extent that the mean item judgments do not correspond to the empirical item difficulties, variability in mean judgments over items would add error to the cut score. In this article, we present two generalizability-theory–based analyses of the proportion of the item variance that contributes to error in the cut score. For one approach, variance components are estimated on the probability (or proportion-correct) scale of the Angoff judgments, and for the other, the judgments are transferred to the theta scale of an item response theory model before estimating the variance components. The two analyses yield somewhat different results but both indicate that it is not appropriate to simply ignore the item variance component in estimating the error variance. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85071933741
Maeda H.; Zhang B.,"Maeda, Hotaka (56018976300); Zhang, Bo (55619300516)",56018976300; 55619300516,Bayesian Extension of Biweight and Huber Weight for Robust Ability Estimation,2020,Journal of Educational Measurement,57,1,,51,70,19,2,10.1111/jedm.12240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071760084&doi=10.1111%2fjedm.12240&partnerID=40&md5=77148c6309f551dcebffaa2f0e0d5872,"When a response pattern does not fit a selected measurement model, one may resort to robust ability estimation. Two popular robust methods are biweight and Huber weight. So far, research on these methods has been quite limited. This article proposes the maximum a posteriori biweight (BMAP) and Huber weight (HMAP) estimation methods. These methods use the Bayesian prior distribution to compensate for information lost due to aberrant responses. They may also be more resistant to the detrimental effects of downweighting the nonaberrant responses. The effectiveness of BMAP and HMAP was evaluated through a Monte Carlo simulation. Results show that both methods, especially BMAP, are more effective than the original biweight and Huber weight in correcting mild forms of aberrant behavior. © 2019 by the National Council on Measurement in Education",,,Article,Final,,Scopus,2-s2.0-85071760084
